---
title: "Layer Normalization"
date: 2025-02-17 18:26:51
categories:
  - 인공지능
tags:
  - Layer normalization
---

<https://arxiv.org/abs/1607.06450>

[Layer Normalization](https://arxiv.org/abs/1607.06450)

**초록**  
최신의 심층 신경망을 훈련시키는 것은 계산 비용이 많이 듭니다. 훈련 시간을 단축하는 한 가지 방법은 뉴런의 활동을 정규화하는 것입니다. 최근에 도입된 기법인 배치 정규화(batch normalization)는 훈련 사례의 미니 배치에서 뉴런에 들어오는 입력의 총합 분포를 이용하여 평균과 분산을 계산하고, 이를 각 훈련 사례에서 해당 뉴런에 대한 총합 입력을 정규화하는 데 사용합니다. 이 방법은 피드포워드 신경망의 훈련 시간을 크게 단축시킵니다. 그러나 배치 정규화의 효과는 미니 배치 크기에 의존하며, 이를 순환 신경망(recurrent neural networks, RNN)에 적용하는 방법은 명확하지 않습니다. 본 논문에서는 배치 정규화를 레이어 정규화(layer normalization)로 전환하여, 단일 훈련 사례에서 한 계층 내의 모든 뉴런에 대한 총합 입력으로부터 정규화에 필요한 평균과 분산을 계산합니다. 배치 정규화와 마찬가지로, 정규화 후 비선형 활성화 함수가 적용되기 전에 각 뉴런에 대해 적응형 바이어스와 게인을 부여합니다. 다만, 배치 정규화와 달리 레이어 정규화는 훈련 시와 테스트 시에 동일한 계산을 수행합니다. 또한, 각 시간 단계마다 정규화 통계를 별도로 계산함으로써 순환 신경망에 간단하게 적용할 수 있습니다. 레이어 정규화는 순환 신경망의 은닉 상태 동역학을 안정화하는 데 매우 효과적이며, 실험 결과 기존 기법에 비해 훈련 시간을 크게 단축할 수 있음을 보여줍니다.

**1. 서론**  
확률적 경사 하강법(Stochastic Gradient Descent)의 여러 변형으로 훈련된 심층 신경망은 컴퓨터 비전 [Krizhevsky et al., 2012] 및 음성 처리 [Hinton et al., 2012]와 같은 다양한 감독 학습 과제에서 기존 방법들을 상당히 능가하는 성능을 보였습니다. 그러나 최신 심층 신경망은 종종 수일간의 훈련을 필요로 합니다. 학습 속도를 높이기 위해 서로 다른 머신에서 훈련 사례의 하위 집합에 대해 기울기를 계산하거나, 신경망 자체를 여러 머신에 분산시키는 방법이 가능하지만 [Dean et al., 2012], 이는 많은 통신과 복잡한 소프트웨어를 요구하며, 병렬화 정도가 증가할수록 수익이 급격히 감소하는 경향이 있습니다. 또 다른 접근법은 신경망의 순전파 과정에서 수행되는 계산을 수정하여 학습을 용이하게 만드는 것입니다. 최근 배치 정규화 [Ioffe and Szegedy, 2015]가 심층 신경망에 추가적인 정규화 단계를 포함시킴으로써 훈련 시간을 단축하는 방법으로 제안되었습니다. 이 정규화 기법은 훈련 데이터 전체에서 각 총합 입력을 해당 평균과 표준 편차를 사용해 표준화합니다. 배치 정규화를 적용한 피드포워드 신경망은 단순한 SGD를 사용하더라도 더 빠르게 수렴합니다. 훈련 시간 단축 외에도, 배치 통계로 인한 확률적 특성이 훈련 과정에서 정규화 효과를 제공합니다.

비록 단순하지만, 배치 정규화는 총합 입력 통계의 이동 평균을 필요로 합니다. 고정 깊이의 피드포워드 신경망에서는 각 은닉층마다 통계를 별도로 저장하는 것이 간단합니다. 그러나 순환 신경망(RNN)의 순환 뉴런에 대한 총합 입력은 시퀀스 길이에 따라 달라지기 때문에, RNN에 배치 정규화를 적용하려면 시간 단계마다 서로 다른 통계가 필요한 것으로 보입니다. 게다가, 배치 정규화는 온라인 학습 과제나 미니 배치 크기가 작아야 하는 매우 큰 분산 모델에는 적용하기 어렵습니다.

본 논문은 다양한 신경망 모델의 훈련 속도를 개선하기 위한 간단한 정규화 기법인 레이어 정규화를 소개합니다. 배치 정규화와 달리, 제안하는 방법은 은닉층 내의 뉴런에 대한 총합 입력으로부터 직접 정규화 통계를 추정하여, 정규화가 훈련 사례 간에 새로운 종속성을 도입하지 않도록 합니다. 우리는 레이어 정규화가 RNN에 효과적으로 작동하며, 여러 기존 RNN 모델의 훈련 시간과 일반화 성능을 모두 개선함을 보입니다.

**2. 배경**  
피드포워드 신경망은 입력 패턴 ?를 출력 벡터 y로 매핑하는 비선형 함수입니다. 심층 피드포워드 신경망의 l번째 은닉층을 고려하고, 해당 층의 뉴런들에 대한 합산 입력을 나타내는 벡터를 aₗ 라고 합시다. 합산 입력은 가중치 행렬 Wₗ 과 하위 계층에서 전달받은 입력 hₗ 을 이용한 선형 변환을 통해 계산되며, 다음과 같이 주어집니다:

aᵢₗ = (wᵢₗ)ᵀ hₗ  
hᵢₗ₊₁ = f(aᵢₗ + bᵢₗ)  (1)

여기서 f(⋅)는 원소별로 적용되는 비선형 함수이며, wᵢₗ은 i번째 은닉 유닛에 들어오는 가중치 벡터, bᵢₗ은 스칼라 바이어스 파라미터입니다. 신경망의 파라미터는 역전파를 통해 계산된 기울기를 사용한 기울기 기반 최적화 알고리즘으로 학습됩니다.

딥러닝의 도전 과제 중 하나는 한 층의 가중치에 대한 기울기가 이전 층의 뉴런 출력에 크게 의존한다는 점입니다. 특히, 이전 층의 출력들이 높은 상관관계로 변화할 경우 그 의존성이 더욱 두드러집니다. 이러한 바람직하지 않은 “공변량 이동(covariate shift)”을 줄이기 위해 배치 정규화 [Ioffe and Szegedy, 2015]가 제안되었습니다. 이 기법은 훈련 사례 전체에 걸쳐 각 은닉 유닛의 합산 입력을 정규화합니다. 구체적으로, l번째 층의 i번째 합산 입력에 대해 배치 정규화 기법은 데이터 분포 하에서의 분산에 따라 합산 입력을 재조정합니다.

āᵢₗ = (gᵢₗ / σᵢₗ) (aᵢₗ − μᵢₗ)  
μᵢₗ = ?₍?∼P(?)₎[aᵢₗ]  
σᵢₗ = ?₍?∼P(?)₎[(aᵢₗ − μᵢₗ)²]  (2)

여기서 āᵢₗ는 l번째 층의 i번째 은닉 유닛에 대해 정규화된 합산 입력이며, gᵢₗ은 비선형 활성화 함수가 적용되기 전에 정규화된 활성화를 스케일링하는 게인 파라미터입니다. 이때 기대값은 전체 훈련 데이터 분포에 대해 계산됨을 주의해야 합니다. 식 (2)의 기대값을 정확하게 계산하기 위해서는 현재의 가중치 집합을 사용하여 전체 훈련 데이터를 순전파해야 하므로, 이는 일반적으로 실용적이지 않습니다. 대신 μ와 σ는 현재 미니 배치로부터 얻은 경험적 샘플들을 사용하여 추정됩니다. 이로 인해 미니 배치의 크기에 제약이 생기며, 순환 신경망에 적용하기 어렵게 됩니다.

**3. 레이어 정규화**  
이제 배치 정규화의 단점을 극복하기 위해 고안된 레이어 정규화 방법을 살펴보겠습니다.

한 층의 출력이 변화하면, 특히 출력이 크게 달라질 수 있는 ReLU 유닛의 경우, 다음 층의 합산 입력에도 높은 상관 관계를 가지는 변화가 발생하게 됩니다. 이는 각 층 내에서 합산 입력의 평균과 분산을 고정함으로써 “공변량 이동(covariate shift)” 문제를 줄일 수 있음을 시사합니다. 따라서, 동일한 층의 모든 은닉 유닛에 대해 레이어 정규화 통계를 다음과 같이 계산합니다:

  μₗ = (1/H) ∑₍ᵢ₌₁₎ᴴ aᵢₗ  
  σₗ = (1/H) ∑₍ᵢ₌₁₎ᴴ (aᵢₗ − μₗ)²  (3)

여기서 H는 한 층에 있는 은닉 유닛의 수를 의미합니다.

식 (2)와 식 (3)의 차이는, 레이어 정규화에서는 동일한 층 내의 모든 은닉 유닛이 동일한 정규화 항(μ와 σ)을 공유하지만, 서로 다른 훈련 사례들은 각기 다른 정규화 항을 가진다는 점입니다. 배치 정규화와 달리, 레이어 정규화는 미니 배치 크기에 어떠한 제약도 두지 않으며, 배치 크기가 1인 순수 온라인 환경에서도 사용할 수 있습니다.

**3.1 레이어 정규화된 순환 신경망**  
최근의 시퀀스 투 시퀀스 모델 [Sutskever et al., 2014]은 자연어 처리에서 순차 예측 문제를 해결하기 위해 소형 순환 신경망(RNN)을 활용합니다. NLP 작업에서는 훈련 사례마다 문장 길이가 다를 수 있는데, 이는 모든 시간 단계에서 동일한 가중치가 사용되는 RNN에서는 쉽게 다룰 수 있습니다. 그러나 배치 정규화를 RNN에 명백한 방식으로 적용하면, 시퀀스의 각 시간 단계마다 별도의 통계를 계산하고 저장해야 하므로, 테스트 시퀀스가 훈련 시퀀스보다 길 경우 문제가 발생할 수 있습니다. 레이어 정규화는 현재 시간 단계의 한 층에 대한 합산 입력만을 고려하기 때문에 이러한 문제가 없으며, 모든 시간 단계에서 공유되는 단일 게인 및 바이어스 파라미터 집합만을 사용합니다.

일반적인 RNN에서는 순환 층의 합산 입력이 현재 입력 ?ₜ와 이전 은닉 상태 벡터 hₜ₋₁로부터 아래와 같이 계산됩니다:  
  aₜ = Wₕʰ · hₜ₋₁ + Wₓʰ · ?ₜ.  
레이어 정규화된 순환 층은 식 (3)과 유사한 추가 정규화 항을 사용하여 활성화를 재중심화 및 재스케일링합니다:

  hₜ = f[ (g/σₜ) ⊙ (aₜ − μₜ) + b ]  
  μₜ = (1/H) ∑₍ᵢ₌₁₎ᴴ aᵢₜ  
  σₜ = (1/H) ∑₍ᵢ₌₁₎ᴴ (aᵢₜ − μₜ)²  (4)

여기서 Wₕʰ는 순환 은닉층 간의 가중치, Wₓʰ는 하위 계층에서 은닉층으로 전달되는 가중치를 나타냅니다.  
⊙는 두 벡터 간의 원소별 곱셈을 의미하며, b와 g는 hₜ와 동일한 차원의 바이어스와 게인 파라미터로 정의됩니다.

일반적인 RNN에서는 순환 유닛에 대한 합산 입력의 평균 크기가 매 시간 단계마다 증가하거나 감소하는 경향이 있어, 기울기가 폭발하거나 소실되는 문제가 발생합니다. 반면, 레이어 정규화된 RNN은 모든 합산 입력의 스케일을 재조정하더라도 불변성을 유지하는 정규화 항 덕분에 은닉층 간의 동역학이 훨씬 안정적으로 유지됩니다.

**4. 관련 연구**  
배치 정규화는 이전에 순환 신경망에 확장되어 적용된 바 있습니다 [Laurent et al., 2015, Amodei et al., 2015, Cooijmans et al., 2016]. 이전 연구 [Cooijmans et al., 2016]에서는 각 시간 단계마다 독립적인 정규화 통계를 유지하는 것이 순환 배치 정규화의 최상의 성능을 달성하는 방법임을 제시합니다. 저자들은 순환 배치 정규화 층에서 게인 파라미터를 0.1로 초기화하는 것이 모델의 최종 성능에 상당한 차이를 만든다는 것을 보여줍니다. 우리의 연구는 또한 weight normalization [Salimans and Kingma, 2016]과 관련이 있습니다. weight normalization에서는 분산 대신에 들어오는 가중치의 L2 노름을 사용하여 뉴런의 합산 입력을 정규화합니다. 예상 통계를 이용하여 weight normalization이나 batch normalization을 적용하는 것은 원래의 피드포워드 신경망을 다른 방식으로 파라미터화하는 것과 동일합니다. ReLU 네트워크에서의 재파라미터화는 Path-normalized SGD [Neyshabur et al., 2015]에서 연구되었습니다. 그러나, 우리가 제안하는 레이어 정규화 방법은 원래의 신경망을 재파라미터화한 것이 아닙니다. 따라서 레이어 정규화된 모델은 다른 방법들과는 다른 불변성 특성을 가지며, 이는 다음 섹션에서 다룰 예정입니다.

---

게인 파라미터(일명 γ 파라미터)는 정규화된 활성화값의 크기를 조정하기 위해 학습되는 매개변수입니다.  
정규화 과정에서는 각 뉴런의 출력값을 평균 0, 분산 1로 조정하지만, 이렇게 하면 모델이 원래의 표현력을 잃을 수 있습니다. 그래서 게인 파라미터를 곱해줌으로써 네트워크가 필요한 범위와 스케일로 다시 조정할 수 있게 해줍니다.

---
