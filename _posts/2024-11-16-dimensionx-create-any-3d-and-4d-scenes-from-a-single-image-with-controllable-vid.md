---
title: "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion"
date: 2024-11-16 20:59:39
categories:
  - 인공지능
---

<https://arxiv.org/abs/2411.04928>

[DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion](https://arxiv.org/abs/2411.04928)

요약

이 논문에서는 단일 이미지와 비디오 확산(video diffusion)을 통해 포토리얼리틱 3D 및 4D 장면을 생성하는 프레임워크인 DimensionX를 소개합니다. 우리의 접근은 3D 장면의 공간적 구조와 4D 장면의 시간적 변화를 비디오 프레임 시퀀스를 통해 효과적으로 표현할 수 있다는 통찰에서 시작합니다. 최근의 비디오 확산 모델들은 생생한 비주얼을 생성하는 데 있어 큰 성공을 보여주었지만, 생성 과정에서의 제한된 공간적 및 시간적 제어 가능성으로 인해 3D/4D 장면을 직접 복구하는 데 한계를 겪고 있습니다. 이를 극복하기 위해 우리는 ST-Director를 제안하며, 이 모델은 차원-가변 데이터로부터 차원-인식 LoRA(Low-Rank Adaptation)을 학습하여 비디오 확산에서 공간적 요소와 시간적 요소를 분리합니다. 이러한 제어 가능한 비디오 확산 접근법은 공간적 구조와 시간적 동역학을 정밀하게 조작할 수 있게 해주며, 공간과 시간 차원의 결합을 통해 프레임 시퀀스에서 3D 및 4D 표현을 재구성할 수 있습니다. 또한, 생성된 비디오와 실제 장면 간의 격차를 줄이기 위해 3D 생성에는 궤적 인식 메커니즘을, 4D 생성에는 정체성 보존 디노이징 전략을 도입하였습니다. 다양한 실제 및 합성 데이터셋에 대한 광범위한 실험을 통해 DimensionX는 기존 방법에 비해 제어 가능한 비디오 생성뿐만 아니라 3D 및 4D 장면 생성에서도 우수한 결과를 달성하였습니다. 프로젝트 페이지: <https://chenshuo20.github.io/DimensionX/>

![](/assets/images/posts/305/img.png)

그림 1: 단 하나의 이미지를 입력으로 하여, 제안된 DimensionX는 공간적 및 시간적 차원을 인식하여 매우 현실적인 비디오 및 3D/4D 환경을 생성할 수 있습니다.

### 1. 서론

컴퓨터 그래픽스와 비전의 맥락에서 3D 및 4D 콘텐츠를 이해하고 생성하는 것은 현실적인 시각적 경험을 창출하는 데 매우 중요합니다 [5, 54]. 공간적(3D) 및 시간적(4D) 차원을 표현함으로써, 비디오는 역동적인 실제 장면을 포착하는 강력한 매체로 작용합니다. 3D 및 4D 재구성 기술이 상당히 발전했음에도 [19, 52, 44, 47], 대규모 3D 및 4D 비디오 데이터셋의 부족으로 인해 단일 이미지에서 고품질의 3D 및 4D 장면을 생성하는 데 한계가 있습니다. 이러한 데이터 부족은 포토리얼리틱하고 상호작용 가능한 환경을 구축하는 데 근본적인 도전 과제가 됩니다.

다행히도, 최근 비디오 확산 모델의 발전은 실제 환경을 이해하고 시뮬레이션하는 데 있어 상당한 가능성을 보여주고 있습니다 [4, 58]. 고도화된 비디오 확산 모델을 바탕으로 최근 연구들 [43, 24, 55, 60]은 비디오 확산에 내재된 공간 및 시간 정보를 활용하여 단일 이미지에서 3D 및 4D 콘텐츠를 생성하려는 시도를 했습니다. 이러한 급속한 발전에도 불구하고, 기존 방법들은 정적 또는 동적 메시 렌더링에 훈련된 비디오 확산을 사용하여 객체 수준 생성에 집중하거나 [43, 24, 55], 장면 수준의 거친 생성에 시간 소모적인 장면별 최적화를 사용합니다 [60] (예: Score Distillation Sampling [36]). 이로 인해 일관되고 현실적인 3D/4D 장면 생성은 여전히 풀리지 않은 과제로 남아 있습니다.

이 논문에서는 DimensionX라는 단일 이미지로부터 제어 가능한 비디오 확산을 통해 고품질의 3D 및 4D 장면을 생성하는 새로운 접근 방식을 제시합니다. 최근의 비디오 확산 모델이 현실적인 결과를 생성할 수 있음에도, 생성 과정에서의 낮은 공간적 및 시간적 제어 가능성으로 인해 이러한 생성된 비디오에서 3D 및 4D 장면을 직접 재구성하는 것은 여전히 어렵습니다. 우리의 핵심 통찰은 비디오 확산에서 시간적 요소와 공간적 요소를 분리하여 각각 개별적으로 그리고 결합하여 정밀하게 제어할 수 있도록 하는 것입니다. 차원 인식 제어를 달성하기 위해, 우리는 공간적 및 시간적 차원이 다양한 데이터를 수집하여 포괄적인 프레임워크를 구축합니다. 이 데이터셋을 바탕으로 우리는 ST-Director를 제안하며, 이는 비디오 확산에서 차원 인식 LoRA(Low-Rank Adaptation)를 통해 공간적 및 시간적 사전 지식을 분리합니다. 또한 비디오 확산의 디노이징 메커니즘을 분석하여, 훈련이 필요 없는 합성 방법을 통해 하이브리드 차원 제어를 달성합니다. 이 제어를 통해 DimensionX는 공간적 및 시간적 변이 프레임의 시퀀스를 생성하여 3D 외형과 4D 동적 움직임을 재구성할 수 있습니다. 복잡한 실제 장면을 다루기 위해, 우리는 3D 생성에는 궤적 인식 접근법을, 4D 생성에는 정체성 보존 디노이징 메커니즘을 설계했습니다. 광범위한 실험 결과, DimensionX는 기존 방법들에 비해 3D 및 4D 장면 생성의 시각적 품질과 일반화 측면에서 뛰어난 성능을 보임을 입증하며, 비디오 확산 모델이 현실적이고 동적인 환경을 만드는 데 유망한 방향임을 시사합니다.

요약하면, 우리의 주요 기여는 다음과 같습니다:

- 우리는 단일 이미지에서 제어 가능한 비디오 확산을 사용하여 포토리얼리틱한 3D 및 4D 장면을 생성하는 새로운 프레임워크인 DimensionX를 제시합니다.
- 우리는 비디오 확산 모델에서 공간 및 시간적 사전 지식을 차원 인식 모듈을 학습함으로써 분리하는 ST-Director를 제안합니다. 또한 비디오 확산 디노이징 과정의 본질에 따라 훈련이 필요 없는 합성 접근 방식을 통해 하이브리드 차원 제어를 강화합니다.
- 비디오 확산과 실제 장면 간의 격차를 줄이기 위해, 우리는 3D 생성을 위한 궤적 인식 메커니즘과 4D 생성을 위한 정체성 보존 디노이징 접근법을 설계하여 보다 현실적이고 제어 가능한 장면 합성을 가능하게 합니다.
- 광범위한 실험을 통해 DimensionX가 비디오, 3D, 4D 생성에서 기존 기준 방법에 비해 우수한 성능을 보임을 확인했습니다.

![](/assets/images/posts/305/img_1.png)

그림 2: DimensionX의 파이프라인. 우리의 프레임워크는 크게 세 부분으로 나뉩니다. (a) ST-Director를 활용한 제어 가능한 비디오 생성. 우리는 차원 인식 LoRA를 학습하여 비디오 확산 모델에서 공간 및 시간 매개변수를 분해하는 ST-Director를 소개합니다. (b) S-Director를 사용한 3D 장면 생성. 주어진 뷰에서 S-Director가 생성한 비디오 프레임으로부터 고품질 3D 장면을 복구합니다. (c) ST-Director를 사용한 4D 장면 생성. 단일 이미지가 주어졌을 때, T-Director가 시간 변이가 있는 비디오를 생성하며, 이 비디오에서 키 프레임을 선택해 공간 변이 참조 비디오를 생성합니다. 참조 비디오의 가이드를 받아 S-Director가 프레임별 공간 변이 비디오를 생성하고, 이들이 결합되어 다중 뷰 비디오가 생성됩니다. T-Director의 다중 루프 정제를 통해 일관된 다중 뷰 비디오가 생성되며, 이를 통해 4D 장면이 최적화됩니다.

### 2. 관련 연구

### 제어 가능한 비디오 확산

조건부 이미지 및 비디오 생성에 추가적인 조건을 통합하는 방식 [62]은 주목할 만한 성공을 거두었습니다. 이전의 비디오 생성 연구 대부분은 확산 모델을 안내하기 위해 다양한 제어 신호를 주입하는 데 집중해왔습니다. 예를 들어, 일부 접근법은 카메라 포즈 궤적을 사용해 비디오 확산을 제어하며, 주로 ControlNet [15, 49], 플뤼커(Plücker) 좌표 임베딩 [56, 2, 21], 또는 다른 좌표 임베딩 [57, 50]을 사용합니다. 다른 방법들은 참조 비디오 [27]나 객체 움직임 궤적 [64, 49]을 활용하여 생성을 유도하기도 합니다. 추가적으로, 행동 기반 제어 [12], 스케치 및 깊이 맵 [14]과 같은 다른 제어 신호들을 탐색한 연구들도 있습니다. 그러나 이러한 방법들은 특정 데이터 주석에 의존하기 때문에 확장성에 한계가 있고, 외부 제어 신호가 비디오 모델에 주입될 때 성능 저하를 초래할 수 있습니다. 추가적인 제어 신호를 주입하는 대신, 일부 방법들은 유사한 패턴을 가진 참조 비디오 집합에 대해 미세 조정하여 비디오 확산 모델을 맞춤화합니다 [13, 65, 51]. 이는 주로 공간 및 시간 계층을 가진 Unet 기반의 비디오 확산 모델에 집중합니다. 하지만 이러한 맞춤화 방식은 일반화가 부족한 경향이 있습니다. 본 연구에서는 3D VAE와 전체 주의를 활용하여 공간 및 시간 정보를 효과적으로 혼합하는 DiT 기반 [35] 비디오 확산 모델인 CogVideoX [58]를 제어하기 위해 설계된 방법을 제안합니다. 우리의 방법은 추가적인 신호 주입이 필요하지 않으며, 성능을 희생하지 않으면서도 확장성과 일반화를 제공합니다.

### 3D 생성과 확산 사전

2D 확산 사전을 활용한 3D 콘텐츠 생성은 3D 생성 분야에 혁신을 가져왔습니다. Score Distillation Sampling (SDS) [36, 48, 25]은 2D 확산 사전을 증류하여 텍스트 입력으로부터 고해상도의 3D 메쉬를 생성합니다. 3D 일관성을 더 향상시키기 위해, 여러 연구들은 다중 뷰 카메라 포즈에 따라 2D 확산 모델을 조건부로 하여 객체 수준 생성을 탐구했습니다 [39, 59, 30, 53, 31]. 유사한 기술들이 장면 수준 생성에도 적용되었습니다 [38]. 최근 접근법들은 비디오 확산 모델을 활용하여 단일 이미지 입력에서 새로운 뷰를 생성하며, 객체 수준 [43, 6, 11] 및 장면 수준 [6, 11, 29] 모두에서 인상적인 결과를 얻었습니다. 추가적으로, ReconX [28]는 비디오 보간 기법을 사용하여 드문 뷰 입력 문제를 해결하며, 비디오 확산 모델이 3D 장면 생성에 갖는 잠재력을 보여줍니다. 본 연구에서는 단일 이미지 입력에서 비디오 확산 모델의 잠재력을 새로운 방식으로 활용하여 3D 객체와 장면을 생성하며, 더 적은 학습 비용과 데이터로 경쟁력 있는 성능을 달성합니다.

### 4D 생성과 확산 사전

3D 생성과 마찬가지로, 4D 생성도 이미지 및 비디오 확산을 포함한 사전 학습된 확산 모델로 상당한 발전을 이루었습니다. 초기 연구들은 [37, 1, 66, 46] 텍스트나 이미지 입력으로부터 장면별 최적화를 통해 4D 표현을 생성하기 위해 SDS 기술을 채택했습니다. 그러나 이러한 방법들은 4D 자산을 생성하는 데 몇 시간이 소요되며 일관성이 떨어지는 문제가 있습니다. 일관성과 생성 효율성을 높이기 위해, 이후 연구들 [55, 24]은 대규모 Objaverse 데이터셋 [8, 9]에서 고품질 동적 메쉬 데이터를 필터링했습니다. 이러한 동적 메쉬를 기반으로, 다중 뷰 비디오를 렌더링하여 다중 뷰 비디오 확산 모델을 학습했습니다. 이러한 모델들은 고품질의 4D 다중 뷰 비디오를 생성할 수 있지만, 주로 객체 중심 설정에 집중하며 복잡한 장면을 다루지는 않습니다. 4D 장면 생성을 위해서는 충분한 데이터의 부족으로 전체 장면을 재구성하는 데 활용되는 다중 뷰 비디오 생성에 상당한 어려움이 있습니다. 최근에는 4Real [60]이 처음으로 사전 학습된 비디오 확산 사전을 SDS 손실과 함께 증류하여 포토리얼리틱한 동적 장면을 생성하는 방안을 제안했습니다. 기존 연구와 달리, 본 연구는 시간적 및 공간적으로 분해된 비디오를 생성하고 이를 결합하여 고품질의 4D 장면 재구성을 위한 다중 뷰 비디오를 생성하는 것을 강조합니다.

### 3. 방법론

단일 이미지가 주어졌을 때, 우리의 목표는 제어 가능한 비디오 확산을 통해 고품질의 3D 및 4D 장면을 생성하는 것입니다. 공간 및 시간 차원에서의 효과적인 제어를 달성하기 위해, 우리는 먼저 차원 가변 데이터셋을 구축하는 체계적인 프레임워크를 개발합니다 (섹션 3.1). 수집된 데이터셋을 바탕으로, 우리는 ST-Director를 도입하여 차원 인식 LoRA를 통해 공간적 및 시간적 기반을 분리하고, 정밀한 차원 인식 제어를 가능하게 합니다. 또한, 비디오 생성 과정에서 디노이징 메커니즘을 탐구하고 효과적인 하이브리드 차원 제어를 위한 훈련이 필요 없는 차원 인식 합성을 도입합니다 (섹션 3.2). 더 나아가 제어 가능한 비디오 확산을 활용하여 고품질 장면을 생성하기 위해, 우리는 3D 생성을 위한 궤적 인식 메커니즘과 4D 생성을 위한 정체성 보존 디노이징 접근법을 설계합니다 (섹션 3.3과 3.4).

### 3.1 차원 가변 데이터셋 구축

비디오 확산에서 공간적 및 시간적 매개변수를 분리하기 위해, 우리는 공개된 데이터셋에서 공간 및 시간 가변 비디오를 수집하는 프레임워크를 도입합니다. 특히, 우리는 공간 가변 데이터를 위해 궤적 계획 전략을 사용하고, 시간 가변 데이터를 위해 흐름 안내를 사용합니다.

#### 공간 가변 데이터의 궤적 계획

![](/assets/images/posts/305/img_2.png)

위 계산을 기반으로 우리는 장면 전체에서 카메라의 분포를 파악했습니다. 다양한 장면을 다루기 위해, 우리는 다음과 같은 규칙을 세워 적합한 데이터를 필터링합니다: (1) 카메라 분포: 장면의 중심을 계산하고, 카메라가 장면 주변을 어떻게 포착하는지 판단합니다. (2) 바운딩 박스 종횡비: 바운딩 박스의 종횡비는 다양한 S-Director 요구사항을 충족해야 합니다. 예를 들어, x축과 y축의 종횡비는 크게 변하지 않아야 하며, 이는 적절한 360도 주변 비디오를 선택하는 데 도움이 됩니다. (3) 카메라와 바운딩 박스 사이의 거리: 각 카메라 위치에서 바운딩 박스의 가장 가까운 평면까지의 거리를 계산하고, 카메라 배치가 더 좋은 데이터를 우선시합니다.

필터링된 데이터셋으로, 장면 내의 점유 필드를 계산하여 렌더링 카메라의 가능한 영역을 계획하는 데 도움이 됩니다. 다중 뷰 이미지를 통해 전체 장면의 3DGS를 재구성한 후, 다중 뷰 이미지와 해당 깊이 맵을 렌더링하고, TSDF [7]를 사용하여 RGB-D 데이터로부터 장면의 메쉬를 추출합니다. 자세한 내용은 부록을 참조하세요.

#### 시간 가변 데이터의 흐름 안내

시간적 제어를 달성하기 위해, 우리는 시간 가변 데이터를 필터링하여 비디오 확산 모델을 미세 조정하는 것을 목표로 합니다. 특히, 우리는 광학 흐름 [42]을 활용해 시간 가변 비디오를 필터링합니다. 시간 가변 비디오의 경우, 광학 흐름 맵이 자주 광범위한 흰 영역을 나타내며, 이는 효과적인 선택 기준으로 작용할 수 있습니다.

![](/assets/images/posts/305/img_3.png)

그림 3: 주의 맵 시각화. 왼쪽 열은 원래 비디오 확산 모델의 디노이징 과정 중 주의 맵을 보여줍니다. 오른쪽 열은 위에서 아래로 각각 S-Director와 T-Director의 주의 맵 변화를 나타냅니다. 0단계부터 시작하여, 초기 디노이징 단계(전체 50단계 중 10단계 이전)에서 출력 비디오의 윤곽과 레이아웃이 결정됩니다. 특히 디노이징 과정에서 공간적 구성 요소가 시간적 정보보다 먼저 복구됩니다.

### 3.2 제어 가능한 비디오 생성을 위한 ST-Director

![](/assets/images/posts/305/img_4.png)

그러나 4차원에서 2차원으로의 이 축소 과정은 대부분의 비디오 생성 방법에서 공간적 및 시간적 차원을 분리할 수 없는 혼합으로 취급되며, 이로 인해 각 차원에 대한 독립적인 제어가 어렵고, 4차원 공간을 복구하기가 어려워집니다. 이를 해결하기 위해, 우리는 두 개의 직교 기저 디렉터, 즉 S-Director(Spatial Director)와 T-Director(Temporal Director)를 도입하여 비디오 생성 과정에서 공간적 및 시간적 변이를 분리할 수 있도록 합니다. 이러한 직교 기저 디렉터를 통해 우리는 비디오 생성을 보다 유연하게 제어할 수 있으며, 단일 축을 따라 프레임을 생성하거나 두 디렉터를 결합하여 4차원 공간 내에서 원하는 관점을 나타내는 투영을 생성할 수 있습니다.

-----

### 전체 맥락

DimensionX의 ST-Director는 4차원 공간(공간적 3차원 + 시간적 1차원)에서 고품질의 3D 및 4D 장면을 생성하는데, 이를 위해 **공간적 요소**와 **시간적 요소**를 분리하고자 합니다. 이는 더 정밀하고 자유로운 제어를 가능하게 하기 위해서입니다.

![](/assets/images/posts/305/img_5.png)

![](/assets/images/posts/305/img_6.png)

![](/assets/images/posts/305/img_7.png)

-----

### 3.2.1 차원 인식 분해

![](/assets/images/posts/305/img_8.png)

![](/assets/images/posts/305/img_9.png)

### 3.2.2 튜닝 없는 차원 인식 합성

![](/assets/images/posts/305/img_10.png)

#### 관찰 결과 1: 디노이징 초기 단계가 비디오 생성의 핵심이다.

주의 맵에서 초기 디노이징 단계 동안, 기저 모델과 두 디렉터 모두 최종 생성 결과와 밀접하게 일치하는 기초적인 스케치를 설정합니다. 이러한 초기 윤곽은 초기 단계에서 필수적인 공간 및 시간적 구조를 포착하여 나머지 디노이징 단계의 방향을 설정합니다. 추가적으로, 이러한 변화가 어떻게 펼쳐지는지에 차이가 있습니다. 기저 모델에서는 시간적 및 공간적 변화가 동시에 발생하여 두 차원이 통합된 진화를 이룹니다. 반면, S-Director와 T-Director를 사용할 때는 디렉터에 따라 시간적 또는 공간적 차원 중 하나만 변화합니다.

#### 관찰 결과 2: 공간 정보가 시간 정보보다 먼저 구축된다.

Motionclone [27]의 발견과 유사하게, 객체 움직임의 합성은 디노이징 초기 단계에서 덜 발달된 상태로 나타납니다. 특히, S-Director를 사용하면 최종 비디오의 구조적 윤곽이 시간적 제어보다 훨씬 더 일찍 나타나는 것을 주의 맵에서 확인할 수 있습니다. 그림 3에서 보듯이, 디노이징 루프의 동일한 단계인 0단계와 3단계에서 T-Director를 사용할 경우 객체는 여전히 정지해 있지만, S-Director는 이미 카메라가 장면을 이동하도록 안내하고 있습니다.

![](/assets/images/posts/305/img_11.png)

![](/assets/images/posts/305/img_12.png)

그림 4: 차원 인식 비디오 생성의 정성적 비교. 동일한 이미지와 텍스트 프롬프트가 주어졌을 때, 첫 번째 행은 시간 가변 비디오 생성(카메라 고정), 두 번째 행은 공간 가변 비디오 생성(카메라 확대), 세 번째 행은 공간 및 시간 가변 비디오 생성(카메라 오른쪽으로 회전)을 나타냅니다.

### 3.3 S-Director를 활용한 3D 장면 생성

![](/assets/images/posts/305/img_13.png)

#### 단일 뷰 장면 생성

![](/assets/images/posts/305/img_14.png)

#### 희소 뷰 장면 생성

![](/assets/images/posts/305/img_15.png)

제안된 긴 비디오 확산 모델과 다양한 S-Directors를 통해, 광범위한 장면들이 생성된 비디오로부터 직접 재구성될 수 있습니다. 특히 희소 뷰(즉, 최소 한 개)의 이미지와 기본 궤적 또는 이러한 움직임 원소들의 조합일 수 있는 선택된 카메라 움직임 유형이 주어졌을 때, 우리의 비디오 확산은 지정된 경로를 따라 일관된 긴 비디오를 생성할 수 있습니다. 생성된 비디오에서 발생할 수 있는 **불일치**를 완화하기 위해, 우리는 3D 장면을 재구성하기 위해 **신뢰도 인식 가우시안 분할 절차**를 채택합니다. DUSt3R [45]에서 생성된 포인트 클라우드와 추정된 카메라 포즈를 초기값으로 하여, \*\*3D 가우시안 분할(3D Gaussian Splatting)\*\*은 추가적인 LPIPS 손실 [63]과 DUSt3R에서 얻은 신뢰도 맵을 사용하여 최적화됩니다. 우리는 3DGS 손실을 다음과 같이 채택합니다:

![](/assets/images/posts/305/img_16.png)

![](/assets/images/posts/305/img_17.png)

그림 5: 희소 뷰 3D 생성의 정성적 비교. 두 개의 큰 각도에서 본 뷰가 주어졌을 때, 우리의 접근 방식은 다른 기준 모델들에 비해 명확히 뛰어난 성능을 보여줍니다.

### 3.4 ST-Director를 활용한 4D 장면 생성

공간 및 시간 제어가 가능한 비디오 확산을 이용하여, 우리는 단일 이미지로부터 고품질의 4D 동적 장면을 복원할 수 있습니다. 직접적인 방법은 각 시간 변이 비디오의 프레임마다 생성된 공간 변이 비디오들을 결합해 다중 뷰 비디오를 만드는 것입니다. 그런 후 이 비디오들을 사용해 4D 장면을 재구성합니다. 그러나 이 방법에는 주요한 문제, 즉 **3D 일관성**의 유지가 있습니다. 공간 변이 비디오 전반에서 배경 및 객체의 외형 일관성을 유지하는 것은 어려우며, 이로 인해 4D 장면에서 큰 흔들림이나 불연속성이 발생할 수 있습니다. 이러한 문제를 해결하기 위해, 우리는 **참조 비디오 잠재 공유**와 **외형 정제 과정**을 포함한 **정체성 보존 디노이징 전략**을 제안하여 모든 공간 변이 비디오의 일관성을 높였습니다.

![](/assets/images/posts/305/img_18.png)

#### 참조 비디오 잠재 공유

![](/assets/images/posts/305/img_19.png)

#### 외형 정제

![](/assets/images/posts/305/img_20.png)

**표 1**: 단일 뷰 및 희소 뷰 시나리오에서의 정량적 비교. 우리의 접근 방식은 단일 뷰 및 희소 뷰 설정 모두에서 다른 기준 방법들에 비해 모든 지표에서 우수한 성능을 보여줍니다.

![](/assets/images/posts/305/img_21.png)

-----

제공된 **Table 1**을 보면, 우리의 접근 방식이 다른 기준 방법들에 비해 \*\*단일 뷰(single-view)\*\*와 **희소 뷰(sparse-view)** 시나리오 모두에서 모든 평가 지표에서 더 우수한 성능을 보이는 것을 확인할 수 있습니다. 이제 각 평가 지표를 중심으로 왜 우리의 접근 방식이 우수한 성능을 보이는지 더 자세히 설명하겠습니다.

### 1. 평가 지표 (PSNR, SSIM, LPIPS)

- **PSNR (Peak Signal-to-Noise Ratio) - 높을수록 좋음**: PSNR은 이미지의 화질을 평가하는데 사용되며, 값이 높을수록 재구성된 이미지가 원본에 가깝다는 의미입니다.
- **SSIM (Structural Similarity Index) - 높을수록 좋음**: SSIM은 이미지의 구조적 유사성을 평가합니다. 값이 높을수록 시각적으로 더 유사한 구조를 가지고 있다는 것을 의미합니다.
- **LPIPS (Learned Perceptual Image Patch Similarity) - 낮을수록 좋음**: LPIPS는 이미지의 시각적 품질을 인식적 유사성 기준으로 평가하는 지표입니다. 값이 낮을수록 인간이 느끼기에 더 유사한 이미지라는 의미입니다.

우리 방법의 성능을 다른 방법들과 비교해보면, 모든 데이터셋과 상황에서 더 높은 **PSNR**과 **SSIM** 값을 보이며, **LPIPS** 값은 더 낮게 나타났습니다. 이러한 결과는 우리의 접근 방식이 다른 기준 방법들에 비해 다음의 측면에서 더 우수한 이유를 잘 설명해줍니다.

### 2. 우리의 접근 방식이 우수한 이유

#### 2.1. **ST-Director 기반의 차원 분리 및 제어**

- 우리의 접근 방식은 **ST-Director**를 통해 공간적 요소와 시간적 요소를 분리하여 각각의 변이를 독립적으로 제어할 수 있습니다. 이를 통해 더 정밀하게 카메라의 움직임과 객체의 동작을 조절할 수 있어, 단일 이미지에서 더 현실적이고 일관된 다중 뷰 비디오 및 4D 장면을 생성할 수 있습니다.
- **S-Director**는 공간적 변이를 제어하고, **T-Director**는 시간적 변이를 제어함으로써, 각각의 디렉터가 장면의 특정 차원을 효율적으로 관리합니다. 이 분리된 제어 방식은 이미지가 불필요하게 왜곡되거나 시간적 불일치가 발생하는 것을 줄여주어, **PSNR**과 **SSIM** 성능이 향상됩니다.

#### 2.2. **정체성 보존 디노이징 전략**

- 우리는 **정체성 보존 디노이징 전략**을 통해 모든 공간 변이 비디오가 일관성을 유지하도록 합니다. 이 전략은 참조 비디오 잠재 공유와 외형 정제 과정으로 구성되어 있으며, 이를 통해 다중 뷰 비디오 간의 시각적 일관성을 유지하면서도 흔들림이나 뷰 간 불일치를 최소화할 수 있습니다.
- 특히, **잠재 공유**를 통해 초기 참조 프레임으로부터 일관된 노이즈 잠재를 사용하고, 이를 바탕으로 디노이징하여 최종적으로 고품질의 동적 장면을 생성하게 됩니다. 이는 비디오 전반에서 시각적 연속성을 보장하여 **SSIM**과 **LPIPS** 지표에서 우수한 성능을 발휘하게 합니다.

#### 2.3. **외형 정제(Appearance Refinement)**

- **외형 정제 과정**은 이미지-이미지 변환 기술에서 영감을 받아 다중 뷰 비디오를 부드럽게 하는 데 기여했습니다. 각 뷰포인트에서 생성된 동적 비디오에 대해 랜덤 노이즈를 적용하고, 여러 단계의 디노이징을 수행하여 각 프레임 간의 미세한 차이를 줄이고 시각적 일관성을 높였습니다.
- 이로 인해 **LPIPS** 값이 낮아졌습니다. 이는 사용자가 느끼기에 더 일관되고 부드러운 시각적 품질을 보장할 수 있음을 의미합니다.

#### 2.4. **궤적 인식 메커니즘(Trajectory-aware Mechanism)**

- 다양한 S-Directors를 사용해 여러 종류의 카메라 궤적을 따를 수 있는 **궤적 인식 메커니즘**을 도입했습니다. 이를 통해 장면의 다양한 각도에서의 비디오를 생성할 때, 일관된 시점 변화와 객체 외형을 유지할 수 있었습니다.
- 예를 들어, 360도 회전 장면을 재구성할 때 일반적인 접근 방식에서는 카메라가 움직일 때마다 장면의 불연속성이 발생할 수 있습니다. 하지만 우리의 접근 방식에서는 각 카메라 궤적마다 적절한 S-Director를 적용함으로써, 장면의 전체적인 일관성을 유지할 수 있어 **PSNR**과 **SSIM**이 개선되었습니다.

### 3. 다른 기준 방법과의 비교

#### 단일 뷰 및 희소 뷰 시나리오에서의 성능 비교

- **단일 뷰 시나리오**에서는 ZeroNVS [38]와 ViewCrafter [61] 같은 기존 방법들보다 더 높은 **PSNR**과 **SSIM**을 달성했고, **LPIPS** 값은 더 낮았습니다. 이는 우리의 모델이 단일 이미지에서도 보다 상세하고 일관된 구조를 복원할 수 있음을 의미합니다.
- **희소 뷰 시나리오**에서도 DNGaussian [22], InstantSplat [10], 그리고 ViewCrafter와 비교했을 때 모든 평가 지표에서 더 나은 성능을 보였습니다. 특히, **PSNR**이 더 높고 **LPIPS**가 더 낮은 것은 우리의 모델이 희소 뷰에서도 높은 충실도와 시각적 품질을 유지한다는 것을 의미합니다.

### 요약

우리의 접근 방식은 **차원 분해 및 제어(ST-Director)**, **정체성 보존 디노이징**, **외형 정제 과정**, 그리고 **궤적 인식 메커니즘**을 결합하여 단일 이미지로부터 일관성 있고 포토리얼리틱한 4D 장면을 생성할 수 있습니다. 이러한 이유로, 단일 뷰와 희소 뷰 모두에서 다른 기준 방법들보다 더 우수한 성능을 보이며, 높은 PSNR과 SSIM, 낮은 LPIPS 값을 기록했습니다.

-----

### 4. 실험

이 섹션에서는 실제 및 합성 데이터셋에서 DimensionX의 제어 가능성, 그리고 ST-Director를 사용한 3D 및 4D 장면 생성 기능을 평가하기 위한 광범위한 실험을 수행합니다. 먼저, 실험의 세부사항을 설명합니다 (섹션 4.1). 그 다음으로, 섹션 4.2에서 제어 가능한 비디오 생성에 대한 정량적 및 정성적 평가를 제공합니다. 이어서 단일 뷰 및 희소 뷰 3D 생성 시나리오에서 다른 기준 방법들과 비교한 정량적 및 정성적 결과를 보고합니다 (섹션 4.3). 다음으로, 4D 장면 생성 결과를 제시합니다 (섹션 4.4). 마지막으로, 설계의 효과성을 평가하기 위한 다양한 **Ablation Study**를 수행합니다 (섹션 4.5).

### 4.1 실험 설정

#### 구현 세부사항

![](/assets/images/posts/305/img_22.png)

#### 데이터셋

전체 프레임워크에서 우리의 비디오 확산 모델은 주로 세 개의 데이터셋으로 훈련됩니다: **DL3DV-10K** [26], **OpenVid** [34], 그리고 **RealEstate-10K** [67]. **OpenVid-1M** [34]는 1백만 개의 비디오 클립을 포함하고 있는 고품질의 공개된 비디오 데이터셋으로, 다양한 모션 동역학과 카메라 제어를 포함하고 있습니다. **DL3DV-10K** [26]는 고해상도 다중 뷰 이미지를 포함한 다양한 실내 및 실외 장면을 가진 광범위하게 수집된 3D 장면 데이터셋입니다. **RealEstate-10K**는 유튜브에서 수집된 데이터셋으로, 주로 실내 장면의 촬영물을 포함합니다. 우리가 설계한 데이터 수집 프레임워크를 적용하여, 우리는 **DL3DV-10K**와 **OpenVid**에서 차원 가변 데이터셋을 구축합니다. **OpenVid**에서 100개의 고품질 시간 가변 비디오를 선택해 **T-Director**를 훈련합니다. 각 **S-Director** 유형에 대해, 특정 카메라 궤적에 따라 100개의 비디오를 렌더링하여 해당 **LoRA**를 훈련합니다. 비디오 프레임을 확장하기 위해, 우리는 **RealEstate-10K** [67]와 **OpenVid** [34]에서 145 프레임을 초과하는 고품질 비디오를 필터링하여 비디오 확산 모델을 전체 미세 조정합니다. 동일한 데이터셋을 사용해 비디오 보간 모델도 처음과 마지막 프레임 지침으로 미세 조정합니다. 우리의 **DimensionX**의 3D 생성 능력을 추가로 검증하기 위해, 우리는 다른 기준 방법들과의 비교를 위해 **Tank-and-Temples** [20], **MipNeRF360** [3], **NeRF-LLFF** [33], **DL3DV-10K** [26]를 사용합니다.

![](/assets/images/posts/305/img_23.png)

그림 6: 4D 장면 생성의 정성적 결과. 실제 또는 합성 단일 이미지가 주어졌을 때, 우리의 DimensionX는 풍부한 특징을 가진 일관성 있고 정교한 4D 장면을 생성합니다.

### 4.2 제어 가능한 비디오 생성

![](/assets/images/posts/305/img_24.png)

**Table 2**: 제어 가능한 비디오 생성에 대한 정성적 비교. 우리의 DimensionX는 **일관성**, **동적성**, **미적 점수**를 포함한 모든 평가 지표에서 기준 모델들을 능가합니다.

#### 기준 모델 및 평가 지표

우리는 DimensionX를 오픈 소스 모델 **CogVideoX** [58] 및 클로즈드 소스 제품인 **Dream Machine 1.6**과 비교했습니다. 우리는 평가 데이터셋으로 수백 개의 이미지를 수집했습니다. 이전 벤치마크인 **VBench** [18]를 따라, 생성된 비디오의 **주제 일관성(Subject Consistency)**, **동적 정도(Dynamic Degree)**, 그리고 \*\*미적 점수(Aesthetic Score)\*\*를 평가 지표로 사용했습니다.

#### 정량적 및 정성적 비교

**표 2**의 정성적 결과는 우리의 접근 방식이 더 나은 시각적 품질과 **3D 일관성**을 포함하여 인상적인 성능을 보여줌을 입증합니다. **그림 4**에서 볼 수 있듯이, 우리의 DimensionX는 비디오 확산 모델의 공간적 및 시간적 매개변수를 효과적으로 분해할 수 있습니다. 반면에 **Dream Machine**은 카메라 움직임과 프롬프트 제약을 사용하더라도 차원 인식 제어를 분리하지 못했습니다.

또한, 공간적 및 시간적 움직임을 포함한 **하이브리드 차원 제어**에 있어, Dream Machine과 비교했을 때 우리의 DimensionX는 더 인상적이고 동적인 비디오를 생성합니다. 정량적 및 정성적 결과 모두 우리의 접근 방식이 동적 움직임과 주제 일관성을 유지하면서 제어 가능한 비디오를 생성할 수 있음을 나타냅니다.

이러한 결과는 우리의 DimensionX가 기존의 기준 모델들보다 공간적 및 시간적 제어가 뛰어나며, 더 높은 일관성과 미적 품질을 제공할 수 있음을 의미합니다.

### 4.3 3D 장면 생성

#### 기준 모델 및 평가 지표

단일 뷰 설정에서는 우리의 접근 방식을 두 가지 생성 방법인 \*\*ZeroNVS [38]\*\*와 \*\*ViewCrafter [61]\*\*와 비교했습니다. 희소 뷰 시나리오에서는 **DNGaussian [22]**, **InstantSplat [10]**, 그리고 \*\*ViewCrafter [61]\*\*를 포함한 두 가지 희소 뷰 재구성 방법과 하나의 희소 뷰 생성 기준 모델을 선택했습니다. 우리는 정량적 결과를 위해 **PSNR**, **SSIM**, **LPIPS**를 평가 지표로 채택했습니다. 구체적으로, 단일 뷰와 희소 뷰 설정 모두에서 주어진 이미지로부터 3D 장면을 재구성한 후, 새로운 뷰에서의 렌더링을 사용해 평가 지표를 계산합니다.

#### 정량적 및 정성적 비교

정량적 비교 결과는 **표 1**에 제시되어 있습니다. DimensionX가 모든 평가 지표에서 기준 모델들을 능가하는 것을 확인할 수 있으며, 이는 우리의 접근 방식이 인상적인 성능을 가지고 있음을 보여줍니다. **그림 5**에서 제시된 것처럼, 단일 뷰(자세한 내용은 부록을 참고하세요)와 희소 뷰 설정 모두에서 우리의 접근 방식은 고품질의 3D 장면을 재구성할 수 있었으며, 다른 기준 모델들은 이러한 어려운 사례들을 처리하는 데 실패했습니다.

### 4.4 4D 장면 생성

우리는 DimensionX를 **실제 데이터셋**과 **합성 데이터셋** 모두에서 평가했습니다. 구체적으로, 우리는 다양한 장면의 고해상도 다중 뷰 비디오를 포함한 \*\*Neu3D [23]\*\*를 사용해 실제 비디오에서 4D 생성을 위한 우리의 접근 방식의 성능을 검증했습니다. **그림 6**에 제시된 것처럼, 단일 이미지가 주어졌을 때 우리의 DimensionX는 큰 각도의 새로운 뷰에서 매우 일관된 동적 비디오를 생성합니다.

이러한 결과는 우리의 접근 방식이 단일 이미지에서도 높은 수준의 일관성을 유지하며 복잡한 4D 장면을 효과적으로 생성할 수 있음을 보여줍니다.

### 4.5 소거 연구 (Ablation Study)

![](/assets/images/posts/305/img_25.png)

**그림 7**: 희소 뷰 3D 생성에 대한 소거 연구: S-Director의 부재는 낮은 재구성 품질을 초래합니다.

#### 3D 생성에서의 궤적 인식 메커니즘

희소 뷰 3D 생성에서는 **S-Director**를 이용해 비디오 보간 모델을 안내합니다. **그림 7**에서 설명된 것처럼, 큰 각도의 희소 뷰를 처리할 때 **S-Director**가 없을 경우 흔히 "야누스 문제(Janus problem)"가 발생하여, 여러 개의 머리가 생성되는 문제가 생깁니다. 이는 재구성 품질을 크게 저하시킵니다.

#### 4D 생성에서의 정체성 보존 디노이징 전략

4D 장면 생성에서는 실제 이미지에 대한 실험을 수행하여 **4D 장면 생성을 위한 정체성 보존 디노이징 전략**을 분석했습니다. **그림 8**에서 볼 수 있듯이, 새로운 뷰의 서로 다른 프레임 간의 일관성 측면에서 **참조 비디오 잠재 공유**와 **외형 정제** 디자인을 소거하여 실험을 진행했습니다. 구체적으로, 프레임별 비디오를 직접 결합하면 배경과 주제의 형태를 포함하여 심각한 불일치가 발생하는 것을 확인할 수 있습니다. **참조 비디오 잠재 공유**를 통해 전체적인 배경과 외형은 서로 다른 프레임 간에 높은 일관성을 보였습니다. 참조 비디오 잠재 공유를 기반으로, **외형 정제**는 외형 세부 사항의 일관성을 더욱 강화했습니다.

![](/assets/images/posts/305/img_26.png)

**그림 8**: 4D 생성에 대한 소거 연구: 참조 비디오 잠재 공유와 외형 정제의 설계를 소거한 결과입니다.

이 실험 결과들은 **S-Director**와 **정체성 보존 디노이징 전략**이 각각 3D 및 4D 장면 생성에서의 일관성과 품질에 큰 영향을 미친다는 것을 보여줍니다.

### 5. 결론

이 논문에서는 단일 이미지로부터 제어 가능한 비디오 확산을 통해 포토리얼리틱한 3D 및 4D 장면을 생성하는 새로운 프레임워크인 **DimensionX**를 소개했습니다. 우리의 핵심 통찰은 **ST-Director**를 도입하여 차원 가변 데이터셋에서 차원 인식 **LoRA**를 학습함으로써 비디오 확산 모델에서 공간적 및 시간적 사전 지식을 분리하는 것입니다. 더 나아가, 우리는 비디오 확산의 **디노이징 과정**을 조사하고, 하이브리드 차원 제어를 달성하기 위한 **튜닝 없이도 가능한 차원 인식 합성**을 도입했습니다. 제어 가능한 비디오 확산을 통해 생성된 순차적인 비디오 프레임으로부터 정확한 3D 구조와 4D 동적 움직임을 복원할 수 있었습니다. 또한, 실제 장면에서의 DimensionX의 일반화를 더욱 향상시키기 위해, 우리는 **3D 장면 생성**을 위한 궤적 인식 전략과 **4D 장면 생성**을 위한 정체성 인식 메커니즘을 맞춤 설계했습니다. 다양한 실제 및 합성 데이터셋에 대한 광범위한 실험을 통해 우리의 접근 방식이 제어 가능한 비디오 생성뿐만 아니라 3D 및 4D 장면 생성에서 최신 성능을 달성했음을 입증했습니다.

### 한계점 및 미래 연구 방향

놀라운 성과에도 불구하고, **DimensionX**는 여전히 **확산 모델 백본**의 한계에 직면해 있습니다. 현재 비디오 확산 모델이 생동감 있는 결과를 합성할 수는 있지만, 여전히 미세한 디테일을 이해하고 생성하는 데 어려움을 겪고 있어 합성된 3D 및 4D 장면의 품질을 제한합니다. 또한, 비디오 확산 모델의 **긴 추론 시간**은 우리의 생성 과정의 효율성을 저하시킵니다. 미래에는 **확산 모델을 통합하여 더 효율적인 엔드 투 엔드 3D 및 4D 생성**을 탐구하는 것이 가치가 있을 것입니다. 우리는 우리의 연구가 비디오 확산 모델을 사용하여 동적이고 상호작용 가능한 환경을 만드는 데 유망한 방향을 제시한다고 믿습니다.
