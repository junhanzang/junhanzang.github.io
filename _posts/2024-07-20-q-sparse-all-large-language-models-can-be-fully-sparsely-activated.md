---
title: "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated"
date: 2024-07-20 20:52:13
categories:
  - 인공지능
---

<https://arxiv.org/abs/2407.10969>

[Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](https://arxiv.org/abs/2407.10969)

## 개요

우리는 Q-Sparse라는 간단하면서도 효과적인 방법을 소개합니다. 이 방법은 대형 언어 모델(LLMs)을 희소하게 활성화하여 훈련시키는 방식입니다. Q-Sparse는 LLM의 활성화를 완전히 희소하게 만들어 추론 시 상당한 효율성을 가져올 수 있습니다. 이는 활성화에 대해 top-K 희소화를 적용하고, 훈련 시 스트레이트-스루 추정기를 사용함으로써 달성됩니다. 이 연구의 주요 결과는 다음과 같습니다:

1. Q-Sparse는 추론 시간에 훨씬 더 효율적이면서도 기준 LLM과 비교 가능한 성능을 달성할 수 있습니다.
2. 우리는 희소하게 활성화된 LLM을 위한 추론 최적화 스케일링 법칙을 제시합니다.
3. Q-Sparse는 처음부터 훈련, 기성 LLM의 계속 훈련, 미세 조정 등 다양한 설정에서 효과적입니다.
4. Q-Sparse는 풀-프리시전 LLM과 1비트 LLM(BitNet b1.58 [WMD+23] 등) 모두에 적용될 수 있습니다. 특히, BitNet b1.58과 Q-Sparse의 시너지(이 둘은 MoE와 함께 사용할 수 있음)는 미래의 LLM의 효율성, 비용, 에너지 소비를 혁신하는 데 중요한 기반과 명확한 길을 제공합니다.

![](/assets/images/posts/213/img.png)

### 그림 1

Q-Sparse는 밀집 모델보다 우수한 추론 최적화 스케일링 법칙을 달성합니다. 활성화의 top-K 희소화를 통해 행렬 곱셈의 연산을 상당히 절약합니다.

## 1. 완전히 희소하게 활성화된 LLMs

대형 언어 모델(LLMs)은 다양한 자연어 처리(NLP) 작업에서 놀라운 성과를 거두고 있습니다. 그러나 LLMs를 실제 응용 프로그램에 배포하는 것은 특히 추론 단계에서 높은 계산 비용과 메모리 점유로 인해 어려움을 겪고 있습니다. 이 문제를 해결하기 위해 최근 연구들은 양자화[MWM+24, WMD+23, FAHA23], 가지치기[XGZC23], 지식 증류[GDWH23], 더 나은 디코딩[LKM23] 등 다양한 접근법을 통해 LLMs의 효율성을 개선하는 데 초점을 맞추고 있습니다. 유망한 접근법 중 하나는 LLMs에서 활성화되는 매개변수 수를 줄이기 위해 희소성을 사용하는 것입니다.

희소성은 LLMs의 효율성에 두 가지 요소를 기여합니다. 첫째, 희소성은 0 요소가 계산되지 않으므로 행렬 곱셈의 계산량을 줄일 수 있습니다. 둘째, 희소성은 메모리와 계산 유닛 간에 매개변수를 전송하는 입출력(I/O) 양을 줄일 수 있습니다. I/O 전송은 LLMs의 추론 단계에서 주요 병목 현상으로 작용합니다. LLMs에서 희소성을 적용하는 일반적인 접근법은 가중치 희소성을 사용하는 것으로, 이는 모델 가중치를 가지치기하여 계산을 절약합니다. 그러나 비구조적 가중치 희소성은 GPU 장치에서 병렬 처리가 어려운 반면, 구조적 가중치 희소성은 모델의 정확성에 큰 영향을 미칩니다.

다른 접근법은 활성화 희소성을 사용하는 것으로, 이는 활성화 텐서에서 활성화된 요소 수를 줄입니다. 활성화 희소성은 전문가 집단(MoE) 메커니즘[LLX+21, FZS21]을 사용하거나, 활성화 함수를 수정[MAM+23, SXZ+24]하거나, 희소화할 위치를 예측[LWD+23]함으로써 달성할 수 있습니다. 그러나 이러한 접근법은 LLMs의 활성화를 완전히 희소화하지 못하여 추론 단계에서의 효율성 향상을 제한할 수 있습니다. 또한 밀집 모델에 비해 희소하게 활성화된 LLMs의 스케일링 법칙은 잘 연구되지 않았습니다.

LLMs에서 희소성의 잠재력을 최대한 탐구하기 위해, 우리는 LLMs에서 활성화의 완전한 희소화를 가능하게 하는 간단하면서도 효과적인 접근법인 Q-Sparse를 소개합니다. LLMs에 대한 주요 수정 사항은 선형 프로젝션(즉, 행렬 곱셈)입니다. 그림 1에서 보듯이, 각 선형 프로젝션에 대해 입력 텐서에서 상위 K 활성화를 선택하는 top-K 희소화 기능이 있습니다. 역전파에서는 활성화의 그래디언트를 계산하기 위해 스트레이트 스루 추정기를 사용합니다. 우리는 또한 피드 포워드 레이어에서 활성화 희소성을 더욱 향상시키기 위해 제곱 ReLU 함수를 도입합니다. Q-Sparse는 풀 프리시전 및 양자화된 LLMs 모두에 사용할 수 있습니다. 희소하게 활성화된 LLMs의 스케일링 법칙을 연구하기 위해 일련의 스케일링 실험을 수행하고, 희소하게 활성화된 LLMs에 대한 추론 최적화 스케일링 법칙을 도출합니다. 스케일링 실험의 결과와 스케일링 법칙의 함의는 다음과 같습니다:

- 희소하게 활성화된 모델의 성능은 동일한 추론 계산 예산(즉, 활성화된 매개변수 또는 FLOPs)으로 밀집 모델보다 우수합니다.
- 매개변수 N이 증가함에 따라 희소하게 활성화된 모델과 밀집 기준 모델 간의 성능 격차가 줄어듭니다.
- 약 40%의 희소성 비율을 가진 희소하게 활성화된 모델의 성능은 동일한 모델 크기와 훈련 토큰을 가진 밀집 기준 모델의 성능과 일치할 수 있습니다.
- 동일한 추론 예산 Na가 주어졌을 때, 45.58%의 희소성 비율(또는 1.84Na 매개변수)을 가진 희소하게 활성화된 풀 프리시전 모델은 최고의 성능을 달성할 수 있습니다. 1.58비트 모델의 경우 최적의 희소성 비율은 61.25%입니다.

우리는 또한 Q-Sparse의 효과를 다양한 설정에서 평가하기 위한 실험을 수행합니다. 여기에는 처음부터 훈련, 기성 LLM의 계속 훈련, 미세 조정이 포함됩니다. 우리는 Q-Sparse가 동일한 훈련 비용으로 기준 LLM과 비교 가능한 결과를 달성하면서도 추론 시간에 훨씬 더 효율적일 수 있음을 보여줍니다.

## 2. Q-Sparse

### 2.1 아키텍처

Q-Sparse 아키텍처는 활성화의 희소성을 가능하게 하기 위해 Transformer 아키텍처[VSP+17, TLI+23]를 기반으로 수정되었습니다.

![](/assets/images/posts/213/img_1.png)

![](/assets/images/posts/213/img_2.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/213/img_3.png)

![](/assets/images/posts/213/img_4.png)

![](/assets/images/posts/213/img_5.png)

![](/assets/images/posts/213/img_6.png)

![](/assets/images/posts/213/img_7.png)

![](/assets/images/posts/213/img_8.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/213/img_9.png)

![](/assets/images/posts/213/img_10.png)

### 그림 2

그림 2는 C4[RSR+19]의 유효 셋의 하위 집합에 대해 300M 모델 크기로 밀집 기준, STE가 적용된 Q-Sparse, STE가 적용되지 않은 Q-Sparse의 각 레이어에 걸친 투영 기울기의 평균 크기를 시각화한 것입니다. STE가 없을 경우 기울기가 소실되는 것을 보여줍니다.

![](/assets/images/posts/213/img_11.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/213/img_12.png)

![](/assets/images/posts/213/img_13.png)

### STE(스트레이트-스루 추정기) 예시

STE는 희소성 함수나 양자화 함수와 같은 비분리 가능 함수에서 기울기를 계산하기 위해 사용되는 방법입니다. STE는 역전파 시 활성화되지 않은 요소들의 기울기가 0이 되는 문제를 해결하여 기울기 소실 문제를 완화합니다.

![](/assets/images/posts/213/img_14.png)

![](/assets/images/posts/213/img_15.png)

![](/assets/images/posts/213/img_16.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 2.3 계속 훈련 및 미세 조정 설정에서 Q-Sparse

Q-Sparse는 처음부터 훈련, 계속 훈련, 미세 조정 등 다양한 설정에서 사용할 수 있습니다. 계속 훈련 및 미세 조정 설정에서는 처음부터 훈련하는 설정과 동일한 아키텍처와 훈련 절차를 사용합니다. 유일한 차이점은 모델을 사전 훈련된 가중치로 초기화하고 희소성 함수가 활성화된 상태로 훈련을 계속하는 것입니다.

피드포워드 레이어에 squared ReLU 함수가 없는 사전 훈련된 모델의 경우, 피드포워드 레이어에서 활성화 함수(예: SiLU) 후에 top-K 희소성 함수를 적용합니다. 이렇게 하면 모델 아키텍처를 변경하지 않고도 활성화의 희소성을 향상시킬 수 있습니다.

![](/assets/images/posts/213/img_17.png)

![](/assets/images/posts/213/img_18.png)

### 그림 3

그림 3은 고정된 희소성 비율 S에서 모델 크기에 따른 희소하게 활성화된 모델의 스케일링 곡선(왼쪽)과 고정된 모델 크기 N에서 희소성 비율에 따른 스케일링 곡선(오른쪽)을 보여줍니다.

### 3.1 스케일링 실험과 발견 사항

희소하게 활성화된 LLM의 스케일링 법칙의 형태를 결정하기 위해 일련의 스케일링 실험을 시작합니다. 실험에서는 다양한 규모(300M부터 7B까지)의 Q-Sparse 모델을 훈련시킵니다. 모델은 Redpajama 데이터셋[Com23]을 사용하여 훈련됩니다. 데이터 전처리에는 LLaMA의 Sentencepiece 토크나이저를 사용합니다. Q-Sparse 외에도 동일한 데이터셋과 설정으로 밀집 기준 모델을 훈련합니다. 더 자세한 내용은 부록 B에서 확인할 수 있습니다.

희소하게 활성화된 모델과 밀집 기준 모델의 관찰된 손실은 그림 3에 나타나 있습니다. 발견 사항을 요약하면 다음과 같습니다:

- 희소하게 활성화된 모델의 성능은 모델 크기와 희소성 비율에 따라 스케일링됩니다.
- 고정된 희소성 비율 S에서, 희소하게 활성화된 모델의 성능은 모델 크기 N에 대해 멱법칙 스케일링 법칙을 따릅니다.
- 고정된 파라미터 N에서, 희소하게 활성화된 모델의 성능은 희소성 비율 S에 대해 지수 법칙 스케일링 법칙을 따릅니다.
- 파라미터 N이 증가함에 따라 희소하게 활성화된 모델과 밀집 기준 모델 간의 성능 차이가 줄어듭니다.

이러한 발견 사항에 따라 우리의 주요 가설은 희소하게 활성화된 모델의 성능이 모델 크기 N에 대해 멱법칙 스케일링 법칙과 희소성 비율 S에 대해 지수 법칙 스케일링 법칙의 조합을 따른다는 것입니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/213/img_19.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/213/img_20.png)

![](/assets/images/posts/213/img_21.png)

![](/assets/images/posts/213/img_22.png)

### 그림 4

그림 4는 풀 프리시전(상단) 및 1.58비트(하단) 가중치의 희소하게 활성화된 모델의 추론 최적화 스케일링 곡선을 보여줍니다. 이는 풀 프리시전 모델의 경우 45.58%의 희소성, 1.58비트 모델의 경우 61.25%의 희소성이 동일한 추론 계산 예산(즉, 활성화된 매개변수 또는 FLOPs)으로 최고의 성능을 달성할 수 있음을 보여줍니다.

![](/assets/images/posts/213/img_23.png)

![](/assets/images/posts/213/img_24.png)

![](/assets/images/posts/213/img_25.png)

### 그림 5

그림 5는 풀 프리시전으로 Q-Sparse와 기준 모델의 훈련 손실 곡선을 보여줍니다. Q-Sparse의 경우 top-K를 70%로 채택하여 전체 희소성을 40%로 만듭니다.

![](/assets/images/posts/213/img_26.png)

### 그림 6

그림 6는 1.58비트 가중치로 Q-Sparse와 기준 모델의 훈련 손실 곡선을 보여줍니다. Q-Sparse의 경우 top-K를 70%로 채택하여 전체 희소성을 40%로 만듭니다.

### 4 실험

우리는 Q-Sparse의 다양한 설정에서의 효과를 평가하기 위해 실험을 수행합니다. 여기에는 처음부터 훈련, 기성 LLM의 계속 훈련, 및 미세 조정이 포함됩니다.

### 4.1 처음부터 훈련

#### 설정

우리는 Q-Sparse를 사용하여 풀 프리시전과 1.58비트에서 일련의 언어 모델을 훈련합니다. 모델들은 Redpajama 데이터셋[Com23]에서 500억 개의 토큰으로 훈련됩니다. 우리는 동일한 데이터셋과 설정으로 Q-Sparse를 밀집 기준 모델과 비교합니다.

#### 결과

희소하게 활성화된 모델과 밀집 기준 모델의 관찰된 손실은 그림 5에 나타나 있습니다. 이는 40%의 희소성 비율을 가진 Q-Sparse가 동일한 모델 크기와 훈련 토큰으로 밀집 기준 모델의 성능을 맞출 수 있음을 보여줍니다.

#### BitNet b1.58 + Q-Sparse

우리는 1비트 LLM에서 Q-Sparse의 효과를 추가로 평가합니다. 우리는 다양한 규모의 Q-Sparse를 가진 BitNet b1.58 모델을 훈련합니다. 우리는 Q-Sparse와 BitNet b1.58 기준 모델의 훈련 손실 곡선을 그립니다. 그림 6은 희소하게 활성화된 BitNet b1.58 모델의 성능이 동일한 추론 계산 예산을 가진 밀집 기준 모델보다 우수함을 보여줍니다. 이는 Q-Sparse가 1비트 LLM과 호환되며, 그 시너지가 추론 시 모델의 성능을 최적화하는 데 사용될 수 있음을 입증합니다.

![](/assets/images/posts/213/img_27.png)

### 그림 7

그림 7은 다양한 희소성 함수의 훈련 손실 곡선(왼쪽)과 전체 희소성 비율(오른쪽)을 보여줍니다. 모든 모델은 300M 크기와 500억 개의 토큰으로 훈련되었습니다.

![](/assets/images/posts/213/img_28.png)

#### 그림 8

그림 8은 각 모델 구성 요소의 다양한 희소성 함수에 따른 희소성 비율을 보여줍니다.

#### top-K 희소성과 STE의 소거 연구

top-K 희소성 함수의 효과를 평가하기 위해 우리는 top-K 희소성 함수와 ReLU 희소성 함수를 가진 희소하게 활성화된 모델의 성능을 비교합니다. 또한, STE의 효과를 평가하기 위해 STE가 있는 모델과 없는 모델을 비교합니다. 그림 7은 결과를 보여줍니다. 이는 STE를 제거하거나 ReLU 함수로 대체하면 성능이 크게 저하됨을 보여줍니다. 또한, ReLU 함수를 사용하는 모델의 희소성 비율은 훈련이 진행됨에 따라 감소하는 반면, top-K 희소성 함수를 사용하는 모델의 희소성 비율은 변하지 않습니다. 그림 8에서 보듯이, 우리는 다양한 구성 요소에서의 희소성 비율의 기여도를 분석하여, 감소하는 희소성이 주로 QKV 프로젝션, 게이팅 프로젝션 및 피드포워드 레이어의 업 프로젝션에서 비롯됨을 발견합니다. 이는 top-K 함수가 ReLU 함수보다 우수함을 증명합니다.

### 4.2 계속 훈련

#### 설정

우리는 Mistral 7B 모델[BBC+23]을 FineWeb-Edu 데이터셋[LBAvWW24]에서 400억 개의 토큰으로 계속 훈련합니다. 데이터 전처리에는 Mistral의 Sentencepiece 토크나이저를 사용합니다. 배치 크기는 400만 개의 토큰이고, 학습률은 5e-5입니다. Adam 옵티마이저를 사용하며, 가중치 감쇠(weight decay)는 0.01로 설정합니다. 더 자세한 훈련 정보는 부록 B에서 확인할 수 있습니다.

![](/assets/images/posts/213/img_29.png)

#### 표 1

표 1은 최종 작업에서 Q-Sparse와 기준 모델의 계속 훈련 결과를 보여줍니다.

![](/assets/images/posts/213/img_30.png)

### 표 2

표 2는 Wikitext2 테스트 세트에서 Q-Sparse와 기준 모델의 계속 훈련에 대한 활성화된 파라미터와 희소성 비율을 보여줍니다.

#### 결과

공정한 비교를 위해, 우리는 Mistral 7B 모델을 밀집 기준 모델과 동일한 레시피로 계속 훈련합니다. 우리는 Q-Sparse를 ReLUfication[MAM+23]과 dReLU 희소화[SXZ+24] 방법과 비교합니다. ReLUfication과 dReLU 희소화 방법은 활성화 함수를 변경하여 모델을 희소화합니다. 원래 논문[MAM+23]을 따라, 우리는 비-ReLU 활성화를 먼저 교체하고, 그 다음 ReLU 함수를 추가하는 두 단계의 훈련 전략을 채택합니다. dReLU 희소화 방법에 대해서는 원래 논문[SXZ+24]을 따라 dReLU 희소화 방법을 구현합니다. 우리는 ARC-Challenge[YBS19], HellaSwag[ZHB+19], Winogrande[SBBC20], MMLU[HBB+21], TruthfulQA[LHE22] 등 다양한 언어 작업에서 이러한 모델들을 평가합니다. 결과는 표 1에 나와 있습니다.

결과는 Q-Sparse가 추론 시간에서 훨씬 더 효율적이면서도 밀집 기준 모델과 비교할 만한 성능을 달성함을 보여줍니다. 또한, Q-Sparse는 성능과 희소성 비율 측면에서 ReLUfication과 dReLU 희소화 방법보다 우수합니다.

모델의 각 구성 요소의 희소성을 세분화하기 위해, 우리는 쿼리, 키, 값, 출력, 업, 다운, 게이트 텐서의 희소성 비율을 표 2에 제시합니다. 이는 Q-Sparse가 ReLUfication과 dReLU 희소화 방법보다 높은 희소성 비율을 달성함을 보여줍니다. 쿼리, 키, 값, 출력, 업, 다운 텐서의 희소성 비율은 40% 이상이고, 게이트 텐서의 희소성 비율은 60% 이상입니다. 이는 Q-Sparse가 LLM의 활성화를 완전히 희소화할 수 있음을 입증합니다.

![](/assets/images/posts/213/img_31.png)

### 표 3

표 3은 최종 작업에서 Q-Sparse와 밀집 기준 모델의 지도 학습 미세 조정 결과를 보여줍니다.

### 4.3 지도 학습을 통한 미세 조정

#### 설정

우리는 Mistral 7B[JSM+23]와 Qwen1.5 7B[BBC+23]의 기본 모델을 Open-Orca 데이터셋[LGP+23]에서 밀집 기준 모델과 Q-Sparse 모두에 대해 미세 조정합니다. 배치 크기는 128로 설정합니다. 학습률은 {3e-6, 5e-6, 7e-6} 중에서 선택합니다. 모든 모델은 공정한 비교를 위해 1 에포크 동안 훈련됩니다. 하이퍼파라미터에 대한 자세한 내용은 부록 B에 나와 있습니다. 우리는 ARC-Challenge[YBS19], HellaSwag[ZHB+19], Winogrande[SBBC20], MMLU[HBB+21], TruthfulQA[LHE22] 등 다양한 언어 작업에서 이러한 모델들을 평가합니다.

#### 결과

결과는 표 3에 나와 있습니다. Q-Sparse가 36억 개의 활성화된 파라미터로 Qwen1.5 4B 밀집 모델보다 훨씬 더 나은 성능을 달성함을 보여줍니다. 또한, Q-Sparse는 약 40억 개의 활성화된 파라미터로 Mistral 7B 모델 및 Qwen1.5 7B 모델과 비교할 만한 성능을 달성합니다. 이는 Q-Sparse가 밀집 사전 훈련된 모델을 거의 정확도 손실 없이 훨씬 더 효율적인 희소 모델로 미세 조정할 수 있음을 입증합니다.

### 5. 논의 및 미래 작업

#### BitNet b1.58 + Q-Sparse + YOCO의 확장

우리는 1비트 LLM(i.e., BitNet b1.58)과 완전한 희소 활성화(i.e., Q-Sparse)를 결합한 유망한 결과를 보여주었습니다. 우리는 모델 크기와 훈련 토큰의 양을 모두 확장하여 훈련을 진행하고 있습니다. 더 나아가, LLM 추론 시 KV 캐시 문제를 해결하기 위해 YOCO [SDZ+24]를 통합할 예정입니다. BitNet, Q-Sparse, YOCO의 통합은 LLM 추론 및 배포에서 모든 데이터 유형을 최적화하기 위한 포괄적인 접근 방식을 제공합니다. 여기에는 모델 가중치, 활성화, KV 캐시의 체계적인 최적화가 포함됩니다.

#### Q-Sparse + MoE

전문가 집합(Mixture-of-Experts)은 LLM에서 희소 활성화를 달성하기 위한 가장 널리 사용되는 방법입니다. Q-Sparse는 이와 독립적이며 MoE와 원활하게 통합될 수 있습니다.

#### 배치 모드에서의 Q-Sparse

현재 Q-Sparse 구현은 배치 훈련 및 추론에 적합하지 않습니다. 우리는 모델링 및 시스템 구현에서 혁신을 통해 Q-Sparse가 배치 모드와 호환되도록 작업하고 있습니다.

[2407.10969v1.pdf

0.70MB](./file/2407.10969v1.pdf)
