---
title: "NICE: Non-linear Independent Components Estimation"
date: 2024-08-15 16:08:57
categories:
  - 인공지능
---

<https://arxiv.org/abs/1410.8516>

[NICE: Non-linear Independent Components Estimation](https://arxiv.org/abs/1410.8516)

**요약**

우리는 복잡한 고차원 밀도를 모델링하기 위한 딥 러닝 프레임워크인 비선형 독립 성분 추정(NICE, Non-linear Independent Component Estimation)을 제안합니다. 이 방법은 데이터를 쉽게 모델링할 수 있는 분포로 만드는 좋은 표현을 찾는다는 아이디어에 기반합니다. 이를 위해, 데이터의 비선형 결정론적 변환을 학습하여 데이터를 잠재 공간(latent space)으로 매핑함으로써 변환된 데이터가 독립적인 잠재 변수를 가지도록 만들어진 인수 분포를 따르게 합니다. 우리는 이 변환을 파라미터화하여 야코비안 행렬과 그 역행렬의 행렬식을 계산하는 것이 매우 쉬우면서도, 간단한 구성 요소의 조합을 통해 복잡한 비선형 변환을 학습할 수 있는 능력을 유지합니다. 이러한 구성 요소 각각은 딥 뉴럴 네트워크에 기반합니다. 학습 기준은 정확한 로그 가능도이며, 이는 계산이 가능합니다. 또한 편향되지 않은 조상 샘플링(ancestral sampling)도 쉽습니다. 우리는 이 접근법이 네 가지 이미지 데이터셋에서 좋은 생성 모델을 생성하며, 인페인팅(inpainting)에도 사용할 수 있음을 보여줍니다.

**1. 서론**

비지도 학습에서 중요한 질문 중 하나는 구조가 알려지지 않은 복잡한 데이터 분포를 어떻게 포착할 수 있는가입니다. 딥 러닝 접근법(Bengio, 2009)은 데이터의 가장 중요한 변이 요소들을 포착할 수 있는 표현을 학습하는 데 의존합니다. 여기서 "좋은 표현이란 무엇인가?"라는 질문이 제기됩니다. 최근 연구(Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014)처럼, 우리는 데이터의 분포가 쉽게 모델링될 수 있는 표현이 좋은 표현이라는 관점을 취합니다. 이 논문에서는 학습자가 데이터를 새로운 공간으로 변환하는 함수를 찾아, 변환된 데이터의 분포가 인수화될 수 있도록 하는 특수한 경우를 고려합니다. 즉, 각 성분이 독립적인 분포를 따르게 됩니다:

![](/assets/images/posts/253/img.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

일반적인 미분을 그대로하면 된다.

y = f(p(x))라면 y' = f'(p(x))\*p'(x)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/253/img_1.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

야코비안 행렬

```
J = [∂f₁/∂x₁  ∂f₁/∂x₂  ...  ∂f₁/∂xₙ]
    [∂f₂/∂x₁  ∂f₂/∂x₂  ...  ∂f₂/∂xₙ]
    [  ...      ...    ...    ...  ]
    [∂fₘ/∂x₁  ∂fₘ/∂x₂  ...  ∂fₘ/∂xₙ]
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

이 논문의 핵심적인 혁신은 이러한 "야코비안 행렬의 행렬식을 쉽게 계산할 수 있는" 및 "역함수를 쉽게 구할 수 있는" 변환 f를 설계하는 것입니다. 이와 동시에, 복잡한 변환을 학습하기 위해 필요한 만큼의 용량(capacity)을 유지할 수 있습니다. 이 아이디어의 핵심은 x를 두 블록 (x1,x2)로 나누고, 다음 형태의 변환을 빌딩 블록으로 적용하는 것입니다:

![](/assets/images/posts/253/img_2.png)

여기서 m은 실험에서 ReLU MLP로 구현된 임의의 복잡한 함수입니다. 이 빌딩 블록은 어떤 m에 대해서도 야코비안 행렬식이 1이 되며, 다음과 같이 역변환도 쉽게 구할 수 있습니다:

![](/assets/images/posts/253/img_3.png)

이후에, 논문의 세부사항, 관련 논의, 그리고 실험 결과가 자세히 다루어질 것입니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

```
f(x) = max(0, x)

f(x) = {
  x,  if x > 0
  0,  if x ≤ 0
}
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/253/img_4.png)

**그림 1: 확률 모델의 계산 그래프**

아래의 수식을 사용하여 확률 모델의 계산 그래프가 설명됩니다.

![](/assets/images/posts/253/img_5.png)

**2. 연속 확률의 전단사 변환 학습**

![](/assets/images/posts/253/img_6.png)

![](/assets/images/posts/253/img_7.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

P\_H가 1 ~ D로 분해된거임

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

NICE는 데이터셋의 역함수가 존재하는 전처리 변환을 학습하는 것으로 볼 수 있습니다. 역함수가 존재하는 전처리는 데이터를 수축시킴으로써 가능성을 임의로 증가시킬 수 있습니다. 우리는 변수 변환 공식(Eq. 1)을 사용하여 이 현상을 정확하게 상쇄하고, 사전 분포 p\_H​의 인수화된 구조를 사용하여 모델이 데이터셋에서 의미 있는 구조를 발견하도록 유도합니다. 이 공식에서, 변환 f의 야코비안 행렬의 행렬식은 수축을 벌하고, 데이터 포인트와 같은 높은 밀도 영역에서 확장을 장려합니다. 이는 원하는 결과입니다. Bengio et al. (2013)에서 논의된 바와 같이, 표현 학습은 입력의 더 "흥미로운" 영역(예: 비지도 학습의 경우 높은 밀도 영역)과 관련된 표현 공간의 부피를 확장하는 경향이 있습니다.

![](/assets/images/posts/253/img_8.png)

![](/assets/images/posts/253/img_9.png)

**그림 2: 커플링 레이어의 계산 그래프**

**3. 아키텍처**

**3.1 삼각 구조**

![](/assets/images/posts/253/img_10.png)

우선 우리는 아핀 변환을 고려합니다. Rezende et al. (2014)과 Kingma와 Welling (2014)은 변환 행렬로 대각 행렬 또는 1차 보정이 포함된 대각 행렬을 사용할 때, 역함수와 행렬식에 대한 공식을 제공합니다. 행렬식 계산이 용이한 또 다른 행렬 집합은 삼각 행렬입니다. 이 행렬의 행렬식은 단순히 대각 요소들의 곱으로 계산됩니다. 테스트 시 삼각 행렬을 역변환하는 것은 계산 측면에서 합리적입니다. 많은 정사각 행렬 M은 상삼각 행렬과 하삼각 행렬의 곱으로 표현될 수 있습니다. 이러한 변환은 합성될 수 있기 때문에, 이러한 합성의 유용한 구성 요소는 야코비안 행렬이 대각선, 하삼각, 또는 상삼각인 경우를 포함합니다.

이 관찰을 활용하는 한 가지 방법은 삼각형 가중치 행렬과 전단사 활성화 함수(bijective activation functions)를 가진 신경망을 구축하는 것입니다. 그러나 이는 아키텍처에 큰 제약을 주어 깊이(depth)와 비선형성 선택에 제한을 줍니다. 대신, 우리는 야코비안 행렬이 삼각형인 함수 집합을 고려할 수 있습니다. 야코비안 행렬의 대각 요소들을 쉽게 계산할 수 있도록 하면, 야코비안 행렬식의 계산도 간단해집니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

대학교 2학년 선형대수학에서 나오는 문제와 동일

```
f(x) = Ax + b

[x']   [a  b] [x]   [tx]
[y'] = [c  d] [y] + [ty]

f(x) = Ax + b

아핀 변환의 특수 케이스: 대각 행렬을 사용한 아핀 변환의 예:

여기서 A = [2  0]
           [0  3]
      b = [1]
          [2]

따라서, f([x1]) = [2x1 + 1]
          [x2]    [3x2 + 2]
          
삼각 행렬의 행렬식: 상삼각 행렬의 예:

M = [3  1  4]
    [0  2  5]
    [0  0  6]

det(M) = 3 * 2 * 6 = 36

야코비안 행렬이 삼각형인 함수의 예:
f(x, y) = [x + y^2]
          [y     ]

야코비안 J = [1  2y]
             [0   1]

det(J) = 1 * 1 = 1

전단사 활성화 함수의 예 (LeakyReLU):
f(x) = {
  x,      if x > 0
  0.01x,  if x ≤ 0
}

삼각형 가중치 행렬을 가진 간단한 신경망 층:
W = [1  0  0]
    [2  3  0]
    [4  5  6]

y = W * x + b
```

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**3.2 커플링 레이어**

이 절에서는 삼각형 야코비안 행렬을 가지며, 따라서 야코비안 행렬식이 계산 가능한 전단사(bijective) 변환 집합을 설명합니다. 이는 변환 f의 빌딩 블록으로 사용될 것입니다.

일반 커플링 레이어

![](/assets/images/posts/253/img_11.png)

![](/assets/images/posts/253/img_12.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/253/img_13.png)

![](/assets/images/posts/253/img_14.png)

![](/assets/images/posts/253/img_15.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

가산적 커플링 레이어(Additive Coupling Layer)

![](/assets/images/posts/253/img_16.png)

**커플링 레이어의 결합**

우리는 여러 개의 커플링 레이어를 조합하여 더 복잡한 계층형 변환을 얻을 수 있습니다. 커플링 레이어는 입력의 일부만 변경하기 때문에, 각 층이 번갈아가며 두 부분 집합의 역할을 교환하여야 하며, 이를 통해 두 개의 커플링 레이어가 모든 차원을 수정하도록 합니다. 야코비안을 검토한 결과, 모든 차원이 서로 영향을 미치게 하려면 최소한 세 개의 커플링 레이어가 필요함을 알 수 있습니다. 우리는 일반적으로 네 개의 커플링 레이어를 사용합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/253/img_17.png)

결국 비선형으로 만든 분포를 야코비안으로 선형 분포로 만들고 이를 선형변환쓰겠다는 것

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

3.3 재스케일링 허용 (Allowing Rescaling)

![](/assets/images/posts/253/img_18.png)

![](/assets/images/posts/253/img_19.png)

3.4 사전 분포(Prior Distribution)

![](/assets/images/posts/253/img_20.png)

**4. 관련된 방법들**

생성 모델 분야에서는 상당한 발전이 이루어졌습니다. 예를 들어, 딥 볼츠만 머신(DBM)과 같은 비지도 그래프 모델은 효율적인 근사 추론 및 학습 기술 덕분에 매우 성공적이었으며, 많은 연구의 주제가 되었습니다. 그러나 이러한 모델은 훈련과 샘플링을 위해 마르코프 체인 몬테 카를로(MCMC) 샘플링 절차가 필요하며, 타겟 분포가 날카로운 모드를 가질 때 이 체인이 느리게 혼합되는 경향이 있습니다. 또한, 로그 가능도(log-likelihood)는 계산이 불가능하며, 가장 잘 알려진 추정 방법인 냉각 중요도 샘플링(AIS)은 과도하게 낙관적인 평가를 할 수 있습니다.

![](/assets/images/posts/253/img_21.png)

![](/assets/images/posts/253/img_22.png)

우리는 또한 변분 기준과 재파라미터화 기법을 결합함으로써, Kingma와 Welling(2014)이 사실상 두 개의 아핀 커플링 레이어를 가진 NICE 모델에서 (x,ϵ)쌍의 결합 로그 가능도를 최대화하고 있다는 것을 관찰합니다(여기서 ϵ은 보조 잡음 변수이며, 가우시안 사전 분포를 사용합니다). 자세한 내용은 부록 C를 참조하십시오.

확률 밀도 함수의 변수 변환 공식은 역변환 샘플링에서 주로 사용됩니다(이 절차는 여기서 샘플링에 사용된 방식과 유사합니다). 독립 성분 분석(ICA) (Hyvarinen and Oja, 2000)에서는, 특히 최대 우도 공식에서 데이터의 직교 변환을 학습하며, 이 과정에서 매개변수 업데이트 사이에 비용이 많이 드는 직교화 절차가 필요합니다. 더 풍부한 변환 계열을 학습하는 방법이 Bengio(1991)에서 제안되었으나, 제안된 변환 클래스인 신경망은 일반적으로 실용적인 추론과 최적화를 위한 구조가 부족했습니다. Chen과 Gopinath(2000)는 계층화된 변환을 가우시안 분포로 학습하지만, 이는 탐욕적인 방식으로 이루어져 샘플링 절차를 제공하지 못합니다.

Rippel과 Adams(2013)은 이러한 변환을 학습하는 아이디어를 다시 도입했지만, 전단사 제약이 없기 때문에 로그 가능도 최대화의 대리로 정규화된 오토인코더 설정에 의존해야 했습니다. 더 원칙적인 로그 가능도 대리로 변분 하한(variational lower bound)이 사용되었으며, 이는 비선형 독립 성분 분석(Hyvarinen and Pajunen, 1999)에서 앙상블 학습을 통해, 그리고 Helmholtz 머신의 변형을 사용한 연구들(Kingma와 Welling, 2014; Rezende et al., 2014)에서 보다 성공적으로 사용되었습니다.

생성적 적대 신경망(GAN) (Goodfellow et al., 2014) 역시 간단한 분포(예: 인수화된 분포)를 데이터 분포로 변환하기 위해 생성 모델을 훈련하지만, 반대 방향으로 가는 인코더가 필요하지는 않습니다. GAN은 샘플링의 어려움을 회피하기 위해, GAN 샘플과 실제 데이터 간의 차이를 구별하는 이차 심층 네트워크를 학습합니다. 이 분류기 네트워크는 GAN 생성 모델에 훈련 신호를 제공하여 출력이 훈련 데이터와 구별되지 않도록 만듭니다.

변분 오토인코더와 마찬가지로, NICE 모델도 추론의 어려움을 피하기 위해 인코더를 사용하지만, NICE의 인코딩은 결정론적입니다. 로그 가능도는 계산이 가능하며, 훈련 절차는 샘플링(데이터의 양자화 해제를 제외하고)을 요구하지 않습니다. NICE에서 계산 가능성을 얻기 위해 사용된 삼각 구조는 또 다른 계열의 계산 가능한 밀도 모델인 신경망 자기회귀 네트워크(neural autoregressive networks)에도 존재합니다. 최근의 성공적인 예로는 신경망 자기회귀 밀도 추정기(NADE) (Larochelle and Murray, 2011)가 있습니다. 사실, NADE 방향성 그래프 모델의 인접 행렬은 엄격히 삼각형 구조를 가지고 있습니다. 그러나 요소별 자기회귀 방식은 고차원 데이터(예: 이미지 데이터)에 대한 생성 작업에서 조상 샘플링 절차를 계산적으로 비효율적이고 병렬화할 수 없게 만듭니다. 하나의 커플링 레이어를 사용하는 NICE 모델은 두 개의 블록을 가진 NADE의 블록 버전으로 볼 수 있습니다.

**5. 실험**

**5.1 로그 가능도와 생성**

우리는 NICE 모델을 MNIST (LeCun and Cortes, 1998), Toronto Face Dataset 2 (TFD) (Susskind et al., 2010), Street View House Numbers 데이터셋 (SVHN) (Netzer et al., 2011) 및 CIFAR-10 (Krizhevsky, 2010)에서 훈련시켰습니다. Uria et al. (2013)에서 제안한 대로, 우리는 데이터의 양자화를 해제한(dequantized) 버전을 사용했습니다: 데이터에 1/256의 균일한 잡음을 추가하고, 이를 [0, 1]^D 범위로 재스케일링했습니다. CIFAR-10의 경우 1/128의 균일한 잡음을 추가하고, 데이터를 [−1, 1]^D 범위로 재스케일링했습니다.

사용된 아키텍처는 네 개의 커플링 레이어로 구성된 스택 구조이며, 마지막 단계에서 대각선 양의 스케일링(지수적으로 매개변수화된 exp(s))이 적용되었습니다. TFD에서는 근사적인 화이트닝을, SVHN과 CIFAR-10에서는 정확한 ZCA를 사용했습니다. 입력 공간은 홀수(I1)와 짝수(I2) 구성 요소로 분리하여 다음과 같은 방정식을 구성했습니다:

![](/assets/images/posts/253/img_23.png)

![](/assets/images/posts/253/img_24.png)

MNIST에서는 1980.50, TFD에서는 5514.71, SVHN에서는 11496.55, CIFAR-10에서는 5371.78의 테스트 로그 가능도를 얻었습니다. 이는 로그 가능도 측면에서 우리가 알고 있는 최고의 결과와 비교했을 때(TFD에서 5250, CIFAR-10에서 3622, Tang et al., 2012의 딥 믹스처 팩터 분석 모델 기준), 여전히 하한이지만, 우수한 성과를 보여줍니다. 연속형 MNIST의 생성 모델은 일반적으로 파르젠 윈도우(Parzen window) 추정을 사용하여 평가되므로 공정한 비교는 어려울 수 있습니다. 훈련된 모델로 생성된 샘플은 그림 5에 표시되어 있습니다.

![](/assets/images/posts/253/img_25.png)

**그림 3:** 아키텍처와 결과. "숨겨진 유닛의 수"는 각 은닉층당 유닛의 수를 나타냅니다.

![](/assets/images/posts/253/img_26.png)

**그림 4:** TFD와 CIFAR-10에서의 로그 가능도 결과. Deep MFA 숫자는 (Tang et al., 2012)에서 얻은 최고의 결과에 해당하지만, 이는 실제로 변분 하한(variational lower bound)입니다.

![](/assets/images/posts/253/img_27.png)

![](/assets/images/posts/253/img_28.png)

![](/assets/images/posts/253/img_29.png)

**그림 6:** MNIST에서의 인페인팅(Inpainting). 위 중앙 그림의 각 행에 대해, 이미지의 가려진 부분의 유형은 위에서 아래로 다음과 같습니다: 상단 행, 하단 행, 홀수 픽셀, 짝수 픽셀, 왼쪽, 오른쪽, 세로로 중앙, 가로로 중앙, 75% 무작위, 90% 무작위. 가려지지 않은 픽셀은 실제 값(ground truth)으로 고정하고, 로그 가능도에 대한 투영 그래디언트 상승(projection gradient ascent)을 통해 가려진 픽셀의 상태를 추정합니다. 가운데 마스크가 적용된 경우, 숫자에 대한 정보가 거의 없다는 점을 주목해야 합니다.

5.2 인페인팅

![](/assets/images/posts/253/img_30.png)

**6. 결론**

이 작업에서 우리는 훈련 데이터를 그 분포가 인수화된 공간으로 매핑하는 매우 비선형적인 전단사 변환을 학습하기 위한 새로운 유연한 아키텍처와 이 변환을 직접적으로 로그 가능도를 최대화하여 달성하는 프레임워크를 제시했습니다. NICE 모델은 효율적이고 편향되지 않은 조상 샘플링을 특징으로 하며, 로그 가능도 측면에서 경쟁력 있는 결과를 달성합니다.

우리 모델의 아키텍처는 토로이달 부분 공간 분석(TSA) (Cohen and Welling, 2014)과 같은 이점들을 활용할 수 있는 다른 귀납적 원칙들을 사용하여 훈련될 수도 있습니다.

또한, 우리는 변분 오토인코더와의 연결을 간단히 언급했으며, NICE가 이러한 모델에서 더 복잡한 근사 후방 분포 가족이나 더 풍부한 사전 분포 가족을 가능하게 하는 보다 강력한 근사 추론을 가능하게 할 수 있음을 지적했습니다.

**감사의 말**

우리는 Yann Dauphin, Vincent Dumoulin, Aaron Courville, Kyle Kastner, Dustin Webb, Li Yao, 그리고 Aaron Van den Oord에게 논의와 피드백을 제공해 준 것에 대해 감사드립니다. Vincent Dumoulin은 시각화를 위한 코드를 제공해 주었습니다. 또한, Theano (Bergstra et al., 2011; Bastien et al., 2012)와 Pylearn2 (Goodfellow et al., 2013)의 개발자들에게 감사드리며, Compute Canada와 Calcul Quebec에서 제공한 계산 자원과 NSERC, CIFAR, 그리고 캐나다 연구 의장(Canada Research Chairs)에서 제공한 연구 자금에 감사드립니다.

![](/assets/images/posts/253/img_31.png)

**(a) MNIST에서 훈련된 모델 (b) TFD에서 훈련된 모델**  
**그림 7: 잠재 공간에서의 구형 구조**  
이 그림들은 모델이 학습한 매니폴드(manifold) 구조의 일부를 보여줍니다.

### A. 추가 시각화

![](/assets/images/posts/253/img_32.png)

B. 근사 화이트닝

![](/assets/images/posts/253/img_33.png)

![](/assets/images/posts/253/img_34.png)

![](/assets/images/posts/253/img_35.png)

C. 변분 오토인코더(VAE)와 NICE

![](/assets/images/posts/253/img_36.png)

![](/assets/images/posts/253/img_37.png)

![](/assets/images/posts/253/img_38.png)

[1410.8516v6.pdf

1.64MB](./file/1410.8516v6.pdf)
