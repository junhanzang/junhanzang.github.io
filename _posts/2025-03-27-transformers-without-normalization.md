---
title: "Transformers without Normalization"
date: 2025-03-27 23:50:23
categories:
  - 인공지능
tags:
  - transformers without normalization
---

<https://arxiv.org/abs/2503.10622?_bhlid=1a87c33b8185a942533ee1886e23e7f6c2d5f90d>

[Transformers without Normalization

Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introdu

arxiv.org](https://arxiv.org/abs/2503.10622?_bhlid=1a87c33b8185a942533ee1886e23e7f6c2d5f90d)

정규화(Normalization) 층은 현대 신경망에서 폭넓게 사용되며 오랫동안 필수적인 요소로 여겨져 왔다. 본 연구는 정규화를 사용하지 않은 트랜스포머(Transformers)에서도 매우 간단한 기법을 통해 기존 모델과 동등하거나 더 우수한 성능을 얻을 수 있음을 보여준다. 이를 위해 본 논문은 동적 탄젠트 함수(Dynamic Tanh, DyT)를 도입하며, 이는 원소 단위(element-wise)로 적용되는 연산으로 정의된다:

![](/assets/images/posts/529/img.png)

DyT는 트랜스포머의 층 정규화(Layer normalization)가 종종 탄젠트 함수와 유사한 'S자 형태'의 입출력 매핑(input-output mapping)을 생성한다는 관찰에서 영감을 얻었다. DyT를 통해 정규화 층을 제거한 트랜스포머는 특별한 하이퍼파라미터 튜닝 없이도 기존 정규화를 적용한 모델과 동등하거나 더 뛰어난 성능을 달성할 수 있다. 본 논문은 이미지 인식에서 생성 모델까지, 지도 학습에서 자기 지도 학습까지, 그리고 컴퓨터 비전에서 자연어 처리 모델까지 다양한 분야에서 DyT를 적용한 트랜스포머의 효용성을 입증하였다. 이러한 결과는 정규화 층이 현대 신경망에서 필수불가결하다는 기존의 통념에 도전하며, 딥 네트워크에서 정규화 층이 갖는 역할에 관한 새로운 통찰을 제시한다.

<https://jiachenzhu.github.io/DyT/>

[Transformers without Normalization - DynamicTanh - DyT

jiachen [dot] zhu [at] nyu [dot] edu zhuangl [at] princeton [dot] edu -->

jiachenzhu.github.io](https://jiachenzhu.github.io/DyT/)

## 1 서론

지난 10년 동안 정규화(normalization) 층은 현대 신경망에서 가장 근본적인 구성 요소 중 하나로 자리매김했다. 이러한 흐름은 2015년 배치 정규화(Batch Normalization, BN)의 등장으로 거슬러 올라갈 수 있다(Ioffe와 Szegedy, 2015). 배치 정규화는 시각 인식 모델의 학습 속도와 수렴 성능을 현저하게 향상시키며 빠르게 인기를 얻었다. 이후로 다양한 네트워크 구조나 응용 분야에 맞춰 정규화 층의 변형들이 다수 제안되었다(Ba 외, 2016; Ulyanov 외, 2016; Wu와 He, 2018; Zhang과 Sennrich, 2019). 현재 거의 모든 현대적 신경망들이 정규화 층을 사용하고 있으며, 특히 트랜스포머(Transformer) 구조에서는 층 정규화(Layer Normalization, LN)가 가장 널리 쓰이고 있다(Vaswani 외, 2017; Dosovitskiy 외, 2020).

정규화 층이 폭넓게 사용되는 주된 이유는 최적화 측면에서의 실질적인 이점 때문이다(Santurkar 외, 2018; Bjorck 외, 2018). 정규화 층은 모델의 성능을 높일 뿐만 아니라, 수렴 속도를 가속화하고 안정화하는 데 도움을 준다. 신경망이 점점 더 넓고 깊어짐에 따라 이러한 특성은 더욱 중요해지고 있다(Brock 외, 2021a; Huang 외, 2023). 그 결과 정규화 층은 딥 네트워크 학습의 효과적인 훈련을 위한 필수적인 요소로 간주된다. 이는 최근의 신경망 설계에서 어텐션(attention)이나 컨볼루션(convolution) 층은 다른 방식으로 대체하려고 하면서도(Tolstikhin 외, 2021; Gu와 Dao, 2023; Sun 외, 2024; Feng 외, 2024), 정작 정규화 층만큼은 거의 항상 유지하고 있는 사실을 통해서도 잘 드러난다.

본 논문은 이와 같은 통념에 도전하며, 트랜스포머 모델 내의 정규화 층을 대체할 수 있는 간단한 대안을 제시한다. 본 연구는 LN 층이 입력을 출력으로 변환할 때 종종 탄젠트(tanh) 함수와 유사한 S자 형태의 곡선으로 매핑하며, 입력 활성값을 스케일링하고 극단적인 값들을 억제한다는 관찰에서 시작되었다. 이러한 통찰에 영감을 얻어, 본 논문은 다음과 같은 원소 단위(element-wise) 연산인 **Dynamic Tanh (DyT)**를 제안한다:

![](/assets/images/posts/529/img_1.png)

여기서 α는 학습 가능한 매개변수이다. DyT는 α를 통해 적절한 스케일링을 학습하고, 경계가 있는 tanh 함수를 사용하여 극단적인 값들을 억제함으로써 LN 층과 유사한 효과를 낸다. 특히 정규화 층과 달리 활성값의 통계량을 계산할 필요 없이 이러한 효과를 달성할 수 있다.

DyT를 사용하는 방법은 매우 간단하다(그림 1 참조). 우리는 비전 및 언어 분야의 트랜스포머를 포함한 기존 신경망 구조에서 정규화 층을 DyT로 바로 교체하여 적용한다. 다양한 환경에서 수행한 실험 결과, DyT를 적용한 모델은 안정적으로 학습되었으며 우수한 최종 성능을 보였다. 심지어 기존 모델의 하이퍼파라미터 튜닝을 별도로 수행하지 않은 상태에서도 좋은 성과를 얻었다. 본 연구의 결과는 현대적 신경망 학습에서 정규화 층이 필수불가결하다는 기존의 통념을 흔들며, 정규화 층의 특성에 대한 경험적 통찰을 제공한다. 또한 예비 실험 결과, DyT가 학습 및 추론 속도까지 향상시키는 것으로 나타나, 효율성을 추구하는 네트워크 설계에서도 잠재적인 후보로 자리잡을 가능성을 보였다.

![](/assets/images/posts/529/img_2.png)

**그림 1:** (왼쪽) 원본 트랜스포머 블록. (오른쪽) 본 연구에서 제안하는 DyT 층이 적용된 트랜스포머 블록. DyT는 일반적으로 널리 사용되는 Layer Norm (Ba 외, 2016) (경우에 따라 RMSNorm (Zhang과 Sennrich, 2019)) 층을 간단히 대체한다. DyT를 적용한 트랜스포머는 기존 정규화된 모델과 동등하거나 더 뛰어난 성능을 보인다.

## 2 배경: 정규화 층(Normalization Layers)

먼저, 정규화 층(normalization layers)에 대해 간략히 살펴본다. 대부분의 정규화 층은 공통된 수식을 공유한다. 입력 데이터 x의 형태(shape)가 (B,T,C)로 주어졌다고 하자. 여기서 B는 배치 크기(batch size), T는 토큰(token)의 개수, C는 각 토큰의 임베딩 차원(embedding dimension)을 나타낸다. 이때 정규화 층의 출력은 일반적으로 다음과 같은 형태로 계산된다.

![](/assets/images/posts/529/img_3.png)

여기서 ϵ은 매우 작은 상수(small constant)이며, γ와 β는 학습 가능한(learnable) 매개변수로서 형태가 (C,)인 벡터이다. γ는 스케일링(scaling), β는 시프팅(shifting)을 담당하는 아핀(affine) 매개변수로서, 이를 통해 출력값이 임의의 범위를 가지도록 조정된다. 여기서 μ와 σ^2는 입력의 평균(mean)과 분산(variance)을 의미하며, 각 정규화 방법에 따라 계산 방식이 달라진다. 따라서 이 통계량들의 차원 또한 각 방법마다 다르며, 실제 계산에서는 브로드캐스팅(broadcasting)을 통해 차원이 맞춰진다.

배치 정규화(Batch Normalization, BN)(Ioffe와 Szegedy, 2015)는 최초로 등장한 현대적 정규화 기법이며, 주로 컨볼루션 신경망(ConvNets) 모델에서 활용되었다(Szegedy 외, 2016; He 외, 2016; Xie 외, 2017). BN의 등장은 딥러닝 모델 설계에 있어서 중요한 이정표로 평가된다. BN은 배치(batch)와 토큰(token) 차원에 걸쳐 평균과 분산을 계산한다. 구체적으로 다음과 같다:

![](/assets/images/posts/529/img_4.png)

컨볼루션 신경망에서 인기가 높은 다른 정규화 기법으로는 그룹 정규화(Group Normalization)(Wu와 He, 2018)와 인스턴스 정규화(Instance Normalization)(Ulyanov 외, 2016) 등이 있으며, 이는 원래 물체 탐지(object detection)나 이미지 스타일 변환(image stylization)과 같은 특수한 작업을 위해 제안되었다. 이들 역시 기본적인 수식 형태는 동일하지만, 평균과 분산을 계산하는 축(axes)과 범위(ranges)가 서로 다르다.

트랜스포머(Transformer) 아키텍처에서 가장 널리 쓰이는 두 가지 정규화 기법은 층 정규화(Layer Normalization, LN)(Ba 외, 2016)와 루트 평균 제곱 정규화(Root Mean Square Normalization, RMSNorm)(Zhang과 Sennrich, 2019)이다. LN은 각 샘플(sample)의 각 토큰(token)에 대해 독립적으로 통계량을 계산한다. 계산 방식은 다음과 같다:

![](/assets/images/posts/529/img_5.png)

RMSNorm(Zhang과 Sennrich, 2019)는 LN을 단순화한 것으로, 입력값에서 평균을 빼주는(mean-centering) 단계를 제거하고, 대신 입력을 다음과 같이 정규화한다:

![](/assets/images/posts/529/img_6.png)

현재 대부분의 현대적 신경망은 단순성과 범용성 때문에 LN을 사용하고 있다. 최근 들어 RMSNorm 또한 널리 사용되고 있으며, 특히 T5(Raffel 외, 2020), LLaMA(Touvron 외, 2023a, b; Dubey 외, 2024), Mistral(Jiang 외, 2023), Qwen(Bai 외, 2023; Yang 외, 2024), InternLM(Zhang 외, 2024; Cai 외, 2024), DeepSeek(Liu 외, 2024; Guo 외, 2025)와 같은 대형 언어 모델에서 자주 쓰이고 있다. 본 논문에서 실험하는 대부분의 트랜스포머 모델은 LN을 사용하며, 예외적으로 LLaMA 모델만이 RMSNorm을 사용한다.

## 3 정규화 층은 어떤 역할을 하는가?

### 분석 환경

먼저 학습된 신경망 내에서 정규화 층(normalization layers)의 동작을 실험적으로 분석하였다. 분석을 위해 다음의 세 가지 모델을 사용하였다.

- **Vision Transformer (ViT-B)** (Dosovitskiy 외, 2020): ImageNet-1K 데이터셋(Deng 외, 2009)으로 학습된 모델.
- **wav2vec 2.0 Large Transformer** (Baevski 외, 2020): LibriSpeech 데이터셋(Panayotov 외, 2015)으로 학습된 음성 모델.
- **Diffusion Transformer (DiT-XL)** (Peebles와 Xie, 2023): ImageNet-1K에서 학습된 생성 모델.

모든 모델은 각 Transformer 블록 내부와 최종 선형(linear) 프로젝션 전에 LN(Layer Normalization)을 적용하였다.

세 모델 각각에서 미니 배치(mini-batch)를 샘플링하고 네트워크의 순방향 전달(forward pass)을 수행하였다. 그리고 각 정규화 층의 입력과 출력(즉, 학습 가능한 아핀 변환(affine transformation) 이전의 값)을 측정하였다. LN은 입력 텐서의 차원을 유지하므로 입력과 출력 요소 간에 일대일 대응을 설정하여 직접적인 관계를 시각화할 수 있다. 이를 통해 얻어진 매핑을 그림 2에 나타내었다.

![](/assets/images/posts/529/img_7.png)

![](/assets/images/posts/529/img_8.png)

![](/assets/images/posts/529/img_9.png)

**그림 2**: ViT(Dosovitskiy 외, 2020), wav2vec 2.0(Baevski 외, 2020), DiT(Peebles와 Xie, 2023) 모델의 특정 LN층에 대한 입력 대비 출력 값을 나타낸 그래프. 각 모델에서 네 개의 LN층을 선택하여 미니 배치의 입력-출력 값을 시각화하였다. 출력 값은 LN에서 아핀 변환 전의 값이다. 그림에서 나타난 S자 형태 곡선은 tanh 함수와 매우 유사하다(그림 3 참조). 초반 층에서 나타나는 선형 형태 역시 tanh 곡선의 중심부로 표현 가능하다. 이 결과를 바탕으로, 서로 다른 축적을 고려하기 위해 학습 가능한 매개변수 α를 가진 **Dynamic Tanh(DyT)**를 제안하였다.

![](/assets/images/posts/529/img_10.png)

**그림 3**: 서로 다른 세 가지 값의 α에 따른 tanh⁡(αx) 함수의 형태.

### 층 정규화에서 나타난 Tanh 형태의 매핑

그림 2의 모든 모델에서 초반 LN층(첫 번째 열)에서는 입력과 출력 간 관계가 대부분 선형적(linear)으로 나타나, x-y 플롯에서 직선에 가까운 모습을 보였다. 그러나 보다 심층부의 LN층에서는 더 흥미로운 현상이 나타났다.

깊은 층에서 특히 주목할 점은, 대부분의 곡선 형태가 완전하거나 부분적인 tanh 함수 형태(S자 형태)와 매우 유사하다는 것이다(그림 3 참조). LN층이 입력 텐서를 선형적으로 변환할 것으로 예상할 수도 있는데, 이는 평균을 빼고 표준편차로 나누는 연산 자체가 본질적으로 선형적이기 때문이다. LN은 각 토큰 단위로 정규화를 수행하며 각 토큰 활성값을 개별적으로만 선형 변환한다. 그러나 실제로는 각 토큰의 평균과 표준편차가 서로 다르기 때문에 전체 텐서의 활성값을 종합적으로 볼 때 선형성은 유지되지 않는다. 그런데도 불구하고, 실제 비선형 변환이 스케일링된 tanh 함수와 매우 유사하다는 것은 놀라운 결과이다.

이러한 S자 형태의 곡선은 중심부(x 값이 0 근처)에선 여전히 선형 형태를 보이며, 대부분의 점(약 99%)은 이 선형 범위에 존재한다. 하지만 ViT 모델에서 x가 50보다 크거나 -50보다 작은 값과 같은 극단적인 범위에 놓이는 점들이 상당수 존재한다. 정규화 층의 주된 역할은 이러한 극단적인 값을 다수의 점들과 유사한 덜 극단적인 값으로 압축(squash)하는 것이다. 이러한 압축 효과는 단순한 아핀 변환으로는 근사될 수 없으며, 우리는 이와 같은 비선형적이며 불균형적인 압축 효과가 정규화 층을 중요하고 필수불가결하게 만드는 이유일 것으로 가설을 세운다.

Ni 외(2024)의 최근 연구 또한 LN층이 가져오는 강력한 비선형성의 효과를 강조하며, 이러한 비선형성이 모델의 표현 능력(representational capacity)을 증대시킨다는 사실을 지적하였다. 또한 이런 압축 현상은 생물학적 뉴런이 큰 입력값에 대해 포화(saturation)되는 현상과 유사하며, 이는 약 100년 전 처음 관찰된 현상이기도 하다(Adrian, 1926; Adrian와 Zotterman, 1926a, b).

### 토큰 단위와 채널 단위로 살펴본 정규화

LN 층이 각 토큰에 대해 선형 변환을 수행하면서도 극단적 값은 어떻게 비선형적으로 압축하는지 이해하기 위해, 데이터를 토큰과 채널 별로 그룹화하여 시각화하였다. 이를 그림 2에서 ViT 모델의 두 번째와 세 번째 그래프를 다시 그리며 명확한 관찰을 위해 일부 점만 선택하여 그림 4에 나타내었다. 시각화 시, 극단적인 값을 가진 채널도 포함되도록 선정하였다.

![](/assets/images/posts/529/img_11.png)

**그림 4**: 두 LN 층에서 입력 대비 출력 값을 나타낸 그래프. 텐서 요소를 채널과 토큰 차원별로 다른 색깔로 표현하였다. 입력 텐서의 형태는 (샘플, 토큰, 채널)이며, 동일 토큰(왼쪽 두 패널)과 동일 채널(오른쪽 두 패널)마다 일정한 색상을 사용하여 시각화하였다.

- **왼쪽 두 패널:** 같은 토큰(같은 색)의 점들은 채널별로 직선을 형성한다. 이는 LN이 각 토큰의 채널 방향으로 선형 연산을 수행하기 때문이다. 그러나 모든 토큰을 함께 나타내면 이 직선들은 tanh 형태의 비선형 곡선을 형성한다.
- **오른쪽 두 패널:** 각 채널은 입력이 다른 범위에 분포하며, tanh 형태의 곡선에 서로 다른 부분을 기여한다. 일부 채널(예: 빨강, 초록, 핑크)은 극단적으로 큰 값을 가지며, 이러한 값들이 LN에 의해 강하게 압축된다.

그림 4의 왼쪽 두 패널에서 같은 색으로 나타낸 각 토큰의 활성값은 실제로 직선을 형성한다. 그러나 각 토큰의 분산이 다르기 때문에 직선의 기울기는 서로 다르다. 입력 범위가 작은 토큰들은 표준편차가 작으므로 LN은 이들의 활성값을 작은 값으로 나누게 되어 기울기가 커진다. 이렇게 모인 직선들이 전체적으로 tanh 형태의 S자 곡선을 형성한다. 오른쪽 두 패널에서는 각 채널마다 활성값의 입력 범위가 매우 다르며, 일부 소수의 채널만이 큰 극단값을 가지는 것이 확인된다. LN 층은 이 채널들의 극단적인 값을 가장 많이 압축한다.

## 4 Dynamic Tanh (DyT)

정규화 층의 입출력 형태가 스케일링된(scaled) tanh 함수와 유사하다는 점에서 영감을 얻어, 본 연구는 정규화 층의 대체로써 **Dynamic Tanh (DyT)**를 제안한다. 입력 텐서 x가 주어졌을 때 DyT 층은 다음과 같이 정의된다:

![](/assets/images/posts/529/img_12.png)

여기서 α는 입력값의 범위(scale)에 따라 입력을 동적으로 스케일링할 수 있도록 학습되는 스칼라 매개변수로, 그림 2에서 나타난 다양한 입력 범위를 처리하기 위해 도입된 것이다. 바로 이 때문에 본 연산을 "동적(Dynamic)" Tanh라 부르게 되었다. 매개변수 γ와 β는 기존의 모든 정규화 층에서 사용되는 것과 동일하게 채널 단위로 학습되는 벡터 매개변수이며, DyT의 출력을 다시 임의의 범위로 조정하는 역할을 한다. 이러한 매개변수는 종종 별도의 아핀(affine) 층으로 간주되기도 하지만, 본 논문에서는 이를 정규화 층에서처럼 DyT 층의 일부로 간주한다. DyT 층의 구현은 PyTorch 스타일의 의사 코드(pseudocode)로 알고리즘 1에 제시하였다.

```
# 입력 x의 형태는 [B, T, C] 
# B: 배치 크기, T: 토큰 수, C: 차원 수
class DyT(Module):
    def __init__(self, C, init_alpha):
        super().__init__()
        self.alpha = Parameter(ones(1) * init_alpha)
        self.gamma = Parameter(ones(C))
        self.beta = Parameter(zeros(C))

    def forward(self, x):
        x = tanh(self.alpha * x)
        return self.gamma * x + self.beta
```

**알고리즘 1:** DyT 층의 의사 코드(pseudocode).

DyT 층을 기존 아키텍처에 통합하는 방법은 매우 간단하며, 기존의 정규화 층을 DyT 층 하나로 교체하는 방식으로 이루어진다(그림 1 참조). 이 방법은 어텐션(attention) 블록 내의 정규화 층, FFN 블록, 그리고 최종 정규화 층 모두에 적용된다. DyT는 활성화 함수(activation function)처럼 보일 수도 있지만, 본 연구에서는 GELU나 ReLU와 같은 기존 아키텍처의 활성화 함수는 전혀 변경하지 않고 오직 정규화 층만 DyT로 대체하였다. 신경망의 다른 구성 요소 역시 변경하지 않았다. 또한, 기존 모델에서 사용한 하이퍼파라미터를 DyT에 맞추기 위해 튜닝할 필요가 거의 없다는 점을 관찰하였다.

### 스케일링 매개변수(Scaling Parameters)에 관하여

DyT의 매개변수 γ는 모든 값이 1인 벡터, β는 모든 값이 0인 벡터로, 기존 정규화 층의 초기화 방법과 동일하게 설정하였다. 스케일링을 위한 매개변수 α는 일반적으로 초기값 0.5로 설정하는 것이 적절하며, 예외적으로 대형 언어 모델(LLM)을 학습할 때만 다를 수 있다. α의 초기화에 관한 상세한 분석은 7장에서 제시된다. 이후 본 논문의 실험에서는 특별한 언급이 없는 한 α 값을 0.5로 초기화하였다.

### 추가 설명(Remarks)

DyT는 엄밀히 말해 새로운 유형의 정규화 층이 아니다. DyT는 정규화 층과 달리 통계량이나 그 어떤 형태의 집계(aggregation)도 계산하지 않으며, 순방향 전달(forward pass) 시 각 입력 요소(element)를 독립적으로 처리한다. 그럼에도 불구하고 DyT는 정규화 층의 효과를 보존한다. 즉, 극단적인 값은 비선형적으로 압축하고, 입력의 중심부(대부분의 값)는 거의 선형적으로 변환하는 특성을 유지한다.

## 5 실험 (Experiments)

본 절에서는 DyT의 효용성을 입증하기 위해 트랜스포머 모델을 비롯한 여러 최신 아키텍처를 다양한 작업과 도메인에 걸쳐 실험하였다. 각 실험에서는 원본 아키텍처에서 사용된 LN이나 RMSNorm 층을 DyT 층으로 교체하고, 공식 오픈 소스 프로토콜에 따라 두 버전의 모델을 학습 및 평가하였다. 결과 재현을 위한 상세한 지침은 부록 A에 제공하였다. 특히 DyT 적용의 간단함을 강조하기 위해 하이퍼파라미터는 원본 모델과 동일하게 유지하였다. 학습률(learning rate)과 α의 초기값 튜닝과 관련된 추가 실험 결과는 부록 B에 제시하였다.

### 비전 도메인의 지도학습 (Supervised learning in vision)

ImageNet-1K(Deng 외, 2009) 분류 작업에서 Vision Transformer(ViT)(Dosovitskiy 외, 2020)와 ConvNeXt(Liu 외, 2022) 모델을 "Base" 및 "Large" 크기로 학습하였다. 두 모델은 각각 attention(ViT)과 convolution(ConvNeXt)이라는 서로 다른 연산 방식을 사용하며 널리 쓰이는 모델이다. 실험 결과는 표 1에 나타냈다. DyT는 모든 아키텍처와 모델 크기에서 LN과 비슷하거나 약간 더 나은 성능을 보였다. 추가로 ViT-B와 ConvNeXt-B 모델의 훈련 손실(training loss) 그래프를 그림 5에 나타냈다. 두 그래프는 DyT와 LN 모델이 매우 유사한 수렴 경향을 보임을 나타낸다.

**표 1:** ImageNet-1K에서의 지도학습 분류 정확도 (Top-1 accuracy).

![](/assets/images/posts/529/img_13.png)

![](/assets/images/posts/529/img_14.png)

![](/assets/images/posts/529/img_15.png)

**그림 5:** ViT-B와 ConvNeXt-B 모델의 훈련 손실 곡선. 두 모델에서 LN과 DyT의 손실 곡선은 유사한 양상을 보이며, 학습 역학(dynamics)이 유사할 수 있음을 시사한다.

### 비전 도메인의 자기 지도 학습 (Self-supervised learning in vision)

비전 도메인의 대표적인 자기 지도 학습 방법인 Masked Autoencoder(MAE, He 외, 2022)와 DINO(Caron 외, 2021)를 사용하여 벤치마크하였다. 두 방법 모두 백본(backbone)으로 ViT를 사용하지만, MAE는 재구성(reconstruction) 손실을, DINO는 공동 임베딩(joint-embedding) 손실을 사용한다. 표준적인 자기 지도 학습 프로토콜에 따라 ImageNet-1K에서 라벨 없이 사전 학습(pretrain)한 후, 분류층을 붙여 라벨을 이용하여 미세조정(fine-tune)하였다. 미세조정 결과는 표 2에 나타냈다. DyT는 자기 지도 학습에서도 LN과 동등한 성능을 보였다.

**표 2:** ImageNet-1K 자기 지도 학습 정확도.

![](/assets/images/posts/529/img_16.png)

### 확산 모델 (Diffusion models)

Diffusion Transformer(DiT, Peebles와 Xie, 2023)를 ImageNet-1K에서 크기(B, L, XL)에 따라 학습하였다. DiT에서는 LN의 아핀 파라미터가 클래스 조건부 처리를 위해 사용되므로, DyT 실험에서도 이를 유지한 채 정규화 연산만 tanh⁡(αx) 함수로 대체하였다. 평가 지표는 표준 ImageNet 기준 배치를 이용한 Fréchet Inception Distance(FID)이며, 결과는 표 3에 나타냈다. DyT는 LN 대비 유사하거나 개선된 FID를 보였다.

**표 3:** ImageNet 이미지 생성 품질 평가(FID, 낮을수록 좋음).

![](/assets/images/posts/529/img_17.png)

### 대형 언어 모델 (Large Language Models)

LLaMA 모델(7B, 13B, 34B, 70B, Touvron 외, 2023a,b; Dubey 외, 2024)을 사전 학습하여 RMSNorm(Zhang과 Sennrich, 2019)와 DyT를 비교하였다. 모델은 The Pile 데이터셋에서 200B 토큰을 사용하여 학습하였다. DyT를 적용한 LLaMA는 초반 임베딩 층 뒤에 학습 가능한 스칼라 파라미터를 추가하고, α의 초기값을 조정하였다(7장 참고). 사전 학습 후 손실 값과 lm-eval(Gao 외)의 15개 zero-shot 평가 작업 평균 성능을 표 4에 제시하였다. DyT는 모든 크기에서 RMSNorm과 비슷한 성능을 달성했다. 손실 곡선은 그림 6에 나타냈으며, 모든 크기에서 유사한 학습 경향을 보였다.

**표 4:** LLaMA 모델의 사전 학습 손실 및 15개 zero-shot 작업 성능.

![](/assets/images/posts/529/img_18.png)

![](/assets/images/posts/529/img_19.png)

![](/assets/images/posts/529/img_20.png)

![](/assets/images/posts/529/img_21.png)

![](/assets/images/posts/529/img_22.png)

**그림 6**: LLaMA 사전 학습 손실. DyT 및 RMSNorm 모델의 손실 곡선은 모델 크기 전반에 걸쳐 밀접하게 정렬되어 있습니다.

### 음성 도메인 자기 지도 학습 (Self-supervised learning in speech)

wav2vec 2.0 Transformer 모델을 LibriSpeech 데이터셋에서 사전 학습하였다. 표 5에 나타낸 결과 DyT는 LN과 동등한 성능을 보였다.

**표 5:** LibriSpeech 음성 사전 학습 검증 손실(validation loss).

![](/assets/images/posts/529/img_23.png)

### DNA 서열 모델링 (DNA sequence modeling)

장거리 DNA 서열 모델링에서 HyenaDNA(Nguyen 외, 2024), Caduceus(Schiff 외, 2024)를 사용하였다. DyT는 표 6에서 LN과 동등한 성능을 보였다.

**표 6:** DNA 분류 정확도(GenomicBenchmarks 평균).

![](/assets/images/posts/529/img_24.png)

---

이 논문에서 **RMSNorm과 DyT의 비교**가 등장한 이유는 논문이 주로 **Transformer 계열의 대형 언어 모델(LLM)** 에서 성능을 평가했기 때문입니다.

설명하신 것처럼 Layer Normalization(LN)은 Transformer 구조에서 가장 흔히 사용되는 정규화 층인데, 최근 몇몇 인기 있는 대형 언어 모델들—예를 들어 **LLaMA** 시리즈—은 RMSNorm(Root Mean Square Normalization)을 기본 정규화로 채택하고 있습니다. 따라서 이 논문은 LLaMA 모델과 같은 최신 대형 언어 모델에서도 DyT가 RMSNorm만큼 우수하게 작동하는지를 확인하기 위해 RMSNorm과 비교 실험을 진행한 것입니다.
---

## 6 분석(Analysis)

본 절에서는 DyT의 중요한 특성에 대한 여러 분석을 수행하였다. 먼저 계산 효율성을 평가한 뒤, tanh 함수와 학습 가능한 스케일링 매개변수 α의 역할에 대한 두 가지 추가 연구를 진행하였다. 마지막으로, 기존에 정규화 층을 제거하려고 시도한 다른 방법들과의 비교를 제시하였다.

### 6.1 DyT의 효율성(Efficiency of DyT)

RMSNorm과 DyT가 적용된 LLaMA 7B 모델을 사용하여, 길이 4096 토큰의 단일 시퀀스에서 100회의 순방향 전달(추론, inference)과 100회의 순방향-역방향 전달(훈련, training)을 수행하는 데 걸린 전체 시간을 측정하였다. 표 7은 Nvidia H100 GPU에서 BF16 정밀도로 측정한 결과이며, RMSNorm 또는 DyT 층 각각과 전체 모델에 소요된 시간을 나타낸다. DyT 층은 RMSNorm 층과 비교하여 계산 시간을 상당히 감소시켰으며, 비슷한 경향이 FP32 정밀도에서도 관찰되었다. DyT는 효율성을 중심으로 설계된 네트워크에서 매우 유망한 선택지가 될 수 있다.

**표 7:** RMSNorm과 DyT가 적용된 LLaMA 7B 모델의 추론 및 훈련 시간(BF16 정밀도).

![](/assets/images/posts/529/img_25.png)

DyT는 추론과 훈련 모두에서 상당한 시간 단축 효과를 보였다.

### 6.2 tanh 함수 및 α 매개변수의 제거 및 교체 실험(Ablations of tanh and α\alphaα)

DyT에서 tanh 함수와 학습 가능한 매개변수 α의 역할을 더 깊이 이해하기 위해, 이 요소들을 변경하거나 제거했을 때의 모델 성능을 평가하는 실험을 수행하였다.

#### tanh 함수의 교체 및 제거 실험(Replacing and removing tanh)

DyT 층에서 tanh를 다른 비선형 압축(squashing) 함수로 교체하고(α는 유지), 그 결과를 평가하였다. 사용된 함수는 **hardtanh와 sigmoid**이며(그림 8 참조), 추가로 tanh를 완전히 제거한 경우(identity 함수 사용)의 영향도 평가하였다. 표 8에 나타난 것처럼, 비선형 압축 함수는 안정적 학습을 위한 필수적인 요소였다. identity 함수를 사용하면 학습이 불안정해지고 발산(divergence)이 발생했지만, 압축 함수들은 안정적인 학습을 가능하게 했다. 이 중 tanh가 가장 우수한 성능을 보였으며, 이는 tanh가 부드럽고(smoothness) 0을 중심으로 하는(zero-centered) 특성을 가지기 때문으로 보인다.

**표 8:** 다양한 압축(squashing) 함수 적용 시 ImageNet-1K 분류 정확도. 모든 실험은 원본 LN 모델과 동일한 학습 설정을 따름. 비선형 압축 함수는 모델 발산 방지에 결정적 역할을 하며, tanh가 가장 높은 성능을 달성했다.

![](/assets/images/posts/529/img_26.png)

"**→ 실패(failed)**"는 일정 수준까지 학습이 진행된 후 발산한 경우이며, 숫자는 발산 직전 도달한 최고 정확도를 나타낸다.

#### α 매개변수의 제거 실험(Removing α\alphaα)

다음으로, 비선형 압축 함수(tanh, hardtanh, sigmoid)는 유지하되, 학습 가능한 매개변수 α를 제거했을 때의 성능을 평가하였다. 표 9에서 보듯이, α를 제거하면 모든 압축 함수에서 성능이 감소하였다. 이는 학습 가능한 α가 모델 성능 향상에 중요한 역할을 한다는 점을 분명히 보여준다.

**표 9:** ViT-B 모델에서의 ImageNet-1K 분류 정확도. 모든 실험은 원본 LN 모델과 동일한 학습 설정을 따름. 학습 가능한 매개변수 α는 모델 성능 향상에 필수적이다.

![](/assets/images/posts/529/img_27.png)

α가 없는 경우 모든 함수에서 성능이 하락하였다.

이러한 분석을 통해 DyT는 효율성과 성능 면에서 기존 정규화 방법에 비해 우수하며, tanh 함수와 학습 가능한 매개변수 α가 모델의 안정성과 성능에 필수적이라는 것을 확인하였다.
---

**Identity 함수**는

![](/assets/images/posts/529/img_28.png)
---

## 6.3 학습 중 α 값 분석(Values of α\alphaα)

**학습 중(Training)**  
분석 결과, DyT의 학습 가능한 매개변수 α는 활성값(activations)의 표준편차의 역수(1/std)를 매우 밀접하게 추적하는 것으로 나타났다. 그림 8의 왼쪽 패널에서 보듯, α는 훈련 초기에는 감소하다가 이후 증가하는 모습을 보였으나, 항상 입력 활성값의 표준편차 변화와 유사한 패턴으로 함께 움직였다. 이러한 결과는 α가 활성값을 적절한 범위로 유지함으로써 안정적이고 효과적인 학습을 가능하게 하는 데 중요한 역할을 함을 시사한다.

**학습 후(After training)**  
학습된 신경망에서 최종적으로 얻어진 α 값을 추가 분석한 결과, 입력 활성값의 표준편차 역수(1/std)와 매우 강한 상관관계가 있음을 발견하였다. 그림 8의 오른쪽 패널에 나타난 것처럼, 일반적으로 높은 1/std 값은 큰 α 값과 관련되어 있었고, 그 반대도 마찬가지였다. 또한, 더 깊은 층으로 갈수록 활성값의 표준편차가 커지는 경향도 관찰되었다. 이러한 현상은 ConvNet에서 관찰된 깊은 잔차 네트워크(residual network)의 특징(Brock 외, 2021a)과 트랜스포머에서 관찰된 특성(Sun 외, 2025)과 일치한다.

이러한 두 가지 분석 결과는 α가 입력 활성값의 1/std 값을 근사적으로 학습하여, 부분적으로 정규화(normalization) 메커니즘처럼 동작함을 시사한다. LN(Layer Normalization)은 토큰별로 활성값을 정규화하는 반면, α는 전체 입력 활성값을 통합적으로 정규화한다. 따라서 α 혼자서는 극단적인 값을 비선형적으로 압축하는 효과를 수행하지는 못한다.

![](/assets/images/posts/529/img_29.png)

**그림 7:** 세 가지 비선형 압축(squashing) 함수(tanh, hardtanh, sigmoid)의 곡선. 모든 함수는 입력을 유한한(bounded) 범위로 압축하지만, DyT 층에 사용할 때 tanh가 가장 좋은 성능을 달성하였다. 이는 tanh 함수의 매끄러운(smoothness) 특성과 0을 중심으로 하는(zero-centered) 특성 때문일 가능성이 있다.

![](/assets/images/posts/529/img_30.png)

**그림 8:**  
(왼쪽 패널) ViT-B 모델에서 두 개의 DyT 층을 선택하여 각 에폭(epoch) 종료 시점의 α 값과 활성값의 표준편차 역수(1/std)를 추적한 결과, 훈련 중 두 값이 함께 변화함을 볼 수 있다.  
(오른쪽 패널) 학습이 끝난 두 모델(ViT-B, ConvNeXt-B)의 최종 α\alphaα 값을 입력 활성값의 1/std 값과 비교했을 때, 두 값 간의 강한 상관관계가 나타난다.
---

### **결론: α\alphaα** 가 하는 일 (쉽게 정리)

정리하자면, **α** 는 DyT가 정규화(normalization)를 수행하는 것을 도와주는 "자동으로 조절되는 스케일링 매개변수"입니다.

- 활성값이 지나치게 퍼져 있을 때(std↑) →α→α를 작게 하여 활성값을 다시 적당한 범위로 줄여줍니다.
- 활성값이 너무 좁게 집중되었을 때(std↓) →α→α를 키워 활성값의 범위를 적절하게 넓혀줍니다.

즉, α는 "자동으로 활성값 범위를 조절하여" 안정적인 학습을 돕는 중요한 역할을 합니다.

반면, LN은 각 토큰별로 평균과 표준편차를 사용하여 정규화를 수행하는데,  
**α**는 이러한 개별적 정규화 대신 **전체 활성값을 한번에** 대략적으로 정규화하는 방식으로 작동합니다.

단, **α**만으로는 극단적인 값을 효과적으로 압축하는 비선형적 역할을 할 수 없기 때문에, DyT에서는 반드시 **tanh와 같은 비선형 함수와 함께 사용** 되어야 합니다.
---

## 6.4 다른 정규화 제거 방법들과의 비교(Comparison with Other Methods)

DyT의 성능을 더 심층적으로 평가하기 위해, 정규화 층 없이도 트랜스포머를 학습할 수 있도록 제안된 기존 방법들과의 비교 실험을 수행하였다. 이러한 방법들은 크게 두 가지 유형으로 나눌 수 있다: 초기화 기반(initialization-based) 방법과 가중치 정규화(weight-normalization-based) 방법이다.

- **초기화 기반 방법(Initialization-based)**: Fixup(Zhang 외, 2019; Huang 외, 2020)과 SkipInit(De와 Smith, 2020; Bachlechner 외, 2021)을 사용하였다. 이 방법들은 학습 초기에 커다란 기울기(gradient)나 활성값이 발생하지 않도록 매개변수 초기값을 조정하여, 정규화 없이 안정적 학습을 가능하게 하는 것을 목표로 한다.
- **가중치 정규화 방법(Weight-normalization-based)**: σReparam(Zhai 외, 2023)을 실험하였다. 이 방법은 가중치의 스펙트럼 노름(spectral norm)을 제어하여 정규화 층이 없어도 안정적인 학습을 유지할 수 있도록 한다.

**표 10:** ImageNet-1K 분류 정확도. DyT는 일관되게 다른 방법들보다 우수한 성능을 달성하였다.

![](/assets/images/posts/529/img_31.png)

표 10은 두 가지 ViT 기반 작업의 결과를 요약하여 제시한다. 각각의 실험은 원본 논문에서 제시한 프로토콜을 최대한 충실히 따랐다. 다만 초기화 기반 방법인 Fixup과 SkipInit의 경우 학습 발산을 방지하기 위해 학습률(learning rate)을 크게 낮춰야 했으며, 공정한 비교를 위해 DyT를 포함한 모든 방법에 대해 간단한 학습률 탐색(search)을 수행하였다. 이로 인해 본 절의 결과는 하이퍼파라미터 튜닝 없이 수행한 5장의 결과와 다소 다를 수 있다. 종합적으로 보았을 때, DyT는 다양한 설정에서 일관되게 기존 방법들보다 더 우수한 성능을 나타냈다.

## 7. α의 초기화(Initialization of α)

본 연구에서는 α의 초기값(이를 α0로 표기함)을 튜닝하는 것이 일반적으로 성능을 크게 향상시키지 않음을 발견했다. 단, 예외적으로 LLM(대형 언어 모델)을 학습할 때만 α0를 신중히 조정하면 눈에 띄는 성능 개선이 있었다. 이 절에서는 α 초기화가 모델 성능에 미치는 영향에 대한 분석 결과를 자세히 설명한다.

## 7.1 Non-LLM 모델에서의 α 초기화

**Non-LLM 모델은 α 초기값(α0)에 비교적 민감하지 않다.**  
그림 9는 여러 작업에서 서로 다른 α0 값을 사용했을 때 검증 성능에 미치는 영향을 나타낸다. 모든 실험은 각 모델 원본 논문에 제시된 세팅과 하이퍼파라미터를 그대로 사용했다. 우리는 대체로 α0​ 값을 넓은 범위에서 변경하더라도 성능이 안정적으로 유지됨을 확인했다. 특히 0.5에서 1.2 사이의 값이 일반적으로 좋은 결과를 제공했다. 또한, α0를 조정해도 대부분 학습 초반부의 손실 곡선에만 영향을 주었다. 다만 한 가지 예외로 ViT-L 지도학습 모델에서 α0가 0.6을 넘으면 학습이 불안정해져 발산하는 현상이 나타났다. 이 경우 학습률을 낮추면 다시 안정적인 학습이 가능했다.

![](/assets/images/posts/529/img_32.png)

**그림 9:** 여러 작업에서 다양한 α0​ 값에 따른 성능. 5장에서 사용한 모든 Non-LLM 모델 작업을 다양한 초기값으로 평가했다. 대부분의 작업은 넓은 α0​ 범위에서 성능이 안정적이었다. 다만 ViT-L 지도학습(오른쪽 위 패널)에서만 α0>0.6일 때 발산이 발생했다.

![](/assets/images/posts/529/img_33.png)

**그림 10:** 다양한 α0 값, 학습률, 모델 크기에서의 안정성 분석. ImageNet-1K 데이터셋에서 지도학습 ViT 모델을 학습한 결과, 큰 모델일수록 LN 및 DyT 모델 모두 불안정해졌다. 이 경우 학습률을 낮추거나 α0를 줄이면 안정성이 개선되었다. 특히 DyT에서 α0=0.5로 설정하면 LN과 유사한 수준의 안정성을 보였다.

**작은 α0 값을 사용하면 학습 안정성이 높아진다.**  
추가적인 분석 결과, 모델 크기나 학습률을 증가시키면 안정적인 학습을 위해 α0를 줄여야 한다는 점을 발견했다. 반대로, 높은 α0를 사용하려면 학습률을 낮춰야 불안정성을 줄일 수 있었다. 그림 10은 ImageNet-1K 지도학습 ViT 모델의 안정성을 학습률, 모델 크기, α0​ 값에 따라 분석한 결과를 보여준다. 큰 모델일수록 불안정해져, 안정적 학습을 위해 작은 α0​ 값이나 낮은 학습률이 필요했다. LN 기반 모델에서도 비슷한 불안정성이 관찰되었으며, DyT에서 α0=0.5를 사용할 경우 LN과 매우 유사한 안정성을 보였다.

**기본값 α0=0.5 설정**  
이러한 분석을 바탕으로 Non-LLM 모델에서는 기본적으로 α0=0.5를 추천한다. 이 값은 기존 LN과 비슷한 수준의 안정성을 제공하면서 우수한 성능을 유지할 수 있었다.

## 7.2 LLM 모델에서의 α 초기화

**α 초기값(α0​) 튜닝은 LLM의 성능을 크게 향상시킨다.**  
앞서 설명한 대로 대부분의 작업에서 기본값 α0=0.5가 충분히 좋은 성능을 보였다. 그러나 LLM 모델에 한해 α0​ 값을 튜닝하면 성능이 크게 개선됨을 확인했다. 이를 위해 30B 토큰으로 LLaMA 모델을 사전학습하면서 여러 α0​ 값을 실험하고 학습 손실(training loss)을 비교했다. 표 11은 각 모델의 최적 α0​ 값을 요약한 것이다.

여기서 중요한 두 가지 발견이 있었다:

1. **모델 크기가 클수록 최적 α0​ 값은 작아진다.** 작은 모델에서 최적값을 먼저 찾으면, 더 큰 모델에서는 탐색 범위를 좁힐 수 있다.
2. **attention 블록에서 더 높은 α0​ 값을 사용할 때 성능이 개선된다.** 구체적으로 attention 블록의 DyT 층은 더 높은 값, 그 외의 층(FFN 블록 및 최종 projection 전 DyT 층)은 더 낮은 값으로 설정할 때 최적의 성능을 얻을 수 있었다.

**표 11:** LLaMA 모델별 최적의 α0​. 큰 모델일수록 작은 값이 필요하며, attention 블록과 나머지 블록(FFN 및 출력층 직전 DyT) 간 초기값을 다르게 설정하는 것이 중요하다.

![](/assets/images/posts/529/img_34.png)

α 초기화 튜닝의 영향을 보다 명확히 보여주기 위해, 그림 11은 두 개의 LLaMA 모델에서 다양한 α0​ 설정에 따른 손실(loss) 값을 히트맵으로 나타낸 것이다. 두 모델 모두 attention 블록에서 높은 α0 값을 사용할 때 학습 손실이 줄어드는 경향을 보였다.

![](/assets/images/posts/529/img_35.png)

![](/assets/images/posts/529/img_36.png)

**그림 11:** 서로 다른 α0 설정에 따른 30B 토큰에서의 손실 값 히트맵. 두 LLaMA 모델 모두 attention 블록에서 높은 α0​ 값이 성능 향상에 도움이 되었다.

## 모델의 너비(width)가 주로 α₀ 선택을 결정한다.

모델의 너비(width)와 깊이(depth)가 최적의 α 초기값(α₀)에 미치는 영향을 분석했다. 그 결과, **모델의 너비가 α₀ 값 선정에 결정적인 역할을 하며, 모델의 깊이는 거의 영향을 주지 않았다**는 점을 발견하였다. 표 12는 다양한 너비와 깊이 조합에서 얻은 최적 α₀ 값을 보여준다. 표에서 볼 수 있듯이, 모델의 너비가 증가할수록 최적 성능을 얻기 위한 α₀ 값이 점점 작아지는 경향이 나타났다. 반면, 모델의 깊이는 α₀ 값 선정에 있어 큰 차이를 보이지 않았다.

![](/assets/images/posts/529/img_37.png)

**표 12:** LLaMA 학습에서 모델 너비와 깊이에 따른 최적의 α₀ 값 (attention 블록 / 기타 블록). 모델 너비는 최적의 α₀ 값 선정에 상당한 영향을 미치며, 너비가 클수록 α₀ 값을 작게 설정해야 한다. 반면, 모델 깊이는 α₀ 선택에 거의 영향을 미치지 않는다.

표 12에서 볼 수 있듯이, 네트워크가 넓어질수록 **attention 블록**과 **다른 블록** 간의 초기값 차이를 더 크게 설정해야 한다. 우리는 대형 언어 모델(LLM)에서 α 초기화가 특별히 민감한 이유가, 다른 일반적인 모델들에 비해 LLM이 훨씬 더 큰 너비(width)를 가지기 때문이라고 가설을 세운다.
---

논문의 7장에서 **α 초기화**를 다룬 이유는 α가 학습은 가능하지만, 초기값을 어떻게 설정하느냐에 따라 학습 성능 및 안정성에 영향을 주기 때문입니다.

- Non-LLM(일반적인 Transformer 모델들)은 α 초기화에 비교적 덜 민감합니다.
- 반면, LLM과 같은 매우 큰 모델은 초기화에 민감하여 세심한 튜닝이 필요합니다.

즉, α 자체는 학습 가능하지만, 초기값을 설정하는 것이 중요하기 때문에 논문에서 별도의 장(7장)을 할애하여 분석한 것입니다.
---

## 8 관련 연구(Related Work)

### 정규화 층의 메커니즘 (Mechanisms of Normalization layers)

정규화(normalization) 층이 어떤 메커니즘을 통해 모델 성능을 향상시키는지에 관한 많은 연구가 이루어져 왔다. 지금까지 밝혀진 주요 메커니즘으로는 학습 과정 중 그래디언트 흐름을 안정화하는 것(Balduzzi 외, 2017; Daneshmand 외, 2020; Lubana 외, 2021), 가중치 초기화에 대한 민감도를 감소시키는 것(Zhang 외, 2019; De와 Smith, 2020; Shao 외, 2020), 특이값의 극단적인 값(outlier eigenvalues)을 완화하는 것(Bjorck 외, 2018; Karakida 외, 2019), 자동으로 학습률을 조정하는 것(Arora 외, 2018; Tanaka와 Kunin, 2021), 그리고 손실 곡면을 부드럽게 만들어 더 안정적인 최적화를 가능하게 하는 것(Santurkar 외, 2018) 등이 있다. 이러한 초기 연구들은 주로 배치 정규화(Batch Normalization)에 초점을 맞추었다. 최근 연구들(Lyu 외, 2022; Dai 외, 2024; Mueller 외, 2024)은 정규화 층이 손실 곡면의 날카로움을 감소시켜 모델의 일반화 성능을 개선한다는 점을 추가로 강조하고 있다.

### Transformer에서의 정규화(Normalization in Transformers)

Transformer(Vaswani 외, 2017)가 인기를 얻으면서 층 정규화(Layer Normalization)에 관한 연구가 활발히 이루어졌으며, 특히 자연어 처리와 같은 순차 데이터에서 우수한 성능이 입증되었다(Nguyen와 Salazar, 2019; Xu 외, 2019; Xiong 외, 2020). 최근 연구(Ni 외, 2024)는 층 정규화가 강력한 비선형성을 도입함으로써 모델의 표현 능력을 향상시킨다는 사실을 밝혔다. 또한 Loshchilov 외(2024), Li 외(2024) 등은 Transformer 내부에서 정규화 층의 위치를 조정하는 것만으로도 모델의 수렴 특성이 개선될 수 있음을 보였다.

### 정규화 층의 제거(Removing normalization)

정규화 층 없이 딥 모델을 학습하는 방법에 대한 연구도 활발히 이루어져 왔다. 여러 연구들(Zhang 외, 2019; De와 Smith, 2020; Bachlechner 외, 2021)은 가중치 초기화 방법을 조정하여 정규화 없이도 안정적인 학습이 가능하도록 하는 방법을 제안했다. 특히 Brock 외(2021a, b)는 다양한 초기화 기법(De와 Smith, 2020), 가중치 정규화(weight normalization)(Salimans와 Kingma, 2016; Huang 외, 2017; Qiao 외, 2019), 그리고 적응적 그래디언트 클리핑(adaptive gradient clipping)(Brock 외, 2021b)을 결합하여 높은 성능을 갖는 ResNet을 정규화 없이 학습할 수 있음을 최초로 보였다. 이외에도 이들의 학습 전략에는 폭넓은 데이터 증강(Cubuk 외, 2020) 및 규제(regularization)(Srivastava 외, 2014; Huang 외, 2016) 기법이 포함되어 있었다. 위 연구들은 주로 ConvNet 기반 모델을 대상으로 이루어졌다.

Transformer 구조에서는 He와 Hofmann(2023)이 Transformer 블록의 수정을 통해 정규화 층 및 skip-connection에 대한 의존성을 낮추는 방법을 연구했다. 또한 Heimersheim(2024)은 사전학습된 네트워크에서 점진적으로 층 정규화를 제거하고 추가적인 파인튜닝을 통해 성능을 유지하는 방법을 제안했다. 기존 연구들과 달리, 본 논문에서 제안한 DyT는 아키텍처와 학습 방식을 최소한으로 변경하면서도 안정적인 학습과 기존 모델에 필적하는 성능을 달성하였다.

## 9 한계점(Limitations)

본 연구는 Transformer와 최신 아키텍처에서 널리 사용되는 층 정규화(LN) 및 RMSNorm 기반 네트워크에 대해 실험을 진행하였다. 그러나 예비 실험(부록 C 참조) 결과, ResNet과 같은 고전적인 네트워크에서 배치 정규화(BN)를 DyT로 직접 대체하는 데 어려움이 있었다. DyT가 다른 종류의 정규화 층을 사용하는 모델에 어떻게 잘 적응할 수 있을지에 대해서는 추가적인 심층 연구가 필요하다.

## 10 결론(Conclusion)

본 논문에서는 현대 신경망, 특히 Transformer를 정규화 층 없이도 학습할 수 있음을 보였다. 이를 위해 기존 정규화 층을 간단히 대체할 수 있는 **Dynamic Tanh(DyT)** 기법을 제안하였다. DyT는 학습 가능한 스케일링 매개변수 α를 사용하여 입력 활성값의 범위를 조정하고, tanh 함수를 통해 극단적인 값을 압축한다. DyT는 간단한 함수이지만 기존 정규화 층의 동작을 효과적으로 재현하였다. 다양한 설정에서 실험한 결과, DyT를 사용한 모델은 정규화를 사용한 모델과 동일하거나 더 나은 성능을 보였다. 이 결과는 현대 딥러닝에서 정규화 층이 필수적이라는 기존의 통념에 도전한다. 또한 본 연구는 딥러닝 네트워크의 가장 근본적인 구성 요소 중 하나인 정규화 층의 메커니즘을 이해하는 데에도 기여한다.
