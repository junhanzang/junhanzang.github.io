---
title: "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation"
date: 2025-07-28 23:17:09
categories:
  - 인공지능
---

<https://arxiv.org/abs/2504.06225>

[Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation](https://arxiv.org/abs/2504.06225)

초록  
디코더 전용(Decoder-only) 대규모 언어 모델(LLM)은 인상적인 성능을 보여주고 있지만, 인코더-디코더(Encoder-Decoder) 모델은 추론 효율성과 풍부한 인코더 표현 덕분에 여전히 실제 응용에서 널리 사용되고 있습니다. 본 논문에서는 사전 학습된 디코더 전용 LLM을 인코더-디코더 구조로 **적응(adaptation)시키는** 새로운 문제를 다루며, 두 접근 방식의 장점을 결합해 품질과 효율성 간의 균형을 개선하는 것을 목표로 합니다. 우리는 이러한 적응이 디코더 전용 LLM의 능력을 계승할 수 있을 뿐만 아니라, 처음부터 사전 학습하는 것에 비해 계산 비용을 줄일 수 있다고 주장합니다. 이를 위해 다양한 사전 학습 목표와 매개변수 초기화·최적화 기법을 체계적으로 탐구했습니다.

Gemma 2 (2B 및 9B) 모델과 새롭게 사전 학습한 mT5 크기(최대 1.6B) 모델을 기반으로 한 광범위한 실험을 통해, **적응 방식의 효과와 인코더-디코더 LLM의 우위**를 입증합니다. 유사한 추론 예산 하에서 인코더-디코더 LLM은 디코더 전용 모델과 동등하거나 종종 더 나은 사전 학습 성능을 보이며, 미세 조정(finetuning) 성능에서는 크게 앞섭니다. 예를 들어, **Gemma 2B-2B 모델은 지시(instruction) 튜닝 후 Gemma 2B 대비 약 7% 높은 성능**을 기록합니다. 또한 인코더-디코더 적응은 다양한 크기의 모델을 유연하게 조합할 수 있게 하며, **Gemma 9B-2B 모델은 Gemma 2B-2B 대비 3% 이상 우수한 성능**을 보였습니다. 적응된 인코더 표현은 SuperGLUE에서도 더 나은 결과를 제공합니다. 우리는 향후 연구를 촉진하기 위해 학습된 체크포인트를 공개할 예정입니다.

### 1. 서론

신경망 아키텍처는 입력 데이터에 대한 특정 가정이나 **귀납적 편향(inductive bias)**을 포함하도록 설계되는 경우가 많으며, 이는 모델 성능 향상이나 계산 효율성 개선, 혹은 두 가지 모두로 이어집니다. 대규모 언어 모델(LLM)에 널리 사용되는 **디코더 전용(decoder-only) 아키텍처**(Brown et al., 2020)와 달리, **인코더-디코더(encoder-decoder) 아키텍처**는 입력 이해를 위한 인코더와 출력 생성을 위한 디코더라는 별도의 모듈을 채택합니다(Vaswani et al., 2017). 이러한 분리는 기능별 매개변수를 독립적으로 관리할 수 있게 하여 문맥 표현(contextual representation)과 복잡한 과제를 다루는 데 더 높은 자유도를 제공합니다(Tay et al., 2022; Wang et al., 2022). 또한 인코더와 디코더의 크기를 유연하게 조정(예: 큰 인코더와 작은 디코더 조합)하여 **품질-효율성(quality-efficiency) 간의 균형**을 제어할 수 있는데(Kasai et al., 2020; Zhang et al., 2022), 이는 LLM 배포에서 점점 더 중요한 요소가 되고 있습니다(Gemini et al., 2024). 그러나 이러한 장점에도 불구하고, **인코더-디코더 LLM**에 대한 연구는 최근 거의 주목받지 못하고 있습니다.

본 논문에서는 다음 질문을 탐구하며 이 고전적인 아키텍처를 다시 조명합니다.  
**“기존 사전 학습된 디코더 전용 LLM을 적응(adaptation)시켜 강력한 인코더-디코더 LLM을 만들 수 있는가?”**

사전 학습은 막대한 자원을 필요로 하지만, 다양한 크기의 강력한 디코더 전용 모델은 이미 널리 사용 가능하므로(Dubey et al., 2024; Team et al., 2024; Liu et al., 2024a; Yang et al., 2024; Jiang et al., 2024), 우리는 **새로운 모델을 처음부터 학습하기보다는 적응 방식이 더 실질적이라고 판단**합니다. 우리의 가설은, 디코더 전용 모델의 매개변수를 재사용함으로써 학습 속도를 가속화하고 내부 지식을 효과적으로 인코더-디코더 구조로 이전시켜 성능을 유지하거나 심지어 향상시킬 수 있다는 것입니다. 또한 적응 방식은 **서로 다른 크기의 디코더 전용 모델을 조합**해 특정 품질-효율성 요구를 충족할 수 있게 합니다. 그러나 **이러한 적응을 위한 최적의 방법과 성능 향상의 정도는 여전히 미해결 과제**이며, 본 논문에서는 이를 체계적으로 분석하고자 합니다.

![](/assets/images/posts/585/img.png)

#### 그림 1: 제안된 접근법 개요

우리는 사전 학습된 디코더 전용 모델을 기반으로 인코더-디코더 모델을 구성합니다. 모델의 아키텍처와 매개변수는 디코더 전용 모델에서 계승하되, **교차 어텐션(cross-attention)**에 대해서는 인코더 및 디코더 크기에 따라 다른 초기화 방법을 적용합니다. “ROPE”는 로터리 임베딩(rotary embedding), “FFN”은 피드포워드(feed-forward) 계층을 의미합니다.

우리는 Gemma 2 (Team et al., 2024)를 실험 환경으로 사용합니다. 그림 1에서 보이듯, 인코더-디코더 아키텍처는 기본적으로 원래 Transformer(Vaswani et al., 2017) 구조를 따르면서 Gemma 2의 수정 사항을 반영합니다. 핵심 아이디어는 **사전 학습된 디코더 전용 모델의 매개변수를 초기화 단계(warmup)로 활용하고, 이후 자기 지도 학습(self-supervised learning)으로 모든 매개변수를 사전 학습 혹은 적응시키는 것**입니다.

또한 인코더와 디코더의 설정이 동일한지 여부에 따라 **교차 어텐션 레이어의 초기화 및 최적화 전략**을 달리 제안합니다. 그리고 지식 증류(knowledge distillation, Hinton et al., 2015)를 결합한 prefix 언어 모델링(prefix LM)과 UL2(Tay et al., 2022) 등 다양한 사전 학습 목표를 비교합니다. Gemma 2 2B와 9B 모델 외에도, **다양한 규모의 소형 모델**을 사전 학습해 규모별 적응 효과를 분석했습니다.

모델 성능 평가를 위해, 사전 학습 모델과 지시 튜닝(instruction-tuning) 모델에 각각 적합한 **다양한 벤치마크**를 사용했습니다. 또한 **SuperGLUE(Wang et al., 2019a)**를 통해 학습된 문맥 표현의 품질을 측정했습니다.

#### 주요 발견 사항

- **사전 학습된 디코더 전용 LLM 활용은 강력한 인코더-디코더 LLM 구축에 효과적**이며, 특히 유사한 추론 FLOPs 환경에서 지시 튜닝 이후 다운스트림 성능이 크게 향상됩니다.
- **제안된 적응 방식은 매우 유연**하며, 예를 들어 **대형 인코더-소형 디코더(9B-2B)** 조합으로 Gemma 2 2B 대비 유사한 생성 지연(latency)에서 상당한 품질 향상을 달성할 수 있습니다.
- 적응 방식은 **계산 효율성이 높을 뿐만 아니라, 처음부터 사전 학습하는 것보다 효과적**입니다.
- **사전 학습 목표의 차이가 중요**합니다. Prefix LM + 지식 증류 모델은 **생성 과제에 유리**하고, UL2 모델은 **인코더 표현 품질이 더 우수**합니다.

### 2. 관련 연구 (Related Work)

디코더 전용(decoder-only) 아키텍처는 현재 LLM의 사실상 표준(de facto standard)으로 자리잡았지만, **인코더-디코더와 디코더 전용 모델링 간의 논쟁은 여전히 결론이 나지 않은 상태**입니다. 이전에 발표된 많은 연구들은 강력한 인코더-디코더 모델을 사전 학습하는 다양한 접근 방식을 제안해 왔습니다. 예를 들어, **MASS**(Song et al., 2019), **T5**(Raffel et al., 2020), **mT5**(Xue et al., 2021), **byT5**(Xue et al., 2022), **BART**(Lewis et al., 2020), **OpenBA**(Li et al., 2023) 등이 있습니다.

Tay et al. (2022)는 서로 다른 사전 학습 목표를 비교하며 **UL2와 인코더-디코더 모델링의 우수성**을 강조했습니다. Zhang et al. (2022)는 기계 번역 환경에서 두 아키텍처의 스케일링 동작(scaling behavior)을 체계적으로 분석하며, 적절한 학습 목표를 적용할 경우 두 모델이 유사한 성능을 낼 수 있음을 보였습니다. Wang et al. (2022)는 다양한 모델링 선택지와 사전 학습 목표를 심층 탐구하며, **LLM의 제로샷 일반화(Zero-Shot Generalization)**에 중점을 두었습니다. 이들은 특히 지시 튜닝(instruction tuning) 이후 인코더-디코더 LLM이 가장 뛰어난 성능을 달성한다는 점을 밝혀냈으며, 이는 본 연구의 실험 결과와도 일치합니다. 다만, 그들의 적응 연구는 **사전 학습 목표 간 적응**을 다룬 것이며, 본 연구처럼 **디코더 전용 LLM에서 인코더-디코더 LLM으로의 적응**은 아닙니다.

#### 사전 학습된 모델을 활용한 인코더-디코더 모델링

**BERT 시대(Devlin et al., 2019)**에는 사전 학습된 BERT를 활용하여 인코더-디코더 성능을 강화하는 다양한 연구가 진행되었습니다. 예를 들어 기계 번역(Zhu et al., 2020; Clinchant et al., 2019; Yang et al., 2020), 문법 오류 교정(Kaneko et al., 2020), 요약(Liu & Lapata, 2019), 텍스트 생성(Chen et al., 2019) 등 다운스트림 과제에 적용되었습니다. 우리 연구 역시 이러한 맥락을 따르지만, **사전 학습된 디코더 전용 LLM**을 기반으로 하며, **범용 인코더-디코더 LLM** 개발에 중점을 둡니다.

#### 추론 친화적(inference-friendly) LLM 연구

다른 관련 방향으로는 **추론 효율성을 개선하는 기술**들이 있습니다. 여기에는

- **양자화(quantization)** (Dettmers & Zettlemoyer, 2023)
- **Key-Value 캐시 최적화** (Corallo & Papotti, 2024)
- **순환 모델링(recurrent modeling)** (Gu & Dao, 2023; Botev et al., 2024)
- **강력한 소형 LLM 개발** (Abdin et al., 2024; Liu et al., 2024b)

등 다양한 기법이 포함됩니다. 이들 기법은 **효율성 향상**에 상당한 기여를 하지만, 본 논문에서 제안하는 **인코더-디코더 적응 방식과는 근본적으로 별개의 초점**을 가지며, **두 접근법을 결합해 더 큰 효율성을 실현할 수 있다는 점에서 상호 보완적**입니다.

### 3. 접근법: 인코더-디코더 적응 (Encoder-Decoder Adaptation)

![](/assets/images/posts/585/img_1.png)

표 1은 모델 구성 요소를 나타내며, **레이어 수(#Layers)**, **모델 차원/FFN 차원/헤드 수(d\_model/ffn/head)**, **쿼리·키-값 헤드 수(q/kv heads)**, 그리고 **모델 파라미터 수(#Params)**를 포함합니다. 인코더-디코더 모델의 경우 균형 잡힌 구조(예: 2B-2B)의 파라미터 수를 표기했습니다. 예를 들어, **9B-2B 모델은 10.4B 파라미터**를 가집니다.

### 3.1 아키텍처 (Architecture)

대규모 언어 모델(LLM)의 사전 학습은 **막대한 계산량과 시간**이 소요됩니다. 이를 줄이기 위해 본 연구는 **기존 디코더 전용 LLM을 인코더-디코더로 적응(adaptation)**시켜 사전 학습된 체크포인트를 초기화에 활용하는 방법을 제안합니다(그림 1 참조).

이로 인해, 우리는 **원래 디코더 전용 모델과 가능한 한 유사한 아키텍처를 유지**하며, 필요한 경우에만 변경을 도입합니다. 결과적으로 다음과 같은 구조를 갖습니다.

1. **인코더(Encoder)**는 디코더 전용 모델과 동일한 구조를 가지지만, **자기 어텐션(self-attention)**을 **인과적(causal)**에서 **양방향(bidirectional)**으로 전환합니다. 섹션 6의 소거 실험(ablations)에서 양방향 어텐션이 다운스트림 성능에 미치는 중요한 효과를 보여줍니다.
2. **디코더(Decoder)** 블록에서는 FFN과 자기 어텐션 부분은 디코더 전용 모델과 동일하며, **교차 어텐션(cross-attention)**은 자기 어텐션과 동일한 헤드 수와 헤드 차원을 가지지만 인코더의 전체 출력을 참조(attend)합니다.

본 연구는 **Gemma 2(Team et al., 2024)**를 기반으로 하지만, 제안된 접근법은 특정 아키텍처에 제한되지 않고 **LLaMA(Dubey et al., 2024), QWen(Yang et al., 2024), DeepSeek(Liu et al., 2024a)** 등 다른 모델 패밀리에도 적용 가능합니다. 이론적으로는 서로 다른 모델 패밀리를 조합하여 **예: LLaMA 인코더 + QWen 디코더**와 같은 형태도 가능합니다.

또한, 본 접근법은 **불균형 인코더-디코더(unbalanced encoder-decoder) 모델**도 지원하며, 이는 인코더가 디코더보다 훨씬 큰 경우입니다. 이러한 구성은 **입력 처리 능력이 생성 능력보다 중요한 응용 분야(예: 요약)**에서 유리합니다. 이 경우, 새로운 정보를 생성할 필요가 없으므로 **생성 시간(latency)**을 크게 줄이면서도 경쟁력 있는 품질을 유지할 수 있습니다.

### 3.2 초기화 (Initialization)

디코더 전용 체크포인트에서 인코더-디코더 모델을 초기화할 때, **각 레이어를 디코더 전용 체크포인트의 가장 유사한 가중치에 매핑**하려고 시도합니다.

- 인코더는 **새로운 가중치를 도입하지 않으므로 디코더 전용 체크포인트에서 완전히 초기화**됩니다.
- 디코더의 **FFN 및 자기 어텐션 부분**은 대응되는 디코더 전용 체크포인트의 가중치에서 초기화됩니다.
- **교차 어텐션(cross-attention)**의 경우:
  - **균형 구조(예: 2B-2B)**에서는 자기 어텐션 가중치로 초기화합니다.
  - 그렇지 않을 경우(불균형 구조)에는 **처음부터 교차 어텐션을 초기화한 후, 초기 K 스텝 동안 교차 어텐션만 미세 조정(finetune)하고 나머지 매개변수는 동결**합니다.
  - 이후 K 스텝이 지나면 모든 모델 매개변수를 함께 튜닝합니다.

### 3.3 사전 학습 목표 (Pretraining Objective)

디코더 전용 사전 학습은 일반적으로 **단일 시퀀스에 대한 인과적 언어 모델링(causal LM)**을 채택합니다. 반면, **인코더-디코더 적응**은 입력 시퀀스와 목표 시퀀스를 분리하여 각각 인코더와 디코더에 공급해야 합니다. 이를 위해 두 가지 고전적 사전 학습 목표를 탐구합니다.

1. **Prefix Language Modeling (PrefixLM)**
   - 인과적 언어 모델링과 유사하지만, **prefix 조건**을 추가한 방식입니다.
   - 전처리를 단순화하기 위해 시퀀스를 절반으로 나누어 **전반부는 입력**, **후반부는 목표(target)**로 사용합니다.
   - 또한, 이 방식은 디코더 전용 모델로부터의 **지식 증류(knowledge distillation)**를 쉽게 적용할 수 있습니다.
2. **UL2 (Tay et al., 2022; Wang et al., 2022)**
   - 더 복잡한 방식으로, **여러 수준의 난이도를 가진 복원(denoising) 작업**으로 구성됩니다.
   - 데이터 준비는 Tay et al. (2022)를 따릅니다.

이 두 가지 사전 학습 목표의 성능은 실험에서 비교 평가합니다.

---

# ? Transformer Attention 구조 비교

? 디코더 전용 (GPT, LLaMA)

입력: "안녕하세요"

↓

Self-Attention 1

↓

Self-Attention 2

↓

Self-Attention N

↓

출력: "안녕하세요!"

? Attention 흐름

각 토큰이 다른 모든 토큰을 참조

"안녕" ↔ "하세요" 관계 학습

? 인코더-디코더 (T5, BART)

입력: "Hello"

↓

인코더 Self-Attention

↓

Cross-Attention

↓

디코더 Self-Attention

↓

출력: "안녕하세요"

? Attention 흐름

인코더: "Hello" 내부 관계

디코더 → 인코더 참조

디코더: "안녕하세요" 내부 관계

### ? 핵심 포인트

**❌ 잘못된 생각:**  
"디코더 전용에는 attention이 없다"

**✅ 올바른 이해:**  
"디코더 전용에는 Self-Attention만 있다"

**? 기억할 점:**  
• **모든 Transformer = Attention 기반**  
• 디코더 전용: Self-Attention만 사용  
• 인코더-디코더: Self-Attention + Cross-Attention  
• GPT도 Self-Attention으로 문맥을 이해합니다!

## 1. 공통점

- **모두 Query, Key, Value로 연산하는 Attention 메커니즘**을 사용
- 수학적으로는 동일한 연산 (scaled dot-product attention)
- 결국 토큰 간 관계를 계산하는 구조라는 점에서는 같습니다

---
