---
title: "Antidistillation Sampling"
date: 2025-04-24 17:09:01
categories:
  - 인공지능
tags:
  - antidistillation sampling
---

<https://arxiv.org/abs/2504.13146>

[Antidistillation Sampling](https://arxiv.org/abs/2504.13146)

**초록**  
고도화된 프런티어 모델(frontier models)은 확장된 추론 과정을 생성하는 과정에서, 모델 증류(distillation)에 유용하게 사용될 수 있는 풍부한 토큰 시퀀스를 무심코 만들어낸다. 이러한 취약성을 인식한 모델 소유자는, 모델 성능을 저해하지 않으면서도 증류의 효율을 제한할 수 있는 샘플링 전략을 찾게 된다. **Antidistillation sampling(안티증류 샘플링)**은 바로 이러한 기능을 제공한다. 이 기법은 모델의 다음 토큰 확률 분포를 전략적으로 수정함으로써, 추론 과정을 ‘오염’시켜 증류에 덜 효과적으로 만들면서도, 모델의 실제 활용성은 그대로 유지한다.

### 1. 서론

확장된 추론 과정을 생성하도록 학습된 대형 언어 모델(Large Language Models, LLMs)은 수학, 코딩, 일반 추론 벤치마크 전반에 걸쳐 인상적인 성능을 보여준다 [예: 1]. 그러나 이러한 생성된 추론 과정(trace)은 이중적인 역할을 수행한다. 모델의 성능을 향상시키는 동시에, 보조 모델이 이러한 추론 과정을 학습 데이터로 활용하여 원 모델의 능력을 모방할 수 있도록 하는 **모델 증류(model distillation)**를 가능하게 한다 [2, 3]. 특히, 모델 증류는 유사한 성능의 모델을 처음부터 훈련시키는 데 드는 연산 비용의 일부만으로도 보조 모델의 성능을 크게 향상시킬 수 있다는 점에서 주목할 만하다 [3].

하지만 모델 증류의 장점에도 불구하고, 이 기술의 효율성과 효과성은 프런티어 추론 모델(frontier reasoning model)을 제공하는 기업에 여러 가지 단점을 초래한다. 첫째, 확장된 추론 과정을 반환하는 것은 기업의 핵심 지식재산(intellectual property)을 포기하는 행위로, 경쟁사들이 프런티어 기술을 저렴하게 복제할 수 있도록 만든다. 둘째, 증류의 가능성은 프런티어 모델 제공자에게 토큰 확률을 숨기거나, 추론 과정을 요약하거나, 전반적으로 사용자-모델 간 상호작용을 제한하도록 유도한다. 마지막으로, 안전한 모델 행동(예: 탈옥 시도에 대한 저항 [4, 5])은 증류된 모델에는 종종 제대로 계승되지 않아, 유해한 콘텐츠를 생성할 위험이 있다 [6].

이러한 문제들을 해결하기 위해, 우리는 **안티증류 샘플링(Antidistillation Sampling)**이라는 기법을 제안한다. 이 기법의 핵심 아이디어는 다음과 같다: 추론 모델의 샘플링 분포를 조정하여, 생성된 추론 과정이 (1) 증류 시도를 '오염시키고(poison)', (2) 원래의 조정되지 않은 확률 분포상에서 높은 확률을 유지하도록 만든다. 이 방법은 고유한 모델 능력을 보호하면서도, 모델이 다양한 실제 응용에 유용하게 사용될 수 있도록 성능을 유지한다.

![](/assets/images/posts/546/img.png)

**그림 1 설명:**  
안티증류 샘플링을 통해 생성된 추론 과정은 증류 시도를 방해하는 동시에, 원래 교사 모델(teacher)의 다운스트림 성능은 유지한다. 위쪽과 아래쪽 행은 각각 MATH [7]와 GSM8K [8] 데이터셋에 대한 결과를 보여준다. 왼쪽 열은 다양한 샘플링 방식에 따른 교사 모델의 정확도를, 오른쪽 열은 해당 방식으로 생성된 추론 과정으로 증류된 학생 모델의 성능을 나타낸다. 주목할 점은, 두 데이터셋 모두에서 동일한 교사 모델 성능을 유지한 상태에서, 안티증류 샘플링은 일반적인 temperature 샘플링에 비해 증류된 모델의 성능을 현저히 저하시킨다는 것이다.

### 1.1 관련 연구

여러 프런티어 AI 연구소들은 모델 증류(model distillation)의 효과를 인정하고 이를 적극 활용하고 있다. 예를 들어, OpenAI는 자사의 API 내에서 모델 증류를 하나의 서비스로 제공하고 있다 [9]. 이러한 증류 파이프라인의 동기를 부여한 모델 증류의 가능성에 대한 인식은 적어도 Schmidhuber의 연구까지 거슬러 올라간다 [10]. 보다 최근에는 Hinton 외 [2]가 증류된 ‘전문 특화 모델(specialist models)’이 다양한 분야에서 인상적인 성능을 달성할 수 있음을 보여주었다. 이후로도 많은 연구들이 증류를 통해 어떻게 모델 능력이 이전되는지를 이해하고자 하였다 [11, 12, 13, 14]. 실제로 일부 연구소에서는 상업용 LLM을 훈련하는 데 있어 증류를 부분적으로 활용하고 있으며, 이때 경쟁사 모델로부터 생성된 확장된 추론 과정을 수집해 사용하는 것으로 추정되기도 한다 [15]. 이러한 관행은 프런티어 모델 운영자들에게 전략적 취약점이 될 수 있으며, 본 논문에서 제안하는 알고리즘의 중요성을 강조하는 근거가 된다.

본 논문이 다루는 위협 모델은 **학생 모델(student model)**이 **교사 모델(teacher model)**이 생성한 데이터로 훈련되는 시나리오이며, 이는 모델 프라이버시 및 보안과 관련된 다양한 주제와 맞닿아 있다. 예컨대, 모델 추출 공격(model extraction attack)은 별도의 훈련이나 증류 없이 쿼리 기반 접근을 통해 모델 가중치를 추정하며 [16], 학습 데이터 추출 공격(training data extraction attack)은 프런티어 모델로부터 직접 학습 데이터를 수집한다 [17]. 안티증류 샘플링이 이러한 공격으로부터 일정 수준의 방어 효과를 제공할 수는 있지만, 이는 본 논문의 주요 범위를 벗어난다. 보다 관련 있는 분야는 데이터 중독(data poisoning)에 관한 문헌이다. 이 분야에서는 악의적으로 조작된 데이터를 모델 학습 과정에 주입해 특정한 원치 않는 결과를 유도한다(예: [18]). Rando와 Tramèr [19]는 선호도 데이터(preference data)에 백도어를 삽입함으로써 RLHF(RL with Human Feedback)로 파인튜닝된 모델을 손상시킬 수 있음을 보여주기도 했다. 본 연구는 데이터 중독 및 프라이버시 보호 기법을 결합하여, 프런티어 모델에 내재된 귀중한 지식을 보호하는 데 목적을 둔다.

끝으로, 안티증류 샘플링은 LLM의 **제어된 디코딩(controlled decoding)**이라는 보다 넓은 틀 내에서 위치지어진다 [20]. 이 분야에서는 부가적인 목적 함수를 통해 디코딩 과정을 조정한다. 기존 연구들로는 생성 품질 향상을 위한 대비 목적 함수(contrastive objectives) 활용 [21], 제약 디코딩을 최적화 문제로 재정식화 [22], 에너지 기반 제약 조건(energy-based constraints)의 도입 [23] 등이 있다. 이러한 접근들과 유사해 보일 수 있으나, 안티증류 샘플링은 서로 다른 문제를 해결하고자 한다. 즉, 디코딩 목적 함수에 증류 인식(distillation-aware) 패널티 항을 도입함으로써, 생성되는 추론 과정을 ‘오염시켜’ 해당 결과를 기반으로 파인튜닝된 모델의 성능을 저하시킨다.

---

### ✅ **핵심 주장 1: 모델 증류는 매우 강력하고 실제로도 널리 사용되고 있다**

- OpenAI를 포함한 주요 연구소들이 모델 증류를 실무에 활용 중이며,
- Hinton 등의 연구를 통해 "전문 특화 모델"로도 증류가 잘 된다는 것이 입증됨.
- 일부 기업은 경쟁사의 reasoning trace를 수집하여 증류에 활용할 가능성도 있음.

? **요점:** 모델 증류는 실제로 매우 효과적이고, 경쟁사의 능력을 복제하는 수단으로 악용될 수 있음.

### ✅ **핵심 주장 2: reasoning trace 기반 증류는 보안적·전략적 위협이 될 수 있다**

- reasoning trace를 반환하는 것 자체가 **지식재산(IP)의 유출**로 이어지고,
- 증류된 모델은 **안전성(safety behavior)을 제대로 계승하지 못함** → 탈옥(jailbreaking) 위험 존재.
- 따라서 모델 제공자는 아예 reasoning trace나 token 확률을 숨기는 등의 제한을 둘 유인이 있음.

? **요점:** reasoning trace 공개는 기술적 유출과 보안 취약점이라는 이중 리스크를 유발함.

### ✅ **핵심 주장 3: 안티증류 샘플링은 이 위협을 방어할 수 있는 새로운 방식이다**

- 기존 연구에서 다룬 데이터 중독(data poisoning), 백도어 삽입 같은 보안 기법과 맥락을 같이하며,
- 동시에 LLM의 controlled decoding 분야에 속하는 새로운 **생성 조절 기법**임.
- 핵심은 “디코딩 과정에서 증류를 어렵게 만드는 목적 함수를 넣는 것”임.

? **요점:** 안티증류 샘플링은 기존 보안/프라이버시 기법과는 다른 방식으로 모델 능력을 보호함.

---
