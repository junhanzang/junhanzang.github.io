---
title: "Deep Reinforcement Learning Driven Aggregate Flow Entries Eviction in Software-defined Networking"
date: 2025-02-06 16:08:04
categories:
  - 인공지능
---

초록 — 소프트웨어 정의 네트워킹(SDN)은 제어 기능을 네트워크 요소로부터 분리하여 SDN 컨트롤러에 논리적으로 중앙집중화함으로써 네트워크에 대한 전반적인 시야와 제어를 제공한다. 스위치와 같은 네트워크 요소들은 컨트롤러가 설치한 플로우 테이블의 항목을 사용하여 데이터를 전달한다. 플로우 테이블의 용량은 제한적이므로 지속적인 관리가 필요하다. 여러 연구에서는 플로우 테이블에 새로운 항목을 위한 공간을 마련하기 위해 퇴거(eviction) 전략을 제안했으나, 이들 연구는 항목과 유입 플로우 간의 1:1 매핑을 전제로 한다. 이는 실제 네트워크에서 하나의 집계 플로우 항목(Aggregate Flow Entry, AFE)이 여러 유입 플로우를 처리할 수 있다는 점에서 큰 한계로 작용한다. 본 논문은 이러한 한계를 극복하기 위해 AFE 퇴거를 위한 딥 강화 학습(Deep Reinforcement Learning, DRL) 프레임워크를 제안한다. 제안된 프레임워크는 AFE가 처리하는 플로우 수(즉, AFE의 정도) 및 기타 매개변수를 계산하여 플로우 테이블 오버플로우를 최소화하는 것을 주 목적으로 퇴거할 AFE를 선택한다. 실험 결과, 제안된 프레임워크는 무작위(Random) 및 최근에 가장 적게 사용된 알고리즘(Least Recently Used, LRU)과 비교하여 오버플로우 수를 45%, 플로우 재설치를 150%, 제어 신호 오버헤드를 58% 각각 감소시킴을 보였다.

키워드 — 소프트웨어 정의 네트워킹, OpenFlow, 집계 플로우 항목, 딥 강화 학습, 딥 Q-러닝

I. 서론  
소프트웨어 정의 네트워킹(SDN)은 SDN 컨트롤러에서 모듈식 중앙집중식 네트워크 제어를 제공함으로써 기업 및 모바일 네트워크의 소프트웨어화(softwarization)를 선도하고 있다 [1]. 데이터 평면의 소프트웨어 스위치와 같은 네트워크 요소들은 데이터 전달 기능만을 수행하며, OpenFlow [2]와 같은 사우스바운드 프로토콜을 사용하여 컨트롤러에 의해 제어된다. 스위치는 플로우를 효율적으로 라우팅하기 위해 컨트롤러가 설치한 플로우 항목들을 포함하는 용량이 제한된 플로우 테이블로 구성된다. 일반적으로 스위치는 빠른 조회 속도를 제공하는 삼진 콘텐츠 주소 지정 메모리(TCAM) [3]를 사용한다. 그러나 TCAM의 용량은 제한적이며 높은 비용 때문에 쉽게 확장할 수 없다. [4]에서 보도된 바와 같이, 상용 스위치의 TCAM 용량은 2000개의 항목 정도인 반면, 데이터센터의 평균 플로우 도착률은 초당 10,000 플로우에 달한다 [5]. 따라서 스위치의 효율적인 기능 유지를 위해 플로우 테이블에 대해 다른 관리 작업이 요구된다.

플로우 테이블 관리 작업의 핵심 중 하나는 플로우 테이블 오버플로우를 최소화하는 것이다. 새로운 유입 플로우에 속하는 패킷이 스위치에 도착하면, 해당 패킷의 헤더가 플로우 테이블의 기존 항목들과 대조된다. 만약 일치하는 항목이 없다면, 스위치는 해당 패킷을 컨트롤러로 전달하며, 이를 테이블 미스(table miss)라고 한다. 컨트롤러는 패킷을 확인하고 다음 홉을 결정한 후, 새로운 플로우에 적합한 플로우 항목을 플로우 테이블에 설치한다. 만약 플로우 테이블이 이미 최대 용량에 도달한 경우, 새로운 플로우 항목을 추가할 수 없으며, 이를 오버플로우라고 한다. 플로우 테이블 오버플로우는 심각한 네트워크 성능 저하를 야기할 뿐만 아니라, 플로우 테이블을 막아 네트워크 공격에 악용될 수 있다 [6]. 오버플로우로 인한 네트워크 성능 저하를 방지하기 위해서는 새로운 플로우를 위한 공간을 마련하기 위해 비활성 또는 중요도가 낮은 플로우 항목들을 제거하는 효율적인 퇴거(eviction) 메커니즘이 필요하다.

OpenFlow 프로토콜은 idle timeout과 hard timeout 기반의 퇴거 메커니즘을 제공한다. idle timeout은 항목과 매칭되는 패킷이 없을 경우 일정 기간 후 해당 항목을 퇴거시키며, hard timeout은 항목이 일정 시간 생존한 후 퇴거시키는 기간을 정의한다. idle timeout과 hard timeout을 사용할 경우, 활성 상태인 항목이 퇴거되거나 일정 시간 비활성 상태임에도 불구하고 항목이 유지되는 가능성이 존재한다. 이러한 한계로 인해 idle timeout과 hard timeout은 플로우 테이블 오버플로우를 완벽하게 방지하지 못하므로, 새로운 유입 플로우를 위한 공간을 마련하기 위한 별도의 퇴거 정책이 요구된다.

오버플로우 상황에서는 선입선출(FIFO)와 최근에 가장 적게 사용된(Least Recently Used, LRU) 알고리즘이 기본적인 퇴거 정책으로 사용된다. 여러 연구에서는 플로우의 특성을 분석하여 개선된 퇴거 정책을 정의하고자 하였다. 한 연구에서는 elephant 플로우와 mice 플로우를 구분하고, elephant 플로우보다 mice 플로우와 관련된 항목을 퇴거시키는 것이 더 효율적이라고 주장한다 [7]. 다른 연구에서는 TCP 헤더 플래그를 활용하여 TCP 플로우 항목을 적시에 퇴거시키는 방법을 제시하였다 [8,9].

최근에는 지도학습(supervised)과 비지도학습(unsupervised) 알고리즘이 퇴거 메커니즘 설계에 사용되고 있다. 머신러닝(ML) 알고리즘인 랜덤 포레스트(Random Forest)는 비활성 플로우 항목을 분류하여 퇴거하는 데 사용된다 [10]. 또 다른 접근 방식으로, 플로우 항목의 매칭 빈도와 최신성을 Deep Reinforcement Learning (DRL)의 매개변수로 활용하여 스위치와 컨트롤러 간의 제어 트래픽을 줄이는 방법이 제안되었다 [11]. 또한, 강화학습(RL)은 [12]에서 UDP 플로우와 관련된 항목의 모니터링 지연을 적응적으로 조정하여, 항목이 비활성 상태가 되는 즉시 퇴거시키는 데 사용되었다. 앞서 언급한 모든 연구들은 유입 플로우와 플로우 항목 간의 1:1 매핑을 전제로 하고 있다. 그러나 실제 네트워크에서는 이 가정이 성립하지 않으며, 그림 1에서 보듯이 하나의 플로우 항목에 여러 플로우가 연결될 수 있고, 패킷 전달은 주로 목적지 주소나 VLAN/MPLS 태그를 기반으로 이루어진다. 따라서 여러 플로우가 하나의 집계 플로우 항목(Aggregate Flow Entry, AFE)과 매칭될 수 있는 상황을 위한 퇴거 메커니즘이 필요하다.

본 논문은 기존 연구의 한계를 극복하기 위해 플로우 테이블에서 AFE를 위한 DRL 기반 퇴거 전략(Eviction Strategy for AFEs, ESAFE)을 제안한다. SDN 컨트롤러에 구현된 ESAFE DRL 프레임워크는 오버플로우 이벤트 발생 시 플로우 테이블 내 모든 AFE의 특성 벡터(feature vector)를 수집하고, 전체적인 오버플로우를 최소화하는 목표 하에 퇴거할 AFE를 선택한다. 만약 선택된 AFE의 퇴거가 다음 오버플로우 이벤트를 지연시킨다면 ESAFE DRL 프레임워크는 보상을 받고, 그렇지 않으면 패널티를 받게 된다. ESAFE의 성능은 AFE에 대한 무작위(Random) 및 LRU 정책과 비교되었으며, 그 결과 ESAFE가 오버플로우 및 퇴거된 AFE의 재설치 횟수를 줄여 SDN 컨트롤러와의 제어 통신 오버헤드를 감소시키는 데 있어 우수함이 입증되었다.

![](/assets/images/posts/503/img.png)

그림 1. 유입 플로우와 1:1 매핑된 항목과 집계 플로우 항목 간의 차이

II. 딥 강화 학습 배경

강화 학습(RL)은 비지도 학습 알고리즘에 속하며, RL 에이전트는 환경에서 시행착오를 통해 학습한다. 환경은 대상 시스템을 나타내며, 에이전트는 환경의 상태를 수집하여 이를 모니터링한다. 에이전트는 환경의 상태를 학습한 후, 알고리즘 형태로 주어진 정책에 따라 적절한 행동을 선택한다. 그 행동은 환경에 적용되어 다음 상태로 전이된다. 만약 그 다음 상태가 정책의 목표를 향해 환경을 전이시킨다면, 에이전트는 보상을 받고, 그렇지 않으면 벌점을 받는다. 시간이 지나면서 에이전트는 상태 수집과 행동 선택의 반복 과정을 통해 가장 높은 보상을 주는 행동을 선택하는 법을 학습하게 된다.

Q-러닝 [13]은 가장 널리 사용되는 RL의 대표적인 알고리즘이다. Q-러닝은 가능한 모든 상태와 행동 쌍에 대해 Q-테이블을 사용하며, Q-함수와 보상 값을 이용해 Q-테이블의 값을 계산한다. 해당 상태에 대해 Q-테이블에서 가장 높은 값을 가진 행동이 선택되며, 각 상태 전이 후 Q-테이블의 값이 갱신된다.

Q-러닝의 한계는 Q-테이블의 유한한 용량에 있다. 환경의 상태와 이에 해당하는 행동이 사실상 무한한 경우, Q-러닝은 비효율적이다. DRL 에이전트는 Q-테이블을 딥 신경망(DNN)으로 대체함으로써 이러한 한계를 극복하며, 이 방법은 Deep Q-Network (DQN) [14]로 불린다. DQN의 Q-네트워크와 Q-타겟 DNN은 이전 상태, 행동, 보상을 기반으로 학습하여 현재 환경 상태에 적합한 행동을 제공한다. 이를 통해 DQN은 연속적인 상태를 갖는 대규모 환경에서도 적절한 행동을 산출할 수 있다. Q-러닝과 유사하게, DQN 역시 보상 값과 Q-함수를 이용한 탐욕적 접근을 통해 정책 값을 탐색하고 갱신하는 오프-폴리시 알고리즘이다.

Q-러닝과 DQN의 차이점은 그림 2에 상세히 제시되어 있다.

![](/assets/images/posts/503/img_1.png)

그림 2. RL과 DRL 간의 차이를 설명한 그림.

III. 집계 플로우 항목에 대한 퇴거 전략  
SDN에서 플로우는 다섯 가지 필드, 즉 소스 IP, 목적지 IP, 소스 포트, 목적지 포트, 그리고 전송 프로토콜에 의해 정의된다. 이들 중 하나라도 값이 변경되면 새로운 플로우로 간주된다. 만약 플로우 테이블의 항목이 이 다섯 가지 필드를 모두 포함한다면, 해당 항목은 동일한 값들을 가진 단 하나의 플로우와만 매칭되며, 이는 플로우와 항목 간의 1:1 매칭을 의미한다.

반대로 항목은 다섯 가지보다 적은 임의의 필드 조합으로 구성될 수 있다. 예를 들어, 항목이 소스 IP와 목적지 IP만으로 구성된다면, 동일한 소스 및 목적지 IP를 가진 모든 플로우가 해당 항목과 매칭된다. 이러한 항목들을 집계 플로우 항목(Aggregate Flow Entries, AFE)이라 하며, AFE의 세분화 수준은 포함된 필드의 개수에 의해 결정된다. 가장 세밀한 항목은 다섯 가지 필드를 포함하여 1:1 매칭을 이루고, 가장 거칠게 구성된 항목은 플로우 패킷이 도착한 스위치 포트 번호만을 매칭한다.

기본 플로우 항목 퇴거 메커니즘은 미리 정의된 기간 또는 항목의 유휴 시간 이후에 항목을 퇴거시키므로 AFE에는 적합하지 않다. AFE는 동시에 여러 플로우를 처리하므로 유휴 상태일 가능성이 낮으며, 만약 활성 상태일 때 퇴거된다면 해당 항목의 재설치를 영향을 받는 모든 플로우에서 요청하게 된다. 게다가 AFE가 비활성/유휴 상태라도 향후 유입될 플로우를 처리할 가능성이 있으므로, AFE를 선제적으로 퇴거시키는 것은 현실적이지 않다. 따라서 오버플로우 이벤트가 발생할 때에만 항목을 퇴거시키기 위한 정책이 요구된다. 앞서 논의한 바와 같이, 오버플로우 이벤트 이후의 항목 퇴거에 관한 기존 연구들은 1:1 매칭 시나리오에만 초점을 맞추었으므로 AFE에는 비효율적이다. 본 논문은 SDN 컨트롤러에서 동작하는 DQN 에이전트를 활용하여 AFE를 위한 퇴거 정책을 제시한다.

제안된 ESAFE에서 SDN 컨트롤러는 현재 스위치 플로우 테이블에 설치된 AFE의 수를 유지한다. 테이블 미스 이벤트에 대한 OpenFlow Packet\_In 메시지를 수신하면, 컨트롤러는 현재 AFE 수와 플로우 테이블 용량을 비교한다. 만약 AFE 수가 용량보다 적으면, 컨트롤러는 Packet\_In 메시지에 명시된 목적지 경로를 결정한 후 Flow\_Mod 메시지를 사용하여 적절한 항목을 플로우 테이블에 설치한다. 반면 AFE 수가 플로우 테이블 용량과 같으면 오버플로우 이벤트가 발생한 것으로 판단하여, 현재 플로우 테이블 내 항목 중 하나를 퇴거시켜야 한다. 퇴거시킬 항목을 선택하기 위해 컨트롤러는 Flow\_State 메시지를 이용해 플로우 테이블 내 모든 AFE의 상태를 수집한 후, 이 상태 정보를 DQN 에이전트에 전달하여 학습을 통해 퇴거할 항목을 결정한다.

그림 3은 SDN 컨트롤러의 아키텍처와, t = 0일 때 비어있는 스위치 플로우 테이블의 타임라인을 보여준다. t = t에서 오버플로우 이벤트가 발생하며, t + 1에서 항목이 퇴거된다. 다음 오버플로우는 t  = t + x 에서 발생하는데, 여기서 는 임의의 시간 간격을 의미하며, DQN이 선택한 AFE의 퇴거는 t + (x+1)에 수행된다.

![](/assets/images/posts/503/img_2.png)

**그림 3. SDN 컨트롤러 내 DQN 구조와 스위치에서의 오버플로우/퇴거 타임라인을 보여주는 ESAFE 아키텍처.**

SDN 컨트롤러에 구현된 DQN 에이전트는 리플레이 메모리(Replay Memory), Q-Current DNN, 그리고 Q-Target DNN으로 구성된다. 리플레이 메모리는 이전 및 현재 상태, 행동, 보상을 저장하며, 이는 Q-Current 및 Q-Target 네트워크를 학습시키는 데 사용된다. 이러한 구조는 두 모델(Q-Current와 Q-Target)의 출력과 리플레이 메모리를 활용하여 Q-Current 네트워크의 가중치를 업데이트하기 위한 오류(error)를 계산한다. 그 결과, Q-Current 네트워크는 오버플로우를 최소화(나아가 재설치 횟수를 줄이고 플로우 테이블 히트 비율을 높이도록) 하기 위해 퇴거할 AFE를 결정한다.

ESAFE에서 제안된 DQN 에이전트의 상태(state), 행동(action), 보상(reward)은 다음과 같이 정의된다.

### 상태 (State)

s\_i번째 오버플로우 이벤트 시점에서의 플로우 테이블 상태는 SDN 컨트롤러에 의해 수집되며, 플로우 테이블 용량(즉, 항목의 최대 개수)을 나타내는 N개의 원소로 구성된 집합

![](/assets/images/posts/503/img_3.png)

![](/assets/images/posts/503/img_4.png)

![](/assets/images/posts/503/img_5.png)

![](/assets/images/posts/503/img_6.png)

![](/assets/images/posts/503/img_7.png)

IV. 성능 평가

### A. 구현(Implementation)

ESAFE는 파이썬 기반의 SDN 프레임워크인 Mininet [15], Ryu SDN 컨트롤러 [16], 그리고 DQN을 위한 Tensorflow 2.5.0 [17]을 이용하여 구현하였다. 컨트롤러와 Open Virtual Switch(OVS) 2.5.5 기반의 소프트웨어 스위치 간에는 OpenFlow v1.3 프로토콜을 사용하여 통신한다. 실험에 사용된 네트워크 토폴로지는 Mininet에서 구현되었으며, 하나의 스위치와 50대의 호스트로 구성된다. 이때 50대 호스트는 모두 소스와 목적지 역할을 겸한다. 소스 호스트들은 Tcpreplay 도구를 사용하여 TCP와 UDP 플로우로 구성된 트래픽을 특정 목적지 호스트로 전송한다. 생성된 플로우는 스위치에 도착하여 플로우 테이블 내 AFE와 매칭된다. 설치되는 AFE들은 소스와 목적지 IP만을 매칭 필드로 사용하며, 나머지 필드는 와일드카드 처리된다. 이는 AFE의 중간 정도 세분화(moderate granularity)를 유지하기 위함이다.

만약 AFE 설치 시 플로우 테이블이 이미 가득 찼다면, 스위치는 Error\_Tablefull 메시지를 컨트롤러로 전송한다. 컨트롤러는 이에 대한 응답으로 Flows\_State 메시지를 이용해 테이블의 모든 AFE 상태를 수집하고, 그 상태 정보를 컨트롤러에 구현된 DQN 모듈에 전달한다. 여기서 기본 퇴거 메커니즘(idle/hard timeout)은 비활성화되어 있으며, 오직 ESAFE만이 AFE 퇴거를 담당한다.

DQN 모듈은 Q-Current DNN과 Q-Target DNN으로 구성되는데, 각 신경망은 300개의 뉴런을 가진 3개의 은닉층과 Rectified Linear Unit(ReLU) 활성화 함수를 사용한다. 출력층은 플로우 테이블 용량만큼의 뉴런을 가지며, 출력 오류는 평균제곱오차(Mean Squared Error, MSE)로 계산된다. DQN의 학습률(learning rate)은 0.1, 감쇄율(decay rate)은 0.9로 설정하였다. 한 에피소드(episode)에서 총 11124개의 플로우가 호스트들에 의해 생성된다. 각 실험은 총 500회의 에피소드로 진행되었으며, 여기서 제시된 결과는 활용(exploitation) 단계에서 얻은 모든 에피소드의 평균값이다.

### B. 실험 결과(Experimental Result)

ESAFE의 성능은 기본 베이스라인으로 사용되는 LRU와 무작위(Random) AFE 퇴거 정책과 비교하여, 정규화된 오버플로우 횟수, 재설치 횟수, 그리고 제어 메시지 오버헤드를 기준으로 평가하였다. 각 평가지표의 정규화는 플로우 테이블 크기가 50, 100, 200일 때의 최대값을 사용하였다.

![](/assets/images/posts/503/img_8.png)

**그림 4.** ESAFE, LRU, 무작위(Random) 퇴거 정책에서의 플로우 테이블 오버플로우 횟수

플로우 테이블 용량이 제한적이기 때문에, 높아진 플로우 도착률에 비해 퇴거가 충분하지 않으면 오버플로우는 불가피하게 발생한다. 플로우 테이블 용량은 오버플로우 발생 횟수에 크게 영향을 미치는데, 용량이 작을수록 오버플로우가 자주 발생하고 용량이 커질수록 오버플로우가 줄어든다. 이에 따라 그림 4는 용량이 50, 100, 200일 때 제안된 ESAFE, LRU, 무작위 정책의 정규화된 오버플로우 횟수를 비교한다. 용량이 50일 경우, ESAFE는 무작위 정책 대비 28%, LRU 대비 45% 낮은 오버플로우 횟수를 보인다. 이는 용량을 100으로 했을 때 각각 43%, 27%로 차이가 줄어들며, 용량을 200으로 했을 때는 각각 31%, 7%로 줄어든다. 이는 용량이 작은 테이블에서 자주 발생하는 오버플로우에 대해, ESAFE가 보다 효과적으로 AFE를 선택해 오버플로우 횟수를 줄임을 의미한다. 반면 테이블 용량이 커지면 오버플로우 발생이 드물어지므로, ESAFE와 LRU, 무작위 정책 간의 성능 격차도 줄어든다. 따라서 리소스가 제한적인 IoT 게이트웨이나 무선 액세스 포인트 같은 네트워크 장비에서 ESAFE가 특히 효과적이라 할 수 있다.

![](/assets/images/posts/503/img_9.png)

그림 5. ESAFE, LRU, 무작위(Random) 퇴거 정책에서의 AFE 재설치 횟수

효율적인 퇴거 정책은 비활성 상태이거나 사용 빈도가 가장 낮은 AFE를 골라 퇴거해야 한다. 활성 상태인 AFE를 퇴거할 경우, 그 AFE와 매칭되는 패킷들이 테이블 미스를 일으켜 컨트롤러가 해당 AFE를 재설치해야 한다. 이는 컨트롤러에 상당한 오버헤드를 유발한다. 그림 5는 테이블 용량이 50, 100, 200일 때 ESAFE, LRU, 무작위 퇴거 정책이 유발하는 재설치 횟수를 비교한다. 용량이 50일 경우 ESAFE는 LRU 대비 114%, 무작위 정책 대비 140% 더 적은 재설치를 보인다. 테이블 용량을 100과 200으로 늘렸을 때 이 차이는 ~150%까지 증가한다. 이는 테이블 용량이 달라져도 AFE 재설치 횟수에는 큰 영향을 주지 않으며, ESAFE가 꾸준히 비활성 또는 사용 빈도가 낮은 AFE를 선택함으로써 재설치를 대폭 줄여준다는 것을 의미한다.

AFE를 설치, 퇴거, 모니터링하는 과정은 스위치와 컨트롤러 간의 제어 패킷 오버헤드를 유발한다. 앞서 언급했듯이 재설치 횟수가 줄어들면 제어 오버헤드 역시 크게 감소하지만, ESAFE는 오버플로우 이벤트마다 테이블 상태를 수집하므로 이에 따른 추가 오버헤드가 발생한다. 따라서 ESAFE, LRU, 무작위 정책 간 제어 오버헤드를 비교하는 것은 중요하다. 그림 6은 테이블 용량이 50, 100, 200일 때 세 정책의 제어 오버헤드를 보여준다. ESAFE는 오버플로우가 발생할 때마다 AFE 상태를 수집하기 때문에 추가적인 오버헤드가 있긴 하지만, 그럼에도 재설치 횟수를 크게 줄여 전반적인 제어 패킷 수를 크게 절감한다. 테이블 용량이 50일 때 ESAFE는 LRU 대비 37%, 무작위 대비 58%의 제어 패킷을 감소시킨다. 용량이 100일 때는 각각 36%, 54%를, 용량이 200일 때는 각각 5%, 19%의 제어 패킷을 절감한다. 이는 ESAFE가 재설치 횟수를 큰 폭으로 줄여주어, 추가적으로 발생하는 상태 수집 오버헤드를 상쇄하고도 LRU와 무작위 정책보다 제어 오버헤드가 전반적으로 낮음을 보여준다.

아울러, 처음으로 테이블 미스를 일으킨 플로우의 AFE를 최초 설치하는 데 필요한 제어 메시지는 세 정책 모두 동일하므로, 이들 메시지는 제어 오버헤드 산정에서 제외하였다.

![](/assets/images/posts/503/img_10.png)

**그림 6.** ESAFE, LRU, 무작위(Random) 정책에서 스위치와 컨트롤러 간에 발생하는 제어 메시지 오버헤드

## V. 결론

본 논문에서는 집계 플로우 항목(Aggregate Flow Entries, AFE)을 대상으로 하는 딥 강화 학습 기반 퇴거 전략(ESAFE)을 제안하였다. 제안된 DRL 프레임워크는 퇴거 결정을 내릴 때 각 AFE와 매칭되는 플로우 수(즉, 차수)를 고려하며, 두 오버플로우 이벤트 사이의 시간을 최대화하여 오버플로우 횟수를 최소화하는 것을 최종 목표로 한다.

ESAFE의 성능은 LRU 및 무작위(Random) 퇴거 정책과 비교되었으며, 그 결과 ESAFE는 오버플로우, 재설치, 제어 오버헤드를 모두 크게 줄여주는 AFE 퇴거를 선택함을 확인할 수 있었다. 오버플로우 이벤트마다 AFE 상태를 수집해야 하는 추가 비용이 발생하긴 하지만, 전반적인 제어 패킷 오버헤드는 오히려 LRU와 무작위 정책보다 더 낮게 나타났다. 이러한 결과를 통해, ESAFE는 기존 베이스라인 기법들에 비해 AFE를 더욱 효율적으로 퇴거시킬 수 있는 정책임을 알 수 있다.

현재는 AFE의 이질적(heterogeneous) 세분화 수준을 고려하여 상태 벡터에 추가 특성을 포함하는 방식으로 ESAFE를 확장하는 연구가 진행 중이며, 더욱 다양하고 견고한 데이터 생성 방식과 대규모 네트워크 토폴로지를 활용하여 다음 논문에서 발표할 예정이다.
