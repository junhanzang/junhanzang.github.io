---
title: "Fast and Simplex: 2-Simplicial Attention in Triton"
date: 2025-07-18 15:58:04
categories:
  - 인공지능
tags:
  - Attention
---

<https://arxiv.org/abs/2507.02754?_bhlid=b823264a61f867fcfc11342f5464010c50351360>

[Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754?_bhlid=b823264a61f867fcfc11342f5464010c50351360)

**초록 (Abstract)**

최근 연구에 따르면, 훈련 손실(training loss)은 모델 크기와 토큰 수가 증가함에 따라 거듭제곱 법칙(power law)을 따르며, 계산(compute)에 최적인 모델을 달성하려면 모델 크기와 토큰 수를 함께 확장해야 한다는 사실이 밝혀졌다. 그러나 이러한 스케일링 법칙은 무한한 양의 데이터가 존재한다는 가정 하에 성립하며, 주로 연산량이 제한된(compute-bound) 상황에 적용된다. 현대의 대형 언어 모델들이 점점 더 대규모 인터넷 기반 데이터셋에 의존함에 따라, 이들이 연산 제한 상황에 있다고 가정하는 것은 점차 타당성을 잃고 있다. 이와 같은 변화는 **토큰 효율성(token efficiency)**을 우선시하는 아키텍처의 필요성을 부각시킨다.

본 연구에서는, 표준 점곱(dot-product) 어텐션을 삼중 선형 함수(trilinear function)로 일반화한 아키텍처인 **2-simplicial Transformer**의 사용을 탐구한다. 이 아키텍처는 효율적인 Triton 커널 구현을 통해 실현되었다. 우리는 2-simplicial Transformer가 표준 Transformer보다 더 높은 토큰 효율성을 달성함을 보인다. 즉, 동일한 토큰 예산 하에서 유사한 크기의 모델이 수학, 코딩, 추론, 논리와 같은 과제에서 기존 점곱 기반 모델보다 더 뛰어난 성능을 보인다.

또한 우리는, 2-simplicial 어텐션이 지식과 추론 과제에 대한 스케일링 법칙의 **지수(exponent)**를 변화시킴으로써, 그 성능 이득을 정량적으로 입증한다.

## 1. 서론

Transformer 아키텍처(Vaswani et al., 2017)를 기반으로 한 대형 언어 모델(LLM)은 GPT-3(Brown et al., 2020), GPT-4(Achiam et al., 2023), Gemini(Team et al., 2023), Llama(Touvron et al., 2023) 등 최첨단 인공지능 시스템의 핵심 기반이 되었다. 이러한 모델의 놀라운 발전은 **신경망 스케일링 법칙(neural scaling laws)**(Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022)에 의해 이끌어졌는데, 이 법칙은 **훈련 손실(training loss)**과 **모델 파라미터 수**, **훈련 데이터량** 사이의 거듭제곱 법칙(power-law) 관계를 실증적으로 보여준다.

이러한 연구에서 도출된 핵심 통찰은, 단순히 모델 크기만 늘리는 것이 아니라 **모델의 파라미터 수와 훈련 데이터 양을 함께 확장해야 최적의 성능**을 낼 수 있다는 점이다. 특히 Hoffmann et al. (2022)은 **계산 자원에 최적인 모델(compute-optimal model)**을 위해서는 균형 잡힌 확장 전략이 필요하다고 강조한다. 그들의 연구에 따르면, **파라미터 수가 700억 개인 Chinchilla 모델**이 **2800억 파라미터를 가진 Gopher 모델**보다 뛰어난 성능을 보였는데, 이는 Chinchilla가 훨씬 더 많은 데이터로 학습되었기 때문이다. 이 결과는 **대형 언어 모델의 성능 향상을 위해 모델 크기와 함께 데이터 규모를 확장하는 것이 얼마나 중요한지**를 잘 보여준다.

그러나 인공지능(AI)이 계속 발전함에 따라, 이제는 **충분히 고품질인 토큰(token)**을 확보하는 것이 점점 더 어려운 과제가 되고 있다. 이러한 전환점에 가까워지면서, 우리는 **기존 Transformer보다 더 효율적으로 스케일링 가능한 새로운 아키텍처**를 탐색할 필요성이 커지고 있다. 하지만 대부분의 아키텍처 개선이나 옵티마이저의 발전은 단지 손실 곡선의 위치(offset)를 이동시킬 뿐, **스케일링 법칙의 지수(exponent)** 자체를 변화시키지는 못한다(Everett, 2025). Kaplan et al. (2020), Shen et al. (2024)는 대부분의 아키텍처 변경이 지수에 영향을 주지 못함을 보였으며, Hestness et al. (2017)은 옵티마이저에 대해서도 유사한 결론을 도출했다. 현재까지 **스케일링 지수에 긍정적인 변화를 준 유일한 영역은 데이터**인데, Sorscher et al. (2022), Bahri et al. (2024), Brandfonbrener et al. (2024)는 **데이터 분포를 바꾸는 것**이 지수에 영향을 줄 수 있음을 보여주었다.

이러한 맥락에서, 우리는 **Transformer의 점곱(dot-product) 어텐션을 삼중선형 형태(trilinear form)**로 일반화한 Clift et al. (2019)의 오래된 연구를 다시 조명한다. 이 구조는 **2-simplicial Transformer**로 불린다. 우리는 또한 RoPE(Su et al., 2024)를 **삼중선형 함수로 일반화**하는 방식을 탐색하고, **회전 불변(rotation-invariant)**을 가지면서도 **2-simplicial attention만큼 표현력이 뛰어난** 삼중선형 형태를 제시한다.

나아가 우리는 2-simplicial Transformer가 **제한된 토큰 예산 하에서 Transformer보다 더 우수한 확장성을 가진다**는 것을 보인다. 동일한 토큰 수를 사용했을 때, 유사한 크기의 2-simplicial Transformer가 수학, 코딩, 추론 과제에서 기존 Transformer보다 더 나은 성능을 보여준다. 뿐만 아니라, 실험 결과에 따르면 2-simplicial Transformer는 **모델 파라미터 수에 대한 스케일링 지수 역시 더 유리한 방향으로 변화**함을 확인할 수 있었다. 이는 기존의 Chinchilla 스케일링(Hoffmann et al., 2022)과 달리, **2-simplicial Transformer는 파라미터 수를 늘릴 때 토큰 수를 더 느린 속도로 증가시켜도 되는 가능성**을 시사한다.

결론적으로, 우리 연구는 **토큰 수가 제한된 환경**에서 2-simplicial Transformer가 기존의 점곱 기반 Transformer보다 **자연어의 비감축 정보(entropy)에 더 근접하게 도달할 수 있는 효율적인 접근 방식**임을 시사한다.

## 2. 관련 연구

Vaswani et al. (2017)의 기념비적인 연구 이후, **어텐션 메커니즘의 일반화**를 시도한 다양한 연구들이 제안되었다. 가장 초기의 흐름 중 하나는 **시퀀스 길이에 따른 어텐션의 이차 복잡도(quadratic complexity)**를 줄이려는 시도였다. 특히 Parmar et al. (2018)은 이미지 생성 문맥에서 **로컬 어텐션(local attention)**을 제안하였고, 이후 여러 연구들(Zaheer et al., 2020; Roy et al., 2021)은 이를 언어 모델링에 다른 방법들과 결합해 적용하였다.

또 다른 접근으로는 **소프트맥스(softmax) 어텐션을 완전히 제거**하는 방식이 있다. 예를 들어, Katharopoulos et al. (2020)은 소프트맥스를 정규화 없는 지수함수(exponential)로 대체하고, 행렬 곱셈의 결합 법칙을 이용해 **선형 시간 복잡도(linear time)**를 갖는 Transformer를 구현하였다. 또 다른 선형 시간 어텐션 방식으로는 **Mamba**(Gu & Dao, 2023)와 같은 **state space 모델**이 있다. 그러나 이러한 선형 어텐션 기법들은 **실제 성능이 Transformer보다 떨어지는 경우가 많아** 널리 채택되지는 않았다. Allen (2025)에 따르면, Mamba가 실전에서 좋은 성과를 낸 주요 요인은 **Conv1D 연산자(conv1d operator)**의 활용 덕분이며, Transformer 아키텍처의 대안으로 제안된 So et al. (2021), Roy et al. (2022)의 연구도 유사한 방향성을 보인다.

반대편 스펙트럼에서는, 어텐션을 **이차(quadratic)에서 고차(higher-order)**로 확장하려는 시도들이 있다. 이 분야에서 가장 먼저 등장한 것으로 알려진 연구는 Clift et al. (2019)의 **2-simplicial attention**이다. 이들은 이 구조가 **심층 강화학습 환경에서 논리적 문제 해결에 적합한 근사 방식**임을 보였다. 유사한 일반화로 Bergen et al. (2021)은 **Edge Transformer**를 제안했으며, 이들은 **삼각형 어텐션(triangular attention)**을 도입하였다. 또한 AlphaFold 논문(Jumper et al., 2021)에서도 단백질의 2D 기하 구조로부터 유도된 **삼각형 자기어텐션(triangle self-attention)**이 사용되었다.

고차 상호작용은 추천 시스템 환경에서 Wang et al. (2021)에 의해 탐색되었으며, 최근 Sanford et al. (2023)은 **n-레이어 2-simplicial Transformer로 해결 가능한 문제의 집합이 dot-product Transformer로 해결 가능한 집합보다 더 크다**는 것을 보였다. 특히 이들은 **Match3**라는 문제 클래스를 정의하고, dot-product 어텐션은 이 과제를 해결하기 위해 시퀀스 길이에 비례해 **지수적으로 많은 레이어**가 필요함을 보였다. 이어지는 연구로 Kozachinskiy et al. (2025)은 **2-simplicial attention의 확장 가능한 근사 기법**을 제안하고, **Strassen 어텐션**과 dot-product 어텐션 간의 **VC 차원(Vapnik, 1968)에 기반한 하한(lower bound)** 관계를 증명하였다. 이는 더 복잡한 추론이 요구되는 태스크에서의 이론적 우위를 시사한다.

이와 관련된 또 다른 연구로는 Dehghani et al. (2018)이 제안한 **루핑(Looping) Transformer layer**이 있다. 이는 **Universal Transformer**로 구현되었으며, 이후 Yang et al. (2023), Saunshi et al. (2025) 등에서도 유사 개념을 다룬 바 있다. **고차 어텐션과 루핑 구조**는 공통적으로 **단위 파라미터당 더 표현력 있는 함수**를 계산하려는 목적을 갖는다. 이러한 연구들은 루핑 Transformer가 논리적 추론 과제에서 더 나은 성능을 보인다는 점을 보여준다. 다만 루핑 구조의 주요 한계는 **대규모 모델로의 확장 시 학습의 어려움**이다. 구체적으로, **k번 루핑하면 모델의 깊이가 k배 증가**하여 **심층 신경망에서 흔히 발생하는 학습 난점들**을 더욱 심화시킬 수 있다. 따라서 **대형 루핑 Transformer의 학습 가능성**은 여전히 불확실하며, 이를 해결하기 위한 추가 연구가 필요하다.

### 표기법 (Notation)

- **벡터**는 **소문자 볼드체**, **행렬 및 텐서**는 **대문자**, **스칼라**는 일반 소문자로 표기한다.
- 두 벡터 **?, ?** 간의 점곱(dot product)은 ⟨?, ?⟩로, 삼중선형(dot product of three vectors)은  
  ⟨?, ?, ?⟩ = ∑₍ᵢ₌₁₎ᵈ ⟨?ᵢ, ?ᵢ, ?ᵢ⟩ 로 표기한다.
- 행렬 곱셈은 @ 기호로 나타내며, 예: (A B) @ C.
- 배열 슬라이싱은 ?[l:l+m] = (a\_l, ..., a\_{l+m−1})로 나타내며, 인덱스는 0부터 시작한다.
- 텐서 연산 중 일부는 Numpy의 아인슈타인 표기법(Einstein summation notation)을 따른다(Harris et al., 2020).
- **FLOPs**는 부동소수점 연산 수를 의미한다.
- 배열의 열 단위 결합은 [?, ?, ?]로 표기한다.
- 정방행렬의 행렬식(determinant)은 det으로 표기한다.

## 3. 신경망 스케일링 법칙 개요

이 절에서는 **Kaplan et al. (2020)**에서 처음 소개된 **신경망 스케일링 법칙(neural scaling laws)**에 대해 간략히 개요를 제시한다. 우리는 **Hoffmann et al. (2022)**에서 제안한 접근 방식을 따르며, 여기서는 **손실 함수 L(N,D)**가 **모델 파라미터 수 N** 및 **토큰 수 D**에 대해 **거듭제곱 법칙(power law)**에 따라 감소한다고 가정한다:

![](/assets/images/posts/582/img.png)

- 첫 번째 항 E는 **비감축 손실(irreducible loss)**을 의미하며, 이는 자연어 텍스트의 **엔트로피(entropy)**에 해당한다.
- 두 번째 항은 **N** 개의 파라미터를 가진 모델이 이러한 이상적인 생성 과정을 얼마나 따라가지 못하는지를 나타낸다.
- 세 번째 항은 모델이 **유한한 양의 데이터**만을 학습하고 **충분히 수렴(convergence)**되지 않았음을 반영한다.

이론적으로는, N→∞, D→∞일 때, 대형 언어 모델은 텍스트 분포의 **비감축 손실 E**에 점점 수렴해야 한다.

### 최적의 파라미터 및 데이터 크기

특정 **계산 자원 예산 C** 하에서, 총 부동소수점 연산량(FLOPs)을 다음과 같이 표현할 수 있다:

![](/assets/images/posts/582/img_1.png)

이 조건 하에서, **최적의 파라미터 수**와 **최적의 데이터 크기**는 다음과 같은 관계를 따른다:

![](/assets/images/posts/582/img_2.png)

Hoffmann et al. (2022)의 연구는 여러 실험을 통해 손실 함수를 파라메트릭 함수로 근사하여, 지수 a와 b를 추정하였다. 다양한 방법론을 통해 일관되게 다음과 같은 값이 도출되었다:

![](/assets/images/posts/582/img_3.png)

이로부터 Hoffmann et al. (2022)의 핵심 주장—즉, **모델의 크기(model size)에 비례하여 토큰 수도 함께 확장해야 한다**—가 도출된다.

### 품질 좋은 토큰의 한계와 법칙의 구조적 불변성

그러나 앞선 **1절**에서 논의했듯이, 현재는 **충분히 고품질인 토큰의 확보 자체가 병목**이 되고 있다. 이로 인해, 우리는 기존 Transformer 구조 외에 **대안적 아키텍처와 학습 알고리즘**을 탐색할 필요가 커지고 있다.

한편, 최근 여러 연구들은 기존 문헌에서 제안된 대부분의 **모델링 및 최적화 기법**이 실제로는 **손실 함수의 오프셋 E**만 이동시킬 뿐, **거듭제곱 법칙의 지수 (α,β)** 자체는 근본적으로 바꾸지 못한다는 점을 보여주었다. 이와 관련해 Everett (2025)의 심층적인 논의를 참고할 수 있다.

---

대부분은 알겠지만 이는 실험적으로 탄생한 공식이다.

---
