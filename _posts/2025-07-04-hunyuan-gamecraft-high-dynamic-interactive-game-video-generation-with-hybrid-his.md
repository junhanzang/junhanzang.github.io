---
title: "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition"
date: 2025-07-04 17:37:14
categories:
  - 인공지능
tags:
  - hunyuan-gamecraft
---

<https://hunyuan-gamecraft.github.io/>

[Hunyuan-GameCraft](https://hunyuan-gamecraft.github.io/)

<https://arxiv.org/abs/2506.17201>

[Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)

**초록**  
최근 확산 기반(diffusion-based) 및 제어 가능한(controllable) 비디오 생성 기술의 발전으로 고품질이며 시간적으로 일관된 비디오 합성이 가능해졌고, 이는 몰입감 있는 인터랙티브 게임 경험의 기반을 마련하고 있습니다. 그러나 기존 방법들은 동적 표현력, 일반화 능력, 장기적인 일관성, 그리고 효율성 측면에서 한계를 가지고 있어, 다양한 게임플레이 영상을 생성하는 데 제약이 있습니다.

이러한 문제를 해결하기 위해, 우리는 **Hunyuan-GameCraft**라는 새로운 프레임워크를 제안합니다. 이는 게임 환경에서 **높은 동적 표현력(high-dynamic)과 상호작용성(interactivity)을 갖춘 비디오 생성**을 가능하게 합니다. 정밀한 동작 제어를 달성하기 위해, 표준 키보드 및 마우스 입력을 통합하여 **공유된 카메라 표현 공간(shared camera representation space)** 상에 매핑함으로써 다양한 카메라 및 이동 동작 간의 부드러운 보간(interpolation)이 가능하도록 합니다.

또한, 게임 장면 정보를 유지하면서 비디오 시퀀스를 자동회귀적으로 확장할 수 있도록 하는 **하이브리드 히스토리 조건부 학습 전략(hybrid history-conditioned training strategy)**을 제안합니다. 추론 효율성과 실시간 사용성을 높이기 위해, 우리는 모델 경량화를 위한 **지식 증류(model distillation)** 기법을 적용하여 연산 비용을 줄이는 동시에, 긴 시간 축에서도 일관성을 유지할 수 있도록 설계했습니다. 이는 복잡한 상호작용 환경에서의 실시간 활용에 적합합니다.

이 모델은 100개 이상의 AAA 게임에서 수집된 **100만 개 이상의 게임플레이 영상**으로 구성된 대규모 데이터셋을 바탕으로 학습되었으며, 정밀성과 제어력을 높이기 위해 정교하게 주석 처리된 합성 데이터셋으로 파인튜닝(fine-tuning) 되었습니다. 정제된 게임 장면 데이터는 시각적 사실성, 현실감, 그리고 동작 제어력을 크게 향상시킵니다.

광범위한 실험 결과, Hunyuan-GameCraft는 기존 모델들보다 게임 비디오 생성의 사실성과 플레이 가능성(playability) 측면에서 월등한 성능을 보임을 확인했습니다.

![](/assets/images/posts/578/img.png)

**그림 2:** Hunyuan-GameCraft의 **다중 동작 제어(multi-actions control)** 결과 예시입니다. 파란색으로 점등된 키는 눌린 상태를 나타냅니다. **W, A, S, D**는 이동(transition movement)을, ↑, ←, ↓, →는 시야 각도(view angle)의 변화를 의미합니다.

![](/assets/images/posts/578/img_1.png)

**그림 1:** Hunyuan-GameCraft는 **단일 이미지와 이에 대응하는 프롬프트만으로도 높은 동적 표현력을 갖춘 인터랙티브 게임 영상 콘텐츠**를 생성할 수 있습니다. 우리는 일련의 동작 신호(action signals)를 시뮬레이션하고, 좌우 프레임에는 서로 다른 입력에 따라 생성된 게임 영상 시퀀스의 주요 순간들이 나타나 있습니다. Hunyuan-GameCraft는 **각 상호작용에 맞춰 정밀하게 콘텐츠를 생성**할 수 있으며, **시간적 및 3D 일관성**을 유지하면서 **장기적인 비디오 생성**도 지원합니다. 또한 시퀀스 전체에서 **과거 장면 정보의 보존**도 효과적으로 수행됩니다. 이 경우에도 **W, A, S, D**는 이동을, ↑, ←, ↓, →는 시야 각도의 변화를 나타냅니다.

### 1. 서론 (Introduction)

생성 모델링(generative modeling)의 빠른 발전은 **엔터테인먼트, 교육 등 다양한 분야**에 큰 변화를 가져왔으며, 이로 인해 **높은 동적 표현력과 몰입감을 갖춘 생성형 게임 경험**에 대한 관심도 점점 높아지고 있습니다. 특히, 최근 **확산 기반(diffusion-based) 비디오 생성 기술**의 획기적인 발전 [1, 2, 31, 6, 19]은 **고품질이며 시간적으로 일관된 동적 콘텐츠 생성**을 가능하게 만들었습니다. 또한, **제어 가능한(controllable) 비디오 생성** 기술의 발전은 **사용자 주도형의 새로운 동적 콘텐츠 제작 방식**을 열어주며, **인터랙티브 디지털 경험의 경계를 넓히는** 데 기여하고 있습니다.

![](/assets/images/posts/578/img_2.png)

**표 1:** 최근의 인터랙티브 게임 생성 모델들과의 비교  
Hunyuan-GameCraft는 **연속적인 동작 신호(continuous action signals)**를 조건으로 하여 **무한 길이의 게임 영상 생성이 가능한 모델**입니다. 이 모델은 **강력한 일반화 능력**, **높은 시간적 동적 표현력**, 그리고 **과거 장면 정보의 효과적인 보존**을 모두 갖춘 것이 특징입니다.

최근의 시각 생성(visual generation) 기술 발전은 **공간 지능(spatial intelligence)**, 즉 일관된 공간 장면의 분석 및 생성에 초점을 맞추고 있습니다. 이러한 모델들은 **상호작용성과 탐색 가능성**을 중심으로, **시공간적 일관성을 갖춘 동적 3D/4D 환경**을 가능하게 합니다. 예를 들어, **WorldLabs [32]**는 정적인 이미지로부터 **고해상도 3D 환경을 재구성할 수 있는 가능성**을 보여주며, **Genie 2 [22]**는 **시간에 따른 물리적으로 일관된 상호작용**을 위해 **잠재적 동작 모델링(latent action modeling)**을 도입하였습니다.

그럼에도 불구하고, 현재의 접근 방식들은 여전히 다음과 같은 주요 영역에서 한계를 겪고 있습니다:

- **실시간 동적 장면 요소의 사실성**,
- **장기적인 시퀀스 일관성**,
- **연산 효율성** 등에서 부족하여, **높은 동적 표현력과 상호작용성을 요구하는 실제 게임 환경**에서는 적용이 어렵습니다. 특히, **게임 상호작용 모델링에서의 실시간 생성과 높은 동적성(dynamicity)**은 **플레이어 경험의 핵심 요소**로 작용합니다.

이러한 문제를 해결하기 위해, 우리는 **Hunyuan-GameCraft**라는 새로운 프레임워크를 제안합니다. 이는 **게임 환경 내에서 높은 동적 표현력과 동작 제어(action-controllability)가 가능한 비디오 생성**을 목표로 합니다. 이 프레임워크는 **텍스트-투-비디오 기반 모델인 HunyuanVideo [18]** 위에 구축되었으며, **이산적인 사용자 동작 신호(discrete user actions)**를 기반으로 **시각적으로 풍부하고 시간적으로 일관된 게임플레이 영상을 생성**할 수 있습니다.

우리는 **W, A, S, D, 방향키, Space** 등 표준 키보드 및 마우스 입력을 **공통된 카메라 표현 공간(shared camera representation space)**으로 통합합니다. 이 통합된 임베딩 공간은 **카메라 및 이동 동작 간의 부드러운 보간(smooth interpolation)**을 가능하게 하여, **물리적 타당성(physical plausibility)**을 유지하면서도, **사용자 중심의 시네마틱한 연출 유연성**(예: 가속, 시야 전환 등)을 제공합니다.

**인터랙티브 게임 비디오 생성에서의 장기적 일관성 유지**를 위해 기존 연구들 [6, 15, 20]은 **훈련 없는 확장(training-free extension)**, **스트리밍 기반 노이즈 제거(streaming denoising)**, 또는 **마지막 프레임 조건부 학습(last-frame conditioning)** 등에 집중해왔습니다. 하지만 이러한 방식은 **품질 저하** 및 **시계열 불일치(temporal inconsistency)** 문제를 겪기 쉬우며, 특히 **인과적 VAE(causal VAEs) 기반 생성에서는 더욱 두드러지는 한계**가 있습니다.

이에 우리는 **하이브리드 히스토리 조건(hybrid history-conditioned) 학습 전략**을 새롭게 제안합니다. 이 전략은 **과거 장면 정보를 통합하고, 마스크 인디케이터(mask indicator)를 활용하여 자동회귀 방식의 오류 누적(error accumulation)을 방지**하면서 시퀀스를 확장할 수 있도록 설계되었습니다.

또한, **추론 속도와 상호작용 경험을 개선**하기 위해, 우리는 **모델 증류 기반 가속 전략(model distillation acceleration strategy) [28]**을 도입합니다. 이를 통해 연산 부담을 줄이면서도 **장시간 시퀀스에서도 일관성을 유지**할 수 있어, **복잡한 실시간 상호작용 환경에서의 실제 적용**에 적합합니다.

우리는 **선별된 게임 장면 데이터셋과 일반 스타일 데이터셋** 모두에서 Hunyuan-GameCraft의 성능을 평가했으며, **기존 모델 대비 유의미한 성능 우위**를 입증했습니다.

### 요약하자면, 우리의 주요 기여는 다음과 같습니다:

- **Hunyuan-GameCraft 프레임워크 제안**: 사용자 맞춤형 동작 입력을 통해 게임 장면 내 동적 콘텐츠를 생성할 수 있는 새로운 인터랙티브 게임 비디오 생성 프레임워크를 제안합니다.
- **이산적인 키보드/마우스 동작 신호의 연속적인 행동 공간으로의 통합**: 속도, 각도 등 복잡하고 정교한 상호작용 입력을 지원하는 통합된 표현 공간을 설계하였습니다.
- **장기적 공간 및 시간 일관성을 유지하는 하이브리드 히스토리 조건 학습 전략**을 도입하였습니다.
- **모델 증류 기반의 추론 속도 개선**을 통해 상호작용 경험을 향상시켰습니다.

## 2 관련 연구 (Related Work)

### 2.1 인터랙티브 게임 장면 월드 모델 (Interactive Game Scene World Model)

최근 연구들은 **비디오 생성 모델을 활용하여 게임 장면에서의 동적 예측 및 상호작용 능력을 향상**시키는 데 점차 초점을 맞추고 있습니다. 표 1에서는 최근의 주요 연구들을 정리하였습니다.

- **WorldDreamer [30]**는 **마스킹된 토큰을 예측하는 방식**으로 **범용 월드 모델(world model)**을 구축하는 방식을 제안하며, 이는 **멀티모달 상호작용**을 지원하고 **자연 장면 및 자율주행 환경**에 적용 가능합니다.
- **GameGen-X [5]**는 오픈월드 게임을 위한 **확산 기반 트랜스포머 모델(diffusion Transformer)**로, **멀티모달 제어 신호**를 통합하여 **인터랙티브 비디오 생성**을 구현합니다.
- **Genie 시리즈 [22]**는 **단일 이미지 프롬프트(single-image prompt)**로부터 **3D 세계를 생성**할 수 있으며,
- **Matrix 모델**은 게임 데이터를 활용하여 **스트리밍 형식(streaming generation format)**으로 **사용자 동작에 따라 무한한 콘텐츠를 생성**할 수 있습니다.

### 2.2 카메라 제어 기반 비디오 생성 (Camera-Controlled Video Generation)

- **MotionCtrl [31]**은 비디오 생성에 적합한 **통합되고 유연한 모션 컨트롤러(motion controller)**를 도입하여, **카메라와 객체의 움직임을 개별적으로 제어**함으로써, 생성된 비디오에서의 **시점 제어(perspective control)**를 정밀하게 수행할 수 있도록 합니다.
- **CameraCtrl [13]**은 **Plücker 임베딩**을 카메라 파라미터의 기본 표현 방식으로 사용하며, **카메라 인코더와 선형 계층만을 학습**하여 카메라 제어를 수행합니다.
- 최근 제안된 **CameraCtrl II [14]**는 **카메라 파라미터 주석이 포함된 고동적(high-dynamic) 데이터셋**을 구축하고, **사전학습된 모델의 동적 표현력을 유지**하기 위해 **경량 카메라 주입 모듈(lightweight camera injection module)** 및 훈련 방식도 함께 설계합니다.

### 2.3 장시간 비디오 확장 (Long Video Extension)

**장시간 비디오 생성(long video generation)**은 **시간적 일관성(temporal consistency)**과 **고화질 시각적 품질**을 장시간 동안 유지하는 데 있어 큰 도전 과제를 안고 있습니다. 초기에는 **GAN 기반 방법**을 활용해 장시간 비디오 생성이 탐색되었으며 [23],  
확산 모델(diffusion model)의 대중화 이후에는 이를 이용한 다양한 해결책이 제안되고 있습니다.

- **StreamingT2V [15]**는 **단기 및 장기 메모리 블록**과 **무작위 혼합(randomized blending)** 방식을 도입하여, **텍스트-투-비디오(text-to-video)** 생성에서의 **일관성과 확장성**을 확보합니다.
- 또 다른 접근으로는 **다음 프레임 예측(next-frame prediction)** [11, 12],
- **다음 토큰 예측과 전체 시퀀스 확산의 결합(DiffusionForcing)** [6],
- **테스트 타임 훈련(test-time training)** [7] 등이 있습니다.

이러한 기존 방법들과 달리, 우리는 **확산 패러다임(diffusion paradigm)** 하에서 **게임 장면 정보를 효과적으로 보존하면서**, **비디오 시퀀스를 자동회귀 방식으로 확장하는 새로운 하이브리드 히스토리 조건 학습 전략(hybrid history-conditioned training strategy)**을 제안합니다.

## 3. 데이터셋 구축 (Dataset Construction)

### 3.1 게임 장면 데이터 큐레이션 (Game Scene Data Curation)

우리는 **Assassin’s Creed**, **Red Dead Redemption**, **Cyberpunk 2077** 등 100개 이상의 AAA 게임 타이틀을 선별하여, **고해상도 그래픽과 복잡한 상호작용이 포함된 다양하고 풍부한 데이터셋**을 구성하였습니다. 그림 3에서 볼 수 있듯이, 우리의 **엔드 투 엔드 데이터 처리 프레임워크**는 총 4단계로 구성되며, **주석이 포함된 게임플레이 데이터의 부족 문제를 해결**함과 동시에 **카메라 제어 기반 비디오 생성의 새로운 기준**을 제시합니다.

![](/assets/images/posts/578/img_3.png)

**그림 3: 데이터셋 구축 파이프라인** 이 파이프라인은 총 네 가지 전처리 단계로 구성됩니다:

1. **장면 및 동작 인지 기반 데이터 분할**,
2. **데이터 필터링**,
3. **상호작용 주석**,
4. **구조화된 캡셔닝**.

![](/assets/images/posts/578/img_4.png)

**그림 4: Hunyuan-GameCraft의 전체 아키텍처**  
입력으로는 **참조 이미지(reference image)**, **프롬프트(prompt)**, 그리고 **키보드/마우스 신호**가 주어집니다. 이들은 모두 **연속적인 카메라 공간(continuous camera space)**으로 변환됩니다. 우리는 입력된 **카메라 궤적(camera trajectory)**을 인코딩하기 위해 **경량 액션 인코더(light-weight action encoder)**를 설계하였고, 비디오 패치를 나눈 후, **액션 및 이미지 특성(action/image features)**을 결합합니다. 장시간 비디오 생성을 위해, **변동 마스크 인디케이터(variable mask indicator)**를 도입하였으며, 여기서 **1은 과거 프레임(history frame)**, **0은 예측 프레임(predicted frame)**을 나타냅니다.

### Scene and Action-aware Data Partition (장면 및 동작 인지 기반 데이터 분할)

우리는 **2단계 영상 분할 접근법**(장면 단위, 동작 단위)을 제안합니다.

- **PySceneDetect [4]**를 사용하여, **2~3시간 분량의 게임플레이 영상을 6초 길이의 시각적으로 일관된 클립**으로 분할합니다 (총 100만 개 이상의 1080p 클립).
- **RAFT [24]**를 통해 **옵티컬 플로우 그래디언트(optical flow gradient)**를 계산하여, **빠른 조준 등 주요 동작 경계(action boundaries)**를 감지하고, 이를 통해 **비디오 생성 훈련에 적합한 정밀한 정렬**이 가능하게 합니다.

### Data Filtering (데이터 필터링)

합성 품질을 높이기 위해 다음과 같은 필터링 단계를 수행합니다:

- **품질 평가 모듈 [17]**을 통해 **저화질 클립을 제거**,
- **OpenCV [3] 기반 휘도 필터링**을 활용하여 **어두운 장면 제거**,
- **VLM [29] 기반 그래디언트 감지**를 활용하여 **다각도의 기준에서 불필요한 클립을 종합적으로 필터링**합니다.

### Interaction Annotation (상호작용 주석)

우리는 **Monst3R [35]**를 활용하여, **6자유도(6-DoF)**의 **카메라 궤적(camera trajectory)**을 복원하고, 이를 통해 **시점 변화(이동/회전)**를 모델링합니다. 각 클립에는 **프레임 단위의 위치/자세(position/orientation)** 정보가 주석으로 포함되어 있으며, 이는 **비디오 생성 학습에서 핵심적인 역할**을 합니다.

### Structured Captioning (구조화된 캡셔닝)

비디오 설명(captioning)을 위해, **게임 특화 VLM [29]**을 이용한 **계층적(hierarchical) 캡셔닝 전략**을 구현합니다.

- (1) **30자 이내의 간결한 요약 캡션**,
- (2) **100자 이상의 상세 설명 캡션**을 함께 생성하며, 이 캡션들은 학습 중 **랜덤 샘플링(random sampling)** 되어 사용됩니다.

### 3.2 합성 데이터 구축 (Synthetic Data Construction)

우리는 선별된 3D 에셋으로부터 약 **3,000개의 고품질 모션 시퀀스**를 렌더링하였으며, **여러 출발 위치(starting positions)**를 체계적으로 샘플링하여, **다양한 카메라 궤적**(이동, 회전, 그리고 이들의 조합)을 생성하고, 이를 **다양한 속도로 재렌더링**하였습니다.

이러한 합성 데이터를 활용한 **다단계 학습 전략(multi-phase training strategy)**을 통해 다음과 같은 효과를 확인할 수 있었습니다:

- 고정밀 렌더링 시퀀스를 도입하면,
  - **시점 전환(viewpoint transition)** 동안의 **모션 예측 정확도**와 **시간적 일관성(temporal coherence)**이 크게 향상되며,
  - **복잡한 카메라 움직임에 대한 기하학적 사전 정보(geometric priors)**를 제공하여, 실제 샘플을 보완하는 데 효과적입니다.

### 3.3 분포 균형 전략 (Distribution Balancing Strategy)

우리는 **혼합 데이터셋 기반의 하이브리드 학습 프레임워크(hybrid training framework)**를 활용하여, **카메라 궤적 내 전방 이동 편향(forward-motion bias)** 문제를 다음의 **이중 전략(two-pronged strategy)**을 통해 해결하였습니다:

1. **시작점-종료점(start-end vector)**에 기반한 **층화 샘플링(stratified sampling)**을 통해, **3D 공간 내 방향 분포의 균형(balance of directional representation)**을 확보하고,
2. **시간 역전 증강(temporal inversion augmentation)**을 적용하여, **후방 이동(backward motion)**의 데이터 커버리지를 2배로 증가시켰습니다.

또한, **균일하게 분포된 렌더링 데이터를 활용한 최종 단계 파인튜닝**을 함께 수행함으로써,

- **제어 신호의 일반화 능력(control signal generalization)**,
- **학습 안정성(training stability)**,
- **방향 전반에 걸친 성능 일관성(cross-directional performance consistency)**이 모두 향상되었습니다.

## 4. 방법 (Method)

본 논문에서는 **Hunyuan-GameCraft**를 제안합니다. 이 모델은 **기존에 오픈소스화된 MM-DiT [9] 기반의 텍스트-투-비디오 모델인 HunyuanVideo [18]**를 바탕으로 한, **고동적(high-dynamic) 인터랙티브 게임 비디오 생성 모델**입니다. 전체 프레임워크는 그림 4에 나타나 있습니다.

**시간적 일관성을 유지하면서도 정밀하게 제어 가능한 게임 영상 생성(fine-grained controllable game video synthesis)**을 달성하기 위해, 우리는 다음과 같은 구성 요소를 설계합니다:

1. **다양한 게임 내 일반적인 키보드/마우스 입력**(W, A, S, D, ↑, ←, ↓, →, Space 등)을 **공유된 카메라 표현 공간(shared camera representation space)**으로 통합합니다 (4.1절). 이후, 이를 인코딩하기 위한 **경량 액션 인코더(light-weight action encoder)**를 설계하여 **카메라 궤적(camera trajectory)**을 효율적으로 표현합니다 (4.1절).
2. 우리는 **하이브리드 히스토리 조건 기반(hybrid history-conditioned)** 비디오 확장 방식을 제안합니다. 이 방식은 **기존에 제거된 노이즈 정보를 기반(historical denoised chunks)**으로, **새로운 잠재 벡터(latent)**를 **자동회귀 방식으로 제거(denoise)**하며 시퀀스를 확장합니다 (4.2절).
3. 마지막으로, **추론 속도를 가속화하고 상호작용 경험을 향상**시키기 위해, 우리는 **Phased Consistency Model [28]**을 기반으로 **모델 증류(model distillation)**를 구현합니다. 이 증류 과정은 **추론 속도를 10~20배 가속**시키며, **행동(action) 당 지연 시간을 5초 미만**으로 줄이는 데 성공하였습니다 (4.3절).

### 4.1 연속 동작 공간과 주입 방식 (Continuous Action Space and Injection)

정밀한 상호작용 효과를 위한 **세밀한 콘텐츠 제어(fine-grained control)**를 달성하기 위해, 우리는 **카메라 파라미터 공간**  
? ⊆ ℝⁿ 내에 존재하는 **직관적이고 연속적인 모션 제어**를 위한 **부분 동작 공간(subset action space) ?**를 정의합니다:

![](/assets/images/posts/578/img_5.png)

여기서,

- **?ₜₐₙₛ(?ₜᵣₐₙₛ)** 및 **?ᵣₒₜ(?ᵣₒₜ)**는 각각 **이동(translation)**과 **회전(rotation)** 방향을 나타내는 **2-구(2-sphere, ?²)** 상의 단위 벡터이며,
- **α** 및 **β**는 각각 이동 속도와 회전 속도를 제어하는 스칼라 값으로, **최대 이동 속도 vₘₐₓ** 및 **최대 각속도 ωₘₐₓ** 내에 정의됩니다. 이 값들은 **프레임 간 이동 시 상대 속도 및 각도의 변화량(differential modulus)**으로 해석됩니다.

우리는 **게임 상황에 대한 사전 지식**과 **일반적인 카메라 제어 관행**을 기반으로,

- **롤(roll) 방향의 자유도는 제거**하고
- **속도 제어 요소(velocity control)**를 포함시킴으로써, **사용자 입력 습관에 맞춘 세밀한 궤적 조작**이 가능하도록 설계하였습니다.

이러한 표현은 기존 방식과 마찬가지로 **표준 카메라 궤적 파라미터(camera trajectory parameters)** 및 **Plücker 임베딩**으로도 변환할 수 있습니다.

또한 우리는 **카메라 정보를 인코딩하기 위한 경량 네트워크(lightweight encoding network)**를 설계하였으며, Plücker 임베딩을 비디오 잠재 표현(video latent)에 정렬시킵니다.

기존 방식들이 잔차 블록(residual block) 또는 트랜스포머 블록을 사용하는 것과 달리,  
우리의 인코딩 네트워크는

- **소수의 컨볼루션 레이어(convolutional layers)**를 통한 **공간 다운샘플링**,
- **풀링 레이어(pooling layers)**를 통한 **시간 축 다운샘플링**으로 구성됩니다. 또한, **학습 가능한 스케일 계수(learnable scaling coefficient)**를 도입하여, **토큰 단위 결합 시의 상대적 가중치**를 자동으로 최적화하고 **안정적이고 적응적인 특징 융합(feature fusion)**을 달성합니다.

이후, 우리는 **MM-DiT 백본**에 카메라 포즈 제어 정보를 주입하기 위해 **토큰 추가(token addition)** 전략을 채택하였습니다.

- **이중 경량 토크나이저(dual lightweight learnable tokenizers)**를 사용하여 **비디오 토큰과 동작 토큰 간의 효과적인 피처 융합(feature fusion)**을 수행함으로써, **효율적인 인터랙티브 제어**가 가능해집니다.

관련된 **추가적인 제거 실험(ablation study)** 및 비교 분석은 **5.3절**에서 자세히 다룹니다.

**MM-DiT 백본의 강력한 멀티모달 융합 및 상호작용 능력**을 활용함으로써, 우리 방법은 **인코더 파라미터 수를 크게 줄였음에도 불구하고**, **추가 연산 비용 없이(state-of-the-art 수준의 상호작용 성능)**을 달성할 수 있었습니다.

![](/assets/images/posts/578/img_6.png)

**그림 5:** 다양한 자동회귀 기반 장기 비디오 확장 기법 비교:  
(i) 학습 없는 추론 방식,  
(ii) 스트리밍 기반 생성,  
(iii) 본 논문에서 제안하는 하이브리드 히스토리 조건 방식.

### 4.2 하이브리드 히스토리 조건 기반 장기 비디오 확장

**장시간 또는 무한 길이의 비디오를 일관되게 생성하는 것**은 **인터랙티브 비디오 생성**에서 여전히 근본적인 과제입니다.  
그림 5에서 볼 수 있듯이, 현재의 비디오 외삽(video extrapolation) 기법은 세 가지 주요 패러다임으로 나뉩니다:

1. 단일 이미지로부터의 **학습 없는 추론(training-free inference)**
2. **불균일한 노이즈 윈도우**를 가진 **스트리밍 기반 생성(streaming generation)**
3. **이전 시퀀스를 기반으로 한 청크 단위 확장(chunk-wise extension using historical segments)**

그림 6(a)에서 보이듯, **training-free 방식**은 외삽 시 **과거 맥락 정보가 부족**하여, 생성 품질의 일관성이 낮고 반복 생성 과정에서 장면 붕괴(scene collapse)가 자주 발생합니다. 또한 **스트리밍 방식**은 우리의 이미지-투-비디오 기반 모델과 아키텍처적으로 큰 부합성을 가지지 않으며, 이는 **인과적 VAE(causal VAE)** 구조가 **초기 프레임과 이후 프레임을 비균등하게 인코딩**함에 따라, 효율성과 확장성 모두에 제약을 줍니다.

이러한 한계를 해결하기 위해, 우리는 **여러 조건 정보를 혼합하여 학습하는 하이브리드 조건 기반 자동회귀 비디오 확장(hybrid-conditioned autoregressive video extension)** 방식을 제안합니다. 이 방식은 **높은 일관성, 사실성, 모델 호환성**을 동시에 달성합니다.

그림 5에 설명된 것처럼, 우리는 **각 자동회귀 단계**를 **헤드 조건(head condition)** 및 **상호작용 신호(interactive signal)**에 의해 안내되는 **청크 잠재 표현(chunk latent)의 노이즈 제거 과정**으로 정의합니다.

- 이 **청크 latent**는 **causal VAE**를 통해 생성된 **글로벌 표현(global representation)**이며, 이후 **입력 동작(action)**에 정밀하게 대응되는 **시간적으로 일관된 비디오 시퀀스**로 복원됩니다.
- **헤드 조건(head condition)**은 다음 세 가지 형태 중 하나일 수 있습니다:  
  (i) 단일 이미지 프레임 latent,  
  (ii) 이전 클립의 마지막 latent,  
  (iii) 이전의 더 긴 latent 시퀀스

우리는 **조건 정보와 노이즈 수준 모두에서의 concat 방식**을 통해 청크 latent의 고품질 노이즈 제거를 실현합니다. 이때, **이진 마스크(binary mask)**는 **헤드 latent 영역에는 1**, **청크 영역에는 0**을 부여하여 **노이즈 제거 대상을 명확히 지정**합니다.

**노이즈 스케줄 내에서는**,

- **헤드 조건은 노이즈 없는 클린 latent**로 유지되고,
- 이 클린 latent가 **후속 청크 latent를 점진적으로 denoise**하여, **새로운 클린 비디오 클립**을 생성하고 다음 생성 반복 단계로 이어지게 됩니다.

우리는 그림 6에 나타난 바와 같이, 세 가지 헤드 조건에 대해 **광범위한 실험**을 수행했습니다. 그 결과, 헤드 조건에 포함된 정보가 많을수록

- **비디오 확장 시 일관성과 생성 품질은 향상**되지만,
- **상호작용 성능은 상대적으로 감소**하는 경향이 나타났습니다.

이러한 **트레이드오프**는 학습 데이터가 주로 **분할된 장기 영상**에서 왔기 때문에 발생합니다. 즉, 후속 클립은 일반적으로 전 클립과 **움직임의 연속성**을 갖기 때문에, 강한 과거 정보(historical prior)는 예측된 다음 클립을 **과거 히스토리와 강하게 결합**시키며, 이는 **변화된 사용자 입력에 대한 반응성 감소**로 이어집니다. 그러나 동시에 **더 풍부한 참조 정보**는 **시간적 일관성과 생성 품질 향상**을 뚜렷하게 이끌어냅니다.

![](/assets/images/posts/578/img_7.png)

**그림 6:** 다양한 비디오 확장 기법 비교

- (a) 단일 이미지 기반 학습 없는 추론 → 품질 붕괴
- (b) 과거 클립 기반 → 제어력 저하
- (c) 본 논문 제안 하이브리드 조건 기반 방식 → 동작 제어력 및 히스토리 보존 능력 우수 (빨간 박스 참고)  
  (W, A, S는 각각 전진, 좌측, 후진 이동을 의미함)

![](/assets/images/posts/578/img_8.png)

**그림 7:** 테스트 벤치마크에서의 질적 비교

- **Matrix-Game**과는 **다중 동작 제어 정확도 및 장기 일관성** 측면에서 비교
- **CameraCtrl**, **MotionCtrl**, **WanX-Cam**과는 **단일 동작 제어 정확도** 비교  
  우리 실험에서는 **파란색으로 점등된 키**가 눌린 상태이며, W, A, S, D는 **이동**, ↑, ←, ↓, →는 **시야 각도 변화**를 나타냅니다.

이러한 트레이드오프 문제를 해결하기 위해, 우리는 **훈련 샘플 구성 및 층화 샘플링(stratified sampling)** 외에도 **세 가지 확장 모드를 모두 혼합하는 하이브리드 조건 기반 학습(hybrid-conditioned training)**을 제안합니다.

이 방식은 **상호작용 능력(interactive capability)**과 **생성 일관성(generation consistency)**이라는 상충 목표를 균형 있게 최적화하여, **최신 성능(state-of-the-art)**을 달성하였습니다.

또한, 이 하이브리드 방식은 실질적인 **배포(deployment) 측면에서도 이점**을 제공합니다.

- 초기 프레임 생성과 장기 비디오 확장이라는 **서로 다른 두 작업을 단일 모델로 통합**하였고,
- **아키텍처 변경 없이** 두 생성 모드 간의 **원활한 전환(seamless transition)**이 가능하게 하여,
- **유연한 제어성과 일관된 장기 비디오 생성**이 동시에 요구되는 **실제 응용 환경에 매우 적합**합니다.

### 4.3 생성 상호작용 가속화 (Accelerated Generative Interaction)

게임플레이 경험을 향상시키고, 생성된 게임 영상과의 상호작용을 더욱 빠르게 지원하기 위해, 우리는 **가속화 기술(acceleration techniques)**을 프레임워크에 통합하여 접근 방식을 확장하였습니다.

그중 특히 주목할 만한 방향은, 우리의 핵심 프레임워크를 **Consistency Models [21]**와 결합하는 것입니다. 이는 **확산 기반 생성(diffusion-based generation)**을 **가속화하는 최신(state-of-the-art) 방식**으로 잘 알려져 있습니다.

특히 우리는 **Phased Consistency Model (PCM) [28]**을 채택하였는데, 이는 원래의 **확산 과정(diffusion process)**과 **Classifier-Free Guidance**를 distillation을 통해 **8단계로 압축된 일관성 모델(consistency model)**로 구현한 것입니다.

또한, 연산 비용을 추가로 줄이고 추론 효율성을 높이기 위해, 우리는 **Classifier-Free Guidance Distillation** 기법을 도입하였습니다. 이 접근은 **외부 가이던스 메커니즘 없이**도 **학생 모델(student model)**이 **가이드된 출력(guided output)**을 직접 생성하도록 학습되며, 이때의 목적 함수(objective function)는 다음과 같이 정의됩니다:

![](/assets/images/posts/578/img_9.png)

- T\_s​는 프롬프트(prompt)를 의미하며,
- w는 guidance weight,
- z\_t​는 시점 t의 latent입니다.

이 함수는 **가이드 없는 예측값과 가이드된 목표 출력 간의 차이**를 최소화하는 방식으로 동작합니다.

이러한 통합을 통해 우리는 **최대 20배의 추론 속도 향상**을 달성하였으며, **초당 6.6 프레임(FPS)**의 **실시간 렌더링 성능**에 도달하여, 시스템의 **상호작용성(interactivity)**과 **플레이 가능성(playability)**을 획기적으로 향상시켰습니다.

## 5. 실험 (Experiment)

### 5.1 실험 설정 (Experimental Setup)

**구현 세부 사항 (Implementation Details)**  
Hunyuan-GameCraft는 **텍스트-투-비디오 기반 모델인 HunyuanVideo [18]** 위에 구축되었으며, **잠재 마스크(latent mask) 메커니즘**과 **하이브리드 히스토리 조건(hybrid history conditioning)**을 도입하여 **이미지-투-비디오 생성 및 장기 비디오 확장**을 가능하게 합니다.

실험은 총 **2단계로 나누어**, **NVIDIA H20 GPU 192대**에서 **전체 파라미터 학습(full-parameter training)**으로 수행되며, **배치 사이즈는 48**입니다.

- **1단계 학습**:
  - 전체 수집 게임 데이터와 합성 데이터를 원래 비율로 사용
  - 총 **30,000회(iterations)** 학습
  - 학습률(learning rate): **3 × 10⁻⁵**
- **2단계 학습**:
  - 3장에서 설명한 **데이터 증강 기법**을 도입해 **동작 분포 균형화**
  - 학습률을 **1 × 10⁻⁵**로 낮춰
  - **추가 20,000회** 학습을 수행하여 **생성 품질과 상호작용 성능 향상**
- 하이브리드 히스토리 조건 비율:
  - 단일 과거 클립: **70%**,
  - 복수 과거 클립: **5%**,
  - 단일 프레임: **25%**
- 시스템은 **초당 25프레임(fps)**의 속도로 동작하며, 각 비디오 청크는 **720p 해상도에서 33프레임**으로 구성됩니다.

**평가 데이터셋 (Evaluation Datasets)**  
온라인에서 수집한 다양한 소스로부터,

- **150개의 다양한 테스트 이미지**와
- **12종의 서로 다른 동작 시그널**을 기반으로 구성된 평가 세트를 구축했습니다.

이 데이터셋은 게임 장면, 스타일화된 아트워크, AI 생성 콘텐츠 등으로 구성되어 있어 **정량적 및 정성적 관점에서의 상호작용 제어 정확도 및 일반화 성능**을 모두 평가할 수 있습니다. 또한, **다양한 시나리오에서의 적응력(cross-scenario adaptability)**을 시연하기 위해 대표적인 결과들도 함께 제공합니다.

**평가 지표 (Evaluation Metrics)**  
다각적인 평가를 위해 다음과 같은 지표를 사용합니다:

- **Fréchet Video Distance (FVD) [25]**: 영상의 사실성을 정량적으로 측정
- **상대 자세 오차 (RPE)**:
  - 위치 오차 (**RPE trans**) 및 회전 오차 (**RPE rot**)
  - 예측된 궤적을 정답 궤적에 **Sim3 Umeyama 정렬**한 후 계산하여 **제어 성능** 평가
- **Matrix-Game 방식**을 따르며,
  - **영상 품질(Image Quality)** 및 **미적 점수(Aesthetic score)**로 시각 품질 평가
  - **Temporal Consistency**를 통해 **시각적·시네마틱 연속성** 평가
- **VBench [16]**에서 제안된 **Dynamic Degree** 지표를 확장하여,
  - 기존의 이진 분류 기반 대신,
  - **Optical Flow의 절대값을 직접 측정한 평균(Dynamic Average)**으로 계산
  - 보다 정밀하고 연속적인 동적 특성 평가 가능
- **사용자 연구(User Study)**를 통해 수집된 **사용자 선호도 점수**도 함께 고려

**비교 대상 모델 (Baselines)**  
우리는 다음과 같은 **4개의 대표적인 최신 기법들과 성능을 비교**합니다:

1. **Matrix-Game**: 현재 오픈소스로 공개된 최신 인터랙티브 게임 모델
2. **CameraCtrl [13]**: 카메라 제어 기반 생성
3. **MotionCtrl [31]**: 카메라 및 객체 제어 기반
4. **WanX-Cam [27]**: VideoX-Fun 기반 카메라 제어 기법

- **CameraCtrl**과 **MotionCtrl**은 **이미지-투-비디오용 SVD** 구현을 기반으로 하며,
- **WanX-Cam**은 **VideoX-Fun 구현체**를 따릅니다.

**표 2: 최신 관련 기법과의 정량적 비교 결과**  
(↑: 클수록 좋음 / ↓: 작을수록 좋음, 최상 성능은 **굵게 표시**)

![](/assets/images/posts/578/img_10.png)

### 5.2 타 방법들과의 비교 (Comparisons with Other Methods)

**정량적 비교 (Quantitative Comparison)**  
우리는 동일한 게임 시나리오 하에서 **현재 가장 앞선 오픈소스 게임 상호작용 모델인 Matrix-Game**과 포괄적인 비교를 수행했습니다. 두 모델은 동일한 기반 모델 [18]을 사용함에도 불구하고, **Hunyuan-GameCraft는 생성 품질, 동적 성능, 제어 정확도, 시간적 일관성** 등 **대다수 핵심 지표에서 의미 있는 개선**을 달성하였으며, 이는 표 2에서 확인할 수 있습니다.

특히, **동적 성능 측면에서 Hunyuan-GameCraft는 Matrix-Game을 능가**하며, **도메인 간 테스트(cross-domain test)**에서는 **상호작용 오차를 55% 감소**시키는 결과를 보였습니다. 이러한 성능 향상은 우리의 **최적화된 학습 전략**과 **조건 기반 주입 메커니즘(conditional injection mechanism)** 덕분에 가능했으며, 이로 인해 **게임 장면뿐만 아니라 다양한 아트 스타일에 걸쳐 강건한 상호작용 생성이 가능**해졌습니다.

또한 동일한 테스트 세트에 대해 **생성 품질 및 제어 정확도**를 평가하였으며, 정량적 결과는 표 2에 제시되어 있습니다. Hunyuan-GameCraft는 다른 모든 비교 모델보다 우수한 성능을 보였으며, 이는 우리의 **동작 공간(action space) 설계**가 **게임 장면 특성을 초월한 카메라 모션의 기본 원리**를 잘 포착했음을 시사합니다.

아울러, 각 베이스라인의 **추론 속도(inference speed)**도 함께 비교하였으며, 우리의 방법은 약간의 동적/시각 품질 손상은 있지만, **거의 실시간 추론에 도달할 수 있어** **실제 게임 상호작용 환경에 더 적합함**을 보여줍니다.

**정성적 비교 (Qualitative Comparison)**  
그림 7을 통해, Hunyuan-GameCraft의 정성적 우수성을 다양한 관점에서 시각적으로 확인할 수 있습니다.

- **(a) 단일 동작 시퀀스** 기반 비교에서는,
  - Matrix-Game 학습에 사용된 **Minecraft 환경**을 활용하여 양측 모델을 비교하였고,
  - **Hunyuan-GameCraft가 훨씬 뛰어난 상호작용 능력**을 보여주었습니다.
  - 특히 좌우 회전 동작의 연속성은, 우리의 **하이브리드 히스토리 조건 학습 전략**이 **역사 정보 보존 능력을 강화**했음을 잘 보여줍니다.
- **(b) 순차적 복합 동작 시나리오**에서는,
  - Hunyuan-GameCraft가 **입력된 복합 상호작용 시그널을 정확히 매핑**할 뿐 아니라,
  - **장기 비디오 확장 시에도 품질과 공간적 일관성(spatial coherence)을 유지**하여,
  - **몰입감 있는 탐색 경험(immersive exploration experience)**을 제공합니다.
- **(c) 모든 비교 대상에 대해 단일 동작 기반 이미지-투-비디오 생성 성능을 평가**한 결과,
  - **Hunyuan-GameCraft는 풍차 회전과 같은 반복적 동작의 일관성을 포함하여**,
  - **동적 표현력 및 전반적 시각 품질**에서 큰 우위를 보였습니다.

**사용자 연구 (User Study)**  
현재까지 게임 및 일반 시나리오 모두에서 **인터랙티브 비디오 생성 모델을 위한 포괄적인 벤치마크가 부족**한 상황을 고려하여, 우리는 **30명의 평가자를 대상으로 사용자 연구(user study)**를 수행하여 평가의 신뢰도를 높였습니다.

표 3에 나타난 바와 같이,  
**우리 모델은 모든 항목에서 압도적인 차이로 가장 높은 점수를 기록**하였으며, **익명 사용자들의 순위 평가**에서 가장 선호되는 모델로 나타났습니다.

**표 3: 사용자 연구 평균 순위 점수**  
(각 항목에 대해 5점은 최고, 1점은 최악을 의미. 우리 방법이 모든 측면에서 가장 높은 선호도를 보임)

![](/assets/images/posts/578/img_11.png)

### 5.3 구성 요소 분석 (Ablation Study)

이 절에서는 **데이터 분포, 제어 주입 방식, 하이브리드 히스토리 조건 기법**의 효과를 검증하기 위한 **포괄적인 실험**을 수행하였습니다.

#### **표 4: 다양한 설정에 따른 구성 요소별 제거 실험 결과**

(↓: 낮을수록 좋음 / ↑: 높을수록 좋음, DA는 Dynamic Average)

![](/assets/images/posts/578/img_12.png)

#### **데이터 분포 (Data Distribution)**

합성 데이터와 실제 게임 데이터 각각의 기여도를 이해하기 위해 구성 요소 제거 실험을 수행하였습니다.

- **합성 데이터**는 **동적 객체가 두드러지지 않으며**, 이는 **동적 장면을 생성하는 계산 비용과 복잡성** 때문입니다.
- 표 4(a)(b)에 따르면,
  - **합성 데이터만 학습한 경우 상호작용 정확도는 향상**되지만,
  - **동적 생성 능력은 크게 저하**됩니다.
  - 반면, **게임플레이 데이터만 사용한 경우 반대의 특성**을 보입니다.

우리가 채택한 **1:5 비율의 데이터 혼합 방식**은 **두 데이터 유형 간의 균형을 효과적으로 달성**하였습니다.

#### **동작 제어 주입 (Action Control Injection)**

카메라 제어 실험을 위해, Plücker 임베딩이 이미 시간적·공간적으로 비디오 latent 표현과 정렬되어 있는 점을 바탕으로,  
다음 **세 가지 카메라 제어 방식**을 실험하였습니다:

1. **Token Addition**
2. **Token Concatenation**
3. **Channel-wise Concatenation**

표 4(c)(d)(g)에 나타난 바와 같이,

- **제어 신호를 초기 단계에 단순히 추가하는 Token Addition 방식이 가장 우수한 제어 성능**을 보여주었습니다.
- 계산 효율까지 고려할 때, 최종적으로 우리는 **Token Addition 방식**을 프레임워크에 채택하였습니다.

#### **하이브리드 히스토리 조건 (Hybrid History Conditioning)**

Hunyuan-GameCraft는 **비디오 생성 및 확장을 위한 하이브리드 히스토리 조건 방식**을 채택합니다.

- **그림 6**에서는 서로 다른 조건 방식에 따른 시각적 결과를 보여주었고,
- 이 절에서는 **정량적 제거 실험 결과**를 표 4(e)(f)(g)에 제공합니다.

분석 결과:

- **단일 프레임 기반 조건**으로 학습 시,
  - 초기에는 **제어 정확도는 만족스러우나**,
  - **히스토리 정보가 부족하여** 긴 시퀀스에서는 **품질 붕괴(quality collapse)**가 발생합니다.
- 반면, **과거 클립 기반 조건**은,
  - **이전 동작과 크게 다른 제어 신호에 대해 정확도가 떨어지는 경향**을 보입니다.
- 우리의 **하이브리드 히스토리 조건**은
  - 이와 같은 **트레이드오프를 효과적으로 균형 조정**하여,
  - **우수한 상호작용 성능**, **장기 일관성**, **시각적 품질**을 **동시에 만족**시킵니다.

![](/assets/images/posts/578/img_13.png)

**그림 8:** 장기 비디오 확장 결과 – Hunyuan-GameCraft는 **분 단위의 긴 비디오 클립**을 생성하면서도 **시각적 품질을 유지**합니다.

![](/assets/images/posts/578/img_14.png)

**그림 9:** 3인칭 게임 시점에서의 인터랙티브 비디오 생성 결과

![](/assets/images/posts/578/img_15.png)

**그림 10:** Hunyuan-GameCraft는 **정확한 카메라 제어**와 함께, **고품질 및 고동적(real-world dynamic) 비디오 생성**을 실현합니다.

## 6. 현실 세계로의 일반화 (Generalization on Real Worlds)

비록 본 모델은 **게임 장면에 최적화되어 설계**되었지만, **사전 학습된 비디오 기반 모델(video foundation model)**을 통합함으로써 **일반화 성능이 크게 향상**되어, **현실 세계 도메인에서도 인터랙티브 비디오 생성**이 가능합니다.

그림 10에서 확인할 수 있듯이, **현실 세계 이미지**를 입력으로 제공하면, **Hunyuan-GameCraft는 조건부 카메라 움직임과 함께 자연스러운 동적 비디오를 성공적으로 생성**할 수 있습니다.

## 7. 한계점 및 향후 연구 방향 (Limitations and Future Work)

Hunyuan-GameCraft는 **인터랙티브 게임 비디오 생성**에 있어 **주목할 만한 성능을 보여주었지만**, 현 단계에서의 **동작 공간(action space)**은 **오픈월드 탐험(open-world exploration)** 위주로 설계되어 있으며, **사격, 투척, 폭발** 등 **다양한 게임 특화 동작들은 포함되어 있지 않습니다**.

향후 연구에서는,

- **보다 다양한 게임플레이 요소를 포함한 데이터셋을 확장**하고,
- **제어력(controllability)**, **장기 비디오 생성(long-form synthesis)**, **히스토리 보존(history preservation)**에서의 강점을 기반으로
- **물리 기반 인터랙션(physical interaction)**과 **플레이 가능성(playability)**이 높은 **차세대 모델 개발**에 집중할 계획입니다.

## 8. 결론 (Conclusion)

본 논문에서는 **Hunyuan-GameCraft**를 소개하며, 이는 **인터랙티브 비디오 생성 분야의 중요한 진전**을 이룬 프레임워크입니다.

- **통합된 동작 표현(unified action representation)**,
- **하이브리드 히스토리 조건 기반 학습**,
- **모델 증류(model distillation)** 기법을 통해
  - **정밀한 제어**,
  - **효율적인 추론**,
  - **확장 가능한 장기 비디오 생성**을 가능하게 합니다.

또한, Hunyuan-GameCraft는

- **사실감(realism)**,
- **즉각적인 반응성(responsiveness)**,
- **시간적 일관성(temporal coherence)** 측면에서도 향상된 성능을 제공합니다.

종합적으로 본 프레임워크는 기존 방법에 비해 **의미 있는 성능 향상**을 보이며, **몰입형 게임 환경에서의 실시간 활용 및 후속 연구를 위한 강력한 기반**을 마련하였습니다.
