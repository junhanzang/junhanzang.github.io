---
title: "Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms"
date: 2025-01-10 01:51:16
categories:
  - 인공지능
tags:
  - ferret-ui 2
---

<https://arxiv.org/abs/2410.18967>

[Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms](https://arxiv.org/abs/2410.18967)

**초록(Abstract)**  
플랫폼의 다양성, 해상도의 차이, 그리고 데이터 제한 등의 근본적인 문제로 인해 사용자 인터페이스(UI)를 이해하는 범용 모델을 구축하는 일은 쉽지 않습니다. 본 논문에서는 iPhone, Android, iPad, 웹페이지, 그리고 AppleTV 등 광범위한 플랫폼에서 보편적인 UI 이해를 수행하도록 설계된 멀티모달 대규모 언어 모델(MLLM)인 **Ferret-UI 2**를 소개합니다. 기존 Ferret-UI를 기반으로 한 Ferret-UI 2는 △다양한 플랫폼 유형 지원, △적응형 스케일링을 통한 고해상도 인식, △GPT-4o와 표식 세트 시각 프롬프트(set-of-mark visual prompting)를 활용한 고급 태스크 학습 데이터 생성이라는 세 가지 핵심 혁신을 제시합니다. 이러한 발전 덕분에 Ferret-UI 2는 복잡하고 사용자 중심적인 상호작용을 수행할 수 있으며, 점차 늘어나는 플랫폼 생태계의 다양성에 대한 높은 유연성과 적응력을 확보합니다. 참조(Referring)·그라운딩(Grounding), 사용자 중심 고급 태스크(9가지 하위 태스크 × 5개 플랫폼), GUIDE 다음 액션 예측 데이터셋, 그리고 GUI-World 멀티 플랫폼 벤치마크에 대한 광범위한 실험 결과, Ferret-UI 2는 Ferret-UI를 크게 능가할 뿐 아니라 강력한 플랫폼 간 전이 성능도 보임을 확인했습니다.  
†University of Texas at Austin. Apple 인턴십 기간 중 진행한 연구.

**1 서론(Introduction)**  
사용자 인터페이스(UI)는 인간-컴퓨터 상호작용에서 핵심 역할을 하며, 사용자가 디지털 시스템과 소통하는 방식을 결정합니다. 스마트폰, 태블릿, 웹 플랫폼, 스마트 TV 등이 급속도로 확산되면서 UI의 복잡도 역시 증가해왔습니다. 이러한 플랫폼 다양성이 커지고 있음에도 불구하고, 특히 멀티 플랫폼 환경에서의 UI 이해와 상호작용(Hong et al., 2023; Wang et al., 2024b; Kapoor et al., 2024)을 다루는 많은 현재 접근 방식들은 여러 한계를 보입니다.

이 분야에서 주목할 만한 연구 중 하나로 \*\*Ferret-UI(You et al., 2024)\*\*가 있습니다. 이는 UI를 지칭(referring)하고 그라운딩하는 분야를 발전시켰습니다. 그러나 Ferret-UI는 (Liu et al., 2024a)에서 제시된 “어떤 해상도라도 처리 가능(any-resolution)”한 접근 방식을 표방함에도 불구하고, 실제로는 336×672 또는 672×336으로 고정된 해상도에 묶여 있습니다. 또한 Ferret-UI는 아이폰이나 안드로이드와 같은 단일 유형의 모바일 플랫폼에만 집중하고 있어, 현재처럼 다양성이 극대화된 플랫폼 환경에서는 활용도가 제한적입니다. 예컨대 그림 1에서 보듯이 아이폰, 아이패드, 웹 UI, 그리고 AppleTV는 각각 기기의 해상도가 전혀 달라, 이들과 직접적으로 호환하여 Ferret-UI를 적용하기가 매우 까다롭습니다.

또 다른 주요 문제는 서로 다른 플랫폼별로 특화된 고품질 데이터가 충분하지 않다는 점입니다. Ferret-UI의 데이터 생성 방법은 이들 플랫폼에도 확장 가능하긴 하지만, 대부분 텍스트 기반 GPT-4 프롬프트에 의존하며 바운딩 박스가 순수하게 텍스트 형식으로만 표현됩니다. 이러한 방식에는 UI 요소 간의 시각적 정보와 공간적 관계가 부족해, 학습 데이터 품질을 저하시킵니다. 이로 인해 최종적으로 모델 성능과 효율성 역시 제한을 받게 됩니다.

![](/assets/images/posts/483/img.png)

**그림 1**: 하나의 Ferret-UI 2 모델이 UI 이해를 위해 서로 다른 네 가지 플랫폼(iPhone, iPad, 웹페이지, AppleTV)에서 상호작용하는 실제 예시입니다. 여러 단계의 상호작용을 포함한 더 많은 예시는 부록 D를 참고하세요.

이러한 한계점을 극복하기 위해, 우리는 Ferret-UI 2를 소개합니다. Ferret-UI 2는 다양한 UI 화면을 이해하고, 여러 플랫폼에서 단일 단계(single-step) 상호작용을 통해 사용자 의도에 대응하도록 설계된 멀티모달 대규모 언어 모델(MLLM)입니다. Ferret-UI(You et al., 2024)를 기반으로 발전된 Ferret-UI 2는 다음 세 가지 핵심 개선을 통해 UI 인식과 사용자 상호작용 역량을 크게 강화합니다: (1) 멀티 플랫폼 지원, (2) 동적인 고해상도 이미지 인코딩, (3) 고품질 멀티모달 학습 데이터 생성.

첫째, Ferret-UI 2는 모바일 플랫폼(iPhone과 Android)뿐만 아니라 태블릿, 웹페이지, 스마트 TV 등으로 범위를 확장합니다. 그림 1은 네 가지 대표적인 화면 유형에서 Ferret-UI 2가 사용자와 상호작용하는 시각적 예시를 보여줍니다. 이를 통해 플랫폼 다양성이 한층 넓어졌으며, 다양한 사용자 환경에 원활하게 확장 적용할 수 있게 되었습니다.

둘째, Ferret-UI 2는 any-resolution 방식(Liu et al., 2024a; Zhang et al., 2024c)을 적용해 고해상도 이미지를 인코딩합니다. 나아가 UI 스크린샷의 원본 해상도를 유지하면서 시각 인식을 강화하기 위해 개선된 적응형 그리딩(adaptive gridding) 기법을 도입했습니다. 이는 사람이 직접 수집한 바운딩 박스 주석 정보를 활용해 UI 요소를 지칭(referring)하고 그라운딩하는 정확도를 높여, UI 구성 요소와 그 관계를 더욱 세밀하게 파악할 수 있게 해줍니다.

셋째, Ferret-UI 2는 기초 과제와 고급 과제 모두에 대해 고품질 학습 데이터를 활용합니다. 기초 과제에서는 단순 referring·grounding 데이터를 대화 형식으로 변환하여, 모델이 다양한 UI 화면에 대한 기초 이해를 구축하도록 합니다. 고급 과제에서는 사용자 중심의 자유로운 대화를 다루기 위해, 텍스트 기반 GPT-4 프롬프트(바운딩 박스가 텍스트로만 표현되는 방식)를 대체하여 GPT-4o와 ‘표식 세트 시각 프롬프트(set-of-mark visual prompting)’(Yang et al., 2023)를 사용해 학습 데이터를 생성합니다. 이를 통해 UI 요소의 공간적 이해를 강화하여 더욱 높은 품질의 학습 데이터를 얻을 수 있습니다. 또한 기존처럼 “[바운딩 박스 위치]를 클릭해라” 같은 단순 지시를 따르는 대신, Ferret-UI 2는 사용자 중심의 단일 단계 상호작용을 수행합니다. 예를 들어 “제출을 확인해 주세요”라는 명령을 받았을 때, 단순 클릭이 아닌 실제 사용자가 원하는 동작의 의도를 이해하고 실행합니다. 우리의 기여 사항을 간단히 요약하면 다음과 같습니다.

- **Ferret-UI 2**를 제시합니다. iPhone, Android, iPad, 웹페이지, AppleTV를 비롯해 폭넓은 플랫폼을 지원하는 멀티모달 LLM으로, 기존 모델과 달리 학습에 활용되는 지도 데이터(instruction-tuning data)를 개선하고, 고해상도 이미지 인코딩을 도입했으며, 플랫폼별 특성에 맞춰 새롭게 설계한 referring·grounding 벤치마크를 제공합니다.
- Ferret-UI 2는 플랫폼별 UI referring·grounding 성능을 크게 향상시킵니다. 3가지 범주의 과제(Referring, Grounding, 사용자 중심 고급 과제로 구성된 9개 하위 태스크 × 5개 플랫폼)에서 Ferret-UI 2는 기존 Ferret-UI보다 뛰어난 성능을 보였으며, GPT-4o와 견줄 만한 경쟁력을 보여줍니다. 또한 Ferret-UI 2는 플랫폼 간 전이 능력도 강력하게 발휘합니다. 마지막으로 GUIDE(Chawla et al., 2024)나 GUI-World(Chen et al., 2024a) 같은 최신 벤치마크에서도 우수한 성능을 달성했습니다.

---

요약하자면 \*\*“여러 플랫폼에서 돌아가는 다양한 UI 화면을 제대로 이해하고 사용자 의도에 맞춰 동작하도록 만드는 모델을 개발하려는 것”\*\*이 Ferret-UI 2의 목표입니다. 이를 위해 **고품질 데이터(특히 UI의 시각적 요소와 공간 정보를 포함한 멀티모달 데이터)를 대량으로 생성**하고, 이 데이터를 활용해 모델을 학습시킵니다. 그렇게 학습된 Ferret-UI 2는 사용자가 “이 버튼 눌러줘”라고 하면 실제로 화면에서 그 버튼이 어디에 있는지 인식하고 눌러주거나, “설정을 열어줘” 같은 명령을 이해해 해당 기능을 수행하게 되는 식이죠.

좀 더 구체적으로는,

1. **여러 플랫폼(iPhone, Android, iPad, Web, AppleTV 등)에 대응**하기 위해, 서로 다른 해상도·레이아웃을 지닌 UI 스크린샷을 고해상도로 정확히 인식할 수 있도록 만들고,
2. \*\*사용자가 말하는 명령(“사용자 의도”)\*\*을 제대로 이해하여, 단순 클릭 이상으로 “어떤 동작이 필요한지”를 파악하고 실행할 수 있도록 설계하며,
3. 이를 위한 **학습 데이터**를 직접 생성(특히 GPT-4o + 시각 정보 활용)해 기존 모델의 한계를 뛰어넘는 성능을 추구합니다.

결국 “다양한 기기와 UI 환경에서도 **사용자가 말하는 것을 이해하고 그에 맞춰 조작**할 수 있는 범용 UI 이해 모델”을 만들고자 하는 거라고 보시면 됩니다.

---
