---
title: "VoiceLDM: Text-to-Speech with Environmental Context"
date: 2024-09-16 12:20:32
categories:
  - 인공지능
---

<https://arxiv.org/abs/2309.13664>

[VoiceLDM: Text-to-Speech with Environmental Context](https://arxiv.org/abs/2309.13664)

**초록**

본 논문에서는 두 개의 서로 다른 자연어 텍스트 프롬프트인 설명 프롬프트와 내용 프롬프트를 정확하게 따르는 오디오를 생성하도록 설계된 모델인 VoiceLDM을 제안합니다. 설명 프롬프트는 오디오의 전반적인 환경적 맥락에 대한 정보를 제공하고, 내용 프롬프트는 언어적 내용을 전달합니다. 이를 달성하기 위해, 우리는 잠재 디퓨전 모델에 기반한 텍스트-투-오디오(TTA) 모델을 채택하고, 추가적인 내용 프롬프트를 조건부 입력으로 통합하도록 기능을 확장했습니다. 사전 학습된 대비적 언어-오디오 사전훈련(CLAP)과 Whisper를 활용하여, VoiceLDM은 수동 주석이나 전사 없이도 대량의 실제 오디오 데이터로 학습되었습니다. 추가로, VoiceLDM의 제어 가능성을 더욱 향상시키기 위해 이중 분류기-프리 가이던스를 적용했습니다. 실험 결과, VoiceLDM은 두 입력 조건 모두에 잘 부합하는 그럴듯한 오디오를 생성할 수 있으며, AudioCaps 테스트 세트에서의 실제 음성의 명료도조차도 능가했습니다. 더욱이, 우리는 VoiceLDM의 텍스트-투-스피치(TTS)와 제로샷 텍스트-투-오디오 기능을 탐구하여 경쟁력 있는 결과를 달성함을 보여주었습니다. 데모와 코드는 <https://voiceldm.github.io에서> 확인하실 수 있습니다.

**색인어**— 텍스트-투-스피치, 텍스트-투-오디오, 잠재 디퓨전 모델, 스타일 제어

**1 서론**

최근 텍스트-투-오디오(TTA) 생성 분야의 발전은 정확성과 다양성 측면에서 놀라운 성능을 보여주었습니다 [1, 2, 3, 4, 5, 6, 7]. 이러한 모델들은 자연어 프롬프트가 제공하는 의미적 맥락을 정확하게 반영하는 오디오를 합성하는 능력을 입증합니다. 그럼에도 불구하고 이러한 모델들의 한계 중 하나는 음성을 생성하도록 지시받았을 때(예: "한 남자가 성당에서 말하고 있다"), 일관된 언어적 출력을 가진 오디오를 생성하는 대신 종종 알아들을 수 없는 웅얼거리는 소리를 생성한다는 점입니다.

이러한 문제를 해결하기 위해, 우리는 TTA 모델에서 영감을 받아 언어적으로 이해 가능한 음성을 생성하는 텍스트-투-스피치(TTS) 모델인 VoiceLDM을 소개합니다. VoiceLDM은 두 가지 유형의 자연어 프롬프트로 제어될 수 있는데, 하나는 발화의 언어적 내용을 지정하는 **내용 프롬프트**이고, 다른 하나는 오디오의 환경적 맥락을 설명하는 **설명 프롬프트**입니다. 우리의 작업은 텍스트-투-스피치와 텍스트-투-오디오의 교차점에 있다고 볼 수 있습니다. 우리가 아는 한, TTS 모델의 음성 명료도를 달성하면서도 TTA 모델에서 발견되는 다양한 오디오 생성 능력을 동시에 갖춘 첫 번째 연구입니다. 그 결과, 우리 모델은 사운드 이펙트가 포함된 음성, 노래하는 목소리, 속삭임 등 다양한 소리를 생성할 수 있습니다.

최근 또는 동시 진행된 TTS 연구 중에서도 생성되는 오디오의 스타일을 제어하기 위해 두 번째 텍스트 프롬프트를 활용할 수 있는 능력을 가진 것들이 있습니다 [8, 9, 10, 11, 12]. 그러나 제어 가능한 다양성은 성별, 감정, 음량 등 음성과 관련된 요인으로만 제한됩니다.

우리는 잠재 디퓨전 모델에 기반한 TTA 시스템인 AudioLDM [1]의 작업을 기반으로 합니다. 우리는 추가적인 내용 프롬프트를 조건부 입력으로 통합하여 모델을 확장했습니다. 대비적 언어-오디오 사전훈련(CLAP) [13]과 Whisper [14]를 활용하여 실제 오디오 데이터를 사용해 모델을 훈련했습니다. 이를 통해 인간의 주석 없이도 대규모의 오디오 데이터셋을 사용하여 더 나은 생성 결과를 위한 모델 훈련에 활용할 수 있었습니다.

실험 결과는 VoiceLDM이 내용 프롬프트와 설명 프롬프트 모두에 잘 부합하는 오디오를 생성함을 보여줍니다. 더욱이, VoiceLDM이 생성한 오디오는 종종 실제 오디오의 언어적 명료도를 능가합니다. 또한, 우리 모델이 일반적인 TTS나 TTA 모델로서 기능할 수 있음을 보여주며, 각 작업에서 경쟁력 있는 결과를 달성함을 입증합니다.

![](/assets/images/posts/282/img.png)

**그림 1:** VoiceLDM은 설명 프롬프트와 내용 프롬프트를 모두 따르는 오디오를 생성하여 텍스트-투-스피치와 텍스트-투-오디오 분야 사이의 격차를 메웁니다.

**2 방법**

![](/assets/images/posts/282/img_1.png)

**그림 2:** VoiceLDM의 개요. VoiceLDM은 대량의 실제 오디오 데이터로 훈련됩니다. **텍스트\_컨텐츠**는 데이터 준비 과정에서 자동 음성 인식(ASR) 모델인 Whisper로 오디오를 처리하여 생성됩니다. **텍스트\_설명**은 추론 시에만 사용됩니다. 자물쇠 아이콘이 있는 모듈은 훈련 중에 동결되어 있음을 나타냅니다.

**2.1 모델 개요**

![](/assets/images/posts/282/img_2.png)

![](/assets/images/posts/282/img_3.png)

**2.2 훈련**

![](/assets/images/posts/282/img_4.png)

![](/assets/images/posts/282/img_5.png)

![](/assets/images/posts/282/img_6.png)

![](/assets/images/posts/282/img_7.png)

**3 실험 설정**

**3.1 데이터 준비**

우리는 훈련을 위해 다음의 공개된 실제 오디오 데이터셋을 사용합니다: AudioSet [22], CommonVoice 13.0 코퍼스의 영어 부분 [23], VoxCeleb1 [24], 그리고 DEMAND [25]. 훈련 데이터셋을 준비하기 위해, 이러한 실제 오디오 데이터셋의 각 오디오를 영어 음성 구간 또는 비음성 구간으로 분류합니다. CommonVoice와 VoxCeleb의 모든 오디오는 음성 구간으로 포함하고, DEMAND의 모든 오디오는 비음성 구간으로 포함합니다.

AudioSet을 처리하기 위해, 우리는 자동 음성 인식 모델인 Whisper [14]를 활용하며, 이 모델의 두 가지 버전을 사용합니다: **large-v2**와 **medium.en**입니다. **large-v2**는 언어 식별 기능을 가진 다국어 모델이며, **medium.en**은 영어에 더 특화되어 있습니다. 우선 모든 오디오를 **medium.en**에 입력하여 전사를 생성합니다. **medium.en**으로부터 얻은 전사를 사용하여, 이해 가능한 영어 음성을 포함한 오디오와 그렇지 않은 오디오를 분류합니다. 오디오가 올바르게 분류되었는지 더 확실히 하기 위해, 음성 구간으로 분류된 오디오에 대해 추가로 **large-v2**를 사용합니다. **large-v2**의 언어 식별 기능을 사용하여 각 오디오에 대해 언어가 영어일 확률을 계산하고 전사를 생성합니다. 언어가 영어일 확률이 50%를 초과하고, **large-v2**와 **medium.en**의 전사 간 단어 오류율(WER)이 50% 미만인 경우에만 오디오를 영어 음성 구간으로 분류합니다.

오디오가 음성 구간과 비음성 구간으로 분류된 후, 우리는 음성 구간의 각 오디오에 대해 **medium.en**이 생성한 전사를 text\_cont​로 사용합니다. AudioSet과 같은 일반적인 오디오에 대해 **medium.en**이 더 정확한 전사를 생성한다는 것을 발견했기 때문에, **large-v2** 대신 **medium.en**의 전사를 사용합니다. 10초보다 긴 오디오의 경우, **medium.en**에 입력하여 전사를 생성하기 전에 오디오의 처음 10초를 사용합니다. 이미 기존 전사가 있고 길이가 10초 미만인 오디오의 경우, 더 나은 성능을 위해 제공된 전사를 사용합니다.

총 243만 개의 음성 구간과 82만 4천 개의 비음성 구간을 수집했습니다. 모든 오디오 파일은 16kHz 샘플링 레이트와 모노 포맷으로 재샘플링되었습니다. 음성 구간의 모든 오디오는 길이가 10초가 되도록 표준화되었는데, 더 긴 클립의 경우 처음 10초를 선택하고, 더 짧은 구간의 경우 제로 패딩을 통해 맞춥니다.

**3.2 모델 구성**

![](/assets/images/posts/282/img_8.png)

우리는 [1]에서 사전 훈련된 VAE와 보코더를 사용합니다. 저자들이 공개한 사전 훈련된 CLAP 모델 [13]2을 사용합니다.

2 <https://huggingface.co/laion/clap-htsat-unfused>

내용 인코더의 경우, 처음부터 Transformer 인코더를 훈련하는 것이 가능합니다. 그러나 성능 향상을 위해, 우리는 TTS를 위해 훈련된 사전 훈련된 SpeechT5 [26] 모델에서 Transformer 인코더 구성 요소를 추출합니다3.

3 <https://huggingface.co/microsoft/speecht5_tts>

**3.3 훈련 설정**

![](/assets/images/posts/282/img_9.png)

우리는 음성 구간의 오디오를 사용하여 VoiceLDM을 훈련합니다. 그러나 음성 구간이 CommonVoice에서 온 경우, 비음성 구간에서 무작위로 선택된 오디오를 0.5의 확률로 즉석에서 혼합합니다. 비음성 구간의 경우, 오디오는 10초의 길이가 되도록 무작위로 잘리거나 패딩되며, [4,20] 범위의 균등 분포에서 무작위로 선택된 신호 대 잡음비(SNR) 값으로 혼합됩니다. 그렇지 않고 음성 구간이 AudioSet이나 VoxCeleb에서 온 경우, 오디오가 이미 충분히 잡음이 있으므로 비음성 오디오를 혼합하지 않습니다.

![](/assets/images/posts/282/img_10.png)

**3.4 평가 지표**

우리는 VoiceLDM의 오디오 품질과 입력 프롬프트의 준수를 평가하기 위해 **정량적** 및 **정성적 지표**를 사용합니다.

**정량적 지표.** 우리는 Frechet Audio Distance(FAD), Kullback-Leiber(KL) 발산, 그리고 CLAP 점수를 보고합니다. 추가로, 음성 명료도를 평가하기 위해 Whisper large-v2를 사용하여 단어 오류율(WER)을 측정합니다. 또한 두 개의 Whisper 모델인 large-v2와 medium.en의 전사 간의 단어 오류율(ΔWER)을 보고합니다. ΔWER 값이 낮을수록 생성된 오디오의 음성 명료도가 높다는 것을 나타냅니다.

**정성적 지표.** 우리는 생성된 오디오의 전반적인 인상(OVL), 오디오와 조건 간의 관련성(REL), 그리고 평균 의견 점수(MOS)를 보고합니다. 정성적 평가를 위해, 우리는 크라우드소싱을 사용하여 참가자들에게 1에서 5 사이의 척도로 오디오를 평가하도록 요청합니다. 우리는 각 오디오가 최소 10명의 다른 평가자에 의해 평가되도록 합니다.

**4 결과**

**4.1 주요 결과**

![](/assets/images/posts/282/img_11.png)

**표 1:** AudioCaps 테스트 세트에서 정량적 지표로 성능 비교. ↑: 높을수록 좋음; ↓: 낮을수록 좋음.

![](/assets/images/posts/282/img_12.png)

**표 2:** AudioCaps 테스트 세트에서 정성적 지표로 성능 비교. 우리는 전체 품질(OVL), 오디오와 설명 프롬프트 간의 관련성(RELdesc), 오디오와 내용 프롬프트 간의 관련성(RELcont)을 보고합니다.

![](/assets/images/posts/282/img_13.png)

정량적 및 정성적 평가 결과는 표 1과 표 2에 나타나 있습니다. VoiceLDM은 두 입력 조건을 동시에 따르는 오디오를 생성할 수 있습니다. 가장 큰 모델인 VoiceLDM-M은 경쟁력 있는 오디오 품질과 설명 프롬프트의 준수를 유지하면서도 실제 오디오의 음성 명료도를 능가했습니다. t\_desc​를 오디오로 대체하면 환경적 맥락을 따르는 면에서 향상된 결과를 얻을 수 있으며, 높은 음성 명료도도 달성합니다. TTS에 중점을 둔 모델인 AudioLDM 2는 프롬프트에 음성과 관련된 요소 이상의 내용이 포함된 경우, 주어진 설명 프롬프트를 따르는 데 실패합니다.

**4.2 텍스트-투-스피치 기능**

![](/assets/images/posts/282/img_14.png)

5 <https://huggingface.co/facebook/fastspeech2-en-200_speaker-cv4>

**표 3:** CommonVoice 테스트 세트에서 TTS 기능의 성능 비교.

![](/assets/images/posts/282/img_15.png)

**표 3**은 TTS 평가 결과를 보여줍니다. CommonVoice 테스트 세트에 대한 평가 결과, 모든 VoiceLDM 모델이 WER 및 ΔWER로 측정된 언어적 명료도 측면에서 실제 오디오를 능가할 수 있음을 알 수 있습니다. 가장 큰 모델인 VoiceLDM-M은 가장 낮은 WER 및 ΔWER을 달성하였으며, 실제 오디오에 필적하는 자연스러움도 달성했습니다. VoiceLDM-M은 또한 모든 지표에서 FastSpeech 2와 SpeechT5를 상당한 차이로 능가했습니다.

**4.3 텍스트-투-오디오 기능**

![](/assets/images/posts/282/img_16.png)

**표 4:** AudioCaps 테스트 세트에서 TTA 기능의 성능 비교.

![](/assets/images/posts/282/img_17.png)

**표 4**의 결과는 TTA를 위해 특별히 훈련되지 않았음에도 불구하고, VoiceLDM이 TTA 모델에서 볼 수 있는 그럴듯한 오디오를 생성할 수 있음을 보여줍니다. VoiceLDM-M은 TTA를 위해 특별히 훈련된 모델인 AudioLDM-S와 비교했을 때, KL 및 CLAP 점수 측면에서 유사한 결과를 달성합니다. ac-filtered 테스트 세트에서 평가했을 때 성능 격차는 더 작아지며, CLAP 점수 측면에서는 AudioLDM-S를 능가하기도 합니다.

**4.4 이중 분류기-프리 가이던스의 효과**

![](/assets/images/posts/282/img_18.png)

**표 5:** 이중 분류기-프리 가이던스의 효과.

![](/assets/images/posts/282/img_19.png)

![](/assets/images/posts/282/img_20.png)

**5 결론**

본 논문은 환경적 맥락으로 TTS 생성을 제어하는 독특한 기능을 도입한 모델인 VoiceLDM을 소개합니다. VoiceLDM은 CLAP과 Whisper를 활용하여 방대한 양의 실제 오디오 데이터로 훈련되었습니다. 우리는 이중 분류기-프리 가이던스를 적용하여 모델의 제어 가능성을 향상시켰으며, 이를 통해 각 조건에 대한 가이던스 강도의 트레이드오프를 제어할 수 있습니다. 정량적 및 정성적 평가 결과, VoiceLDM은 TTS와 TTA 모델에서 발견되는 음성 합성과 일반 오디오 합성 기능을 동시에 달성할 수 있음을 보여주었습니다. 더욱이, VoiceLDM이 기존의 TTS 또는 TTA 모델로서도 기능할 수 있음을 보여주어, 두 분야의 일반화된 확장으로서의 위치를 확립했습니다.

[2309.13664v1.pdf

0.50MB](./file/2309.13664v1.pdf)
