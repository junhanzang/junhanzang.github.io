---
title: "Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild"
date: 2024-06-22 23:46:49
categories:
  - 인공지능
---

<https://www.projectaria.com/datasets/nymeria/>

[Nymeria Dataset](https://www.projectaria.com/datasets/nymeria/)

요약. 우리는 Nymeria를 소개합니다. 이는 여러 다중모달 자가 중심 장치를 사용하여 자연 상태에서 수집된 대규모, 다양하고 풍부하게 주석이 달린 인간 동작 데이터셋입니다. 이 데이터셋은 다음과 같은 내용을 포함합니다: a) 전신 3D 동작 기준 진실; b) Project Aria 장치의 자가 중심 다중모달 녹화 데이터(RGB, 그레이스케일, 시선 추적 카메라, IMU, 자력계, 기압계 및 마이크 포함); c) 제3자 시점을 제공하는 추가 "관찰자" 장치. 우리는 모든 센서에 대해 장치와 캡처 세션 전반에 걸쳐 월드 정렬 6자유도(6DoF) 변환을 계산합니다. 이 데이터셋은 또한 3D 장면 점 구름과 교정된 시선 추적을 제공합니다. 우리는 세부적인 자세 설명부터 원자적 행동 및 대략적인 활동 요약까지 맥락 속 인간 동작의 계층적 언어 설명을 주석화하는 프로토콜을 개발했습니다. 우리가 아는 한, Nymeria 데이터셋은 자연스럽고 다양한 활동을 포함한 세계 최대의 자연 상태 인간 동작 수집 데이터셋입니다. 이는 동기화되고 지역화된 다중 장치 다중모달 자가 중심 데이터를 제공하는 최초의 데이터셋이며, 언어 설명이 포함된 세계 최대의 동작 데이터셋입니다. 이 데이터셋은 50개 위치에서 264명의 참가자가 399km를 이동하면서 총 300시간의 일상 활동을 담은 1200개의 녹화본을 포함합니다. 동작-언어 설명은 6545개의 어휘로 구성된 310,500개의 문장에서 8,640,000단어를 제공합니다. 이 데이터셋의 잠재력을 입증하기 위해 자가 중심 신체 추적, 동작 합성 및 행동 인식을 위한 주요 연구 과제를 정의하고 여러 최첨단 기준 알고리즘을 평가했습니다. 데이터와 코드는 오픈 소스로 제공될 예정입니다.

키워드: 동작 캡처 · 자가 중심 · 다중모달 · 대규모 데이터셋

## 1. 도입

AI의 등장으로 스마트 글래스와 기타 AI 기반 웨어러블 장치 응용 프로그램이 급증하고 있습니다 [1–5,8,10,27]. 이러한 장치들은 LLM 기반 AI 어시스턴트에 원활하게 접근할 수 있을 뿐만 아니라, 즉각적이고 장기적인 개인 맞춤형 컨텍스트를 캡처할 수 있는 다중 모달 데이터 캡처 장치로 사용됩니다. 이를 통해 이러한 AI 어시스턴트는 인간 중심의 차세대 컨텍스트화된 AI로 진화할 수 있으며, 장기적으로는 AR/VR 기술과 결합하여 새로운 세대의 컨텍스트화된 컴퓨팅을 가능하게 합니다.

중요한 컨텍스트에는 착용자의 자체 동작(자가 모션)과 그들의 행동이 포함됩니다. 이러한 컨텍스트를 신체 착용 장치만으로 신뢰성 있게 추정하는 것은 매우 어려운데, 이는 일반적으로 센서 측정만으로는 전신 자세를 완전히 제어하기에 충분하지 않아 전체 몸 동작 합성 분야의 생성적 방법과 기술을 사용해야 하기 때문입니다. 실제로, 이 방향의 대부분의 연구는 기존의 훈련 데이터에 심각하게 제한을 받고 있으며, 현실감과 완전성이 부족한 시뮬레이션이나 [11,12,40,41,47,85], 규모와 다양성이 부족한 제한된 데이터 수집에 의존해야 합니다 [34, 44–46, 66, 90]. 여기에는 세 가지 주요 기술적 도전 과제가 있습니다:

- 전신 동작 기준 진실 얻기: 광학 마커를 사용하는 비전 기반 솔루션이나 [44,58,66] 다중 뷰 비디오 [30,43,50,96]는 시선 가시성에 의해 부정적인 영향을 받으며, 제한된 범위의 동작을 다루기 위해 복잡한 다중 카메라 설정이 필요합니다. 관성 기반 솔루션은 [6,9] 대안이 될 수 있지만, 오류가 누적되어 전역 정확도 면에서 열등합니다 [59, 78].
- 다중 장치 동기화: 여러 캡처 장치 또는 기준 시스템을 결합하려면 정확한 시간 및 6자유도 정렬이 필요하며, 이는 수정할 수 없거나 범용 프로토콜을 지원하지 않는 기성 솔루션을 사용할 때 어려움을 겪을 수 있습니다. 대신 대부분의 기존 데이터셋은 시각적 신호 [30,43,96] 또는 오디오 [34] 신호를 사용하여 이 문제를 해결합니다. 그러나 이러한 접근 방식은 정확성과 신뢰성에 한계가 있습니다. 특히 긴 녹화의 경우, 시계 드리프트가 누적되어 이러한 방법이 활동을 방해하고 중단시킬 수 있습니다. 결과적으로 기존 데이터셋의 평균 동작 길이는 매우 짧습니다 (cf. Tab. 2).
- 데이터 처리 및 주석: 이는 데이터셋이 최대 잠재력을 발휘하기 위해 매우 중요합니다. 원시 신체 동작, 다중 장치 공동 위치 및 3D 장면 표현 외에도, 우리는 자연어 설명이 미래 연구 방향에 중요한 역할을 한다고 믿습니다. 기존 작업은 장면 컨텍스트가 없는 간단한 설명을 제공하며 [23,31,71], 이는 일반적으로 동작 클립을 재생하여 주석자에게 보여주는 방식으로 이루어집니다. LLM 훈련을 위한 텍스트 코퍼스와 비교할 때 [13, 65, 86], 동작-언어 데이터셋의 규모는 상당히 작습니다.

![](/assets/images/posts/171/img.png)

### 표 1: Nymeria 데이터셋의 전례 없는 규모

Nymeria 데이터셋의 전례 없는 규모를 설명합니다. 우리는 자연 상태에서 20개의 시나리오가 포함된 50개 장소에서 264명의 사람들의 300시간 일상 활동을 담은 1200개의 시퀀스를 캡처했습니다. 이 데이터는 2억 6천만 개의 신체 포즈, 2억 120만 개의 이미지, 117억 개의 IMU 샘플, 1080만 개의 시선 포인트를 제공합니다. 머리 이동 궤적은 399km를 초과합니다. 동작-언어 설명은 6545개의 어휘로 구성된 864만 단어에서 31만 500개의 문장을 제공합니다.

![](/assets/images/posts/171/img_1.png)

### 표 2: 공개 날짜별 인간 동작 데이터셋

**표 2**는 공개 날짜에 따른 인간 동작 데이터셋을 보여줍니다. 2열에서 5열은 각각 녹화된 활동 시간(q/h), 신체 포즈 수 (백만 프레임 단위, p/M), 평균 시퀀스 길이(분 단위, μ/m), 참가자 수(pp)를 나타냅니다. 이후 우리는 자연어 주석이 달린 동작 데이터의 설명 수(nl/K)와 어휘 크기(voc)를 비교합니다. 나머지 열들은 다음과 같은 기능을 다룹니다: 기준 진실 동작을 위한 파라메트릭 인간 모델과 메쉬(pm)를 사용, 자가 중심의 머리에 장착된 장치(hd)를 사용, 제3자 시점(3p)을 사용, 자가 중심의 손목 착용 장치(wd)를 사용, 전역 동작 위치 지정(ga)을 사용, 실외 시나리오(od)를 포함, 시선 추적(gz)을 포함, 3D 장면 표현(sr)을 포함, 다중 사람 시나리오(mp)를 포함, 인간 간 상호작용(hh)을 포함. 참고로 EgoExo4D는 논문에서 1422시간을 보고하며, 이는 카메라로 녹화된 시간을 합산한 것입니다. 총 활동 시간은 180시간이며, 88.8시간은 MSCOCO 키포인트로 주석이 달렸습니다. 밑줄로 표시된 숫자는 전체 데이터셋에 대해 보고된 것입니다.

연구의 격차를 메우고 가속화하기 위해, 우리는 Nymeria 데이터셋을 소개합니다. 이는 세계 최대의 자연 상태에서 수집된 인간 동작 데이터셋으로, 풍부하고 다중 모달의 자가 중심 캡처와 주석이 포함되어 있습니다. 표 1은 우리 데이터셋의 핵심 통계를 요약한 것입니다. 이 데이터셋은 50개의 장소에서 264명의 참가자로부터 300시간 동안 2억 6천만 개의 포즈를 포함하고 있습니다. 일상 활동에 중점을 두어, 우리는 사람들이 환경 및 타인과 상호 작용하는 다양성과 진정성을 포착하기 위해 20개의 실내 및 실외 시나리오를 설계했습니다. 예를 들어 친구와 게임/스포츠를 하는 것, 카페테리아에서 식사하는 것, 사무실에서 일하는 것, 숲에서 하이킹하는 것, 도로에서 자전거를 타는 것 등이 있습니다. 모든 녹화는 15분 길이로, 자연스러운 활동과 즉흥적이고 스크립트 없는 행동과 상호 작용을 충분히 포착합니다.

우리는 전신 동작을 캡처하기 위해 관성 기반 XSens Mocap [6] 장비를 사용합니다. 모든 녹화에서 RGB, 그레이스케일, 시선 추적(ET) 비디오, 다중 채널 오디오, 관성 측정 장치(IMU), 자력계 및 기압계의 원시 데이터를 수집하기 위해 Project Aria [27] 안경을 사용합니다. 양쪽 손목에서도 비슷한 다중 모달 데이터를 수집하기 위해 Project Aria 안경의 전자 장치와 센서를 손목에 착용할 수 있도록 재구성한 손목 장치(miniAria)를 사용합니다. 모든 장치와 각 세션의 전체 기간 동안 서브 밀리초 수준의 시간 동기화를 달성하기 위해 비침습적인 맞춤형 하드웨어 솔루션을 사용합니다. 우리는 Project Aria Machine Perception Service (MPS) [7]를 사용하여 헤드셋과 손목 장치의 정확한 6자유도 장치 궤적, 3D 반밀도 점 구름 및 교정된 시선을 얻고 모든 것을 하나의 단일 미터 세계 좌표계에 정렬합니다. 추가로 XSens 동작도 최적화를 통해 드리프트를 보정하여 동일한 기준에 등록합니다. 전체적으로 이 데이터셋은 참가자들이 총 399km를 이동하며 2억 120만 개의 자가 중심 이미지, 117억 개의 IMU 샘플 및 1,080만 개의 시선 포인트를 캡처했습니다.

Nymeria 데이터셋은 세계 최대의 동작-언어 데이터셋으로도 돋보입니다. 주석자들은 녹화된 세션에서 렌더링된 자가 중심 보기, 제3자 보기 및 인간 동작의 동기화된 재생을 관찰하여 맥락 속 내레이션을 생성했습니다. 우리는 다양한 세부 수준에서 설명을 얻을 수 있도록 계층적 주석 체계를 설계했습니다. 총 38.6시간의 세부적인 신체 포즈, 207시간의 단순화된 원자적 행동 및 196시간의 활동 요약을 주석으로 달았습니다. 총 설명은 65,45개 어휘로 구성된 31만 500개의 문장에서 864만 단어를 포함하고 있습니다.

### 2 관련 연구

#### 2.1 인간 동작 데이터셋

**대규모 동작 데이터셋:** 대규모 데이터셋은 실제 및 합성 데이터를 통해 개인 및 그룹의 움직임을 분석하는 데 귀중한 3D 주석을 제공합니다. AMASS [58]는 15개의 모캡 데이터셋을 통합하여 통일된 SMPL [52] 형식을 제공하여 광범위한 동작 분석을 용이하게 합니다. 3DPW [59]는 IMU와 모바일 카메라를 사용하여 최초의 다인용, 자연 상태 데이터셋을 3D 키포인트로 구성합니다. DivaTrack [90]는 동기화된 신체 동작 데이터와 IMU 신호를 통합하여 다양한 신체 유형과 동작 클래스를 지원합니다. GTA-V 엔진을 활용한 GTA-Human [15]은 풍부한 3D 렌더링 동작 배열을 합성하여 보통 실내에서 수집되는 실제 데이터를 보완합니다. 상호작용에 중점을 둔 ParaHome [44]과 HPS [34]는 자가 중심 및 제3자 시점에서 인간의 역학을 포착합니다. ParaHome [44]은 통일된 공간에서 정확한 가정 활동과 물체 이동을 기록하며, HPS [34]는 3D 장면 스캔, 비디오, IMU 데이터 및 환경과 상호 작용하는 인간의 재구성을 포함하는 방대한 자가 중심 데이터셋을 구성하여 상호작용 인식 및 합성 연구를 풍부하게 합니다.

**자가 중심 다중 모달 동작 데이터셋:** 1인칭 인식 연구를 발전시키기 위해 여러 자가 중심 비디오 데이터셋 [20–22,29]이 개발되었으며, 이는 손-물체 상호작용 및 동작 인식에 중점을 두지만 종종 3D 기준 데이터는 생략됩니다. 이러한 격차를 해결하기 위해, 인간 활동을 3D 장면 내에서 더 잘 포착하기 위해 포괄적인 다중 모달 센서 데이터와 주석이 포함된 데이터셋이 도입되었습니다. UnrealEgo [96]은 동적인 3D 환경에서 자연스러운 단일 인물 스테레오 이미지와 시퀀스를 제공합니다. EgoBody [96]는 최초로 자가 중심 및 제3자 시점을 제공하는 방대한 데이터셋으로, 개인 및 장면 모두에 대해 상세한 3D 주석을 제공합니다. EgoHumans [43]는 자가 중심 시점에서 다인용 3D 활동의 첫 번째 수집을 제공합니다. Ego-Exo4D [30], AEA [56, 57] 및 ADT [66]는 숙련된 또는 일반적인 일상 활동을 문서화하여 이 분야를 더욱 풍부하게 합니다.

**동작-언어 데이터셋:** 인간의 동작 의미를 이해하는 것은 주석 깊이가 다양한 의미적 레이블이 있는 데이터셋을 통해 향상됩니다. 일부 모션캡처(mocap) 데이터셋 [14,28,33,35,37,80]은 제한된 동작 범주로 희소한 주석을 제공하는 반면, 다른 데이터셋은 상세한 의미적 레이블을 제공합니다. BABEL [72]은 크라우드소싱을 통해 AMASS [58] 시퀀스에 밀도 높고 정확하게 정렬된 동작 레이블을 추가합니다. KIT Motion-Language Dataset [71]은 이동 동작에 대한 완전한 자연어 설명을 처음으로 제공합니다. HumanML3D [31]은 HumanAct12 [33] 및 AMASS [58] 동작 클립을 세 개의 고유한 텍스트 설명으로 풍부하게 합니다. Motion-X [50]는 얼굴 표정과 손 제스처를 포함한 3D 전신 동작의 포괄적인 수집물로, 시퀀스와 프레임 수준에서 세부적인 텍스트 주석을 포함한 첫 번째 데이터셋입니다.

**표 2**는 개요 요약을 보여줍니다. 기존의 인간 동작 데이터셋은 자가 중심 장치 없이 캡처되었거나 [18, 31, 44, 50, 58, 90], 규모가 제한적이며 [18,34,43,44,56,66,96], 동작 표현을 위한 파라메트릭 인간 모델이 부족합니다 [30]. Nymeria 데이터셋은 모든 축에서 향상시켜 이 분야를 발전시킵니다.

### 2.2 자가 중심 동작 이해

#### 모션 센서를 통한 신체 추적 및 합성

IMU 기반 모캡 시스템은 실제 시나리오에서 동작을 포착하기 위해 널리 채택되고 있습니다. 실무에서는 더 적은 수의 IMU를 사용하여 추적하는 것이 선호되지만, 이는 정식화되지 않은 신체 추적 문제를 야기합니다. 희소성과 노이즈 문제를 해결하기 위해 최근의 방법들은 대규모 데이터셋으로부터 모션 모델을 학습하기 위해 데이터 기반 접근 방식을 활용합니다. DIP [36]는 AMASS [58]에서 합성된 IMU 측정을 기반으로 훈련된 양방향 RNN을 사용한 프라이어를 사용합니다. Transpose [93]는 추적 정확도를 더욱 향상시키고 루트 번역 추정 문제를 해결합니다. PIP [92]는 물리층을 추가하여 관절 방향과 루트 모션 예측을 정제합니다. TIP [41]는 정지점을 예측하여 루트 모션 추정을 개선합니다. 일부 이전 방법들은 특정 기성품 IMU 장치에 맞춰 추적 시스템을 조정하며, 일부는 머리와 손을 위한 AR/VR 장치를 활용합니다 [39, 40, 90], 반면 다른 방법들은 모바일 장치를 사용합니다 [60]. DiffusionPoser [89]는 미리 훈련된 확산 모델에서 인페인팅 기법을 사용하여 추론 중에 IMU 수를 결정하는 반대 접근 방식을 취합니다. 대안으로, 희소한 IMU를 다른 자가 중심 모달리티와 결합하여 추적 정확도를 향상시킬 수 있습니다. EgoLocate [91]는 IMU와 단안 카메라를 결합하여 동작 추적, 장치 위치 추정 및 매핑을 수행합니다. MocapEveryone [46]은 두 개의 손목 IMU와 머리 장착 카메라를 사용하여 다양한 환경에서 3D 인간 동작을 캡처합니다.

#### 텍스트로부터의 동작 합성

텍스트 조건부 동작 합성은 BABEL [72] 및 HumanML3D [31]와 같은 주석이 달린 인간 동작 데이터셋의 가용성으로 인해 활발한 연구 분야입니다. 초기 접근 방식은 텍스트 설명으로부터 다양한 동작을 생성하기 위해 변이 오토인코더(VAE)를 활용합니다 [32, 70]. 이후 연구는 벡터 양자화 변이 오토인코더(VQ-VAE)를 사용하여 인간 동작을 토큰화하고 텍스트로부터 동작을 합성하는 법을 학습합니다 [38, 98]. MotionClip [82]은 변형기 기반 오토인코더를 사용하여 CLIP [73]과 정렬된 잠재 공간을 학습함으로써 표현 학습을 탐구합니다. 확산 모델의 등장과 함께, 연구자들은 간단한 훈련 과정을 통해 텍스트-동작 합성에서 더 높은 품질과 다양성을 달성할 수 있음을 보여주었습니다 [19, 79, 83]. 진행 중인 연구는 공간 제약을 통합하여 이러한 텍스트 조건부 확산 모델을 정제하고 [42], 더 긴 동작 시퀀스를 생성하며 [83, 99], 생성 속도와 품질을 향상시키기 위해 잠재 공간을 학습하고 [17], 세밀한 텍스트 설명에 응답하여 동작을 향상시킵니다 [94, 95]. 기존의 텍스트 설명이 포함된 인간 동작 데이터셋과 달리, Nymeria 데이터셋은 다양한 시나리오의 계층적 언어 설명을 포함합니다. 우리는 우리 데이터셋이 제공하는 전례 없는 다양성이 이 분야에서 획기적인 발전을 일으킬 주요 요소가 될 것이라고 믿습니다.

### 3 Nymeria 데이터셋

우리의 목표는 실제 환경에서 장기간의 다양한 일상 활동을 기록하는 것입니다. 모션 캡처와 헤드셋 외에도 손목에 착용하는 센서가 현재의 AR/VR 헤드셋과 매우 유사하며 자가 중심 신체 추적을 더 잘 제어할 수 있음을 보여주는 손목 장치도 고려합니다. 나머지 섹션에서는 캡처 설정, 녹화 프로토콜, 시나리오 및 핵심 통계를 자세히 설명합니다.

![](/assets/images/posts/171/img_2.png)

그림 2: 캡처 설정. 왼쪽에서 오른쪽으로: (a) 모캡 수트, 프로젝트 아리아 안경, 미니아리아 손목 밴드 2개, 동기화 장치를 착용한 참가자, (b) 웨어러블 기록 장치 세트, (c) 프로젝트 아리아의 센서 세트, (d) 미니아리아 손목 밴드와 손목에 착용하는 방법.

#### 3.1 데이터 캡처 시스템

**하드웨어:** 캡처 설정은 그림 2로 설명됩니다. 데이터를 기록하기 위해 참가자는 모캡 수트, Project Aria 안경 한 쌍, 두 개의 손목밴드, 그리고 동기화 장치를 착용합니다. 우리는 모션 캡처를 위해 17개의 관성 트래커와 자력계가 연결된 XSens MVN Link [6]를 사용합니다. 다른 관성 기반 시스템과 비교하여 MVN Link는 장치 내 녹화를 지원하여 자연 상태의 모션 캡처에 이상적입니다. 다중 모달 자가 중심 데이터를 기록하기 위해 Project Aria 안경 [27]을 가벼운 헤드셋으로 사용합니다. 이 센서 스위트에는 1개의 RGB 카메라, 2개의 그레이스케일 주변 카메라, 2개의 ET 카메라, 2개의 IMU, 1개의 기압계, 1개의 자력계, 7개의 마이크로폰, 1개의 온도계, GNSS, WiFi 및 BT가 포함됩니다. 미래의 웨어러블 장치와 AR/VR 설정을 닮게 하기 위해, 우리는 전자 장치와 센서를 재포장하여 miniAria라는 새로운 손목 착용 장치를 만들었습니다. miniAria 손목밴드는 마이크로폰, 기압계, 자력계 및 ET 카메라를 제외한 Project Aria 안경의 센싱 기능을 일치시킵니다. 센서 구성 및 녹화 프로필에 대한 자세한 내용은 부록 A에 제공됩니다.

**동기화:** Project Aria는 동기화를 돕기 위해 외부에서 제공된 시간 신호를 기록할 수 있습니다. 우리는 MVN Link가 동일한 신호를 수용할 수 있는 능력을 추가로 개발했습니다. 모든 장치를 시간 정렬하기 위해, 동기화 장치를 개발하여 Project Aria 안경, miniAria 손목밴드 및 모캡 수트에 시간 신호를 공급합니다. 이 장치는 무선 서버에서 시간 신호를 수신할 수 있으며, 이는 여러 Project Aria 장치 간의 시간 정렬을 서브 밀리초 정확도로 가능하게 합니다. 모캡과 Aria 간의 동기화는 1 모션 프레임 이내, 즉 4.2ms 내에 이루어집니다. 여러 사람을 동시에 캡처하기 위해, 각 참가자마다 설명된 설정을 복제하고 공통 시간 서버를 활용합니다.

![](/assets/images/posts/171/img_3.png)

그림 3: 다양한 사람들의 다양한 시나리오. 다양한 참가자가 다양한 장소에서 다양한 실내 및 실외 활동을 수행하는 모습을 보여줍니다. 각 하위 그림의 왼쪽 상단, 왼쪽 하단 및 오른쪽 보기는 각각 자기 중심 RGB 카메라 보기, 보조 3인칭 보기 및 모션을 보여줍니다.

![](/assets/images/posts/171/img_4.png)

그림 4: 성별 인구 통계. 참가자는 여성과 남성이 거의 비슷하게 나뉘어 있습니다. 인종은 백인(Cauc.), 히파닉, 아프리카계 미국인(AA), 동아시아인(EA), 남아시아계(SA), 동아시아 남부(SEA) 및 혼혈을 포함합니다.

### 3.2 녹화 프로토콜

**물류:** 우리는 성별, 연령, 키, 체중 등의 인구통계학적 특성을 고르게 분포하도록 다양한 참가자를 모집합니다. 녹화 중에는 Aria 안경을 착용한 관찰자가 참가자와 일정 거리를 두고 이동하는 카메라 역할을 합니다. 이 제3자 전체 관점은 동작-언어 내레이션을 향상시키기 위해 중요한 추가 컨텍스트를 제공합니다. 참가자와 관찰자 외에도 현장에서 수집을 관리하는 두 명의 운영자가 있습니다. 모든 사람들은 시나리오에 따라 자연스럽게 상호작용하여 연출된 동작과는 대조적으로 풍부한 역동성을 이끌어냅니다. 수집을 확대하기 위해 운영자들은 각 녹화를 완료하기 위해 3단계 절차를 따르도록 훈련받습니다. 이는 참가자가 XSens를 위한 <1분 동작 보정 수행, 시선 추정을 위한 <1분 시선 보정, 그리고 15-20분 활동을 수행하는 것으로 시작됩니다. 한 참가자 수집의 경우, 각 사람은 반나절 동안 5가지 시나리오를 수행합니다. 두 명의 참가자가 참여하는 세션의 경우, 사람들은 하루 종일 고용되어 8가지 시나리오를 협력하여 수행합니다. 대량의 데이터는 가정에서 캡처됩니다. 각 장소는 2-4일 동안 임대됩니다.

**시나리오:** 우리는 일반적인 일상 활동의 20가지 시나리오를 정의합니다 (정의는 부록 C.1 참조). 일반적인 실내 시나리오에는 물건 찾기, 집 탐험, 요리, 식사, 작업, 엔터테인먼트 등이 포함되며, 실외 활동에는 하이킹, 자전거 타기, 스포츠, 정원 가꾸기 등이 포함됩니다. 자연스럽고 진정한 동작을 기록하기 위해 참가자들에게 높은 수준의 지침을 제공합니다. 예를 들어, "다음 15-20분 동안 카페테리아에서 음식을 가져와 파티오에서 먹겠습니다." 대부분의 시나리오는 인간 상호작용을 장려하며, 이는 운영자가 상황에 맞는 활동을 유도하여 역동성을 증가시킬 수 있도록 합니다. 그림 1과 그림 3은 다양한 시나리오의 예를 시각화합니다.

### 4 데이터 처리 및 주석

데이터 처리는 세 단계로 이루어집니다. 먼저, XSens 소프트웨어를 사용하여 스켈레톤 모션을 얻고, Project Aria MPS [7] (부록 B.1 참조)을 사용하여 장치 위치 추정, 장면 표현 및 시선 추정을 수행합니다. 다음으로, 모션을 파라메트릭 인간 모델에 리타겟팅하고 최적화를 통해 Aria 장치의 좌표에 등록합니다. 마지막으로, 데이터에 언어 설명을 주석으로 추가합니다.

#### 4.1 전신 모션 캡처 및 리타겟팅

우리는 권장 절차를 따라 240Hz로 모션을 기록합니다:

1. 참가자의 신체 치수를 신중하게 측정합니다.
2. 매 녹화 전에 보정을 수행합니다.
3. 단일 또는 다층 활동을 명시하여 최고 XSens 품질로 처리합니다.

![](/assets/images/posts/171/img_5.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/171/img_6.png)

![](/assets/images/posts/171/img_7.png)

#### 최종 결과

최적화가 완료되면, 모델의 팔꿈치와 손목의 위치가 실제 측정 데이터와 최대한 일치하게 됩니다. 이를 통해 모션 캡처 데이터로부터 정확한 3D 스켈레톤 모델을 생성할 수 있습니다.

이 예시는 단순화된 팔 움직임에 대한 것이지만, 동일한 원리가 전신 모션 캡처에도 적용됩니다. 각 관절의 위치와 회전 각도를 조정하여 전체 신체 동작을 정확하게 재현합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 4.2 6자유도(6DoF) 위치 추정 및 매핑

동일한 위치에서 기록된 데이터는 Project Aria MPS [7]를 통해 단일 측정 3D 월드로 전역 정렬됩니다. 이 시스템은 최신 비주얼 관성 주행 거리계(VIO), SLAM 및 매핑 알고리즘을 사용합니다 [26,61,63]. 요약하자면, 먼저 각 개별 녹화에 대해 독립적으로 SLAM을 실행합니다. 그 후, 결과 맵을 루프 클로징하고 시각-관성 번들 조정을 통해 공동으로 최적화합니다. 출력 결과는 매우 정확한 1KHz 궤적이며, 이는 예를 들어 머리와 손목의 움직임 클러스터를 각각 시각화할 수 있게 합니다.

![](/assets/images/posts/171/img_8.png)

#### 그림 5: 위치별 전역 정렬된 궤적 및 포인트 클라우드

우리는 정원과 함께 약 5시간의 녹화를 포함하는 다층 주택의 예를 보여줍니다. 왼쪽은 누적된 궤적의 상단 뷰를 보여주며, 빨간색, 녹색 및 파란색은 각각 머리, 왼손목 및 오른손목을 나타냅니다. 오른쪽에는 인간 3D 동작 분포에서 클러스터가 나타나는 클로즈업 뷰를 샘플링하여 보여줍니다.

![](/assets/images/posts/171/img_9.png)

#### 그림 6: 관찰자 뷰로 투영된 XSens 스켈레톤

우리는 1.5km 이동 궤적에 걸쳐 20분 동안 녹화된 포즈를 샘플링하고, 로컬라이즈된 XSens 스켈레톤을 관찰자의 RGB 뷰에 투영하여 다중 장치 위치 추정, 동작 등록 및 시간 동기화의 끝에서 끝까지의 질적 평가를 수행합니다. 주제는 일반 코트 아래에 XSens 수트를 착용하고 있습니다.

![](/assets/images/posts/171/img_10.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 예시: 실제 값을 사용한 두 장치 간의 궤적 정렬

#### 설정 및 준비

1. **XSens와 Aria 장치 사용**:
   - XSens와 Aria 장치를 사용하여 데이터를 수집합니다.
   - XSens는 신체의 여러 관절에 부착된 센서를 통해 데이터를 수집합니다.
   - Aria는 머리에 착용되어 머리의 움직임을 기록합니다.

![](/assets/images/posts/171/img_11.png)

![](/assets/images/posts/171/img_12.png)

![](/assets/images/posts/171/img_13.png)

![](/assets/images/posts/171/img_14.png)

![](/assets/images/posts/171/img_15.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/171/img_16.png)

그림 7: 다단계 주석 예시. 위에서부터 아래까지 모션 내레이션, 원자 동작 및 활동 요약에 대한 결과를 보여줍니다.

### 4.3 맥락 속 동작의 언어 설명

신체 동작, 자연어 및 활동 인식 간의 연결을 구축하기 위해, 우리는 인간 주석자들에게 데이터셋의 재생 비디오를 보면서 맥락 속 인간 동작에 대한 텍스트 설명을 작성하도록 요청합니다. 주석자들은 비디오를 클립으로 나누고 미리 정의된 질문에 답함으로써 설명을 작성합니다. 주석자들이 동작에 대한 전체적인 이해를 얻을 수 있도록, 재생 비디오는 자가 중심 비디오, 제3자 비디오 및 3D 장면으로 렌더링된 인간 동작의 동기화된 뷰를 포함합니다.

우리는 서로 다른 수준의 세부 사항을 다루기 위해 세 가지 주석 작업을 정의하여 인간의 노력을 의미 있게 확장합니다. 가장 세밀한 수준은 동작 내레이션으로, 상세한 신체 자세를 설명하는 작업입니다. 예를 들어, 동작 방향, 속도, 상체와 하체가 다른 사람 및 환경과 어떻게 상호작용하는지, 집중하는 대상 등을 설명합니다. 다음 수준의 주석은 원자적 행동입니다. 정교한 설명을 포함한 동작 내레이션과 비교하여, 주석자들은 가능한 한 동작 동사를 사용하도록 권장됩니다. 예를 들어, "다리를 약간 벌리고 몸을 오른쪽으로 돌리면서 양팔을 흔들다" 대신 "춤추기"로 설명하는 것입니다. 마지막으로, 우리는 활동 요약을 정의합니다. 이전 작업이 3-5초의 스니펫을 생성하는 것과 달리, 요약은 비디오를 15-30초 클립으로 나누어 고수준 활동의 한 문장 요약을 작성합니다.

그림 7은 각 주석 작업의 예를 제공합니다. 이 그림은 세 가지 동기화된 뷰를 모두 갖는 이점을 보여줍니다. 자가 중심 뷰는 손-물체 상호작용과 환경에 대한 클로즈업 관찰을 제공하고, 제3자 뷰와 동작 렌더링은 주석자들이 동작에 대한 전체적인 이해를 얻는 데 도움이 됩니다.

![](/assets/images/posts/171/img_17.png)

#### 그림 8: 언어 설명의 분포

워드 클라우드는 각각의 작업에 대한 주석과 모든 데이터를 결합한 주석을 시각화합니다. 튜플 (N, X, Y, Z)은 N 시간의 데이터가 X개의 문장, Y개의 단어, Z개의 어휘 크기로 설명되었음을 의미합니다.

### 4.4 데이터 처리 요약

우리는 XSens 데이터를 수동으로 검사하여 품질이 낮은 녹화를 식별합니다. 우리는 92%의 양질의 비율을 달성했으며, 이는 1200개의 녹화로 구성됩니다. 실패 사례는 주로 부정확한 신체 측정, 보정 및 케이블 연결과 같은 수동 작업 오류로 인해 발생합니다. MPS는 매우 견고하여 녹화의 작은 부분만 간헐적으로 추적을 잃으며, 구체적으로 헤드셋은 0.38%, 손목밴드는 6.3%의 녹화에서 추적을 잃습니다. 대부분의 경우 추적은 곧 재개됩니다. 손목밴드 시점에서 발생하는 빠른 동작과 상당한 가림 현상이 더 높은 오류율을 초래할 것으로 예상됩니다. 시나리오 및 위치별 세부 통계는 부록의 표 7 및 표 8에 나와 있습니다.

언어 설명의 경우, 우리는 38.6시간의 동작 내레이션, 207시간의 원자적 행동, 196시간의 활동 요약을 주석으로 달았습니다. 내레이션 및 원자적 행동의 평균 비디오 세그먼트 길이는 5초이고, 요약의 경우 30초입니다. 전체적으로, 데이터셋은 6545개의 고유 단어로 구성된 310,500개의 문장과 864만 단어를 제공합니다. 문장당 평균 단어 수는 27.8로, 기존의 동작-언어 내레이션보다 상당히 깁니다. 그림 8은 언어 분포를 시각화합니다.

![](/assets/images/posts/171/img_18.png)

표 3: 실제 입력과 합성 입력을 사용한 전신 3점 트래킹 기준선 AvatarPoser[40].

![](/assets/images/posts/171/img_19.png)

표 4: 확산 기반 모션 생성 기준선(1점 EgoEgo 및 3점 BoDiffusion 포함).

### 5 벤치마크 작업 및 기준선

우리가 수집한 방대한 양의 다양하고 맥락적인 동작 데이터는 독특한 다중 모달 자가 중심 감각 입력과 풍부한 주석 지도를 통해 동작 추적, 합성 및 이해 작업에 대한 솔루션을 구축할 수 있는 전례 없는 기회를 제공합니다. 우리는 세 가지 일반적인 문제에 대한 기준 결과를 제시하지만, 이는 우리 데이터셋의 잠재력을 일부만 다룹니다.

#### 5.1 희소한 입력으로부터의 동작 생성

사용자가 VR 헤드셋을 착용하고 선택적으로 컨트롤러를 들거나, AR 안경을 착용하고 선택적으로 손목밴드를 착용할 때, 우리는 온보드 센서만으로 그들의 전체 신체 움직임을 재구성하고자 합니다. 이는 "원포인트" [48,55] 또는 "쓰리포인트" [16,25,40] 추적 문제라고 합니다. 기존 연구는 모션 캡처 데이터셋에서 파생된 합성 센서 입력에만 의존할 수 있습니다. Nymeria는 모델 훈련과 정량적 평가를 위한 실제 센서 입력과 신체 동작 기준 진실의 첫 번째 대규모 데이터셋을 제공합니다. 또한 TotalCapture [87] 또는 DivaTrack [90]과 같은 데이터셋과 달리, Nymeria는 머리 및 손목 장치의 실제 IMU 데이터를 제공합니다. 결과적으로, 미래의 연구는 IMU 및 기타 온보드 센서와 함께 6D 장치 포즈를 증강하거나 [90], 카메라 뷰 [48, 54]를 활용할 수 있으며, 또한 우리 데이터셋의 풍부한 주석을 자가 중심 동작 생성의 맥락적 조건으로 사용할 수 있습니다.

기준선으로서, 우리는 세 가지 최첨단 모델을 평가합니다: AvatarPoser [40] (쓰리포인트 회귀 모델), BoDiffusion [16] (쓰리포인트 확산 모델), 및 EgoEgo [48] (원포인트 확산 모델). 표 3은 실제 및 합성된 쓰리포인트 입력으로 AvatarPoser의 재구성 오류를 평가합니다. 실제 센서 입력을 사용할 때 작은 차이가 있지만 눈에 띄는 품질 저하가 있음을 알 수 있습니다. 표 4는 두 확산 기반 모델의 모션 품질을 추가로 평가하며, 모션 FID 점수 [33]를 측정하여 평가합니다.

![](/assets/images/posts/171/img_20.png)

표 5: 모션 제거 VQVAE [mm]. 제품 정량화가 성능 향상에 가장 크게 기여하고, 코드북 크기를 늘리면서 잠재적 차원을 줄이는 것이 그 뒤를 이었습니다.

![](/assets/images/posts/171/img_21.png)

표 6: 모션 이해의 정량적 결과. 자연어 사전 학습이 부족했던 TM2T는 T5[75]를 언어 백본으로 채택한 MotionGPT보다 성능이 떨어집니다. 이는 내레이션의 복잡성과 다양성을 나타냅니다.

### 5.2 생성적 동작 프라이어

우리 데이터셋의 데이터 양과 범위는 인간 동작의 기초 모델을 구축하기에 이상적입니다. 이전의 자세 [68, 84] 또는 동작 [51, 77] 매니폴드를 학습하려는 시도는 약 40시간의 고립된 동작과 이동 또는 전문적 공연에 중점을 둔 AMASS [58] 데이터셋에 의존했습니다. 이에 반해, Nymeria는 일상 활동을 하는 사람들의 300시간의 일상 활동과 동작 중인 주변 환경에 대한 정보를 포함합니다. 우리의 동작 분포는 일상적인 인간 활동을 다루는 다운스트림 애플리케이션에 더 적합한 프라이어 모델을 제공합니다.

여기서는 동작 데이터의 벡터 양자화 변이 오토인코더(VQ-VAE) [24,64]를 훈련하는 실험 결과를 표 5에 제공합니다(자세한 내용은 부록 참조). 우리는 제품 양자화(PQ)에 사용된 코드북의 수, 코드북(CB) 크기 및 잠재 차원(Dim)을 평균 관절 위치 오류(MPJPE), Procrustes-정렬(PA-)MPJPE 및 관절 위치 가속도(ACC) 오류와 비교합니다. 훈련된 인코더는 언어 모델의 텍스트 토크나이저와 유사하게 입력 동작을 이산 코드로 압축하거나 [38], 노이즈가 많은 동작을 자연스러운 동작 매니폴드로 투영하는 "동작 토크나이저"로 사용할 수 있습니다 [77]. 결과적인 잠재 코드는 조건 유무에 관계없이 생성적 동작 모델에서 자가 회귀 동작 예측에 유용하며 [53], 또는 동작 이해 작업을 위한 텍스트 토큰과 함께 언어 모델에서 사용할 수 있습니다 [32, 38]. 잘 훈련된 디코더는 최소한의 신호 손실로 전체 동작을 성공적으로 복원할 수 있습니다.

### 5.3 자연어로 동작 이해

우리 데이터셋에서 특히 가치 있는 주석 유형은 고품질의 계층적 내레이션입니다. 우리는 38.6시간의 밀집된 동작 내레이션, 207시간의 원자적 행동, 그리고 196시간의 활동 요약을 영어로 제공합니다. HumanML3D [31]과 비교했을 때, 우리의 내레이션은 더 긴 자연스러운 문장으로 작성되었으며, 물체와 환경에 대한 맥락적 설명이 포함되어 있습니다. 이는 다양한 세부 수준에서 텍스트에서 동작 생성 및 동작에서 텍스트 설명 작업을 모두 지원할 수 있습니다. 더욱 중요하게도, 맥락적 설명은 인간 동작과 쌍을 이루는 것뿐만 아니라 비디오, 포인트 클라우드 및 기타 감각 데이터 및 주석과도 쌍을 이룹니다. 언어와 동작 연구를 물리적 세계에 연결하는 데 있어서 2D 및 3D 환경 정보와 언어의 상호 확인은 흥미로운 기회를 제공합니다.

기준선으로서, 우리는 표 6에서 동작에서 텍스트 작업을 위한 MotionGPT [38] 및 TM2T [32]의 훈련 및 테스트 메트릭스를 제공합니다. 구체적으로, 우리는 BERT 점수 [97], BLEU 점수 [67], CIDEr 점수 [88] 및 ROUGE-L 점수 [49]를 보고합니다. 모델 및 훈련 세부 사항에 대해서는 부록을 참조하십시오.

### 6 토론 및 결론

우리는 자가 중심 동작 이해 연구를 가속화하기 위해 전례 없는 규모의 Nymeria 데이터셋을 제안합니다. 이 데이터셋은 50개의 장소에서 264명의 참가자가 수행한 300시간의 일상 활동과 2억 6천만 개의 신체 포즈를 포함한 세계 최대의 자연 상태 인간 동작 수집 데이터셋입니다. 우리는 정확한 6자유도(6DoF) 추적, 3D 장면 표현 및 시선 추정을 제공하며, 모든 모달리티가 동기화되어 단일 측정 좌표계로 정렬됩니다. 이 데이터셋은 총 201.2M의 자가 중심 이미지, 11.7B의 IMU 샘플 및 10.8M의 시선 포인트를 포함하여 참가자들이 399Km를 이동하는 동안 캡처되었습니다. Nymeria 데이터셋은 또한 310,500개의 문장에서 8.64M 단어로 구성된 세계 최대의 동작-언어 데이터셋으로, 38.6시간의 세밀한 동작 내레이션, 207시간의 원자적 행동 및 196시간의 활동 요약에 걸쳐 분포되어 있습니다.

#### 한계

모캡 수트와 미니Aria 손목밴드는 비디오의 시각적 외형을 부자연스럽게 만들어 컴퓨터 비전 연구에 유용성을 제한할 수 있습니다. 이 설정은 또한 동작 범위에 직접적인 영향을 미치는 일부 제약을 초래합니다. XSens 데이터 품질은 동작 보정과 신체 측정에 영향을 받으며, 사용자가 일관되지 않은 정확성을 가질 수 있습니다. 우리의 데이터셋은 일반적인 일상 활동의 일부만을 다루고 있음을 인지하고 있습니다. 공공 장소에서의 쇼핑과 같은 중요한 활동이나 고속 운동 스포츠와 같은 동적 동작은 우리의 솔루션이 한계를 가지는 영역입니다.

#### 사회적 영향

자가 중심의 전신 동작을 이해하는 것은 맥락화된 AI를 구축하는 데 매우 중요합니다. 그러나 데이터셋과 알고리즘은 개인 데이터를 많이 포함합니다. 기술의 경계를 확장하기 위해 가치 있는 데이터셋을 구축하는 동안, 우리는 데이터 동의, 비식별화 및 최소 데이터 보유를 통해 개인 정보를 최대한 존중하려고 노력합니다.
