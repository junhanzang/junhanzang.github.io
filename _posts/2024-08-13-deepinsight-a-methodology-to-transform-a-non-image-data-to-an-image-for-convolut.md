---
title: "DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture"
date: 2024-08-13 20:56:40
categories:
  - 인공지능
---

<https://www.nature.com/articles/s41598-019-47765-6>

**초록**

유전체나 다른 종류의 데이터에서 나타나는 작은 변이를 포착하여 표현형 또는 범주를 구분하는 것은 매우 중요하지만, 어려운 과제입니다. 풍부한 데이터가 존재하지만, 유전자 또는 요소들로부터의 정보가 임의적으로 분산되어 있어, 식별에 필요한 세부 사항을 추출하는 것이 어렵습니다. 그러나 유사한 유전자를 클러스터로 정리하면 이러한 차이를 더 쉽게 파악할 수 있으며, 개별 요소를 다루는 것보다 숨겨진 메커니즘(예: 경로)의 강력한 식별이 가능합니다. 본 연구에서는 비이미지 샘플을 잘 정리된 이미지 형태로 변환하는 "DeepInsight"를 제안합니다. 이를 통해, GPU 활용을 포함한 컨볼루션 신경망(CNN)의 강점을 비이미지 샘플에도 적용할 수 있습니다. 더 나아가, DeepInsight는 CNN을 통해 비이미지 샘플에서 필수적인 정보를 추출할 수 있게 하여 유망한 결과를 보여주었습니다. 우리가 아는 한, 이는 RNA-seq, 모음, 텍스트, 인공 데이터 등 다양한 종류의 비이미지 데이터셋에 CNN을 동시에 적용한 첫 번째 연구입니다.

**서론**

포스트 유전체 시대에 들어서면서, 방대한 데이터에 접근할 수 있게 되었지만, 정보가 고차원 데이터 공간에 무차별적으로 퍼져 있어 표현형을 구분하는 것이 어렵습니다. 이와 같은 문제는 다른 종류의 데이터(예: 모음, 텍스트)에서도 관련 특성을 클래스 라벨과 연관시키는 데 나타납니다. 따라서, 적절한 방식으로 요소를 배열하여 분석에 필요한 관련 특성을 추출할 수 있도록 하는 것이 매우 중요합니다. 따라서, 정보를 정렬하고 올바른 순서로 배열하는 과정이 이후 단계에서 중요한 단계가 됩니다. 우리는 이 단계를 '요소 배열 단계'라고 부릅니다. 요소 배열, 특성 추출 및 적합한 분류기를 개발하는 세 가지 단계를 따르면 표현형 또는 클래스 라벨의 식별 또는 분류를 개선할 수 있습니다.

분류 또는 탐지 문제에 대한 기존의 머신 러닝(ML) 기법은 특징 벡터(즉, 크기가 p×1인 열 벡터) 형태의 샘플을 필요로 합니다. 이 특징 벡터는 특징 추출 기법을 통해 얻어지며, 정의된 그룹 중 하나로 분류됩니다. 이 벡터 형태의 특징들은 일반적으로 ML 기법에 의해 서로 독립적이라고 간주됩니다(특히 나타나는 순서에서). 따라서, 특징의 순서를 변경하는 것은 분류나 표현형 탐지에 직접적인 영향을 미치지 않으며, 이는 랜덤 포레스트나 결정 트리와 같은 최신 ML 분류기에서 요소 배열 단계를 불필요하게 만듭니다. 그러나 ML 기법의 신뢰성은 특징 추출 기법에 의존합니다.

반면에, 딥 뉴럴 네트워크의 한 종류인 컨볼루션 신경망(CNN) 아키텍처는 샘플을 이미지(즉, 크기가 m×n인 행렬)로 받아들여, 은닉층(예: 컨볼루션 층, RELU 층, 맥스풀링 층)을 통해 특징 추출 및 분류를 수행합니다. CNN은 추가적인 특징 추출 기법을 필요로 하지 않으며, 원시 요소로부터 자동으로 특징을 도출합니다. 두 번째 장점은 이미지의 고차원 통계와 비선형 상관관계를 발견할 수 있다는 것입니다. 세 번째로, CNN의 컨볼루션 뉴런은 수용 영역 또는 제한된 부분 영역에서 데이터를 처리하므로, 큰 입력 크기를 처리하기 위해 매우 많은 뉴런이 필요하지 않아도 되어, 더 적은 파라미터로 훨씬 깊은 네트워크를 가능하게 합니다. 또 다른 특징적인 속성은 가중치 공유입니다. 즉, 많은 수용 영역이 동일한 가중치와 바이어스(또는 필터)를 공유함으로써, 기존 신경망과 비교하여 메모리 사용량을 줄일 수 있습니다. CNN 아키텍처는 이미지를 효과적으로 다룰 수 있으며, 산업용 애플리케이션(예: 자율 주행 자동차)에서 높은 정확도를 보장하는 데 중요한 역할을 합니다. 이미지의 경우, 지역적으로 인접한 픽셀들이 공간적으로 일관된 정보를 포함하고 있습니다. 즉, 가까운 픽셀들이 유사한 정보를 공유합니다. 따라서, CNN 아키텍처에서 픽셀을 임의로 배열하면 특징 추출 및 분류 성능이 저하될 수 있습니다. 따라서, CNN에서 사용하는 이미지의 인접 픽셀 순서는 더 이상 ML 기법에서처럼 독립적이지 않습니다. CNN은 이웃한 픽셀 집합을 사용할 때 추가적인 정보를 포착하며, 이는 ML 기법이 개별 특징을 사용할 때와는 다릅니다. 이러한 성공의 공로는 GPU와 같은 하드웨어의 발전에도 있으며, 이를 통해 매우 복잡한 모델을 훨씬 더 빠르고 저렴하게 학습시킬 수 있습니다. 또한, 새로운 딥러닝 아키텍처와 라이브러리의 개발로 인해 모델을 신속하게 구축하고 학습할 수 있습니다. 다행히도, CNN의 경우 캡처된 이미지는 일반적으로 물리적 객체를 묘사하며, 카메라 렌즈가 객체의 해당 음영을 픽셀에 올바르게 배치하기 때문에 픽셀을 재배열할 필요가 없습니다.

유전체, 전사체, 메틸화, 돌연변이, 텍스트, 음성, 금융 및 은행과 같은 많은 데이터는 비이미지 형태로 존재하며, 이 분야에서는 ML 기법이 주로 사용됩니다. 또한, CNN은 입력으로 이미지를 필요로 하기 때문에 사용될 수 없습니다. 그러나 비이미지 데이터를 잘 조직된 이미지 형태로 변환할 수 있다면, CNN을 사용하여 더 높은 분류 성능을 얻을 수 있습니다. 이를 위해 효과적으로 요소 배열을 수행할 수 있는 방법을 개발해야 합니다. 탐지율을 향상시키기 위해, 우리는 제안된 DeepInsight 방법에서 요소 배열, 특징 추출 및 분류의 세 단계를 통합했습니다. DeepInsight는 유사한 요소나 특징을 함께 배치하고, 비슷하지 않은 요소들은 더 멀리 배치하여 이미지를 구성하며, 이웃한 요소를 집합적으로 사용할 수 있도록 합니다. 이 집합적인 요소 배열 접근 방식은 숨겨진 메커니즘(예: 경로)을 발견하거나 특징 집합 간의 관계(예: 텍스트, 모음에 대한 이해)를 이해하는 데 유용할 수 있습니다. 따라서, 유사한 특징(또는 원시 요소)을 클러스터로 삽입하여 이미지를 변환하는 것은 개별 특징을 다루는 것보다(이웃 정보 무시) 더 의미 있고 견고합니다. 이는 중요한 정보를 통합할 수 있으며, 목표나 결과에 대한 특징의 상대적 중요성을 탐구할 수 있는 잠재력을 가지고 있습니다. 요소 배열은 중요한 정보를 해제하는 열쇠입니다. 주어진 데이터셋에서 더 많은 정보를 추출할 수 있는 전략을 깊이 생각하는 것이 중요합니다. 또한, DeepInsight는 CNN을 활용한 특징 추출 및 분류를 가능하게 하여 CNN을 비이미지 케이스로 확장할 수 있으며, CNN의 일반화된 결과를 제공합니다. 본 논문에서는 DeepInsight가 유전자 발현, 모음, 텍스트 및 인공 데이터와 같은 다양한 종류의 데이터에 유용함을 보여줍니다.

CNN이 이미지를 효과적으로 처리하기 위해 제안된 다양한 버전들이 있습니다. 예를 들어, He 등은 매우 깊은 네트워크를 더 쉽게 학습할 수 있도록 잔차 네트워크 아키텍처를 제안했습니다. 그들은 ImageNet 데이터셋에서 152개의 층 깊은 잔차 네트워크를 사용했습니다. Singh 등은 히스톤 변형 데이터를 입력으로 사용하여 유전자 발현을 분류하기 위해 CNN 기반 기술을 개발했습니다. Liu 등은 종양 유전자 발현 샘플을 열 벡터로 사용하고 1차원 CNN을 적용하여 분류를 수행했습니다. 그들은 샘플을 이미지로 변환하지 않았습니다. Zeng 등은 현장 하이브리드 유전자 발현 패턴에서 CNN을 사용하여 특징을 추출했습니다. 입력 샘플은 자연 이미지였습니다. Gao 등은 DNA 서열을 사용하여 4차원 이진 코드로 변환했습니다. 이러한 이진 코드는 DNA 서열에 따라 배열된 후, CNN에 적용되어 폴리애데닐화 위치를 예측했습니다. Xu 등은 텍스트 해싱에 CNN을 적용하여 텍스트를 이진 코드로 변환한 후, 1차원 컨볼루션에 입력했습니다. 즉, 이러한 특징들은 더 이상 컨볼루션 층에서 이미지로 취급되지 않습니다. Zhang 등은 텍스트를 원시 신호로 간주하고 1차원 CNN을 적용하여 분류했습니다. Lyu와 Haque는 최근 RNA-seq 데이터에 CNN을 적용하여 먼저 유전자 선택을 수행한 후 염색체 위치 기반으로 이미지를 구성했습니다. 이 방법은 유전자 발현을 이미지 샘플로 변환하고 CNN을 적용한 첫 번째 사례일 것입니다. 이 방법은 염색체 위치 정보를 필요로 하기 때문에 다른 종류의 데이터셋에는 사용할 수 없습니다. 대부분의 방법들은 CNN에 이미지를 입력으로 사용하거나 1차원 CNN을 사용했습니다. 따라서 비이미지 샘플을 CNN 응용을 위해 이미지를 보편적으로 변환하는 것에 대한 문헌은 거의 없습니다.

**결과**

**실험 설정**

DeepInsight 방법을 테스트하기 위해 네 가지 종류의 데이터셋을 사용했으며, 이 방법으로 얻은 결과를 최신 분류기들과 비교했습니다. 사용된 데이터셋은 1개의 유전자 발현 데이터셋, 1개의 텍스트 데이터셋, 1개의 모음 데이터셋, 그리고 2개의 인공 데이터셋입니다. 주요 목표는 비이미지 데이터를 DeepInsight 방법을 통해 CNN 아키텍처를 활용하여 처리할 수 있음을 보여주는 것입니다.

이 연구에 사용된 데이터셋은 각각 80:10:10 비율로 훈련, 검증, 테스트 세트로 나누었습니다. 모델의 학습은 훈련 세트에서 이루어지며, 모델의 적합성은 검증 세트에서 평가됩니다. 검증 오류가 최소화되는 하이퍼파라미터를 선택합니다. 테스트 세트는 훈련 또는 모델 적합 단계에서 사용되지 않았습니다. 테스트 세트에서의 분류 정확도를 계산하여 최종 모델에 대한 편향되지 않은 평가를 제공합니다. 분류 정확도는 테스트 세트에서 올바르게 분류된 샘플의 비율로 정의됩니다.

이 데이터셋들의 설명은 다음과 같습니다. 첫 번째는 RNA-seq 또는 유전자 발현 데이터셋으로, TCGA(<https://cancergenome.nih.gov)의> 공개 데이터셋이며, 6216개의 샘플을 포함하고 있으며 각 샘플은 60483개의 유전자 또는 차원을 가집니다. 이는 10개의 암 유형을 나타내는 10-클래스 데이터셋입니다. 두 번째는 TIMIT 코퍼스에서 추출한 음성 데이터셋입니다. 여기서는 10개의 서로 다른 단모음(monophthong) 모음이 추출되었고, 각 모음은 세 개의 세그먼트로 나누어졌으며, 각 세그먼트는 에너지-델타-가속(MFCC\_EDA) 특징 벡터를 사용하여 멜 주파수 켑스트럼 계수로 생성되었습니다. 총 12579개의 샘플과 39개의 차원이 있습니다. 세 번째 데이터셋은 Relathe(텍스트)로, 뉴스그룹 문서에서 파생된 것이며, 서로 다른 뉴스그룹에 균등하게 분할되었습니다. 1427개의 샘플과 4322개의 차원이 포함되어 있습니다. 이는 이진 클래스 문제입니다. 다음 두 개는 인공 데이터셋입니다. 하나는 Madelon으로, 2600개의 샘플과 500개의 차원을 가지고 있습니다. 이 데이터셋은 연속적인 입력 변수를 가진 이진 클래스 분류 문제로, 다변량이고 고도로 비선형적입니다. 다른 하나는 ringnorm-DELVE로, 이는 Leo Breiman의 ringnorm 예제를 구현한 것입니다. 이 데이터셋은 20차원, 2 클래스 분류로, 7400개의 샘플을 포함합니다. 각 클래스는 다변량 정규 분포에서 추출되었으며, 클래스 1은 평균이 0이고 공분산은 단위 행렬의 4배이며, 클래스 2는 평균이

![](/assets/images/posts/250/img.png)

​

이고 공분산은 단위 행렬입니다. 이러한 데이터셋은 표 1에 요약되어 있습니다.

![](/assets/images/posts/250/img_1.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

- 샘플 (Sample):
  - 사진이 아닌, 각각의 데이터 항목을 의미합니다.
  - 예를 들어, RNA-seq 데이터셋에서는 각 환자나 조직 표본이 하나의 샘플입니다.
- 차원 (Dimension):
  - 각 샘플에 대해 측정된 특성(feature)의 수입니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**비교 및 분류 성능**

비교 목적으로 기존의 최신 분류기인 랜덤 포레스트, 결정 트리, 아다부스트가 사용되었습니다. 경쟁 방법들의 하이퍼파라미터는 그리드 서치 최적화를 사용하여 최적화되었습니다. 4.3절과 보충 파일 1에서 논의된 바와 같이, DeepInsight 방법은 두 가지 종류의 정규화(norm-1과 norm-2)를 사용하며, 이 두 정규화에 대해 검증 오류가 평가됩니다. 검증 오류가 가장 낮은 정규화 방법이 이후 처리에 사용됩니다. 픽셀 프레임 크기는 120 × 120으로 고정되어 있습니다. 그러나 RNA-seq 데이터셋의 경우, 요소 또는 특징의 수가 매우 많아(60483개) 다른 데이터셋과 비교했을 때 손실 압축(보충 파일 2에서 논의된 바와 같이)이 발생하므로 분석은 200 × 200 픽셀 크기에서 수행되었습니다. DeepInsight를 실행한 후 모든 데이터셋에 대해 두 가지 정규화 방법의 검증 오류는 보충 파일 3에 나타나 있습니다. 검증 세트에서 가장 적합한 모델이 별도의 테스트 세트에서 성능을 평가하는 데 사용됩니다.

이 비교의 목적은 DeepInsight가 다양한 종류의 데이터셋에서 경쟁력 있는 성능을 낼 수 있음을 보여주는 것입니다. 분류 정확도와 관련된 성능은 표 2에 나타나 있으며(코드에 대한 간단한 논의는 보충 파일 4를 참조하십시오).

![](/assets/images/posts/250/img_2.png)

**표 2** 다양한 모델을 사용하여 여러 종류의 데이터셋에서의 분류 정확도.

DeepInsight는 RNA-seq 데이터의 테스트 세트에서 99%의 분류 정확도를 기록했으며, 이는 최신 랜덤 포레스트 방법보다 3% 높은 결과입니다. 모음 데이터셋에서는 DeepInsight가 97%의 분류 정확도를 기록한 반면, 랜덤 포레스트는 90%를 기록했습니다. 이 개선은 본 연구에서 비교된 기존 방법 중 가장 성능이 좋은 방법보다 약 7% 더 나은 것입니다. 다음으로, 텍스트 데이터에서는 DeepInsight가 92%의 정확도를 기록한 반면, 랜덤 포레스트는 90%를 기록했습니다. 동일한 경향이 인공 데이터셋인 Madelon과 ringnorm에서도 발견되었습니다. Madelon에서는 DeepInsight가 88%의 정확도를 기록했고, ringnorm에서는 98%를 기록했습니다. 이는 각각 두 번째로 좋은 방법보다 23%와 4% 더 나은 성과입니다. 다섯 개의 데이터셋에 대한 평균 분류 정확도도 계산되었습니다. Ada-boost는 73%의 평균 분류 정확도를 기록했으며, 결정 트리는 80%를 기록하여 Ada-boost보다 더 나은 성과를 보였습니다. 랜덤 포레스트는 86%를 기록하여 기존에 연구된 기술 중 가장 높은 성과를 보였으나, DeepInsight는 95%의 유망한 평균 분류 정확도를 기록하여 두 번째로 좋은 방법보다 훨씬 더 나은 성과를 보였습니다.

**논의**

예상한 대로, 제안된 DeepInsight 방법은 매우 유망한 결과를 보여주었습니다. 얻어진 결과는 다양한 종류의 비이미지 데이터셋에 CNN 아키텍처를 사용할 수 있게 해주며, 이는 딥러닝 네트워크를 활용할 수 있는 가능성을 증가시킵니다. 이 알고리즘을 다양한 응용 분야에 적용할 수 있는 엄청난 가능성을 상상할 수 있습니다.

이 연구에서 우리는 DeepInsight 방법을 통해 비이미지 샘플에 대해 CNN의 여러 특성을 통합할 수 있었습니다. 벡터 형태의 비이미지 샘플을 CNN 처리를 위해 의미 있는 이미지로 변환했습니다. 이 전략은 유전체 데이터와 관련된 모든 문제를 해결하지는 않지만, CNN의 장점을 통합하는 데 한 걸음 나아간 것입니다. 딥 뉴럴 네트워크 아키텍처는 특징 추출, 차원 축소, 희소하고 고차원적인 데이터에서 숨겨진 구조 발견, 데이터 증강 및 업샘플링, 라벨이 있는/없는 샘플을 통한 반지도 학습, 그리고 시계열 데이터에서 최적의 행동 선택 등 많은 장점을 포괄하고 있습니다. 따라서, 더 넓은 맥락에서, 딥 뉴럴 네트워크 아키텍처는 DNA 서열에서 단백질 서열(시계열 데이터로 간주될 수 있음), RNA-seq 또는 오믹스 데이터에 이르기까지 다양한 입력 샘플에 대한 유전체 분석 솔루션을 제공할 가능성이 있습니다.

DeepInsight 방법은 CNN 아키텍처의 다용성을 증가시킵니다. CNN의 자동 특징 추출, 뉴런의 필요성을 줄여 더 깊은 모델 학습을 가능하게 하는 것, 메모리 요구 사항을 완화하기 위한 가중치 공유 기능, 인접 정보의 활용(즉, 픽셀 프레임의 부분 영역을 한 번에 처리), GPU 활용 등의 특성은 CNN을 분류 및 분석에 강력한 도구로 만듭니다. 제안된 기술은 이러한 CNN의 속성을 비이미지 사례에 적용합니다. 또한, 우리는 여러 종류의 데이터셋에서 DeepInsight의 효과를 보여주었으며, 매우 유망한 결과를 얻었습니다. RNA-seq 데이터에서 DeepInsight가 달성한 최대 분류 정확도는 99%였습니다. 모음, 텍스트, Madelon, ringnorm 데이터셋에서의 정확도는 각각 97%, 92%, 88%, 98%였습니다.

현재 알고리즘 버전의 추가 확장이 고려될 수 있습니다. 현재 기술은 분류를 위해 그레이스케일 또는 단일 레이어(즉, 2D 매트릭스)를 사용하고 있습니다. 이는 다층을 포함하도록 확장될 수 있으며, 따라서 다중 오믹스 데이터(예: 유전자 발현, 메틸화, 돌연변이)와 관련된 문제를 해결하는 데 적용될 수 있습니다. 또한, 다양한 종류의 데이터(예: 임상 및 비임상 데이터)를 단일 레이어로 정규화하여(컴퓨팅 자원 때문에 다층이 금지되는 경우) 분석 및 분류할 수 있습니다. 이 기술은 데이터가 이미지 형태가 아닌 여러 응용 분야에 유용할 수 있습니다.

**DeepInsight 방법**

![](/assets/images/posts/250/img_3.png)

![](/assets/images/posts/250/img_4.png)

**DeepInsight 파이프라인**  
(a) 특징 벡터에서 특징 행렬로의 변환 예시.  
(b) 특징 벡터를 이미지 픽셀로 변환하는 DeepInsight 방법론의 예시.

만약 데이터의 차원이 매우 크고 하드웨어 제한으로 인해 처리하기 어렵다면, DeepInsight를 적용하기 전에 차원 축소 기법(DRT)을 고려할 수 있습니다. DRT는 문제의 성격에 따라 특징 선택 또는 특징 추출의 형태일 수 있습니다. DRT를 적용하면 더 작은 특징 집합이 제공되어 처리 속도가 빨라지지만, 분류 성능에 위험이 있을 수 있습니다. 반면에, 만약 잡음이 많거나 중복된 특징이 제거된다면 처리 속도뿐만 아니라 정확도도 높일 수 있습니다. DRT의 적용은 사례에 따라 다르기 때문에, 우리는 DRT를 적용하지 않고 DeepInsight를 설명했습니다.

**DeepInsight 파이프라인**

![](/assets/images/posts/250/img_5.png)

**특징 정규화**

이미지의 단일 레이어는 256가지의 음영을 가지며, 이는 [0, 1] 범위로 정규화됩니다. 따라서 이미지 변환을 적용하기 전에 특징 값도 정규화해야 합니다. 이 연구에서는 두 가지 유형의 정규화를 수행했습니다: (1) 각 특징이 독립적으로 가정되어, 해당 특징의 최소값과 최대값으로 정규화하는 방법, (2) 전체 훈련 세트에서 단일 최대값으로 정규화하여 상호 특징의 토폴로지를 어느 정도 유지하는 방법. 이러한 정규화 방식은 보충 파일 1에서 자세히 설명되어 있습니다. DeepInsight는 이 두 가지 정규화 방식에 대해 검증 세트 성능을 평가하고, 검증 오류가 가장 낮은 방식을 채택합니다.

**CNN 아키텍처**

이 섹션에서는 DeepInsight 방법의 CNN 아키텍처에 대해 설명합니다. 특징 벡터가 이미지로 변환되면, 이를 CNN 아키텍처로 추가 처리할 수 있습니다(이미지 형태로 변환된 두 종류의 암 샘플에 대한 예시는 그림 2a에 나와 있습니다).

![](/assets/images/posts/250/img_6.png)

**DeepInsight 네트워크: 일러스트레이션**

(a) DeepInsight 방법의 이미지 변환 방법론을 사용하여 두 가지 유형의 종양을 시각화한 예시입니다. 이 두 유형 간의 차이는 여러 지점에서 시각화될 수 있습니다. 이러한 이미지 샘플들은 이후 딥러닝 아키텍처(DLA)로 처리됩니다. 즉, 같은 그림의 (b) 부분에 묘사된 병렬 CNN을 사용합니다.  
(b) DeepInsight에서 사용된 병렬 CNN 아키텍처. 이 아키텍처는 두 개의 병렬 CNN 아키텍처로 구성되며, 각각 네 개의 컨볼루션 층을 포함하고 있습니다. 파라미터는 베이지안 최적화 기법을 사용하여 조정됩니다.

우리는 모델을 효과적으로 학습시키기 위해 다양한 필터 크기를 사용할 수 있도록 병렬 CNN 아키텍처를 개발했습니다. 우리의 CNN 아키텍처는 그림 2b에 나와 있습니다. 이 아키텍처에서는 네 개의 병렬 층이 있으며, 각 층은 2D 컨볼루션 층, 배치 정규화 층, ReLU 활성화 층, 그리고 맥스 풀링 층으로 구성됩니다. 배치 정규화는 학습 중 과적합을 방지하기 위해 사용되며, 맥스 풀링 층은 각 층에서 이미지 크기를 다운샘플링하는 데 사용됩니다. 병렬 아키텍처의 네 번째 컨볼루션 층의 출력은 결합되어 완전 연결 층으로 전달됩니다. 마지막으로, 소프트맥스(SoftMax) 층이 클래스 라벨을 출력하기 위해 사용됩니다.

DeepInsight의 CNN 아키텍처에는 컨볼루션 층, 필터 크기, 학습 속도 등 다양한 하이퍼파라미터가 있습니다. 우리는 모든 실험에서 베이지안 최적화 기법을 적용하여 이러한 하이퍼파라미터를 조정했습니다. 검증 세트에서 최고의 성능을 보인 하이퍼파라미터 세트를 얻었습니다. 파라미터의 세부 사항과 학습 단계에서의 검증 오류는 보충 파일 2와 보충 파일 3에서 논의되었습니다.

CNN 모델이 최적의 하이퍼파라미터로 학습되면, 새로운 샘플을 어느 하나의 카테고리나 클래스로 식별할 수 있습니다.

예시로, 서로 다른 유형의 암, 텍스트, 모음에서 두 샘플을 추출하여 샘플 간의 차이를 관찰했습니다. DeepInsight 방법으로 변환된 샘플은 그림 3에 나와 있습니다. 이 방법은 요소 배열을 수행하여 흥미로운 지역을 제공하며, 특징 추출 및 분류를 통해 특징의 차이를 추가로 포착합니다. 또한, 이제 이러한 샘플을 시각화할 수 있으며, 특정 영역에서의 상대적 차이는 서로 다른 클래스 라벨(또는 표현형)로 이어질 수 있습니다.

![](/assets/images/posts/250/img_7.png)

**DeepInsight로 패턴을 드러내기**  
이 예시는 DeepInsight가 유전자 발현(다양한 종류의 암), 텍스트(두 종류의 텍스트), 모음(두 종류의 모음)에서 얻은 다양한 패턴을 보여줍니다. 각 플롯은 변환된 샘플을 보여주며, 샘플 간의 차이를 이제 쉽게 알아볼 수 있습니다.

**데이터 이용 가능성**  
RNA-seq 데이터는 TCGA(<https://cancergenome.nih.gov)에서> 이용할 수 있습니다. 모음 데이터는 TIMIT Acoustic-Phonetic Continuous Speech Corpus(<https://catalog.ldc.upenn.edu/LDC93S1)에서> 추출할 수 있습니다. 텍스트 데이터는 <http://featureselection.asu.edu/datasets.php에서> 이용할 수 있습니다. Madelon 데이터셋은 UCI 리포지토리(<http://archive.ics.uci.edu/ml/datasets/madelon)에서>, ringnorm 데이터셋은 토론토 대학교(<https://www.cs.toronto.edu/~delve/data/ringnorm/desc.html)에서> 이용할 수 있습니다.

**코드 이용 가능성**모든 소스 코드, 샘플 데이터셋, 그리고 관련 문서는 <http://www.riken.jp/en/research/labs/ims/med_sci_math/> 또는 <http://www.alok-ai-lab.com에서> 이용할 수 있습니다.

[s41598-019-47765-6.pdf

1.78MB](./file/s41598-019-47765-6.pdf)

이 논문이 기억났다면

<https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding>

[Stanford Ribonanza RNA Folding | Kaggle](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding)

에서 사용했을지도...
