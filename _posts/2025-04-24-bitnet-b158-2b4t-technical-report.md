---
title: "BitNet b1.58 2B4T Technical Report"
date: 2025-04-24 00:43:52
categories:
  - 인공지능
tags:
  - BITNET
---

<https://github.com/microsoft/BitNet>

[GitHub - microsoft/BitNet: Official inference framework for 1-bit LLMs](https://github.com/microsoft/BitNet)

<https://huggingface.co/microsoft/bitnet-b1.58-2B-4T>

[microsoft/bitnet-b1.58-2B-4T · Hugging Face](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T)

<https://arxiv.org/abs/2504.12285>

[BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)

이전에 읽었지만, tech report가 나와서 다시 한번 recap용

**초록 (Abstract)**  
우리는 파라미터 수가 20억에 달하는 최초의 오픈소스 네이티브 1비트 대규모 언어 모델(LLM)인 **BitNet b1.58 2B4T**를 소개합니다. 이 모델은 총 4조 토큰으로 구성된 말뭉치로 학습되었으며, 언어 이해, 수학적 추론, 코딩 능력, 대화 능력 등을 포괄하는 다양한 벤치마크를 통해 철저히 평가되었습니다. 그 결과, **BitNet b1.58 2B4T**는 유사한 규모의 오픈 가중치 기반의 일반 정밀도(Full-Precision) LLM들과 동등한 성능을 보이는 동시에, **메모리 사용량, 에너지 소비, 디코딩 지연(latency)** 측면에서 **현저한 연산 효율성의 이점**을 제공합니다.

연구 및 활용을 촉진하기 위해, 본 모델의 가중치는 **Hugging Face**를 통해 공개되며, GPU 및 CPU 아키텍처용 오픈소스 추론(inference) 구현도 함께 제공됩니다.

- **BitNet b1.58 2B4T (1.58-bit)**: bitnet-b1.58-2B-4T
  - 추론 전용으로 사용되는 **압축 가중치(packed weight)**
- **BitNet b1.58 2B4T (bf16)**: bitnet-b1.58-2B-4T-bf16
  - 학습 전용으로 사용되는 **마스터 가중치(master weight)**
- **BitNet b1.58 2B4T (gguf)**: bitnet-b1.58-2B-4T-gguf
  - bitnet.cpp에서 사용되는 **GGUF 포맷 가중치**
- **BitNet b1.58 2B4T 코드**: bitnet.cpp  **데모**: [aka.ms/bitnet-demo](https://aka.ms/bitnet-demo)

[Bitnet](https://aka.ms/bitnet-demo)

![](/assets/images/posts/545/img.png)

**그림 1 설명**  
**BitNet b1.58 2B4T**는 파라미터 수 30억 미만의 주요 오픈 가중치 LLM들이 정의한 **Pareto 최적선(Pareto frontier)**을 성능 대비 메모리 사용 측면에서 **뛰어난 효율성**으로 **앞서 나가고 있음**을 보여줍니다.

**1. 서론 (Introduction)**

오픈소스 대규모 언어 모델(LLM)은 첨단 인공지능 기술에 대한 접근을 민주화하고, 혁신을 촉진하며, 자연어 처리, 코드 생성, 비전 컴퓨팅 등 다양한 분야의 연구를 가능하게 하는 데 핵심적인 역할을 해왔습니다(Dubey et al., 2024; Yang et al., 2024; Bai et al., 2025). 이러한 모델들의 공개는 광범위한 실험과 적응을 가능하게 합니다. 그러나 이러한 오픈 LLM의 **광범위한 활용에는 중요한 장벽**이 존재합니다. 바로 **추론 및 배포에 요구되는 막대한 계산 자원**입니다. 최신 오픈 LLM들은 **대용량 메모리**, **높은 에너지 소비**, **상당한 추론 지연(latency)**을 요구하기 때문에, 엣지 디바이스나 자원이 제한된 환경, 실시간 애플리케이션에서는 사용이 사실상 어렵습니다.

이러한 문제를 해결하기 위한 유망한 접근 방식으로 **1비트 LLM**이 주목받고 있습니다. 이는 모델 양자화(quantization)의 극단적 형태로, 가중치(및 경우에 따라 활성값)를 **{-1, +1} 이진(binary)** 또는 **{-1, 0, +1} 삼진(ternary)** 값으로 제한하는 방식입니다. 이러한 모델은 **가중치 저장에 필요한 메모리를 극적으로 줄이고**, **비트 단위 연산(bitwise computation)을 효율적으로 수행할 수 있어**, **배포 비용 절감**, **에너지 소비 감소**, **추론 속도 향상**의 잠재력을 가지고 있습니다.

이전 연구에서도 1비트 모델이 시도된 바 있으나, 기존 오픈소스 접근 방식은 대체로 두 가지 범주로 나뉩니다:

1. **사전 학습된 고정밀 모델**에 **사후 양자화(post-training quantization, PTQ)**를 적용하는 방식 – 이 경우, 성능 저하가 클 수 있음 (Xu et al., 2024b; Team, 2024)
2. **처음부터 1비트 가중치로 학습된(native) 모델** – 주로 소규모 모델로 개발되어 왔으며, 예: OLMo-Bitnet-1B [(<https://huggingface.co/NousResearch/OLMo-Bitnet-1B>)] → 이들은 여전히 대규모 full-precision 모델에 비해 성능 격차가 존재하여, 실제 적용 사례는 제한적이었음.

이러한 **효율성과 성능 간의 간극(gap)**을 해소하고자, 본 논문에서는 **BitNet b1.58 2B4T**를 소개합니다.  
이는 **처음부터 1비트로 학습된 최초의 오픈소스 대규모 LLM**으로, 총 **20억 개의 파라미터**를 갖추고 있으며, **4조 토큰 규모의 데이터셋**으로 학습되었습니다. 이 모델은 1비트 패러다임에 최적화된 구조 및 학습 기법을 활용하였으며, 본 연구의 핵심 기여는 **적절한 방식으로 대규모 학습이 이루어진 native 1비트 LLM도, 유사 크기의 최신 full-precision 오픈 LLM과 성능 면에서 충분히 경쟁 가능하다는 것을 입증한 것**입니다.

이 기술 보고서에서는 **BitNet b1.58 2B4T의 개발 과정과 평가 결과**를 자세히 다룹니다. 모델의 구조와 학습 방법론을 설명하고, 언어 이해, 수학적 추론, 코딩 능력, 다중 턴 대화 능력을 평가하는 **표준 벤치마크**에서의 성능을 종합적으로 제시합니다.  
결과적으로, 본 모델은 기존 full-precision 기준 모델들과 비교해 **우수한 효율성과 함께 강력한 성능을 입증**하였습니다.  
마지막으로, **BitNet b1.58 2B4T의 가중치를 Hugging Face를 통해 공개**하며, **GPU 및 CPU 환경에서의 추론을 위한 오픈소스 코드**도 함께 제공하여, 고효율 LLM의 추가 연구 및 실제 활용을 촉진하고자 합니다.

**2. 아키텍처 (Architecture)**

**BitNet b1.58 2B4T의 아키텍처는 표준 Transformer 모델**(Vaswani et al., 2017)을 기반으로 하며, **BitNet 프레임워크**(Wang et al., 2023a; Ma et al., 2024)를 바탕으로 한 여러 가지 중요한 수정 사항이 포함되어 있습니다. 이 모델은 처음부터 완전히 새로 학습된 모델입니다.

핵심적인 아키텍처 혁신은, 기존의 **정밀도 기반 선형 계층(torch.nn.Linear)을 BitLinear라는 커스텀 계층**으로 교체한 것입니다. 이는 BitNet 접근 방식의 핵심 요소로 작용합니다.  
**BitLinear 계층 내부에서는 다음과 같은 양자화 방식이 적용됩니다:**

- **가중치 양자화(Weight Quantization)**:  
  모델의 가중치는 **forward pass 시점에 1.58비트로 양자화**됩니다. 이는 **absmean(절대 평균 기반)** 양자화 기법을 사용하여, 가중치를 {−1, 0, +1}의 **삼진(ternary)** 값으로 매핑합니다. 이로 인해 모델 크기가 대폭 축소되며, 효율적인 수학 연산이 가능해집니다.
- **활성값 양자화(Activation Quantization)**:  
  선형 프로젝션을 통과하는 활성값은 **8비트 정수로 양자화**됩니다. **absmax(절대 최대값 기반)** 양자화 전략을 사용하며, **토큰 단위**로 적용됩니다.
- **정규화(Normalization)**:  
  **양자화 학습 환경에서의 안정성 향상**을 위해, subln 정규화 기법(Wang et al., 2022)이 도입됩니다.

BitLinear 계층 외에도, 성능 및 안정성을 높이기 위해 다음과 같은 최신 LLM 기법들이 통합되었습니다:

- **활성 함수(FFN)**:  
  피드포워드 네트워크(FFN) 서브 계층에서는 일반적으로 사용되는 **SwiGLU 활성 함수**(Shazeer, 2020) 대신, **제곱 ReLU (ReLU²)** 함수를 사용합니다. 이 선택은 **모델 희소성(sparsity) 향상** 및 **1비트 연산 효율성**을 고려한 것입니다(Wang et al., 2024a; 2024b).
- **위치 임베딩(Positional Embeddings)**:  
  **RoPE(Rotary Position Embeddings)** (Su et al., 2024)가 사용되어 위치 정보를 주입합니다. 이는 최신 고성능 LLM에서 일반적으로 채택되는 방식입니다.
- **바이어스 제거(Bias Removal)**:  
  LLaMA와 같은 아키텍처들과 마찬가지로, **모든 선형 계층과 정규화 계층에서 bias 항을 제거**하여 파라미터 수를 줄이고, 양자화 단순화에도 기여합니다.

**토크나이저(tokenizer)**는 **LLaMA 3용으로 개발된 토크나이저**(Dubey et al., 2024)를 채택하였습니다.  
이 토크나이저는 **바이트 단위 BPE(Byte-Pair Encoding)** 방식을 사용하며, **어휘 집합 크기는 128,256개 토큰**입니다. 이를 통해 다양한 자연어 및 코드 데이터를 효과적으로 처리할 수 있으며, 오픈소스 생태계와의 통합이 용이합니다.

**3. 학습 (Training)**

**BitNet b1.58 2B4T의 학습 과정은 크게 세 단계**로 구성됩니다:  
**대규모 사전 학습(pre-training)**, 이어지는 **지도 미세조정(Supervised Fine-Tuning, SFT)**, 그리고 **직접 선호 최적화(Direct Preference Optimization, DPO)**입니다.  
수학 및 연쇄적 사고(chain-of-thought) 추론 능력을 더욱 강화하기 위해 **Proximal Policy Optimization (PPO)**나 **Group Relative Policy Optimization (GRPO)** 같은 고급 기법(Schulman et al., 2017; Shao et al., 2024)을 사용할 수도 있으나, 현재 버전의 BitNet b1.58 2B4T는 **사전학습, SFT, DPO 세 단계만**으로 구성되어 있습니다. 강화학습 기반 방법은 향후 연구 방향으로 남겨두었습니다.

### **3.1 사전 학습 (Pre-training)**

이 단계의 목적은 **세계에 대한 폭넓은 지식과 언어의 기초 능력**을 모델에 학습시키는 것이었습니다. 기존 LLM의 일반적인 학습 전략(Dubey et al., 2024)을 기반으로 하되, **1비트 아키텍처에 맞게 특화된 조정**을 적용하였습니다.

#### **3.1.1 학습률 스케줄 (Learning Rate Schedule)**

2단계 학습률 스케줄이 사용되었습니다.

1. **1단계 (고학습률 단계)**:  
   초기에는 **코사인 감소(cosine decay)** 스케줄을 적용하되, **상대적으로 높은 초기 최대 학습률**로 시작했습니다. 이는 1비트 모델이 일반 full-precision 모델보다 학습 안정성이 높다는 관찰에 기반하여, 보다 **공격적인 초기 학습**을 가능하게 합니다.
2. **2단계 (쿨다운 단계)**:  
   전체 학습 토큰의 중반 시점쯤에 학습률을 **급격히 낮추고**, 이후에는 **훨씬 낮은 최대값의 코사인 스케줄**로 유지합니다. 이 "쿨다운(cooldown)" 단계에서는 모델이 **고품질 데이터에 대해 표현을 정제**할 수 있도록 설계되어 있습니다 (3.1.3 참고).

#### **3.1.2 가중치 감쇠 스케줄 (Weight Decay Schedule)**

학습률 조정에 더해, **2단계 가중치 감쇠(weight decay) 전략**도 함께 적용되었습니다.

1. **1단계**:  
   초기 단계에서는 **코사인 스케줄을 따라 가중치 감쇠**를 적용하였으며, **최대 0.1**까지 도달하도록 하였습니다. 이는 초기 고학습률 구간에서 **과적합 방지**에 도움을 줍니다.
2. **2단계**:  
   쿨다운 단계에서는 **가중치 감쇠를 0으로 설정**하여 사실상 비활성화하였습니다. 이를 통해 **낮은 학습률과 고품질 데이터의 조합**에 따라 모델 파라미터가 더 정교한 최적점에 수렴할 수 있도록 하였습니다.

#### **3.1.3 사전 학습 데이터 (Pre-training Data)**

사전 학습에는 **공개된 텍스트 및 코드 데이터셋**이 혼합되어 사용되었습니다.  
예를 들어, **대규모 웹 크롤링 데이터인 DCLM**(Li et al., 2024b), **교육용 웹사이트 데이터인 FineWeb-EDU**(Penedo et al., 2024) 등이 포함되었습니다.  
또한, **수학적 추론 능력 향상**을 위해 **합성(synthetic) 수학 데이터**도 함께 사용되었습니다.

데이터 제시 전략 역시 학습 2단계 전략에 맞추어 설계되었습니다:

- **일반 웹 데이터**는 1단계에서 주로 사용되었고,
- **고품질 큐레이션 데이터셋**은 학습률이 낮아지는 2단계 쿨다운 시점에서 **중점적으로 사용**되었습니다.

**3.2 지도 미세조정 (Supervised Fine-tuning, SFT)**

사전 학습 이후, 모델은 **지시 따르기 능력(instruction-following)** 및 **대화형 상호작용 형식에서의 성능 향상**을 위해 **지도 미세조정(SFT)** 과정을 거쳤습니다.

### **3.2.1 SFT 데이터**

SFT 단계에서는 **공개된 다양한 지시 따르기 및 대화형 데이터셋**을 사용했습니다. 사용된 대표적인 데이터셋은 다음과 같습니다:

- **WildChat** (Zhao et al., 2024)
- **LMSYS-Chat-1M** (Zheng et al., 2024)
- **WizardLM Evol-Instruct** (Xu et al., 2024a)
- **SlimOrca** (Lian et al., 2023)

또한, **추론 능력(reasoning)**과 **복잡한 지시 처리 능력**을 강화하기 위해, **GLAN**(Li et al., 2024a) 및 **MathScale**(Tang et al., 2024) 등의 기법으로 생성된 **합성 데이터셋(synthetic datasets)**을 추가로 활용했습니다.

### **3.2.2 채팅 템플릿 (Chat Template)**

SFT 및 추론 중 대화형 태스크에서는 다음과 같은 채팅 형식의 템플릿을 사용했습니다:

```
<|begin_of_text|>System: {system_message}<|eot_id|>
User: {user_message_1}<|eot_id|>
Assistant: {assistant_message_1}<|eot_id|>
User: {user_message_2}<|eot_id|>
Assistant: {assistant_message_2}<|eot_id|>
...
```

### **3.2.3 최적화 세부사항 (Optimization Details)**

SFT 중에는 다음과 같은 최적화 전략이 주요한 역할을 했습니다:

- **손실 함수 집계(Loss Aggregation)**:  
  일반적으로는 배치 내 토큰에 대한 **교차 엔트로피 손실을 평균(mean reduction)** 내지만, 본 모델은 **합산(sum reduction)**을 사용했습니다.  
  실험 결과, 손실을 합산할 경우 수렴 속도와 최종 성능이 향상됨을 확인했습니다.
- **하이퍼파라미터 튜닝**:  
  학습률 및 에폭 수에 대한 **세심한 조정**을 실시했습니다.  
  사전 학습 결과와 일치하게, 1비트 모델은 일반 full-precision 모델보다 **상대적으로 높은 학습률**에서 더 좋은 효과를 보였습니다.  
  또한, **동일 규모의 full-precision 모델보다 더 많은 에폭 수**를 통해 최적 수렴에 도달했습니다.

## **3.3 직접 선호 최적화 (Direct Preference Optimization, DPO)**

모델의 **도움됨(helpfulness)** 및 **안전성(safety)**을 **사용자 선호에 맞게 조정**하기 위해, SFT 이후 **DPO (Direct Preference Optimization)** 단계가 수행되었습니다(Rafailov et al., 2023).  
DPO는 전통적인 RLHF(보상 모델 기반 강화 학습)를 대체하는 **효율적인 방식**으로, **보상 모델 없이도** 선호 데이터를 통해 **직접 언어 모델을 최적화**할 수 있습니다.  
이 단계는 모델의 **대화 능력 및 실사용에서의 응답 정렬**을 더욱 정제하는 역할을 합니다.

### **3.3.1 DPO 학습 데이터**

DPO 학습에 사용된 **선호 데이터셋은 공개된 고품질 자원들을 결합**하여 구성되었으며, 사람의 다양한 판단을 반영하도록 설계되었습니다.  
사용된 주요 데이터셋은 다음과 같습니다:

- **UltraFeedback** (Cui et al., 2024)
- **MagPie** (Xu et al., 2024c)

이러한 데이터의 조합은 **다층적인 선호 신호**를 제공하여, 모델이 **인간 기대에 더 부합하는 응답**을 생성하도록 유도합니다.

### **3.3.2 DPO 학습 세부사항**

DPO 학습은 총 **2 에폭(epoch)** 동안 진행되었으며, 다음과 같은 세팅이 사용되었습니다:

- 학습률: 2 × 10⁻⁷
- **DPO의 β 파라미터**: 0.1 (참조 정책과의 차이를 조절하는 역할)
- **Liger Kernel 라이브러리(Hsu et al., 2024)**의 최적화 커널을 통합하여 학습 효율 향상

**정성적 평가 결과**, DPO는 모델의 **응답 스타일을 선호 방향으로 효과적으로 유도**하였으며, **사전 학습 및 SFT에서 형성된 핵심 능력을 훼손하지 않고** 유지함을 확인했습니다.

**표 1**:  
**BitNet b1.58 2B4T**와 유사한 크기(1~2B 파라미터)의 **주요 오픈 가중치 full-precision LLM**들을 대상으로, **효율성과 다양한 벤치마크에서의 성능을 비교**한 결과입니다.  
모든 비교 모델은 **instruction-tuned 버전**입니다.

![](/assets/images/posts/545/img_1.png)

![](/assets/images/posts/545/img_2.png)

![](/assets/images/posts/545/img_3.png)

## **4. 평가 (Evaluation)**

![](/assets/images/posts/545/img_4.png)

**해설**:  
BitNet b1.58 2B는 **GPTQ/AWQ로 양자화된 Qwen2.5 모델들보다 낮은 메모리 사용량과 유사한 성능**을 보이며, 일부 벤치마크에서는 오히려 더 뛰어난 결과를 보입니다.  
모든 모델은 **instruction-tuned 체크포인트** 기반입니다.

### **표 3: BitNet b1.58 2B4T vs. 다른 오픈소스 1비트 LLM 성능 비교**

![](/assets/images/posts/545/img_5.png)

**해설**:  
BitNet b1.58 2B4T는 **기존 오픈소스 1비트 모델들보다 전반적으로 우수한 성능**을 보이며, 특히 **ARC, BoolQ, CommonsenseQA 등 주요 언어 이해 벤치마크에서 높은 정확도**를 기록했습니다.

### **벤치마크 설명**

- **언어 이해 및 추론**  
  ARC-Easy/Challenge (Yadav et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), PIQA (Bisk et al., 2019), OpenbookQA (Mihaylov et al., 2018), CommonsenseQA (Talmor et al., 2019)
- **세계 지식 평가**  
  TruthfulQA (Lin et al., 2022), MMLU (Hendrycks et al., 2021a)
- **독해력 평가**  
  TriviaQA (Joshi et al., 2017), BoolQ (Clark et al., 2019)
- **수학 및 코드**  
  GSM8K (Cobbe et al., 2021), MATH-500 (Hendrycks et al., 2021b), HumanEval+ (Liu et al., 2023)
- **지시 따르기 및 대화**  
  IFEval (Zhou et al., 2023), MT-bench (Zheng et al., 2023)

### **비교 대상 모델**

BitNet b1.58 2B4T는 다음과 같은 주요 오픈 가중치 full-precision LLM들과 비교되었습니다:

- **LLaMA 3.2 1B** (Dubey et al., 2024)
- **Gemma-3 1B** (Team et al., 2025)
- **Qwen2.5 1.5B** (Yang et al., 2024)
- **SmolLM2 1.7B** (Allal et al., 2025)
- **MiniCPM 2B** (Hu et al., 2024)

모든 모델은 **instruction-tuned 버전**이며, **공개된 평가 파이프라인을 통해 동일한 조건 하에 재평가**되었습니다.  
자세한 평가 세부 사항은 부록(Appendix)에 수록되어 있으며, 주요 결과는 본 섹션의 표에서 확인할 수 있습니다.

**4.1 주요 결과 (Main Results)**

표 1에서 확인할 수 있듯이, **BitNet b1.58 2B4T는 탁월한 자원 효율성**을 보여줍니다.  
디코딩 중 **임베딩을 제외한 메모리 사용량**과 **추정 에너지 소비량**(Horowitz, 2014; Zhang et al., 2022)은 평가에 포함된 모든 full-precision 모델보다 **현저히 낮아**,  
**운영 비용 절감 및 저사양 장치에 대한 배포 용이성** 측면에서 뚜렷한 이점을 나타냅니다.

과제 성능(Task Performance) 면에서도, **BitNet b1.58 2B4T는 매우 경쟁력 있는 성능**을 보입니다.  
**추론(reasoning), 지식(knowledge), 수학(math)** 등 다양한 분야의 벤치마크에서 **최고 성능**을 기록했으며,  
다른 벤치마크에서도 **최상위 full-precision 모델들과 거의 유사한 성능**을 달성했습니다.  
일부 특정 과제나 평균 점수에서 full-precision 모델이 약간 앞설 수 있으나, **BitNet b1.58 2B4T는 전체적으로 균형 잡힌 강력한 성능**을 보이며,  
**동급 모델 대비 유사한 성능을 유지하면서도 효율성은 대폭 향상**된 결과를 입증합니다.

**4.2 사후 양자화(Post-training Quantized) 모델과의 비교**

BitNet b1.58 2B4T는 또한 **INT4 방식(GPTQ, AWQ)**으로 사후 양자화된 경쟁 모델 **Qwen2.5 1.5B**와 비교를 통해 **성능-효율성 트레이드오프**도 분석하였습니다.  
그 결과는 표 2에 요약되어 있습니다.

INT4 양자화는 full-precision 모델의 메모리 사용량을 줄이긴 하지만, **BitNet b1.58 2B4T는 본질적인 1비트 아키텍처 덕분에 훨씬 더 낮은 메모리 요구량**을 달성합니다.  
중요한 점은, **이러한 메모리 효율성 향상이 성능 저하로 이어지지 않는다는 것**입니다.  
표준 PTQ 방식은 일반적으로 full-precision 모델 대비 **체감 가능한 성능 저하**를 일으키는 반면,  
**BitNet b1.58 2B4T는 Qwen2.5 1.5B의 INT4 양자화 버전보다 전반적으로 높은 성능**을 유지합니다.

이러한 비교는 **BitNet b1.58 2B4T가 기존 아키텍처에 INT4 PTQ를 적용하는 것보다 훨씬 더 유리한 효율-성능 지점을 차지**하고 있으며,  
**더 적은 자원으로 더 나은 성능을 제공**한다는 점을 시사합니다.

**4.3 오픈소스 1비트 모델과의 비교**

마지막으로, BitNet b1.58 2B4T를 **1비트 또는 근접 양자화 수준**을 목표로 설계된 다른 모델들과 비교하였습니다.  
여기에는 **소규모의 native 1비트 모델**들과, **사후 양자화를 통해 1.58비트 수준으로 양자화된 대규모 모델들**이 포함되며, 비교 결과는 표 3에 제시되어 있습니다.

평가 결과는 **BitNet b1.58 2B4T가 해당 범주의 최고 성능 모델임을 명확히 보여줍니다.**  
비교된 모든 1비트 모델보다 **전반적으로 월등한 성능**을 기록하였고, 대부분의 벤치마크에서 **최고 점수**를 달성했습니다.  
특히 주목할 점은, 이 모델이 **소규모 native 1비트 모델들뿐 아니라**, **파라미터 수가 훨씬 많은 후처리 양자화 모델들조차 능가**했다는 점입니다.

이는 **BitNet b1.58 2B4T의 native 1비트 학습 방식의 효과성**을 잘 보여주며,  
**극단적인 양자화 수준에서도 새로운 수준의 성능(State-of-the-Art)을 구현**할 수 있음을 입증합니다.  
즉, 단순한 후처리 양자화가 아닌 **설계 단계부터 1비트 기반으로 학습한 접근 방식**이 더 높은 효율성과 성능을 동시에 달성할 수 있음을 강조합니다.

**5. 추론 구현 (Inference Implementation)**

대규모 언어 모델(LLM)의 **효율적인 추론(inference)**은, 특히 **자원이 제한된 환경에서의 실용적 배포**를 위해 매우 중요합니다.  
**BitNet b1.58 2B4T**는 **1.58비트 가중치와 8비트 활성값**을 사용하는 **독자적인 양자화 방식(W1.58A8)**을 채택하고 있으며,  
표준 딥러닝 라이브러리들은 이러한 **혼합 정밀도의 저비트 형식에 최적화된 커널**을 일반적으로 제공하지 않기 때문에,  
이를 해결하기 위해 **GPU 및 CPU용 전용 추론 라이브러리**를 개발하고 오픈소스로 공개하였습니다.  
코드는 <https://aka.ms/bitnet>에서 확인할 수 있습니다.

### **5.1 GPU 추론 (GPU Inference)**

현재의 GPU 아키텍처 및 소프트웨어 라이브러리(cuBLAS, PyTorch 커널 등)는 주로 **FP16, BF16, INT8/INT4**와 같은  
표준 데이터 형식에 최적화되어 있어, BitNet b1.58 2B4T에서 요구하는 **W1.58A8 행렬 곱 연산에 대한 고성능 지원이 부족**합니다.  
이로 인해 1비트 모델이 이론적으로 제공할 수 있는 효율성을 실제 하드웨어에서 완전히 활용하기 어렵습니다.

이를 해결하기 위해, 우리는 **W1.58A8 행렬 곱 전용 커스텀 CUDA 커널**을 새로 설계하였습니다.

- **가중치(1.58비트)**는 {-1, 0, +1}의 **삼진(ternary)** 값으로 표현되며, 이를 표준 데이터 형식으로 저장하는 데 비효율적입니다.
- 우리는 이를 위해 **4개의 ternary 값을 하나의 int8로 인코딩하여 High Bandwidth Memory(HBM)에 저장**합니다.
- 계산 시에는, CUDA 커널이 HBM에서 int8 형태의 가중치를 **GPU의 빠른 Shared Memory(SRAM)**으로 로딩하고,  
  이를 **행렬 곱 직전**에 **삼진 값으로 디코딩(unpack)**하여, 8비트 활성값과 함께 곱셈 연산을 수행합니다.

이러한 **‘pack → store → load → unpack → compute’** 전략은 **메모리 대역폭 사용을 최소화**하면서,  
**맞춤형 계산 명령어를 활용**하여 효율적인 추론을 가능하게 합니다.  
추가적인 구현 세부사항과 최적화 기법은 **Ladder 프레임워크**(Wang et al., 2023b)에 상세히 설명되어 있습니다.

> 주의할 점: 현재 상용 GPU 아키텍처는 이러한 1비트 모델에 최적화되어 있지 않으므로,  
> **향후 저비트 연산 전용 하드웨어 로직이 포함된 GPU**가 BitNet b1.58 같은 모델의 **성능·에너지 효율성 극대화에 핵심적**일 것입니다.

### **5.2 CPU 추론 (CPU Inference)**

**GPU 없이도 실행 가능한 범용 환경(엣지 디바이스, 노트북, 일반 서버 등)**에서도  
BitNet b1.58을 사용할 수 있도록, 우리는 **C++ 기반의 bitnet.cpp 라이브러리**를 개발하였습니다.  
이는 **1비트 LLM 추론을 위한 공식 CPU 레퍼런스 구현**으로 제공됩니다.

- bitnet.cpp는 **표준 CPU 아키텍처에 최적화된 커널**을 제공하며,  
  모델의 양자화 방식(W1.58A8)에 맞춰 설계되어 **불필요한 일반 양자화 라이브러리 사용**이나  
  **복잡한 비트 조작 연산 없이도 높은 효율**을 제공합니다.
- 또한, BitNet b1.58의 학습 방식과 일관되게 **가중치를 처리함으로써**,  
  **학습 시와 동일한 수치 정확도(lossless inference)**를 보장합니다.

이 방식은 **1.58비트 모델을 CPU에서 직접 빠르고 정확하게 추론**할 수 있도록 하며,  
구현 세부사항 및 사용법은 bitnet.cpp 저장소 및 관련 기술 보고서(Wang et al., 2025)에서 확인할 수 있습니다.

**6. 결론 (Conclusion)**

이 기술 보고서는 **BitNet b1.58 2B4T**를 소개하였습니다. 이는 **고효율이면서도 강력한 성능을 갖춘 대규모 언어 모델(LLM)**로 나아가는 중요한 진전을 의미합니다.  
BitNet b1.58 2B4T는 **파라미터 수 20억, 학습 토큰 수 4조 규모로 처음부터 학습된 최초의 오픈소스 네이티브 1비트 LLM**이며,  
본 연구는 **극단적인 양자화(extreme quantization)**가 **학습 단계에서 직접 적용될 수 있음을 실증**했습니다.

언어 이해, 추론, 수학, 코딩, 대화 등 다양한 벤치마크 전반에 걸친 종합적인 평가 결과,  
BitNet b1.58 2B4T는 **동일한 규모의 최신 오픈 가중치 full-precision 모델과 견줄 만한 성능**을 보여주었습니다.  
무엇보다 중요한 점은, 이러한 성능을 **기존 모델 대비 훨씬 낮은 연산 자원 요구**로 달성했다는 것입니다 —  
즉, **메모리 사용량, 에너지 소비, 추론 지연(latency)**을 크게 절감할 수 있습니다.

또한, **실제 활용과 추가 연구를 촉진하기 위해**, 우리는 GPU용 커스텀 CUDA 커널과 CPU용 bitnet.cpp 라이브러리를 포함한 **최적화된 추론 구현**을 공개하였으며,  
모델 가중치 또한 Hugging Face를 통해 자유롭게 사용할 수 있도록 배포하였습니다.

BitNet b1.58 2B4T는 **고성능 LLM에 반드시 full-precision 가중치가 필요하다는 통념에 도전**하며,  
자원이 제한된 환경에서도 **강력한 언어 모델을 배포할 수 있는 가능성**을 열어,  
**고급 AI 기술에 대한 접근을 보다 민주화(democratize)**하는 데 기여할 수 있습니다.

**7. 향후 연구 방향 (Future Directions)**

BitNet b1.58 2B4T는 인상적인 결과를 보여주었지만, 앞으로도 여러 흥미로운 연구 과제가 남아 있습니다:

- **스케일링 법칙 및 대형 모델 확장**:  
  네이티브 1비트 LLM의 **스케일링 특성**을 분석하는 것은 매우 중요합니다.  
  향후에는 **7B, 13B 이상의 대형 모델**을 학습시키고, **더 큰 데이터셋**을 활용하여  
  full-precision 모델과의 성능 격차가 유지되는지를 탐색할 예정입니다.
- **하드웨어 공동 설계 및 최적화**:  
  현재 하드웨어는 1비트 모델의 잠재력을 완전히 활용하기에는 제약이 있습니다.  
  기존 하드웨어(GPU, CPU, NPU)에 대한 **고도로 최적화된 커널** 개발과 함께,  
  **1비트 연산 및 데이터 전송에 특화된 하드웨어 가속기 공동 설계**가  
  **속도 및 에너지 효율성을 획기적으로 개선**할 열쇠가 될 것입니다.
- **시퀀스 길이 확장**:  
  BitNet b1.58 2B4T가 처리할 수 있는 **최대 시퀀스 길이를 확장**하는 것도 중요합니다.  
  이는 긴 문서 요약이나 복잡한 문제 해결 등, **장기 문맥 이해가 필요한 과제**에 필수적이며,  
  특히 **chain-of-thought 추론**에서의 성능 향상에 핵심이 됩니다.  
  **저비트 모델에 적합한 효율적인 어텐션 메커니즘** 탐색이 중요 과제입니다.
- **다국어 지원(Multilingual Capabilities)**:  
  현재 모델은 **영어 중심 데이터**로 학습되었습니다.  
  향후에는 사전 학습 말뭉치를 확장하고, 필요시 아키텍처를 조정하여  
  **다국어 처리 성능을 강화**하는 것이 보다 폭넓은 활용을 위한 주요 방향입니다.
- **멀티모달 통합(Multimodal Integration)**:  
  1비트 원칙을 **멀티모달 아키텍처에 적용하는 연구** 역시 유망한 분야입니다.  
  텍스트와 이미지 등 **다양한 모달리티의 정보를 효율적으로 융합**할 수 있는  
  **저비트 기반 멀티모달 처리 기술**은 새로운 응용 가능성을 열어줄 수 있습니다.
- **이론적 이해(Theoretical Understanding)**:  
  **대규모 1비트 학습이 효과적인 근본 원인**에 대한 이론적 분석도 미해결 영역입니다.  
  **학습 동역학, 손실 지형(loss landscape), 표현 특성** 등을 정밀하게 분석함으로써  
  향후 1비트 모델의 설계와 발전에 유의미한 통찰을 제공할 수 있습니다.

이러한 방향들을 지속적으로 탐색함으로써, 우리는 **1비트 LLM의 성능과 효율성을 한 단계 더 발전시키고**,  
**보다 지속 가능하고 접근 가능한 인공지능**을 실현하는 데 기여하고자 합니다.  
BitNet b1.58 2B4T와 그 관련 도구의 오픈소스 공개는, 이러한 노력을 이어갈 **공동체의 기반**을 제공할 것입니다.
