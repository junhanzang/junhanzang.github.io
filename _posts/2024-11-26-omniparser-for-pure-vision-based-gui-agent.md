---
title: "OmniParser for Pure Vision Based GUI Agent"
date: 2024-11-26 17:34:00
categories:
  - 인공지능
---

<https://microsoft.github.io/OmniParser/>

[SOCIAL MEDIA TITLE TAG](https://microsoft.github.io/OmniParser/)

**초록**

최근 대형 비전 언어 모델의 성공은 사용자 인터페이스에서 동작하는 에이전트 시스템에 큰 잠재력을 보여주고 있습니다. 그러나 GPT-4V와 같은 멀티모달 모델이 다양한 애플리케이션을 아우르는 여러 운영 체제에서 일반적인 에이전트로 작동하는 데 있어서 그 잠재력이 과소평가되었다고 주장합니다. 이는 다음 두 가지를 충족할 수 있는 견고한 화면 분석 기술이 부족하기 때문입니다: 1. 사용자 인터페이스 내에서 상호작용 가능한 아이콘을 신뢰성 있게 식별하고, 2. 스크린샷의 다양한 요소의 의미를 이해하며 의도된 동작을 화면 상의 해당 영역에 정확하게 연관짓는 능력입니다. 이러한 격차를 메우기 위해, 우리는 사용자 인터페이스 스크린샷을 구조화된 요소로 파싱하는 종합적인 방법인 **OMNIPARSER**를 소개합니다. 이는 GPT-4V가 인터페이스의 해당 영역에 정확하게 근거한 동작을 생성할 수 있는 능력을 크게 향상시킵니다. 먼저, 인기 있는 웹 페이지와 아이콘 설명 데이터를 이용해 상호작용 가능한 아이콘 탐지 데이터셋을 제작했습니다. 이 데이터셋은 다음과 같은 특화된 모델을 미세 조정하는 데 사용되었습니다: 화면의 상호작용 가능한 영역을 파싱하기 위한 탐지 모델과 감지된 요소의 기능적 의미를 추출하기 위한 캡션 모델입니다. **OMNIPARSER**는 ScreenSpot 벤치마크에서 GPT-4V의 성능을 크게 향상시켰습니다. 또한 Mind2Web과 AITW 벤치마크에서는 스크린샷 입력만으로도 추가 정보가 필요한 GPT-4V의 기준 성능을 능가했습니다.

![](/assets/images/posts/342/img.png)

![](/assets/images/posts/342/img_1.png)

**OmniParser에 의해 파싱된 스크린샷 이미지와 로컬 의미의 예시** OmniParser의 입력은 사용자 작업과 UI 스크린샷이며, 이를 통해 다음을 생성합니다: 1. 바운딩 박스와 숫자 ID가 오버레이된 파싱된 스크린샷 이미지, 2. 추출된 텍스트와 아이콘 설명을 모두 포함한 로컬 의미.

**상호작용 가능한 영역 탐지 및 아이콘 기능 설명을 위한 데이터셋 큐레이션** 우리는 67,000개의 고유한 스크린샷 이미지를 포함한 상호작용 가능한 아이콘 탐지 데이터셋을 큐레이션했습니다. 각 이미지에는 DOM 트리에서 유도된 상호작용 가능한 아이콘의 바운딩 박스가 라벨링되어 있습니다. 먼저 ClueWeb 데이터셋에서 인기 있는 공개 URL 중 100,000개를 균등하게 샘플링하고, 각 URL의 DOM 트리에서 웹페이지의 상호작용 가능한 영역의 바운딩 박스를 수집했습니다. 또한 캡션 모델의 미세 조정을 위해 7,000개의 아이콘-설명 쌍도 수집했습니다.

![](/assets/images/posts/342/img_2.png)

**상호작용 가능한 영역 탐지 데이터셋의 예시** 바운딩 박스는 웹페이지의 DOM 트리에서 추출된 상호작용 가능한 영역을 기반으로 합니다.

**결과** 우리는 우리 모델을 SeeClick, Mind2Web, 그리고 AITW 벤치마크에서 평가했습니다. 그 결과, 우리 모델이 모든 벤치마크에서 GPT-4V 기준 성능을 능가함을 보여줍니다. 또한 스크린샷 입력만으로도 추가 정보가 필요한 GPT-4V 기준 성능을 능가함을 보여줍니다.

![](/assets/images/posts/342/img_3.png)

![](/assets/images/posts/342/img_4.png)

![](/assets/images/posts/342/img_5.png)

**다른 비전 언어 모델에 플러그인으로 사용할 준비 완료** OmniParser가 기성 비전 언어 모델의 플러그인 선택지가 될 수 있음을 추가로 증명하기 위해, 우리는 최근 발표된 비전 언어 모델인 Phi-3.5-V와 Llama-3.2-V와 결합한 OmniParser의 성능을 보여줍니다. 표에서 볼 수 있듯이, 우리 모델의 미세 조정된 상호작용 가능한 영역 탐지(ID) 모델은 GPT-4V, Phi-3.5-V, Llama-3.2-V의 모든 하위 카테고리에서 로컬 의미와 함께 사용한 Grounding DINO 모델(w.o. ID)과 비교하여 작업 성능을 상당히 개선시켰습니다. 또한, 아이콘 기능의 로컬 의미는 모든 비전 언어 모델의 성능에 크게 기여합니다. 표에서 LS는 아이콘 기능의 로컬 의미를, ID는 우리가 미세 조정한 상호작용 가능한 영역 탐지 모델을 의미합니다. 설정 'w.o. ID'는 미세 조정되지 않은 Grounding DINO 모델로 ID 모델을 대체하고, 로컬 의미를 사용하는 경우를 뜻합니다. 'w.o. ID and w.o. LS' 설정은 Grounding DINO 모델을 사용하며, 텍스트 프롬프트에서 아이콘 설명도 사용하지 않는 경우를 의미합니다.

![](/assets/images/posts/342/img_6.png)

[2408.00203v1.pdf

5.73MB](./file/2408.00203v1.pdf)
