---
title: "SAM 2: Segment Anything in Images and Videos"
date: 2024-07-30 22:52:21
categories:
  - 인공지능
---

<https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/>

[SAM 2: Segment Anything in Images and Videos | Research - AI at Meta](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)

<https://ai.meta.com/blog/segment-anything-2/>

[Introducing SAM 2: The next generation of Meta Segment Anything Model for videos and images](https://ai.meta.com/blog/segment-anything-2/)

우리는 Segment Anything Model 2 (SAM 2)를 소개합니다. 이는 이미지와 비디오에서 프롬프트 가능한 시각적 분할 문제를 해결하기 위한 기초 모델입니다. 우리는 사용자의 상호작용을 통해 모델과 데이터를 개선하는 데이터 엔진을 구축하여 현재까지 가장 큰 비디오 분할 데이터셋을 수집했습니다. 우리의 모델은 실시간 비디오 처리를 위한 스트리밍 메모리가 있는 간단한 트랜스포머 아키텍처입니다. 우리의 데이터로 훈련된 SAM 2는 다양한 작업에서 강력한 성능을 제공합니다. 비디오 분할에서는 이전 접근 방식보다 3배 적은 상호작용을 사용하면서 더 나은 정확도를 보였습니다. 이미지 분할에서는 우리의 모델이 Segment Anything Model (SAM)보다 6배 더 빠르고 더 정확합니다. 우리는 우리의 데이터, 모델 및 통찰력이 비디오 분할 및 관련 인지 작업에서 중요한 이정표가 될 것이라 믿습니다. 우리는 모델의 버전, 데이터셋 및 인터랙티브 데모를 공개할 예정입니다.

데모: <https://sam2.metademolab.com>

코드: <https://github.com/facebookresearch/segment-anything-2>

웹사이트: <https://ai.meta.com/sam2>

### 1. 소개

Segment Anything (SA)는 이미지에서 프롬프트 가능한 분할을 위한 기초 모델을 소개했습니다 (Kirillov et al., 2023). 그러나 이미지는 시각적 세그먼트가 복잡한 움직임을 보일 수 있는 실제 세계의 정적 스냅샷일 뿐이며, 멀티미디어 콘텐츠의 급속한 증가와 함께 비디오 데이터는 이제 시간적 차원을 포함하여 기록됩니다. AR/VR, 로보틱스, 자율주행차, 비디오 편집 등의 중요한 응용 프로그램은 이미지 수준의 분할을 넘어 시간적 위치를 요구합니다. 우리는 범용 시각 분할 시스템이 이미지와 비디오 모두에 적용될 수 있어야 한다고 믿습니다.

비디오에서의 분할은 시간적, 공간적 범위를 결정하는 것을 목표로 하며, 이는 이미지에서의 도전 과제 외에도 고유한 도전 과제를 제시합니다. 개체는 움직임, 변형, 가림, 조명 변화 등의 요인으로 인해 외형에 큰 변화를 겪을 수 있습니다. 비디오는 카메라 움직임, 흐림, 낮은 해상도로 인해 종종 이미지보다 품질이 낮습니다. 또한 많은 수의 프레임을 효율적으로 처리하는 것이 주요 도전 과제입니다. SA는 이미지에서의 분할 문제를 성공적으로 해결했지만, 기존 비디오 분할 모델과 데이터셋은 비디오에서 "모든 것을 분할"할 수 있는 유사한 능력을 제공하지 못합니다.

우리는 비디오와 이미지 분할을 위한 통합 모델인 Segment Anything Model 2 (SAM 2)를 소개합니다 (이미지는 단일 프레임 비디오로 간주합니다). 우리의 작업은 과제, 모델, 데이터셋을 포함합니다 (그림 1 참조). 우리는 이미지 분할을 비디오 도메인으로 일반화한 Promptable Visual Segmentation (PVS) 과제에 중점을 둡니다. 이 과제는 비디오의 모든 프레임에서 세그먼트 관심 지점을 정의하기 위해 점, 상자, 또는 마스크를 입력으로 받아 시간적-공간적 마스크(즉, '마스크렛')를 예측합니다. 마스크렛이 예측되면 추가 프레임에서 프롬프트를 제공하여 반복적으로 개선할 수 있습니다.

우리의 모델 (§4)은 단일 이미지와 비디오 프레임 전반에서 관심 객체의 분할 마스크를 생성합니다. SAM 2는 객체와 이전 상호작용에 대한 정보를 저장하는 메모리를 갖추고 있어 비디오 전반에 걸쳐 마스크렛 예측을 생성하고 이전 프레임에서 관찰된 객체의 저장된 메모리 컨텍스트를 기반으로 이를 효과적으로 수정할 수 있습니다. 우리의 스트리밍 아키텍처는 비디오 도메인으로의 SAM의 자연스러운 일반화로, 비디오 프레임을 하나씩 처리하며 대상 객체의 이전 메모리를 주목하기 위한 메모리 주의 모듈을 갖추고 있습니다. 이미지에 적용될 때, 메모리는 비어 있으며 모델은 SAM처럼 작동합니다.

![](/assets/images/posts/227/img.png)

그림 1 우리는 프롬프트 가능한 시각적 분할 과제를 해결하기 위해 Segment Anything Model 2 (SAM 2)를 소개합니다. (a) 우리의 기초 모델을 사용하여 (b), 데이터 엔진을 통해 수집된 대규모 SA-V 데이터셋에서 훈련된 모델입니다. SAM 2는 스트리밍 메모리를 활용하여 이전 프롬프트와 예측을 저장하고, 하나 또는 여러 비디오 프레임에서 프롬프트(클릭, 상자 또는 마스크)를 통해 대화형으로 영역을 분할할 수 있습니다.

우리는 주석자와 함께 루프 내에서 우리의 모델을 사용하여 새로운 도전적인 데이터를 대화형으로 주석하여 훈련 데이터를 생성하는 데이터 엔진 (§5)을 사용합니다. 대부분의 기존 비디오 분할 데이터셋과 달리, 우리의 데이터 엔진은 특정 범주의 객체로 제한되지 않고 유효한 경계를 가진 모든 객체를 분할하기 위한 훈련 데이터를 제공하는 것을 목표로 합니다. 기존의 모델 지원 접근 방식과 비교하여, SAM 2와 함께 루프 내에서 작동하는 우리의 데이터 엔진은 비슷한 품질에서 8.4배 더 빠릅니다. 최종 Segment Anything Video (SA-V) 데이터셋 (§5.2)은 50.9K 비디오에서 35.5M 마스크로 구성되어 있으며, 기존의 모든 비디오 분할 데이터셋보다 53배 더 많은 마스크를 포함합니다. SA-V는 비디오 전반에 걸쳐 다시 나타나고 가려지는 작은 객체와 부품으로 도전적입니다. 우리의 SA-V 데이터셋은 지리적으로 다양하며, SAM 2의 공정성 평가에서는 인식된 성별에 따라 비디오 분할 성능 차이가 거의 없고, 평가한 세 가지 인식된 연령 그룹 간에 거의 차이가 없음을 나타냅니다.

우리의 실험 (§6)은 SAM 2가 비디오 분할 경험에 큰 변화를 가져온다는 것을 보여줍니다. SAM 2는 이전 접근 방식보다 3배 적은 상호작용을 사용하면서 더 나은 분할 정확도를 제공할 수 있습니다. 또한, SAM 2는 다양한 평가 설정에서 기존의 비디오 객체 분할 벤치마크에서 이전 작업을 능가하며, 이미지 분할 벤치마크에서도 SAM보다 더 나은 성능을 제공하며 6배 더 빠릅니다. SAM 2는 비디오와 이미지 분포 전반에 걸쳐 다양한 제로 샷 벤치마크(비디오 분할 17개, 단일 이미지 분할 37개)에서 효과적임을 보여줍니다.

우리는 우리의 작업을 허가가 있는 오픈 라이선스 하에 공개할 예정이며, SA-V 데이터셋(CC by 4.0), SAM 2 모델의 버전(Apache 2.0), 대화형 온라인 데모(<https://sam2.metademolab.com)도> 포함됩니다.

### 2 관련 연구

#### 이미지 분할

Segment Anything (SA)은 프롬프트 가능한 이미지 분할 과제를 도입하여, 관심 객체를 지칭하는 바운딩 박스나 점과 같은 입력 프롬프트를 주면 유효한 분할 마스크를 출력하는 것을 목표로 합니다(Kirillov et al., 2023). SA-1B 데이터셋으로 훈련된 SAM은 유연한 프롬프트를 사용하여 제로샷 분할을 가능하게 하여 다양한 하위 응용 프로그램에서 채택되었습니다. 최근의 연구들은 SAM의 품질을 향상시키는 방향으로 확장되었습니다. 예를 들어, HQ-SAM(Ke et al., 2024)은 고품질 출력 토큰을 도입하고 세밀한 마스크로 모델을 훈련하여 SAM을 향상시킵니다. 또 다른 연구들은 SAM의 효율성을 높여 실제와 모바일 응용 프로그램에서 더 널리 사용될 수 있도록 하였습니다. 예를 들어 EfficientSAM(Xiong et al., 2023), MobileSAM(Zhang et al., 2023a), FastSAM(Zhao et al., 2023) 등이 있습니다. SAM의 성공은 의료 영상(Ma et al., 2024; Deng et al., 2023; Mazurowski et al., 2023; Wu et al., 2023a), 원격 탐사(Chen et al., 2024; Ren et al., 2024), 동작 분할(Xie et al., 2024), 위장된 객체 탐지(Tang et al., 2023) 등 다양한 응용 분야에서 채택으로 이어졌습니다.

#### 대화형 비디오 객체 분할 (iVOS)

대화형 비디오 객체 분할은 사용자 안내를 통해 비디오에서 객체 분할을 효율적으로 얻기 위해 중요한 과제로 떠오르고 있으며, 종종 낙서, 클릭, 바운딩 박스 형태로 나타납니다. 초기 접근법(Wang et al., 2005; Bai & Sapiro, 2007; Fan et al., 2015)은 그래프 기반 최적화를 통해 분할 주석 프로세스를 안내했습니다. 최근 접근법(Heo et al., 2020; Cheng et al., 2021b; Delatolas et al., 2024)은 모듈식 설계를 채택하여 사용자 입력을 단일 프레임에서 마스크 표현으로 변환한 후 이를 다른 프레임으로 전파합니다. 우리의 작업은 비디오 전반에 걸쳐 객체를 분할하려는 유사한 목표를 공유하며, 이 목표를 달성하기 위해 강력한 모델과 큰 규모의 다양한 데이터셋을 구축합니다. 특히, DAVIS 대화형 벤치마크(Caelles et al., 2018)는 여러 프레임에서 낙서 입력을 통해 대화형으로 객체를 분할할 수 있게 합니다. 우리는 프롬프트 가능한 비디오 분할 과제를 위한 대화형 평가 설정을 §6.1에 채택했습니다.

클릭 기반 입력은 대화형 비디오 분할을 위해 수집하기 더 쉽습니다(Homayounfar et al., 2021). 최근 연구들은 이미지에서 SAM과 마스크 기반 비디오 추적기를 결합하여 사용합니다(Cheng et al., 2023b; Yang et al., 2023; Cheng et al., 2023c) 또는 점(Rajič et al., 2023) 기반으로 합니다. 그러나 이러한 접근법에는 한계가 있습니다. 추적기는 모든 객체에 대해 작동하지 않을 수 있으며, SAM은 비디오의 이미지 프레임에서 잘 작동하지 않을 수 있으며, 모델의 오류를 대화형으로 수정할 메커니즘이 없으며, 잘못된 프레임에서 SAM을 처음부터 다시 주석하고 거기서부터 추적을 재시작하는 것 외에는 방법이 없습니다.

#### 반지도 비디오 객체 분할 (VOS)

반지도 VOS는 일반적으로 첫 번째 프레임에서 객체 마스크를 입력으로 받아 비디오 전반에 걸쳐 정확히 추적해야 합니다(Pont-Tuset et al., 2017). 이는 첫 번째 프레임에서만 객체 외형의 감독 신호로 볼 수 있는 입력 마스크 때문에 "반지도"라고 불립니다. 이 과제는 비디오 편집, 로보틱스, 자동 배경 제거 등 다양한 응용 프로그램에서의 관련성으로 인해 큰 주목을 받았습니다. 초기 신경망 기반 접근법은 종종 첫 번째 비디오 프레임(Caelles et al., 2016; Perazzi et al., 2016; Yoon et al., 2017; Maninis et al., 2017; Hu et al., 2018a; Bhat et al., 2020; Robinson et al., 2020) 또는 모든 프레임(Voigtlaender & Leibe, 2017)에서 온라인 미세 조정을 사용하여 모델을 대상 객체에 맞췄습니다. 빠른 추론은 첫 번째 프레임(Hu et al., 2018b; Chen et al., 2018) 또는 이전 프레임(Oh et al., 2018; Yang et al., 2018, 2020)을 조건으로 하는 오프라인 훈련 모델을 통해 달성되었습니다. 이러한 다중 조건부는 RNN(Xu et al., 2018a) 및 교차 주의(Oh et al., 2019; Cheng et al., 2021a; Li et al., 2022a; Yang et al., 2021b, 2024; Cheng & Schwing, 2022; Yang & Yang, 2022; Wang et al., 2022; Cheng et al., 2023a; Goyal et al., 2023)와 함께 모든 프레임으로 확장되었습니다. 최근 접근법(Zhang et al., 2023b; Wu et al., 2023b)은 단일 비전 트랜스포머를 확장하여 현재 프레임과 이전 모든 프레임 및 관련 예측을 공동으로 처리하여 단순한 아키텍처를 만들었지만 추론 비용이 매우 큽니다. 반지도 VOS는 첫 번째 비디오 프레임에서 마스크 프롬프트만 제공하는 것과 같으므로 우리의 프롬프트 가능한 시각적 분할(PVS) 과제의 특수한 경우로 볼 수 있습니다. 그러나 첫 번째 프레임에서 필요한 고품질 객체 마스크를 주석하는 것은 실질적으로 어렵고 시간이 많이 걸립니다.

#### 비디오 분할 데이터셋

VOS 과제를 지원하기 위해 많은 데이터셋이 제안되었습니다. 초기 VOS 데이터셋(Prest et al., 2012; Li et al., 2013; Ochs et al., 2014; Fan et al., 2015), 예를 들어 DAVIS(Pont-Tuset et al., 2017; Caelles et al., 2019)는 고품질 주석을 포함하지만, 그 제한된 크기는 딥러닝 기반 접근법을 훈련하기에 충분하지 않습니다. 94개의 객체 카테고리를 4천 개 이상의 비디오에 걸쳐 다루는 YouTube-VOS(Xu et al., 2018b)는 VOS 과제를 위한 최초의 대규모 데이터셋입니다. 알고리즘이 개선되고 벤치마크 성능이 포화되기 시작하면서 연구자들은 차폐(Qi et al., 2022; Ding et al., 2023), 긴 비디오(Hong et al., 2023, 2024), 극단적 변형(Tokmakov et al., 2022), 객체 다양성(Wang et al., 2021b, 2023) 또는 장면 다양성(Athar et al., 2022)에 중점을 두어 VOS 과제의 난이도를 높이기 위해 노력해 왔습니다. 우리는 현재의 비디오 분할 데이터셋이 "비디오에서 모든 것을 분할"할 수 있는 능력을 달성하기에 충분한 범위를 제공하지 않는다고 봅니다. 이들의 주석은 일반적으로 전체 객체(부분이 아닌)를 다루며 데이터셋은 종종 사람, 차량, 동물과 같은 특정 객체 클래스 중심입니다. 이러한 데이터셋과 비교하여, 우리가 공개하는 SA-V 데이터셋은 전체 객체뿐만 아니라 객체 부분도 광범위하게 다루고 있으며, 마스크 수가 한 자릿수 이상 더 많습니다.

![](/assets/images/posts/227/img_1.png)

### 그림 2 SAM 2를 사용한 대화형 분할.

#### 단계 1 (선택):

프레임 1에서 SAM 2에 프롬프트를 주어 목표 객체(혀)의 세그먼트를 얻습니다. 초록색/빨간색 점은 각각 긍정/부정 프롬프트를 나타냅니다. SAM 2는 자동으로 세그먼트를 다음 프레임으로 전파하여 마스크렛을 형성합니다(파란색 화살표). SAM 2가 객체를 잃어버리면(프레임 2 이후), 새로운 프레임에 추가 프롬프트를 제공하여 마스크렛을 수정할 수 있습니다(빨간색 화살표).

#### 단계 2 (정제):

프레임 3에서의 단일 클릭으로 객체를 복구하고 올바른 마스크렛을 얻을 수 있습니다. 분리된 SAM + 비디오 추적기 접근법은 프레임 3에서 객체를 올바르게 다시 주석하기 위해 여러 번의 클릭이 필요합니다(프레임 1에서처럼). 그러나 SAM 2의 메모리를 사용하면 단일 클릭으로 혀를 복구할 수 있습니다.

### 3 과제: 프롬프트 가능한 시각적 분할 (PVS)

PVS 과제는 비디오의 임의의 프레임에서 모델에 프롬프트를 제공할 수 있게 합니다. 프롬프트는 긍정/부정 클릭, 바운딩 박스, 또는 마스크로 객체를 정의하거나 모델이 예측한 것을 정제하는 데 사용할 수 있습니다. 대화형 경험을 제공하기 위해 특정 프레임에서 프롬프트를 받으면, 모델은 즉시 이 프레임에서 객체의 유효한 분할 마스크를 반환해야 합니다. 초기 프롬프트(하나 또는 여러 개, 동일 프레임 또는 다른 프레임)를 받은 후, 모델은 비디오 전체에 걸쳐 객체의 마스크렛을 얻기 위해 이러한 프롬프트를 전파해야 하며, 이는 비디오의 모든 프레임에서 대상 객체의 분할 마스크를 포함합니다. 추가 프롬프트는 비디오 전체에 걸쳐 세그먼트를 정제하기 위해 임의의 프레임에서 제공될 수 있습니다 (그림 2의 예시 참조). 과제에 대한 자세한 내용은 §A를 참조하십시오.

다음 섹션에서 소개하는 SAM 2 (§4)는 SA-V 데이터셋을 구축하기 위해 PVS 과제에 데이터 수집 도구로 적용됩니다 (§5). 모델은 여러 프레임에 걸친 주석을 포함하는 대화형 비디오 분할 시나리오를 시뮬레이션하여 온라인 및 오프라인 설정에서 평가됩니다 (§6). 또한, 주석이 첫 번째 프레임으로 제한된 기존 반지도 VOS 설정과 SA 벤치마크에서의 이미지 분할도 평가됩니다.

### 4 모델

우리의 모델은 비디오(및 이미지) 도메인으로의 SAM의 일반화로 볼 수 있습니다. SAM 2 (그림 3)는 비디오 전체에 걸쳐 분할될 객체의 공간적 범위를 정의하기 위해 개별 프레임에서 점, 상자, 마스크 프롬프트를 지원합니다. 이미지 입력의 경우, 모델은 SAM과 유사하게 작동합니다. 프롬프트 가능하고 가벼운 마스크 디코더는 현재 프레임에서 프레임 임베딩과 프롬프트(있는 경우)를 받아 해당 프레임의 분할 마스크를 출력합니다. 프롬프트는 프레임에서 마스크를 정제하기 위해 반복적으로 추가될 수 있습니다.

SAM과 달리, SAM 2 디코더에서 사용되는 프레임 임베딩은 이미지 인코더에서 직접 가져오는 것이 아니라 과거 예측 및 프롬프트된 프레임의 메모리에 의해 조건화됩니다. 프롬프트된 프레임은 현재 프레임보다 "미래"에서 올 수도 있습니다. 프레임의 메모리는 현재 예측을 기반으로 메모리 인코더에 의해 생성되어, 후속 프레임에서 사용하기 위해 메모리 뱅크에 저장됩니다. 메모리 주의 작업은 이미지 인코더에서 프레임별 임베딩을 가져와 메모리 뱅크에 조건화된 임베딩을 생성하여 마스크 디코더로 전달합니다.

개별 구성 요소와 훈련에 대해서는 아래에 설명하며, 더 자세한 내용은 부록 C에서 제공됩니다.

![](/assets/images/posts/227/img_2.png)

### 그림 3

SAM 2 아키텍처. 주어진 프레임에 대해, 분할 예측은 현재 프롬프트와/또는 이전에 관찰된 메모리에 조건화됩니다. 비디오는 이미지 인코더에 의해 한 번에 하나의 프레임씩 소비되는 스트리밍 방식으로 처리되며, 이전 프레임에서의 대상 객체의 메모리에 교차 주의를 기울입니다. 마스크 디코더는 선택적으로 입력 프롬프트도 받아 해당 프레임의 분할 마스크를 예측합니다. 마지막으로, 메모리 인코더는 예측과 이미지 인코더 임베딩(그림에는 표시되지 않음)을 변환하여 향후 프레임에서 사용합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

강화학습에서 자주보던, Experience Replay Memory처럼 과거의 경험을 가져오는 형식인데 여기서는 미래에 대한 값도 있는 방식. 이를 바탕으로 simple logic들에 대해서도 진행하는 방법을 알았다.

일정 공간안에 사람이 그린것처럼 도면을 그릴때, frame처럼 단계별로 우리도 나누어서 해당 알고리즘의 이동으로 하면 될 것이다.

autocad 같은 곳에도 적용가능해보임

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 이미지 인코더

임의 길이의 비디오를 실시간으로 처리하기 위해 스트리밍 접근 방식을 사용하여 비디오 프레임을 이용할 수 있게 될 때마다 소비합니다. 이미지 인코더는 전체 상호작용 동안 한 번만 실행되며, 각 프레임을 나타내는 비조건화된 토큰(피처 임베딩)을 제공합니다. 우리는 계층적 구조로 다중 스케일 피처를 디코딩에 사용할 수 있게 해주는 MAE(He et al., 2022)로 사전 훈련된 Hiera(Ryali et al., 2023; Bolya et al., 2023) 이미지 인코더를 사용합니다.

### 메모리 어텐션

메모리 어텐션의 역할은 현재 프레임 피처를 과거 프레임 피처와 예측 및 새로운 프롬프트에 조건화하는 것입니다. 우리는 L개의 트랜스포머 블록을 쌓는데, 첫 번째 블록은 현재 프레임의 이미지 인코딩을 입력으로 받습니다. 각 블록은 자기 어텐션을 수행한 후 메모리 뱅크(아래 참조)에 저장된 프레임과 객체 포인터(아래 참조)의 메모리에 교차 어텐션을 수행하며, 그 다음 MLP를 수행합니다. 우리는 효율적인 어텐션 커널의 최근 발전을 활용하기 위해 일반 어텐션 연산을 사용합니다(Dao, 2023).

### 프롬프트 인코더 및 마스크 디코더

우리의 프롬프트 인코더는 SAM과 동일하며 클릭(긍정 또는 부정), 바운딩 박스, 또는 마스크로 주어진 프레임에서 객체의 범위를 정의할 수 있습니다. 희소 프롬프트는 각 프롬프트 유형에 대한 학습된 임베딩과 더해진 위치 인코딩으로 표현되며, 마스크는 컨볼루션을 사용하여 임베딩되고 프레임 임베딩과 합쳐집니다.

우리의 디코더 디자인은 SAM을 대부분 따릅니다. 우리는 프롬프트와 프레임 임베딩을 업데이트하는 "양방향" 트랜스포머 블록을 쌓습니다. SAM에서처럼, 단일 클릭과 같은 모호한 프롬프트의 경우, 여러 호환 가능한 대상 마스크가 있을 수 있습니다. 우리는 여러 마스크를 예측합니다. 이 설계는 모델이 유효한 마스크를 출력하도록 보장하는 데 중요합니다. 비디오에서는 모호성이 여러 비디오 프레임에 걸쳐 확장될 수 있어, 모델은 각 프레임에서 여러 마스크를 예측합니다. 후속 프롬프트가 모호성을 해결하지 못하면, 모델은 현재 프레임에 대해 가장 높은 예측 IoU( Intersection over Union )를 가진 마스크만 전파합니다.

SAM과 달리 긍정 프롬프트가 주어진 경우 항상 유효한 객체가 분할되어야 하지만, PVS 과제에서는 일부 프레임에서 유효한 객체가 존재하지 않을 수 있습니다(예: 가림으로 인해). 이러한 새로운 출력 모드를 처리하기 위해 우리는 관심 객체가 현재 프레임에 존재하는지 예측하는 추가 헤드를 추가합니다. 또 다른 차이점은 메모리 어텐션을 우회하여 마스크 디코딩을 위해 계층적 이미지 인코더에서 고해상도 정보를 통합하는 것입니다 (§C 참조).

### 메모리 인코더

메모리 인코더는 출력 마스크를 컨볼루션 모듈을 사용하여 다운샘플링하고 이미지 인코더에서 비조건화된 프레임 임베딩과 요소별로 합친 후, 경량 컨볼루션 레이어를 통해 정보를 융합하여 메모리를 생성합니다.

### 메모리 뱅크

메모리 뱅크는 비디오에서 대상 객체에 대한 과거 예측 정보를 유지하여 최대 N개의 최근 프레임의 메모리를 유지하는 FIFO 큐와 최대 M개의 프롬프트된 프레임의 메모리를 유지하는 FIFO 큐를 유지합니다. 예를 들어, 초기 마스크가 유일한 프롬프트인 VOS 과제에서 메모리 뱅크는 항상 첫 번째 프레임의 메모리와 최근의 N개의 프레임(프롬프트되지 않은)의 메모리를 유지합니다. 두 메모리 집합은 공간적 피처 맵으로 저장됩니다.

공간적 메모리 외에도, 우리는 각 프레임의 마스크 디코더 출력 토큰을 기반으로 객체의 고수준 의미 정보를 위한 경량 벡터로 객체 포인터 목록을 저장합니다(Meinhardt et al., 2022). 우리의 메모리 어텐션은 공간적 메모리 피처와 이 객체 포인터 모두에 교차 어텐션을 수행합니다. 우리는 최근 N개의 프레임의 메모리에 시간적 위치 정보를 임베딩하여 모델이 단기 객체 움직임을 나타낼 수 있도록 하지만, 프롬프트된 프레임의 메모리에는 임베딩하지 않습니다. 왜냐하면 프롬프트된 프레임의 훈련 신호는 더 희소하고 훈련 중에 본 시간적 범위와 매우 다른 시간적 범위에서 프롬프트된 프레임이 올 수 있는 추론 설정으로 일반화하기 더 어렵기 때문입니다.

### 훈련

모델은 이미지와 비디오 데이터를 함께 훈련합니다. 이전 작업(Kirillov et al., 2023; Sofiiuk et al., 2022)과 유사하게, 우리는 모델의 대화형 프롬프트를 시뮬레이션합니다. 우리는 8개의 프레임 시퀀스를 샘플링하고 프롬프트로 최대 2개의 프레임을 무작위로 선택하며, 훈련 중에 모델 예측 및 실제 마스크렛을 사용하여 확률적으로 수정 클릭을 받습니다. 훈련 과제는 순차적으로(및 "대화형으로") 실제 마스크렛을 예측하는 것입니다. 모델에 대한 초기 프롬프트는 확률 0.5로 실제 마스크, 확률 0.25로 실제 마스크에서 샘플링된 긍정 클릭, 또는 확률 0.25로 바운딩 박스 입력일 수 있습니다. 자세한 내용은 §C를 참조하십시오.

### 5 데이터

비디오에서 "모든 것을 분할"하는 능력을 개발하기 위해, 우리는 대규모이면서도 다양한 비디오 분할 데이터셋을 수집하기 위한 데이터 엔진을 구축했습니다. 우리는 인간 주석자와 상호작용하는 모델을 사용합니다. Kirillov et al. (2023)과 유사하게, 주석된 마스크렛에 대한 의미적 제약을 두지 않으며, 전체 객체(예: 사람)와 부품(예: 사람의 모자)에 집중합니다. 우리의 데이터 엔진은 주석자에게 제공되는 모델 지원 수준에 따라 세 단계로 구분됩니다. 다음으로 각 데이터 엔진 단계와 SA-V 데이터셋을 설명합니다.

#### 5.1 데이터 엔진

**단계 1: SAMper 프레임** 초기 단계에서는 인간 주석을 지원하기 위해 이미지 기반 대화형 SAM (Kirillov et al., 2023)을 사용했습니다. 주석자는 SAM과 "브러시"와 "지우개"와 같은 픽셀 정밀 수동 편집 도구를 사용하여 비디오의 매 프레임에서 6 FPS로 목표 객체의 마스크를 주석해야 했습니다. 다른 프레임으로 마스크를 시간적으로 전파하는 추적 모델이 없기 때문에 이 방법은 프레임별로 모든 프레임에서 마스크 주석이 필요하여 프로세스가 느리며, 우리의 실험에서 평균 주석 시간은 프레임당 37.8초였습니다. 그러나 이는 프레임별 고품질 공간 주석을 제공합니다. 이 단계에서는 1.4K 비디오에서 16K 마스크렛을 수집했습니다. 우리는 또한 평가 중 SAM 2의 잠재적 편향을 줄이기 위해 SA-V 검증 및 테스트 세트를 주석하는 데 이 접근 방식을 사용했습니다.

**단계 2: SAM+SAM2 마스크** 두 번째 단계에서는 SAM 2를 추가하여 루프 내에 SAM 2 마스크를 프롬프트로만 받아들이도록 했습니다. 우리는 이 버전을 SAM 2 마스크라고 부릅니다. 주석자는 1단계에서와 같이 SAM 및 기타 도구를 사용하여 첫 번째 프레임에서 공간 마스크를 생성한 다음, SAM 2 마스크를 사용하여 주석된 마스크를 다른 프레임으로 시간적으로 전파하여 전체 시간-공간 마스크렛을 얻습니다. 이후의 비디오 프레임에서 주석자는 SAM, "브러시" 및/또는 "지우개"로 마스크를 처음부터 주석하여 SAM 2 마스크로 다시 전파함으로써 SAM 2 마스크의 예측을 공간적으로 수정할 수 있으며, 이 과정을 반복하여 마스크렛을 올바르게 수정할 수 있습니다. SAM 2 마스크는 처음에 1단계 데이터와 공개된 데이터셋으로 훈련되었습니다. 2단계 동안 수집된 데이터를 사용하여 SAM 2 마스크를 두 번 재훈련하고 업데이트했습니다. 2단계에서는 63.5K 마스크렛을 수집했습니다. 주석 시간은 프레임당 7.4초로 줄어들어 1단계보다 약 5.1배 빨라졌습니다. 주석 시간은 개선되었지만, 이 분리된 접근 방식은 이전 메모리 없이 중간 프레임에서 마스크를 처음부터 주석해야 합니다. 우리는 상호작용 이미지 분할 및 마스크 전파를 통합 모델에서 수행할 수 있는 완전한 기능의 SAM 2를 개발하는 방향으로 나아갔습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

"마스크렛(masklet)"은 비디오 분할 작업에서 사용되는 용어로, 특정 객체의 분할 마스크가 비디오의 여러 프레임에 걸쳐 연결된 것을 의미합니다. 이는 단일 프레임의 분할 마스크가 아니라, 비디오 전체에서 특정 객체의 분할을 나타내는 일련의 마스크를 가리킵니다. 마스크렛은 시간적, 공간적 일관성을 유지하며 객체의 움직임과 변형을 따라가도록 설계되었습니다.

즉, "마스크렛"은 비디오의 각 프레임에서 동일한 객체를 추적하고 분할하는 데 사용되는 일련의 마스크로, 객체가 비디오 전반에 걸쳐 어떻게 이동하고 변화하는지를 반영합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**단계 3: SAM 2** 마지막 단계에서는 포인트 및 마스크를 포함한 다양한 프롬프트를 수용하는 완전한 기능의 SAM 2를 사용합니다. SAM 2는 시간적 차원에 걸친 객체의 메모리를 활용하여 마스크 예측을 생성합니다. 이는 주석자가 중간 프레임에서 예측된 마스크렛을 편집하기 위해 가끔씩만 수정 클릭을 제공하면 되며, 공간적 SAM으로 처음부터 주석할 필요가 없음을 의미합니다. 3단계 동안 수집된 주석을 사용하여 SAM 2를 다섯 번 재훈련하고 업데이트했습니다. 루프 내에서 SAM 2를 사용하면 프레임당 주석 시간이 4.5초로 줄어들어 1단계보다 약 8.4배 빨라졌습니다. 3단계에서는 197.0K 마스크렛을 수집했습니다.

![](/assets/images/posts/227/img_3.png)

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  |  |  |  |
| 모델 사용 단계 | 프레임당 시간 | 편집된 프레임 | 클릭된 프레임당 클릭 수 | 마스크 정렬 점수 (IoU>0.75) |
| 1단계 SAM만 | 37.8초 | 100.00 % | 4.80 | - |
| 2단계 SAM + SAM 2 마스크 | 7.4초 | 23.25 % | 3.61 | 86.4 % |
| 3단계 SAM 2 | 4.5초 | 19.04 % | 2.68 | 89.1 % |

표 1 데이터 엔진 단계의 진화를 보여주며, 프레임당 평균 주석 시간, 마스크렛당 편집된 프레임의 평균 비율, 클릭된 프레임당 수동 클릭 수 및 마스크 크기별 1단계와의 마스크 정렬을 나타냅니다.

### 품질 검증

주석의 높은 기준을 유지하기 위해 검증 단계를 도입했습니다. 별도의 주석자 그룹이 각 주석된 마스크렛의 품질을 "만족스러움" (모든 프레임에서 목표 객체를 정확하고 일관되게 추적) 또는 "불만족" (목표 객체는 명확한 경계로 잘 정의되었으나 마스크렛이 정확하거나 일관되지 않음)으로 검토하는 작업을 맡습니다. 불만족스러운 마스크렛은 정제를 위해 주석 파이프라인으로 다시 보내집니다. 잘 정의되지 않은 객체를 추적하는 마스크렛은 전적으로 거부됩니다.

### 자동 마스크렛 생성

주석의 다양성을 보장하는 것은 모델의 범용성을 확보하는 데 중요합니다. 인간 주석자가 주로 두드러진 객체에 집중할 수 있기 때문에, 자동으로 생성된 마스크렛("Auto"라고 함)을 추가하여 주석을 보강합니다. 이는 주석 범위를 확장하고 모델 실패 사례를 식별하는 두 가지 목적을 제공합니다. 자동 마스크렛을 생성하기 위해, 첫 번째 프레임에서 SAM 2에 정규 그리드의 점으로 프롬프트를 제공하여 후보 마스크렛을 생성합니다. 그런 다음 이 후보들은 마스크렛 검증 단계로 보내져 필터링됩니다. "만족스러움"으로 태그된 자동 마스크렛은 SA-V 데이터셋에 추가됩니다. "불만족"으로 식별된 마스크렛(즉, 모델 실패 사례)은 샘플링되어 주석자에게 제공되어 데이터 엔진의 3단계에서 SAM 2와 함께 정제됩니다. 이러한 자동 마스크렛은 크고 두드러진 중앙 객체뿐만 아니라 배경의 다양한 크기와 위치의 객체도 포함합니다.

### 분석

표 1은 각 데이터 엔진 단계의 주석 프로토콜을 제어된 실험을 통해 비교한 결과입니다(자세한 내용은 §D.2.2 참조). 프레임당 평균 주석 시간, 마스크렛당 수동 편집된 프레임의 평균 비율, 클릭된 프레임당 평균 클릭 수를 비교합니다. 품질 평가를 위해 Phase 1 마스크 정렬 점수를 정의하며, 이는 해당 마스크의 IoU가 Phase 1의 대응 마스크와 비교하여 0.75를 초과하는 마스크의 비율을 의미합니다. Phase 1 데이터는 프레임별로 높은 품질의 수동 주석을 포함하므로 기준으로 선택되었습니다. SAM 2가 포함된 Phase 3는 효율성과 비교 가능한 품질을 증가시키며, Phase 1보다 8.4배 빠르고, 편집된 프레임 비율과 프레임당 클릭 수가 가장 적으며, 더 나은 정렬 결과를 제공합니다.

표 2에서는 각 단계의 끝에서 사용 가능한 데이터로 훈련된 SAM 2의 성능을 비교하여 추가 데이터의 영향을 측정합니다. 우리는 자체 SA-V 검증 세트와 9개의 제로샷 벤치마크(자세한 내용은 §E.1 참조)에서 3-클릭으로 프롬프트를 주어 표준 J&F 정확도 메트릭(높을수록 좋음)을 사용하여 평가합니다. 각 단계를 통해 데이터를 반복적으로 포함한 후 일관된 개선이 관찰되었으며, 이는 SA-V 검증 세트뿐만 아니라 9개의 제로샷 벤치마크에서도 나타났습니다.

![](/assets/images/posts/227/img_4.png)

|  |  |  |
| --- | --- | --- |
|  |  |  |
| 훈련 데이터 | SA-V 검증 | 9 제로샷 |
| VOS + SA-1B | 50.0 | 62.5 |
| + Phase 1 | 53.0 | 66.9 |
| + Phase 2 | 58.8 | 70.9 |
| + Phase 3 | 62.5 | 71.2 |
| + Auto | 63.2 | 71.5 |

표 2 각 데이터 엔진 단계에서 데이터를 추가하여 분할 정확도(J&F 메트릭)가 개선된 결과를 보여줍니다. "VOS"는 비디오 객체 분할 데이터셋을 의미합니다. 자세한 내용은 §E에 있습니다.

### 5.2 SA-V 데이터셋

우리의 데이터 엔진을 사용하여 수집한 SA-V 데이터셋은 50.9K개의 비디오와 642.6K개의 마스크렛으로 구성됩니다. 표 3에서는 SA-V 구성 요소를 기존의 일반적인 VOS 데이터셋과 비디오 수, 마스크렛 수, 마스크 수를 기준으로 비교합니다. 특히, 주석된 마스크 수는 기존의 VOS 데이터셋보다 53배(자동 주석을 제외하면 15배) 더 많아 향후 연구를 위한 중요한 자원을 제공합니다. 우리는 SA-V를 자유로운 라이선스 하에 공개할 예정입니다.

#### 비디오

우리는 크라우드 워커들이 촬영한 50.9K개의 새로운 비디오 세트를 수집했습니다. 비디오는 54% 실내 장면과 46% 실외 장면으로 구성되어 있으며, 평균 지속 시간은 14초입니다. 비디오는 다양한 일상적인 시나리오를 다루며, "야생에서" 다양한 환경을 특징으로 합니다. 우리의 데이터셋은 기존의 VOS 데이터셋보다 더 많은 비디오를 포함하고 있으며, 그림 5에 표시된 바와 같이, 비디오는 47개국에 걸쳐 있으며 다양한 참가자들에 의해 촬영되었습니다(자체 보고된 인구 통계).

![](/assets/images/posts/227/img_5.png)

#### 그림 4

SA-V 데이터셋의 예제 비디오로, 수동 및 자동으로 생성된 마스크렛이 겹쳐져 있습니다. 각 마스크렛은 고유한 색상을 가지며, 각 행은 하나의 비디오에서 1초 간격으로 캡처된 프레임을 나타냅니다.

### 표 3: SA-V 데이터셋과 오픈 소스 VOS 데이터셋 비교

![](/assets/images/posts/227/img_6.png)

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |
| 데이터셋 | 비디오 수 | 지속 시간 | 마스크렛 수 | 마스크 수 | 프레임 수 | 소실율 |
| DAVIS 2017 (Pont-Tuset et al., 2017) | 0.2K | 0.1시간 | 0.4K | 27.1K | 10.7K | 16.1% |
| YouTube-VOS (Xu et al., 2018b) | 4.5K | 5.6시간 | 8.6K | 197.3K | 123.3K | 13.0% |
| UVO-dense (Wang et al., 2021b) | 1.0K | 0.9시간 | 10.2K | 667.1K | 68.3K | 9.2% |
| VOST (Tokmakov et al., 2022) | 0.7K | 4.2시간 | 1.5K | 175.0K | 75.5K | 41.7% |
| BURST (Athar et al., 2022) | 2.9K | 28.9시간 | 16.1K | 600.2K | 195.7K | 37.7% |
| MOSE (Ding et al., 2023) | 2.1K | 7.4시간 | 5.2K | 431.7K | 638.8K | 41.5% |
| Internal | 62.9K | 281.8시간 | 69.6K | 5.4M | 6.0M | 36.4% |
| SA-V Manual | 50.9K | 196.0시간 | 190.9K | 10.0M | 4.2M | 42.5% |
| SA-V Manual+Auto | 50.9K | 196.0시간 | 642.6K | 35.5M | 4.2M | 27.7% |

SA-V Manual은 수동으로 주석된 레이블만 포함합니다. SA-V Manual+Auto는 수동으로 주석된 레이블과 자동으로 생성된 마스크렛을 결합한 것입니다.

![](/assets/images/posts/227/img_7.png)

### 그림 5: 데이터셋 분포

(a) 마스크렛 크기 분포 (비디오 해상도로 정규화됨) (b) 비디오의 지리적 다양성 (c) 비디오를 녹화한 크라우드워커의 자체 보고된 인구 통계

### 마스크렛

주석에는 190.9K개의 수동 마스크렛 주석과 데이터 엔진을 사용하여 수집한 451.7K개의 자동 마스크렛이 포함됩니다. 그림 4에는 수동 및 자동으로 생성된 마스크렛이 겹쳐진 예제 비디오가 표시되어 있습니다. SA-V는 가장 큰 VOS 데이터셋보다 53배(자동 주석을 제외하면 15배) 더 많은 마스크를 보유하고 있습니다. SA-V 수동 주석의 소실율(최소한 한 프레임에서 사라졌다가 다시 나타나는 주석된 마스크렛의 비율)은 42.5%로, 기존 데이터셋 중 경쟁력 있는 수준입니다. 그림 5a는 DAVIS, MOSE, YouTubeVOS와 비교한 마스크 크기 분포(비디오 해상도로 정규화됨)를 보여줍니다. SA-V 마스크의 88% 이상이 정규화된 마스크 면적이 0.1 미만입니다.

### SA-V 훈련, 검증 및 테스트 분할

SA-V는 비디오 작성자(및 그들의 지리적 위치)를 기준으로 분할하여 유사한 객체의 중복을 최소화합니다. SA-V 검증 및 테스트 세트를 생성하기 위해, 우리는 도전적인 시나리오에 중점을 두고 비디오를 선택하며, 주석자에게 빠르게 움직이거나 다른 객체에 의해 복잡하게 가려지거나 소실/재등장 패턴을 보이는 도전적인 목표를 식별하도록 요청합니다. 이러한 목표는 §5.1의 1단계 설정을 사용하여 6 FPS로 주석되었습니다. SA-V 검증 세트는 293개의 마스크렛과 155개의 비디오로 구성되어 있으며, SA-V 테스트 세트는 278개의 마스크렛과 150개의 비디오로 구성되어 있습니다.

### 내부 데이터셋

우리는 또한 훈련 세트를 추가로 증강하기 위해 내부적으로 사용 가능한 라이센스 비디오 데이터를 사용했습니다. 내부 데이터셋은 훈련을 위해 §5.1의 2단계와 3단계에서 주석된 62.9K개의 비디오와 69.6K개의 마스크렛으로 구성되어 있으며, 테스트를 위해 1단계를 사용하여 주석된 96개의 비디오와 189개의 마스크렛(내부 테스트)으로 구성되어 있습니다. 데이터 엔진 및 SA-V 데이터셋에 대한 자세한 내용은 부록 D를 참조하십시오.

### 6 제로샷 실험

여기에서는 제로샷 비디오 과제 (§6.1) 및 이미지 과제 (§6.2)에서 SAM 2를 이전 연구와 비교합니다. 우리는 비디오 작업에 대해 표준 J&F 메트릭(Pont-Tuset et al., 2017)을, 이미지 작업에 대해 mIoU( Mean Intersection over Union ) 메트릭을 보고합니다. 별도로 언급하지 않는 한, 이 섹션에서 보고된 결과는 Hiera-B+ 이미지 인코더를 사용하여 1024 해상도로 훈련된 데이터셋 전체 조합에서 기본 설정을 따릅니다. 이는 표 7에 나와 있는 SAM 2 (Hiera-B+)입니다(자세한 내용은 §C.2 참조).

### 6.1 비디오 과제

#### 6.1.1 프롬프트 가능한 비디오 분할

![](/assets/images/posts/227/img_8.png)

(a) 오프라인 평균 J&F (3-클릭) (b) 온라인 평균 J&F (3-클릭)

그림 6 상호작용 오프라인 및 온라인 평가 설정에서 9개 데이터셋에 대한 제로샷 정확도.

우리는 먼저 사용자 경험을 유사하게 시뮬레이션하는 대화형 설정을 포함하는 프롬프트 가능한 비디오 분할을 평가합니다. 우리는 두 가지 설정을 사용합니다: 오프라인 평가(비디오를 여러 번 반복하여 가장 큰 모델 오류에 따라 상호작용할 프레임을 선택)와 온라인 평가(비디오를 한 번의 전방 패스로 통해 프레임을 주석). 이러한 평가는 프레임당 3개의 클릭(Nclick = 3)을 사용하여 9개의 밀집 주석된 제로샷 비디오 데이터셋에서 수행됩니다(자세한 내용은 §E.1 참조).

우리는 비디오 객체 분할을 위한 두 가지 최신 모델, XMem++ (Bekuzarov et al., 2023)과 Cutie (Cheng et al., 2023a)를 기반으로 두 개의 강력한 기준선(SAM+XMem++ 및 SAM+Cutie)을 생성합니다. XMem++를 사용하여 하나 또는 여러 프레임에서 마스크 입력을 기반으로 비디오 분할을 생성합니다. SAM은 초기 마스크를 제공하거나 현재 분할을 마스크 프롬프트로 사용하여 출력을 정제하는 데 사용됩니다. SAM+Cutie 기준선의 경우, Cutie를 수정하여 여러 프레임에서 마스크 입력을 받을 수 있도록 합니다.

그림 6에서 우리는 상호작용한 프레임 Nframe = 1, . . . , 8에 대한 평균 J&F 정확도를 보고합니다. SAM 2는 오프라인 및 온라인 평가 설정 모두에서 SAM+XMem++ 및 SAM+Cutie를 능가합니다. 모든 9개 데이터셋(데이터셋별 결과는 §E.1 참조)에서 SAM 2는 두 방법을 압도하며, 몇 번의 클릭으로 고품질 비디오 분할을 생성할 수 있음을 확인합니다. 또한 추가 프롬프트로 결과를 지속적으로 정제할 수 있습니다. 전체적으로, SAM 2는 3배 적은 상호작용으로 더 나은 분할 정확도를 생성할 수 있습니다.

### 6.1.2 반지도 비디오 객체 분할

![](/assets/images/posts/227/img_9.png)

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |
| 방법 | 1-클릭 | 3-클릭 | 5-클릭 | 바운딩 박스 | 실제 마스크‡ |
| SAM+XMem++ | 56.9 | 68.4 | 70.6 | 67.6 | 72.7 |
| SAM+Cutie | 56.7 | 70.1 | 72.2 | 69.4 | 74.1 |
| SAM 2 | 64.3 | 73.2 | 75.4 | 72.9 | 77.6 |

표 4 반지도 VOS 평가에서 다양한 프롬프트를 사용하여 17개의 비디오 데이터셋에 대한 제로샷 정확도. 표는 첫 번째 비디오 프레임에서 각 유형의 프롬프트(1, 3 또는 5 클릭, 바운딩 박스, 실제 마스크‡)에 대한 평균 J&F를 보여줍니다 (‡: 이 경우 XMem++ 또는 Cutie에 직접 마스크를 입력으로 사용하여 SAM을 사용하지 않음).

우리는 다음으로 비디오의 첫 번째 프레임에서 클릭, 박스 또는 마스크 프롬프트만 사용하여 반지도 비디오 객체 분할(VOS) 설정(Pont-Tuset et al., 2017)을 평가합니다. 클릭 프롬프트를 사용할 때, 첫 번째 비디오 프레임에서 1, 3 또는 5개의 클릭을 상호작용적으로 샘플링한 후, 이러한 클릭을 기반으로 객체를 추적합니다. §6.1.1의 대화형 설정과 유사하게, 우리는 XMem++ 및 Cutie와 비교하며, 클릭 및 박스 프롬프트를 위해 SAM을 사용하고, 마스크 프롬프트를 사용할 때는 기본 설정을 사용합니다. 우리는 표준 J&F 정확도(Pont-Tuset et al., 2017)를 보고하며, VOST(Tokmakov et al., 2022)에서는 해당 프로토콜에 따라 J 메트릭을 보고합니다. 결과는 표 4에 나와 있습니다. SAM 2는 다양한 입력 프롬프트를 사용하여 17개의 데이터셋에서 두 기준선을 능가합니다. 결과는 SAM 2가 이 다른 작업들이 특별히 설계된 마스크 입력을 사용하는 전통적인 비대화형 VOS 작업에서도 뛰어나다는 것을 강조합니다. 자세한 내용은 §E.1.3에 있습니다.

### 6.1.3 공정성 평가

우리는 인구통계학적 그룹 간의 공정성을 평가하기 위해 SAM 2를 평가합니다. Ego-Exo4D (Grauman et al., 2023) 데이터셋의 사람 카테고리에 대한 주석을 수집했으며, 이 데이터셋에는 비디오의 주체가 제공한 자가 보고된 인구통계 정보가 포함되어 있습니다. 우리는 SA-V 검증 및 테스트 세트와 동일한 주석 설정을 사용하여 제3자(외부) 비디오의 20초 클립에 적용했습니다. 첫 번째 프레임에서 1클릭, 3클릭, 실제 마스크를 사용하여 SAM 2를 평가했습니다.

표 5는 성별 및 연령에 따른 사람 분할의 J&F 정확도에서 SAM 2의 비교 결과를 보여줍니다. 3클릭 및 실제 마스크 프롬프트에서는 차이가 거의 없습니다. 1클릭 예측을 수동으로 검사한 결과, 모델이 사람 대신 부분적인 마스크를 자주 예측하는 것을 발견했습니다. 사람이 올바르게 분할된 클립으로 비교를 제한하면, 1클릭의 격차가 상당히 줄어듭니다(J&F 남성 94.3, 여성 92.7), 이는 프롬프트의 모호성에 기인할 수 있음을 시사합니다. 부록 G에서는 SA-V에 대한 모델, 데이터 및 주석 카드를 제공합니다.

#### 표 5: 보호된 인구통계 그룹에 대한 SAM 2의 공정성 평가 (J&F 메트릭 기준)

![](/assets/images/posts/227/img_10.png)

이 결과는 SAM 2가 대부분의 경우 공정하게 작동함을 보여주며, 특히 3클릭 및 실제 마스크 프롬프트에서 성별 및 연령에 따른 분할 성능 차이가 거의 없음을 나타냅니다. 1클릭의 경우, 프롬프트의 모호성 때문에 성능 차이가 나타날 수 있지만, 올바르게 분할된 경우 그 차이는 크게 줄어듭니다.

### 6.2 이미지 작업

우리는 37개의 제로샷 데이터셋을 대상으로 Segment Anything 과제에서 SAM 2를 평가합니다. 여기에는 이전에 SAM 평가에 사용된 23개의 데이터셋이 포함됩니다. 표 6에는 1-클릭 및 5-클릭 mIoU가 보고되어 있으며, 데이터셋 도메인별 평균 mIoU와 단일 A100 GPU에서의 모델 속도(FPS)가 나와 있습니다.

첫 번째 열(SA-23 All)은 SAM의 23개 데이터셋에서의 정확도를 보여줍니다. SAM 2는 추가 데이터 없이 SAM보다 더 높은 정확도(1-클릭에서 58.9 mIoU 대 58.1 mIoU)를 달성하며, 6배 더 빠릅니다. 이는 주로 SAM 2의 작지만 더 효율적인 Hiera 이미지 인코더 덕분입니다. 하단 행은 우리의 SA-1B 및 비디오 데이터 혼합으로 훈련한 경우, 23개 데이터셋에서 평균 61.4%로 정확도가 더 향상될 수 있음을 보여줍니다. 우리는 또한 SA-23의 비디오 벤치마크와 추가한 14개의 새로운 비디오 데이터셋에서 큰 성능 향상을 확인할 수 있습니다.

#### 표 6: 37개 데이터셋에서 Segment Anything(SA) 과제의 제로샷 정확도

![](/assets/images/posts/227/img_11.png)

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |
| 모델 | 데이터 | SA-23 전체 | SA-23 이미지 | SA-23 비디오 | 14개의 새로운 비디오 | FPS |
| SAM | SA-1B | 58.1 (81.3) | 60.8 (82.1) | 54.5 (80.3) | 59.1 (83.4) | 21.7 |
| SAM 2 | SA-1B | 58.9 (81.7) | 60.8 (82.1) | 56.4 (81.2) | 56.6 (83.7) | 130.1 |
| SAM 2 | 우리 혼합 | 61.4 (83.7) | 63.1 (83.9) | 59.1 (83.3) | 69.6 (86.0) | 130.1 |

표 6는 다양한 도메인(이미지/비디오)에서 SAM과 비교한 SAM 2의 평균 1-클릭 및 5-클릭 mIoU를 보여줍니다. 우리는 SAM이 사용한 23개 데이터셋(SA-23)과 14개의 추가 제로샷 비디오 데이터셋에서 평균 메트릭을 보고합니다(자세한 내용은 §E.3 참조).

종합적으로, 결과는 상호작용 비디오 및 이미지 분할에서 SAM 2의 이중 능력을 강조하며, 이는 비디오와 정적 이미지를 포함한 다양한 시각 도메인에서의 훈련 데이터 덕분입니다. 데이터셋별 상세 결과는 §E.3에 나와 있습니다.

### 7 최첨단 반지도 VOS와의 비교

우리의 주요 초점은 일반적인 대화형 PVS 과제이지만, 역사적으로 일반적인 프로토콜인 반지도 VOS 설정(첫 번째 프레임에서 실제 마스크가 프롬프트로 제공되는 경우)도 다룹니다. 표 7에서는 기존 최첨단 방법들과의 비교를 제시하며, 표준 프로토콜을 사용하여 정확도를 보고합니다. 우리는 서로 다른 속도와 정확도 절충을 가진 두 가지 이미지 인코더 크기(Hiera-B+ 및 Hiera-L)로 구성된 두 가지 버전의 SAM 2를 평가합니다. SAM 2는 정확도와 추론 속도(FPS, 마지막 열에 표시) 모두에서 기존 최첨단 방법들보다 상당한 개선을 보입니다. 전반적으로 최상의 결과를 얻기 위해 더 큰 이미지 인코더를 사용하는 것이 전반적인 정확도 향상에 큰 도움이 됩니다.

우리는 또한 "모든" 객체 클래스를 포함하는 오픈월드 세그먼트의 성능을 측정하는 SA-V 검증 및 테스트 세트에서도 기존 작업을 평가합니다. 이 벤치마크와 비교할 때, 대부분의 이전 방법들은 대략 같은 정확도에서 최고치를 기록합니다. 이전 작업의 SA-V 검증 및 테스트에서의 최고 성능은 상당히 낮아, "비디오에서 모든 것을 분할"하는 능력과의 격차를 보여줍니다. 마지막으로, SAM 2는 LVOS 벤치마크 결과에서 장기 비디오 객체 분할에서도 주목할 만한 성능 향상을 가져옵니다.

#### 표 7: 기존 작업과의 VOS 비교

![](/assets/images/posts/227/img_12.png)

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |  |
| 방법 | MOSE 검증 | DAVIS 2017 검증 | LVOS 검증 | SA-V 검증 | SA-V 테스트 | YTVOS 2019 검증 | FPS |
| STCN (Cheng et al., 2021a) | 52.5 | 85.4 | - | 61.0 | 62.5 | 82.7 | 13.2 |
| SwinB-AOT (Yang et al., 2021b) | 59.4 | 85.4 | - | 51.1 | 50.3 | 84.5 | - |
| SwinB-DeAOT (Yang & Yang, 2022) | 59.9 | 86.2 | - | 61.4 | 61.8 | 86.1 | - |
| RDE (Li et al., 2022a) | 46.8 | 84.2 | - | 51.8 | 53.9 | 81.9 | 24.4 |
| XMem (Cheng & Schwing, 2022) | 59.6 | 86.0 | - | 60.1 | 62.3 | 85.6 | 22.6 |
| SimVOS-B (Wu et al., 2023b) | - | 88.0 | - | 44.2 | 44.1 | 84.2 | 3.3 |
| JointFormer (Zhang et al., 2023b) | - | 90.1 | - | - | - | 87.4 | 3.0 |
| ISVOS (Wang et al., 2022) | - | 88.2 | - | - | - | 86.3 | 5.8 |
| DEVA (Cheng et al., 2023b) | 66.0 | 87.0 | 55.9 | 55.4 | 56.2 | 85.4 | 25.3 |
| Cutie-base (Cheng et al., 2023a) | 69.9 | 87.9 | 66.0 | 60.7 | 62.7 | 87.0 | 36.4 |
| Cutie-base+ (Cheng et al., 2023a) | 71.7 | 88.1 | - | 61.3 | 62.8 | 87.5 | 17.9 |
| SAM 2 (Hiera-B+) | 75.8 | 90.9 | 74.9 | 73.6 | 74.1 | 88.4 | 43.8 |
| SAM 2 (Hiera-L) | 77.2 | 91.6 | 76.1 | 75.6 | 77.6 | 89.1 | 30.2 |

SAM 2는 첫 번째 프레임의 실제 마스크 프롬프트 기반 비디오 분할에서 정확도(J&F, G)와 속도(FPS) 모두에서 매우 우수한 성능을 보여줍니다. SAM 2는 SA-V 검증/테스트에서 기존 방법보다 훨씬 뛰어난 성능을 보입니다. 모든 FPS 추정치는 A100 GPU에서 측정되었습니다. 우리 모델 외의 FPS 추정치는 Cheng et al. (2023a)에서 가져왔습니다.

우리는 또한 "모든" 객체 클래스를 포함하는 오픈월드 세그먼트의 성능을 측정하는 SA-V 검증 및 테스트 세트에서도 기존 작업을 평가합니다. 이 벤치마크와 비교할 때, 대부분의 이전 방법들은 대략 같은 정확도에서 최고치를 기록합니다. 이전 작업의 SA-V 검증 및 테스트에서의 최고 성능은 상당히 낮아, "비디오에서 모든 것을 분할"하는 능력과의 격차를 보여줍니다. 마지막으로, SAM 2는 LVOS 벤치마크 결과에서 장기 비디오 객체 분할에서도 주목할 만한 성능 향상을 가져옵니다.

### 8 데이터 및 모델 소거 실험

이 섹션에서는 SAM 2의 설계 결정을 알리기 위해 수행한 소거 실험을 제시합니다. 우리는 MOSE 개발 세트("MOSE dev")에서 평가를 수행했으며, 이 세트는 MOSE 훈련 분할에서 무작위로 샘플링된 200개의 비디오를 포함하고 있으며, 소거 실험에서는 훈련 데이터에서 제외되었습니다. 또한, SA-V 검증 세트와 9개의 제로샷 비디오 데이터셋의 평균을 평가에 사용했습니다. 비교를 위한 지표로, 첫 번째 프레임에서 3클릭 입력 하에 J&F를 보고하여 1클릭과 VOS 스타일 마스크 프롬프트 간의 균형을 맞춥니다. 추가로, 이미지를 대상으로 한 SA 과제에서 SAM이 사용한 23개 데이터셋 벤치마크에서 평균 1클릭 mIoU를 보고합니다. 특별히 명시되지 않는 한, 소거 실험은 512 해상도에서 SA-V 수동 주석 데이터와 SA-1B의 10% 부분집합을 사용하여 수행됩니다. 추가 세부 사항은 §C.2에 나와 있습니다.

### 8.1 데이터 소거 실험

#### 데이터 혼합 소거 실험

표 8에서는 서로 다른 데이터 혼합으로 훈련된 SAM 2의 정확도를 비교합니다. 우리는 SA-1B로 사전 훈련한 후, 각 설정에 대해 별도의 모델을 훈련합니다. 반복 횟수(200k)와 배치 크기(128)는 고정하고, 실험 간 훈련 데이터만 변경합니다. 우리는 SA-V 검증 세트, MOSE, 9개의 제로샷 비디오 벤치마크, 그리고 SA-23 과제(§6.2)에 대한 정확도를 보고합니다. 첫 번째 행(row 1)은 VOS 데이터셋(Davis, MOSE, YouTubeVOS)만으로 훈련된 모델이 도메인 내 MOSE dev에서는 좋은 성능을 보이지만, 9개의 제로샷 VOS 데이터셋을 포함한 다른 모든 데이터셋에서는 성능이 저조함을 보여줍니다(59.7 J&F).

우리의 데이터 엔진 데이터를 훈련 혼합에 추가하면 엄청난 이점을 얻을 수 있으며, 9개의 제로샷 데이터셋에서 평균 성능이 12.1% 향상됩니다(11행 vs 1행). 이는 VOS 데이터셋의 제한된 범위와 크기 때문입니다. SA-1B 이미지를 추가하면 이미지 분할 과제에서 성능이 향상되며(3행 vs 4행, 5행 vs 6행, 9행 vs 10행, 11행 vs 12행), VOS 기능이 저하되지 않습니다. SA-V 및 SA-1B만으로 훈련하면 모든 벤치마크에서 강력한 성능을 얻을 수 있습니다(MOSE 제외). 전반적으로, 모든 데이터셋(VOS, SA-1B, 데이터 엔진 데이터)을 혼합할 때 최상의 결과를 얻습니다(12행).

#### 데이터 양 소거 실험

다음으로 훈련 데이터 규모의 효과를 연구합니다. SAM 2는 SA-1B로 사전 훈련한 후 다양한 크기의 SA-V로 훈련합니다. 우리는 3개의 벤치마크(SA-V 검증 세트, 제로샷, MOSE dev)에서 첫 번째 프레임에서 3번의 클릭으로 프롬프트를 주었을 때의 평균 J&F 점수를 보고합니다. 그림 7은 모든 벤치마크에서 훈련 데이터 양과 비디오 분할 정확도 간의 일관된 거듭제곱 법칙 관계를 보여줍니다.

#### 표 8: 다양한 데이터 혼합으로 훈련된 모델의 정확도

![](/assets/images/posts/227/img_13.png)

|  |  |  |
| --- | --- | --- |
|  |  |  |
| 데이터 혼합 | J&F | mIoU |
| VOS | Internal | SA-V |
| 1 ✓ | 48.1 | 60.2 |
| 2 ✓ | 57.0 | 72.2 |
| 3 ✓ | 63.0 | 72.6 |
| 4 ✓ ✓ | 62.9 | 73.2 |
| 5 ✓ ✓ | 63.0 | 73.2 |
| 6 ✓ ✓ ✓ | 63.6 | 75.0 |
| 7 ✓ ✓ | 50.0 | 63.2 |
| 8 ✓ ✓ | 54.9 | 71.5 |
| 9 ✓ ✓ | 61.6 | 72.8 |
| 10 ✓ ✓ ✓ | 62.2 | 74.1 |
| 11 ✓ ✓ ✓ | 61.8 | 74.4 |
| 12 ✓ ✓ ✓ ✓ | 63.1 | 73.7 |

표 8에서 우리는 VOS(Davis, MOSE, YouTubeVOS), 내부 훈련 데이터 세트, SA-V, SA-1B의 하위 집합을 포함하는 다양한 데이터 혼합으로 모델을 훈련합니다. SA-V 검증 세트와 Internal 테스트 세트, MOSE, 9개의 제로샷 데이터셋에서 첫 번째 프레임에 3번의 클릭으로 프롬프트를 주었을 때 J&F 정확도를 보고하며, SA-23 데이터셋에서의 평균 1클릭 mIoU를 보고합니다.

![](/assets/images/posts/227/img_14.png)

### 그림 7 설명

그림 7은 SA-V 데이터 양에 따른 SAM 2의 성능을 보여줍니다. 우리는 SA-V 검증 세트(왼쪽), 9개의 제로샷 데이터셋(중앙), MOSE 개발 세트(오른쪽)에서 첫 번째 프레임에 3번의 클릭으로 프롬프트를 주었을 때의 J&F 정확도를 보고합니다.

### 데이터 품질 소거 실험

표 9에서는 품질을 위한 필터링 전략을 실험합니다. 우리는 SA-V에서 50,000개의 마스크렛을 무작위로 또는 주석자가 가장 많이 수정한 마스크렛을 선택하여 샘플링합니다. 수정된 프레임 수를 기준으로 필터링하면 데이터의 25%만 사용하여 강력한 성능을 발휘하며, 무작위 샘플링보다 성능이 우수합니다. 그러나 전체 190,000개의 SA-V 마스크렛을 사용하는 것보다는 성능이 떨어집니다.

![](/assets/images/posts/227/img_15.png)

#### 표 9: SA-V 데이터 품질 소거 실험 결과

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |
| 설정 | SA-V 검증 | 내부 테스트 | MOSE 개발 | 9 제로샷 | SA-23 |
| SA-1B + SA-V 50k 무작위 샘플링 | 63.7 | 70.3 | 72.3 | 68.7 | 59.1 |
| SA-1B + SA-V 50k 가장 많이 수정됨 | 66.2 | 73.0 | 72.5 | 69.2 | 58.6 |
| SA-1B + SA-V 전체 190k | 69.9 | 73.8 | 73.9 | 70.8 | 59.8 |

우리는 SA-V 수동 데이터의 서로 다른 하위 집합(50k 무작위 샘플링 마스크렛, 가장 많이 수정된 50k 마스크렛, 전체 190k 마스크렛)으로 모델을 훈련합니다.

### 8.2 모델 아키텍처 소거 실험

여기서는 설계 결정을 안내한 모델 소거 실험을 제시합니다. 우리는 비디오(J&F) 및 이미지(mIoU) 작업에 대한 분할 정확도와 비디오 분할 속도(FPS)를 보고합니다. 이미지 및 비디오 구성 요소에 대한 설계 선택이 대부분 분리된 것으로 나타났습니다. 이는 우리의 모듈식 설계와 훈련 전략에 기인할 수 있습니다.

![](/assets/images/posts/227/img_16.png)

#### 표 10: 용량 소거 실험

우리는 입력 크기(해상도, 프레임 수), 메모리 크기(메모리 수, 메모리 채널 차원), 모델 크기(메모리 어텐션, 이미지 인코더)와 관련하여 모델 용량을 소거 실험합니다. 기본 소거 설정은 회색으로 표시되어 있습니다.

### 8.2.1 용량 소거 실험

#### 입력 크기

훈련 중에 고정된 해상도와 고정된 길이의 프레임 시퀀스를 샘플링합니다(여기서는 # 프레임으로 표시). 표 10a, 10b에서 그 영향을 소거 실험합니다. 높은 해상도는 이미지 및 비디오 작업 전반에 걸쳐 상당한 개선을 가져옵니다. 프레임 수를 늘리면 비디오 벤치마크에서 눈에 띄는 성능 향상이 있으며, 속도와 정확성의 균형을 맞추기 위해 기본값으로 8을 사용합니다.

#### 메모리 크기

메모리의 (최대) 수를 늘리는 것은 일반적으로 성능 향상에 도움이 되지만, 표 10c에서와 같이 약간의 변동이 있을 수 있습니다. 우리는 시간적 맥락 길이와 계산 비용의 균형을 맞추기 위해 기본값으로 과거 6 프레임을 사용합니다. 메모리에 사용되는 채널 수를 줄이면 성능 저하가 크게 발생하지 않으면서도, 저장에 필요한 메모리 크기를 4배 줄일 수 있습니다(표 10d).

#### 모델 크기

백본 또는 메모리-어텐션의 용량을 늘리면 일반적으로 더 나은 결과를 얻을 수 있습니다(표 10e, 10f). 백본을 확장하면 이미지 및 비디오 메트릭 모두에서 이점이 있으며, 메모리-어텐션을 확장하면 비디오 메트릭만 개선됩니다. 우리는 속도와 정확성의 균형을 제공하는 B+ 백본을 기본으로 사용합니다; B+ 백본만으로도 SAM 2는 이미 SAM ViT-H를 크게 능가할 수 있습니다(표 6 참조).

![](/assets/images/posts/227/img_17.png)

### 표 11: 상대적 위치 인코딩

우리는 메모리 어텐션에서 2d-RoPE를 사용하면서 이미지 인코더에서는 기본적으로 RPB를 제거합니다(회색). 이 표에서 512 해상도에서는 FPS가 유사하지만, RPB를 제거하면 FlashAttention-2(Dao, 2023)를 사용할 수 있어 1024 해상도에서 속도가 크게 향상됩니다. 1024의 높은 해상도에서는 2d-RoPE(첫 번째 행)와 RoPE가 없는 기준선(세 번째 행) 사이의 FPS 차이가 훨씬 작아집니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

"RPB를 제거합니다"는 상대적 위치 바이어스(Relative Positional Bias, RPB)를 모델에서 사용하지 않는다는 의미입니다. 더 구체적으로, 이는 이미지 인코더 및 메모리 어텐션의 일부 또는 전체 레이어에서 RPB를 사용하지 않는 것을 의미합니다. 이러한 결정은 모델의 성능과 속도에 영향을 미칠 수 있으며, 해당 실험에서는 이를 통해 얻어진 결과를 분석합니다.

### RPB 제거의 의도와 효과

1. **성능 유지**:
   - SA-23과 같은 데이터셋에서는 RPB를 제거해도 성능 저하가 없음을 발견했습니다.
   - 비디오 벤치마크에서는 최소한의 성능 저하만 발생했습니다.
2. **속도 향상**:
   - 512 해상도에서는 FPS가 유사하지만, 1024 해상도에서는 RPB를 제거하면 FlashAttention-2를 사용할 수 있어 속도가 크게 향상됩니다.
   - 이는 고해상도 작업에서 모델의 처리 속도를 크게 증가시킵니다.

### 상대적 위치 인코딩 실험 결과

**설정**:

- 메모리 어텐션에서 2d-RoPE 사용.
- 이미지 인코더에서 RPB 제거.

**효과**:

- 512 해상도에서는 FPS가 유사하지만, 1024 해상도에서는 속도가 크게 향상됨.
- 성능 면에서 SA-23에서는 저하가 없고, 비디오 벤치마크에서는 최소한의 저하만 발생.

이러한 결정은 모델의 효율성을 높이고, 특히 고해상도 작업에서 성능을 최적화하는 데 목적이 있습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 8.2.2 상대적 위치 인코딩

기본적으로 우리는 이미지 인코더와 메모리 어텐션 모두에서 절대적 위치 인코딩을 항상 사용합니다. 이 섹션에서는 상대적 위치 인코딩 설계 선택을 연구합니다. 여기서 우리는 장기 비디오 객체 분할의 벤치마크로 LVOSv2(Hong et al., 2024)에서 첫 번째 프레임에 3번의 클릭을 사용하여 평가합니다.

SAM(Kirillov et al., 2023)은 모든 백본 레이어에 상대적 위치 바이어스(RPB)를 추가하는 Li et al. (2022b)를 따르지만, Bolya et al. (2023)은 글로벌 어텐션 레이어를 제외한 모든 레이어에서 RPB를 제거하고 "절대-승" 위치 인코딩을 채택하여 속도 향상을 가져옵니다. 우리는 모든 백본에서 RPB를 제거함으로써 이를 더욱 개선하였으며, SA-23에서는 성능 저하 없이, 비디오 벤치마크에서는 최소한의 성능 저하로 1024 해상도에서 속도를 크게 향상시킵니다(표 11 참조). 또한 메모리 어텐션에서 2d-RoPE(Su et al., 2021; Heo et al., 2024)를 사용하는 것이 유익하다는 것을 발견했습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 절대적 위치 인코딩과 상대적 위치 인코딩의 차이

#### 절대적 위치 인코딩 (Absolute Positional Encoding)

- 각 위치에 고유한 임베딩을 부여하여 위치 정보를 명시적으로 인코딩합니다.
- 각 위치는 고유의 값을 가지며, 입력 시퀀스의 길이에 따라 미리 정의된 고정 임베딩을 사용합니다.

#### 상대적 위치 인코딩 (Relative Positional Encoding)

- 위치 간의 상대적 거리를 인코딩합니다.
- 각 위치 간의 상대적 차이를 인코딩하여, 특정 위치가 다른 위치와 얼마나 떨어져 있는지를 나타냅니다.

### 예제

```
import torch
import torch.nn.functional as F
import math

class AbsolutePositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(AbsolutePositionalEncoding, self).__init__()
        self.encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        self.encoding[:, 0::2] = torch.sin(position * div_term)
        self.encoding[:, 1::2] = torch.cos(position * div_term)
        self.encoding = self.encoding.unsqueeze(0)

    def forward(self, x):
        return x + self.encoding[:, :x.size(1), :]

class RelativePositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, num_heads, max_len=5000):
        super(RelativePositionalEncoding, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.max_len = max_len
        self.relative_bias_table = torch.nn.Parameter(torch.zeros((2 * max_len - 1), num_heads))
        torch.nn.init.normal_(self.relative_bias_table, std=0.02)

    def forward(self, q, k, v):
        batch_size, num_heads, len_q, d_k = q.size()
        len_k = k.size(2)
        
        range_q = torch.arange(len_q, device=q.device)
        range_k = torch.arange(len_k, device=k.device)
        relative_position_index = range_q[:, None] - range_k[None, :]
        relative_position_index += (self.max_len - 1)
        relative_position_bias = self.relative_bias_table[relative_position_index.view(-1)].view(len_q, len_k, -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        relative_position_bias = relative_position_bias.unsqueeze(0).expand(batch_size, -1, -1, -1)
        
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
        attn = attn + relative_position_bias
        attn = F.softmax(attn, dim=-1)
        output = torch.matmul(attn, v)
        return output

# 절대적 위치 인코딩 테스트
def test_absolute_positional_encoding():
    d_model = 512
    max_len = 5000
    seq_len = 60
    batch_size = 10
    pe = AbsolutePositionalEncoding(d_model, max_len)
    x = torch.zeros(batch_size, seq_len, d_model)
    x = pe(x)
    print("Absolute Positional Encoding Output Shape:", x.shape)

# 상대적 위치 인코딩 테스트
def test_relative_positional_encoding():
    d_model = 512
    max_len = 5000
    seq_len = 60
    batch_size = 10
    num_heads = 8
    d_k = d_model // num_heads
    
    pe = RelativePositionalEncoding(d_model, num_heads, max_len)
    q = torch.randn(batch_size, num_heads, seq_len, d_k)
    k = torch.randn(batch_size, num_heads, seq_len, d_k)
    v = torch.randn(batch_size, num_heads, seq_len, d_k)
    output = pe(q, k, v)
    print("Relative Positional Encoding Output Shape:", output.shape)

# 실행 및 비교
def main():
    test_absolute_positional_encoding()
    test_relative_positional_encoding()

if __name__ == "__main__":
    main()
```

### 결과는 다음과 같다.

Absolute Positional Encoding Output Shape: torch.Size([10, 60, 512])   
Relative Positional Encoding Output Shape: torch.Size([10, 8, 60, 64])

절대적 위치 인코딩은 입력 차원에 고정된 위치 정보를 추가하는 반면, 상대적 위치 인코딩은 위치 간의 상대적인 차이를 인코딩합니다. 이를 통해 상대적 위치 인코딩은 상대적인 위치 관계를 반영할 수 있지만, 이로 인해 출력 차원이 입력 차원과 다르게 변형될 수 있습니다.

### 요약

- **절대적 위치 인코딩**은 각 위치에 고유한 임베딩을 부여하여 위치 정보를 명시적으로 인코딩합니다.
- **상대적 위치 인코딩**은 위치 간의 상대적 거리를 인코딩하여 위치 정보를 나타냅니다.

이 두 가지 접근 방식은 모델의 구조와 사용 사례에 따라 선택할 수 있으며, 각각의 장단점이 있습니다. 절대적 위치 인코딩은 구현이 간단하고 직관적이지만, 상대적 위치 인코딩은 더 유연하고 일반화된 위치 정보를 제공할 수 있습니다.

2D Rotary Positional Encoding (2D-RoPE)은 고차원 공간에서 위치 정보를 효과적으로 인코딩하는 방법입니다. 이 방법은 주로 Transformer 모델에서 사용되며, 특히 비디오 및 이미지와 같은 2차원 데이터를 다룰 때 유용합니다. RoPE는 입력 벡터의 각 요소에 대해 회전 변환을 적용하여 위치 정보를 인코딩합니다.

2D-RoPE는 각 차원별로 회전 변환을 적용하여 위치 정보를 인코딩하며, 이를 통해 Transformer 모델이 위치 정보를 더 잘 이해할 수 있게 합니다.

### 2D-RoPE 코드 예제

```
import torch
import torch.nn as nn
import math

class RotaryPositionalEncoding2D(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(RotaryPositionalEncoding2D, self).__init__()
        self.d_model = d_model
        self.max_len = max_len

        # Create the base positional encodings
        pos_x = torch.arange(max_len, dtype=torch.float).unsqueeze(1)
        pos_y = torch.arange(max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe_x = torch.zeros(max_len, d_model)
        pe_y = torch.zeros(max_len, d_model)
        
        pe_x[:, 0::2] = torch.sin(pos_x * div_term)
        pe_x[:, 1::2] = torch.cos(pos_x * div_term)
        pe_y[:, 0::2] = torch.sin(pos_y * div_term)
        pe_y[:, 1::2] = torch.cos(pos_y * div_term)

        self.register_buffer('pe_x', pe_x)
        self.register_buffer('pe_y', pe_y)

    def forward(self, x):
        """
        Args:
            x: Tensor of shape [batch_size, seq_len, seq_len, d_model]
        """
        batch_size, seq_len_x, seq_len_y, d_model = x.size()
        assert d_model == self.d_model, "Dimension mismatch"

        pe_x = self.pe_x[:seq_len_x, :].unsqueeze(1).repeat(1, seq_len_y, 1)  # Shape: [seq_len_x, seq_len_y, d_model]
        pe_y = self.pe_y[:seq_len_y, :].unsqueeze(0).repeat(seq_len_x, 1, 1)  # Shape: [seq_len_x, seq_len_y, d_model]

        pe = pe_x + pe_y  # Combine x and y positional encodings
        pe = pe.unsqueeze(0).repeat(batch_size, 1, 1, 1)  # Shape: [batch_size, seq_len_x, seq_len_y, d_model]

        return x + pe

# 2D-RoPE 테스트
def test_rotary_positional_encoding_2d():
    d_model = 512
    max_len = 5000
    seq_len = 60
    batch_size = 10
    pe = RotaryPositionalEncoding2D(d_model, max_len)
    x = torch.zeros(batch_size, seq_len, seq_len, d_model)
    x = pe(x)
    print("2D-RoPE Output Shape:", x.shape)

# 실행
def main():
    test_rotary_positional_encoding_2d()

if __name__ == "__main__":
    main()
```

### 2D-RoPE Output Shape: torch.Size([10, 60, 60, 512])

### 설명

- **RoPE의 핵심 아이디어**: RoPE는 각 위치에 대해 사인과 코사인 함수를 사용하여 주기적인 위치 인코딩을 생성합니다. 이 인코딩은 고차원 공간에서 위치 정보를 보존합니다.
- **2D-RoPE**: 이 구현에서는 2차원 입력(예: 이미지 또는 비디오 프레임)에서 각 위치에 대해 두 개의 1차원 위치 인코딩(하나는 x축, 하나는 y축)을 결합합니다.
- **forward 메서드**: 입력 텐서 x에 대해 위치 인코딩을 추가하여 최종 인코딩된 출력을 생성합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 8.2.3 메모리 아키텍처 소거 실험

#### 순환 메모리

메모리 피처를 메모리 뱅크에 추가하기 전에 GRU에 피딩하는 효과를 조사합니다. §8.2.2와 유사하게, LVOSv2에서 장기 객체 분할에 대한 추가 벤치마크로 평가를 수행합니다. 이전 연구에서는 GRU(Cho et al., 2014) 상태를 추적 과정에 메모리를 통합하는 수단으로 자주 사용했지만, 표 12의 결과는 이 접근 방식이 성능을 향상시키지 않는다는 것을 보여줍니다(LVOSv2에서 약간의 개선 제외). 대신에, 우리는 메모리 피처를 직접 메모리 뱅크에 저장하는 것이 더 간단하고 효율적임을 발견했습니다.

#### 객체 포인터

다른 프레임에서 마스크 디코더 출력의 객체 포인터 벡터에 교차 어텐션하는 영향에 대한 소거 실험을 수행합니다(§4 참조). 표 12에 제시된 결과는 객체 포인터에 교차 어텐션하는 것이 9개의 제로샷 데이터셋 전반에서 평균 성능을 향상시키지는 않지만, SA-V 검증 데이터셋 및 도전적인 LVOSv2 벤치마크(검증 분할)에서 성능을 크게 향상시킨다는 것을 보여줍니다. 따라서 우리는 메모리 뱅크와 함께 객체 포인터에 교차 어텐션하는 것을 기본으로 사용합니다.

### 표 12: 메모리 설계에 대한 소거 실험

기본적으로 객체 포인터를 사용하고(회색), 순환 GRU 메모리도 연구합니다.

![](/assets/images/posts/227/img_18.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 해석

- **순환 메모리**: GRU를 사용하여 메모리 피처를 메모리 뱅크에 추가하는 것이 성능을 향상시키지 않음을 발견했습니다.
  - LVOSv2에서는 약간의 개선이 있었으나, 다른 벤치마크에서는 큰 차이가 없었습니다.
- **객체 포인터**: 객체 포인터에 교차 어텐션하는 것이 SA-V 검증 데이터셋 및 LVOSv2 벤치마크에서 성능을 크게 향상시킵니다.
  - 9개의 제로샷 데이터셋에서는 평균 성능에 큰 영향을 미치지 않았습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 9 결론

우리는 Segment Anything을 비디오 도메인으로 자연스럽게 확장하여 세 가지 주요 측면에 기반한 발전을 제시합니다: (i) 프롬프트 가능한 분할 과제를 비디오로 확장, (ii) 비디오에 적용할 때 메모리를 사용할 수 있도록 SAM 아키텍처를 장착, (iii) 비디오 분할을 위한 훈련 및 벤치마킹을 위한 다양한 SA-V 데이터셋. 우리는 SAM 2가 시각적 인식에서 중요한 발전을 나타내며, 우리의 기여가 이 분야에서 연구와 응용을 더욱 촉진할 중요한 이정표가 될 것이라고 믿습니다.

### 10 감사의 글

프로젝트 방향에 대한 논의에 대해 Alexander Kirillov와 Jitendra Malik에게 감사드립니다. 데모 작업에 대해 Andrew Huang, Sahir Gomez, Miguel Martin, Devansh Kukreja, Somya Jain에게 감사드리며, 데이터셋 시각화 도구를 제작해 준 Aohan Li와 Meng Wang에게 감사드립니다. 데이터셋 준비 작업에 대해 Shoubhik Debnath와 Sagar Vaze에게 감사드립니다. 디자인 전문 지식에 대해 William Ngan과 Sasha Mitts에게 감사드리며, 제품 관리 리더십에 대해 Grant Gardner와 George Orlin에게 감사드립니다. 귀중한 논의에 대해 Joelle Pineau, Daniel Bolya, Kate Saenko, Pengchuan Zhang, Christopher Chedeau에게 감사드립니다. 데이터 지원에 대해 Rene Martinez Doehner와 Baishan Guo에게 감사드리며, 주석 엔지니어링 및 관리 파트너인 Robert Kuo, Rishi Godugu, Bob Kamma, Ida Cheng, Claudette Ward, Kai Brown, Jake Kinney, Jenny Truong, Karen Bergan에게 감사드립니다. 컴퓨팅 및 인프라 지원에 대해 Vispi Cassod, Parth Malani, Shiva Koduvayur, Alexander Miller, Caleb Ho에게 감사드립니다. 마지막으로, 프로젝트 지원에 대해 Azita Shokrpour, Mallika Malhotra, Rodrick Shepard, Jonathan Torres, Luc Dahlin, David Soofian, Alex Bosenberg, Amanda Kallet에게 감사드립니다.

![](/assets/images/posts/227/img_19.png)

### 그림 9: 마스크 디코더 아키텍처

디자인은 주로 SAM을 따르며, 업샘플링하는 동안 이미지 인코더의 stride 4와 stride 8 피처를 추가로 포함합니다. 우리는 또한 출력 마스크에 해당하는 마스크 토큰을 객체 포인터로 사용하고, 관심 객체가 현재 프레임에서 보이는지를 나타내는 폐색 점수(occlusion score)를 생성합니다.

#### 요약

- **기본 구조**: SAM의 디자인을 주로 따릅니다.
- **업샘플링 단계**: 이미지 인코더의 stride 4와 stride 8 피처를 사용하여 업샘플링합니다.
- **마스크 토큰**: 출력 마스크에 해당하는 마스크 토큰을 객체 포인터로 사용합니다.
- **폐색 점수**: 관심 객체가 현재 프레임에서 보이는지를 나타내는 폐색 점수를 생성합니다.

[453323338\_287900751050452\_6064535069828837026\_n.pdf

12.01MB](./file/453323338_287900751050452_6064535069828837026_n.pdf)
