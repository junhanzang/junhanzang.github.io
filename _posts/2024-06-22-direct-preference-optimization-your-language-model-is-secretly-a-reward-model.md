---
title: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
date: 2024-06-22 00:01:53
categories:
  - 인공지능
---

<https://arxiv.org/abs/2305.18290>

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

초록

대규모 비지도 언어 모델(LMs)은 광범위한 세계 지식과 일부 추론 기술을 학습하지만, 완전히 비지도 학습으로 인해 그들의 행동을 정밀하게 제어하는 것은 어렵습니다. 이러한 조작 가능성을 얻기 위한 기존 방법들은 모델 생성의 상대적 품질에 대한 인간의 레이블을 수집하고, 비지도 언어 모델을 이 선호도에 맞추도록 미세 조정하며, 종종 인간 피드백을 통한 강화 학습(RLHF)을 사용합니다. 그러나 RLHF는 복잡하고 종종 불안정한 절차로, 먼저 인간의 선호도를 반영하는 보상 모델을 적합시키고, 그 다음 강화 학습을 사용하여 추정된 보상을 최대화하면서 원래 모델에서 너무 멀어지지 않도록 대규모 비지도 언어 모델을 미세 조정해야 합니다. 이 논문에서는 RLHF의 새로운 보상 모델 매개변수를 도입하여 해당 최적 정책을 닫힌 형태로 추출할 수 있게 하여, 단순한 분류 손실만으로 표준 RLHF 문제를 해결할 수 있게 합니다. 결과적으로 Direct Preference Optimization (DPO)이라 부르는 이 알고리즘은 안정적이고 성능이 우수하며 계산적으로 가벼워, 미세 조정 중 LM에서 샘플링하거나 중요한 하이퍼파라미터 튜닝을 수행할 필요가 없습니다. 우리의 실험 결과, DPO는 기존 방법들만큼 또는 그보다 더 잘 인간의 선호도에 맞게 언어 모델을 미세 조정할 수 있음을 보여줍니다. 특히, DPO를 사용한 미세 조정은 PPO 기반의 RLHF보다 생성의 감정 제어 능력에서 뛰어나며, 요약 및 단일 턴 대화에서 응답 품질을 맞추거나 개선하면서 구현 및 훈련이 훨씬 간단합니다.

### 1. 소개

대규모 데이터셋으로 훈련된 대규모 비지도 언어 모델(LMs)은 놀라운 능력을 획득합니다[11, 7, 40, 8]. 그러나 이러한 모델은 다양한 목표, 우선순위, 그리고 기술을 가진 인간에 의해 생성된 데이터로 훈련됩니다. 이들 목표와 기술 중 일부는 모방하기에 바람직하지 않을 수 있습니다. 예를 들어, AI 코딩 보조 도구가 일반적인 프로그래밍 실수를 이해하여 이를 수정할 수 있게 하는 것은 좋지만, 코드 생성 시에는 훈련 데이터에 있는 (잠재적으로 드문) 고품질 코딩 능력 쪽으로 모델을 편향시키고 싶을 것입니다. 비슷하게, 언어 모델이 50%의 사람들이 믿고 있는 일반적인 오해를 알고 있기를 바라지만, 모델이 이 오해를 50%의 질의에서 진실이라고 주장하는 것은 원치 않을 것입니다!

다시 말해, 모델의 폭넓은 지식과 능력에서 원하는 응답과 행동을 선택하는 것은 안전하고 성능이 우수하며 제어 가능한 AI 시스템을 구축하는 데 중요합니다[26]. 기존 방법은 일반적으로 강화 학습(RL)을 사용하여 언어 모델을 인간의 선호에 맞추지만, 우리는 기존 방법이 사용하는 RL 기반 목표를 단순한 이진 교차 엔트로피 목표로 정확히 최적화할 수 있음을 보여줄 것입니다. 이는 선호 학습 파이프라인을 크게 단순화합니다.

기본적으로 기존 방법은 인간이 안전하고 유용하다고 생각하는 행동 유형을 나타내는 인간 선호의 정제된 집합을 사용하여 언어 모델에 원하는 행동을 주입합니다. 이 선호 학습 단계는 대규모 텍스트 데이터셋에 대한 대규모 비지도 사전 훈련 단계 이후에 발생합니다. 선호 학습에 대한 가장 직관적인 접근 방식은 고품질 응답의 인간 시연을 통한 지도 학습 미세 조정이지만, 가장 성공적인 방법 클래스는 인간(또는 AI) 피드백을 통한 강화 학습(RLHF/RLAIF; [12, 2])입니다. RLHF 방법은 인간 선호 데이터셋에 보상 모델을 맞추고, 그런 다음 RL을 사용하여 언어 모델 정책을 최적화하여 높은 보상을 받도록 응답을 생성하면서 원래 모델에서 지나치게 벗어나지 않도록 합니다. RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 상당히 복잡하여 여러 언어 모델 훈련과 학습 중 정책 샘플링을 포함하며, 이는 상당한 계산 비용을 초래합니다.

이 논문에서는 명시적인 보상 모델링이나 강화 학습 없이 인간 선호에 맞도록 언어 모델을 직접 최적화하는 방법을 보여줍니다. 우리는 Direct Preference Optimization(DPO)을 제안합니다. 이 알고리즘은 기존 RLHF 알고리즘과 동일한 목표(보상 최대화 및 KL 발산 제약)를 암묵적으로 최적화하지만 구현이 간단하고 훈련이 직관적입니다. 직관적으로, DPO 업데이트는 선호되는 응답에 대한 로그 확률을 상대적으로 증가시키지만, 단순 확률 비율 목표로 발생하는 모델 악화를 방지하는 예제별 동적 중요도 가중치를 포함합니다. 기존 알고리즘처럼 DPO는 주어진 보상 함수가 실증적 선호 데이터와 얼마나 잘 맞는지를 측정하는 이론적 선호 모델(예: Bradley-Terry 모델; [5])에 의존합니다. 그러나 기존 방법은 선호 모델을 사용하여 보상 모델을 훈련하기 위한 선호 손실을 정의하고, 학습된 보상 모델을 최적화하는 정책을 훈련하는 반면, DPO는 변수를 변경하여 정책의 함수로 선호 손실을 정의합니다. 모델 응답에 대한 인간 선호 데이터셋을 사용하여, DPO는 간단한 이진 교차 엔트로피 목표를 사용하여 정책을 최적화할 수 있으며, 선호 데이터에 맞춰진 암묵적 보상 함수에 최적 정책을 생성합니다.

우리의 주요 기여는 Direct Preference Optimization(DPO)으로, 선호로부터 언어 모델을 훈련하기 위한 간단한 RL-프리 알고리즘입니다. 우리의 실험은 DPO가 감정 조절, 요약, 대화 등의 작업에서 선호를 학습하는 데 있어 최대 6B 파라미터를 가진 언어 모델을 사용하여 PPO 기반 RLHF를 포함한 기존 방법만큼 효과적임을 보여줍니다.

### 2. 관련 연구

점점 더 규모가 커지는 자기지도 언어 모델은 제로샷(Zero-shot)으로 일부 작업을 수행하거나 몇 개의 샷 프롬프트(few-shot prompts)로 작업을 수행하는 것을 배웁니다[31]. 그러나 이러한 모델의 다운스트림 작업 성능과 사용자 의도와의 일치는 지침과 인간이 작성한 완성본 데이터셋에 대해 미세 조정을 함으로써 크게 향상될 수 있습니다[23, 36, 13, 39]. 이러한 '지침 튜닝' 절차는 LLM이 지침 튜닝 세트 외부의 지침으로 일반화할 수 있게 하여 사용성을 전반적으로 증가시킵니다[13]. 지침 튜닝의 성공에도 불구하고, 응답 품질에 대한 인간의 상대적 판단은 전문가 시연보다 수집하기가 더 쉬우며, 따라서 후속 연구들은 인간의 선호 데이터셋으로 LLM을 미세 조정하여 번역[18], 요약[38, 49], 이야기 작성[49], 지침 준수[26, 32]에서의 능력을 향상시켰습니다. 이러한 방법들은 먼저 선호 모델(예: Bradley-Terry 모델[5]) 아래에서 선호 데이터셋과의 호환성을 위한 신경망 보상 함수를 최적화한 다음, 주어진 보상을 최대화하기 위해 REINFORCE[45], 근접 정책 최적화(PPO; [37]) 또는 그 변형[32]과 같은 강화 학습 알고리즘을 사용하여 언어 모델을 미세 조정합니다. 밀접하게 관련된 연구는 지침 준수를 위해 인간 피드백으로 미세 조정된 LLM을 활용하여 안전성 또는 무해성과 같은 특정 속성을 위한 추가 합성 선호 데이터를 생성하는 것입니다[2]. 이는 LLM의 주석에 대한 텍스트 기준 형태로 약한 감독만을 사용합니다. 이러한 방법들은 강화 학습을 통해 다양한 목표를 달성하기 위한 언어 모델 훈련에 관한 연구[33, 27, 46]와 인간 선호로부터 학습하는 일반적인 방법에 관한 연구[12, 19]의 두 연구 분야의 융합을 나타냅니다. 상대적 인간 선호를 사용하는 매력에도 불구하고, 강화 학습을 통해 대규모 언어 모델을 미세 조정하는 것은 여전히 주요한 실질적 도전 과제로 남아 있습니다; 이 연구는 RL 없이 상대적 선호를 최적화하는 이론적으로 정당한 접근 방식을 제공합니다.

언어의 맥락을 벗어나, 선호로부터 정책을 학습하는 것은 밴딧(bandit) 및 강화 학습 설정에서 연구되었으며, 여러 접근 방식이 제안되었습니다. 행동의 보상 대신 선호나 순위를 사용하는 맥락적 밴딧 학습은 맥락적 결투 밴딧(CDB; [48, 14])으로 알려져 있습니다. 절대적인 보상이 없는 경우, CDB의 이론적 분석은 최적 정책의 개념을 폰 노이만 승자로 대체합니다. 폰 노이만 승자는 다른 모든 정책에 대해 최소 50%의 기대 승률을 가지는 정책입니다[14]. 그러나 CDB 설정에서는 선호 레이블이 온라인으로 제공되지만, 인간 선호로부터 학습하는 경우에는 일반적으로 오프라인 선호 주석이 달린 고정된 배치의 행동 쌍으로부터 학습합니다[47]. 비슷하게, 선호 기반 강화 학습(PbRL)은 보상 대신 알 수 없는 '점수 매기기' 함수에 의해 생성된 이진 선호로부터 학습합니다[9, 35]. PbRL에 대한 다양한 알고리즘이 존재하며, 정책 외 선호 데이터를 재사용할 수 있는 방법도 있지만, 일반적으로 먼저 잠재 점수 함수(즉, 보상 모델)를 명시적으로 추정하고 이를 최적화하는 단계를 포함합니다[16, 9, 12, 34, 19]. 우리는 대신 선호를 만족시키기 위해 정책을 직접 최적화하는 단일 단계 정책 학습 접근 방식을 제시합니다.

### 3. 기본 사항

우리는 Ziegler et al.과 나중의 다른 연구들에서 RLHF 파이프라인을 검토합니다[38, 1, 26]. 일반적으로 이는 세 가지 단계를 포함합니다:

1. 지도 미세 조정(SFT)
2. 선호 샘플링 및 보상 학습
3. RL 최적화

**SFT**: RLHF는 일반적으로 고품질 데이터에 대한 지도 학습으로 사전 훈련된 언어 모델을 미세 조정하여, 특정한 과제의 높은 품질의 응답을 얻기 위해 시작됩니다[4, 13].

![](/assets/images/posts/169/img.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### Bradley-Terry 모델

![](/assets/images/posts/169/img_1.png)

![](/assets/images/posts/169/img_2.png)

### 보상 모델 학습

다음으로, 보상 모델을 학습하는 과정을 설명하겠습니다. 보상 모델은 인간이 제공한 선호 데이터를 사용하여 최적화됩니다. 이를 위해 다음과 같은 손실 함수를 사용합니다:

![](/assets/images/posts/169/img_3.png)

![](/assets/images/posts/169/img_4.png)

### RL 미세 조정 단계

마지막 단계는 모델을 최적화하는 과정입니다. 여기서는 보상 함수를 최대화하기 위해 모델을 조정합니다. 이를 위해 다음과 같은 최적화 문제를 해결합니다:

![](/assets/images/posts/169/img_5.png)

![](/assets/images/posts/169/img_6.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 4. Direct Preference Optimization (DPO)

#### 배경 및 목표

강화 학습 알고리즘을 대규모 문제, 특히 언어 모델의 미세 조정에 적용하는 데 따른 어려움 때문에, 우리는 선호도를 직접 사용하여 정책을 최적화하는 간단한 접근 방식을 도출하고자 합니다. 기존의 RLHF(RL을 통한 인간 피드백) 방법은 보상 모델을 학습한 후 이를 강화 학습(RL)을 통해 최적화합니다. 그러나 우리의 접근 방식은 보상 모델의 특정 매개변수화를 활용하여, RL 학습 루프 없이 최적 정책을 닫힌 형태로 추출할 수 있게 합니다.

#### 주요 통찰

우리의 주요 통찰은 보상 함수에서 최적 정책으로의 분석적 매핑을 활용하여, 보상 함수에 대한 손실 함수를 정책에 대한 손실 함수로 변환할 수 있다는 것입니다. 이러한 변수 변환 접근 방식은 명시적이고 독립적인 보상 모델을 맞추는 것을 피하면서도 Bradley-Terry 모델과 같은 기존의 인간 선호 모델 아래에서 최적화를 수행할 수 있습니다. 본질적으로, 정책 네트워크는 언어 모델과 (암묵적) 보상 모델 둘 다를 나타냅니다.

### 요약

DPO는 기존의 강화 학습 루프를 피하고, 선호도 데이터를 직접 사용하여 언어 모델의 정책을 최적화하는 새로운 방법입니다. 이는 보상 함수에서 최적 정책으로의 분석적 매핑을 활용하여, 정책 네트워크가 언어 모델과 보상 모델을 동시에 나타내도록 합니다. 이렇게 하면, 명시적인 보상 모델 없이도 인간 선호에 따라 언어 모델을 효과적으로 최적화할 수 있습니다.

### DPO 목적 도출

DPO 목표를 도출하는 과정을 설명하겠습니다. 우리는 이전 작업에서 사용된 동일한 강화 학습 목표를 기반으로 시작합니다. 식 3에 있는 KL 제약을 가진 보상 최대화 문제의 최적 해답이 다음과 같은 형태를 취한다는 것을 보여주는 것은 간단합니다:

![](/assets/images/posts/169/img_7.png)

![](/assets/images/posts/169/img_8.png)

### 요약

DPO는 기존의 강화 학습 방법과 달리, 보상 함수에서 최적 정책을 직접 추출할 수 있는 방법을 사용합니다. 이를 통해, 정책 네트워크는 언어 모델과 보상 모델을 동시에 나타냅니다. DPO의 목표는 선호 데이터를 기반으로 정책을 최적화하여 더 나은 응답을 생성하는 것입니다. 또한, 우리의 절차는 재모수화된 브래들리-테리 모델을 맞추는 것과 동일하기 때문에 선호도 데이터 분포의 적절한 가정 하에서 일관성과 같은 특정 이론적 속성을 지니고 있습니다[4]. 섹션 5에서는 다른 작업과 관련하여 DPO의 이론적 속성에 대해 자세히 설명합니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/169/img_9.png)

어디서 많이 본 형태 아닌가?

KL Divergence에서 보던 식이다

![](/assets/images/posts/169/img_10.png)

이를 변경해서 사용한 것이다.

즉, PPO의 변형이다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### DPO 업데이트가 하는 일

DPO의 기계적 이해를 위해, L\_DPO​ 손실 함수의 기울기를 분석하는 것이 유용합니다. 매개변수 θ에 대한 기울기는 다음과 같이 표현할 수 있습니다:

![](/assets/images/posts/169/img_11.png)

### DPO 절차

일반적인 DPO 절차는 다음과 같습니다:

![](/assets/images/posts/169/img_12.png)

### 5. DPO의 이론적 분석

이 섹션에서는 DPO 방법에 대한 추가 해석과 이론적 근거를 제공하고, DPO의 장점을 RLHF에 사용되는 액터 비평 알고리즘(예: PPO [37])의 문제와 연관시킵니다.

### 5.1 언어 모델이 보상 모델이다

![](/assets/images/posts/169/img_13.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

### Plackett-Luce 모델

Plackett-Luce 모델은 주어진 선택지 집합에서 선택지의 순위를 매기는 문제를 해결하는 데 사용됩니다. 이 모델은 선택지 각각의 점수를 기반으로 확률을 계산합니다.

![](/assets/images/posts/169/img_14.png)

#### 순위 매김

Plackett-Luce 모델은 순위 매김 문제에서도 사용됩니다. 예를 들어, 주어진 선택지 집합에서 1위를 결정한 후, 나머지 선택지 집합에서 다시 1위를 결정하는 방식으로 순위를 매깁니다. 이렇게 해서 전체 순위를 계산할 수 있습니다.

### Bradley-Terry 모델

Bradley-Terry 모델은 두 선택지 간의 상대적인 선호를 모델링하는 데 사용됩니다. 이는 두 선택지 중 하나를 선택할 확률을 계산합니다.

![](/assets/images/posts/169/img_15.png)

따라서 다음의 식으로 변환되고

![](/assets/images/posts/169/img_16.png)

동등성의 증명

![](/assets/images/posts/169/img_17.png)

따라서 동일하다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

증명은 간단하며 부록 A.5로 미루겠습니다. 첫 번째 수식은 Plackett-Luce 모델 계열의 잘 알려진 사양 부족 문제입니다[30]. 이러한 과소 지정으로 인해 일반적으로 식 2 [4]의 MLE 추정치를 보장하기 위해 추가적인 식별성 제약 조건을 부과해야 합니다. 두 번째 정리는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 산출한다는 것을 의미하므로, 최종 목표를 위해 우리는 최적 클래스에서 임의의 보상 함수를 복구하는 데에만 관심이 있습니다. 부록 A.6에서 다음 정리를 증명합니다:

아래는 위의 증명과 동일하다

### 정리 1

![](/assets/images/posts/169/img_18.png)

![](/assets/images/posts/169/img_19.png)

### 5.2 액터-크리틱 알고리즘의 불안정성

![](/assets/images/posts/169/img_20.png)

### 6. 실험

이 섹션에서는 선호도 데이터를 직접 사용하여 정책을 학습하는 DPO의 능력을 실험적으로 평가합니다. 먼저, 잘 통제된 텍스트 생성 설정에서, DPO가 보상을 극대화하고 참조 정책과의 편차를 최소화하는 데 있어 PPO와 같은 강화 학습 알고리즘과 어떻게 비교되는지 평가합니다. 다음으로, 대형 모델과 보다 복잡한 RLHF 작업에서 DPO의 성능을 평가합니다. 우리는 하이퍼파라미터 튜닝 없이도 DPO가 RLHF와 같은 기준선보다 성능이 뛰어나거나 동등한 성능을 보여줌을 관찰했습니다. 실험 설정에 대한 자세한 내용은 부록 C에 설명되어 있습니다.

### 데이터셋

모든 실험에서 우리는 서로 다른 오픈 엔드 텍스트 생성 작업을 사용합니다. 기본적으로, 실험은 다음과 같은 세 가지 작업으로 구성됩니다:

![](/assets/images/posts/169/img_21.png)

마지막으로, 이 설정에서 사전 훈련된 SFT 모델은 사용 가능하며, 이는 SFT 모델을 형성하는 데 사용되는 데이터와 상관없는 온 더 셀프 언어 모델이므로 사전 훈련된 SFT 모델의 예측 완성을 선호합니다.

### 평가

우리의 실험에서는 두 가지 다른 평가 접근 방식을 사용합니다. 제약된 보상 최대화 목표를 최적화하는 각 알고리즘의 효과를 분석하기 위해, 통제된 감정 생성 설정에서 각 알고리즘을 참조 정책과의 KL 발산 및 달성한 보상의 경계(frontier)로 평가합니다. 이 경계는 우리가 실제 보상 함수(감정 분류기)에 접근할 수 있기 때문에 계산 가능합니다. 그러나 실제 세계에서는 실제 보상 함수를 알 수 없습니다. 따라서 요약 및 단일 턴 대화 설정에서 요약 품질과 응답 유용성에 대한 인간 평가의 대리자로 GPT-4를 사용하여 알고리즘을 기준 정책과의 승률로 평가합니다. 요약의 경우, 테스트 세트의 참조 요약본을 기준으로 사용하고, 대화의 경우, 테스트 데이터셋에서 선호되는 응답을 기준으로 사용합니다.

기존 연구들은 언어 모델이 기존 메트릭보다 더 나은 자동 평가자가 될 수 있음을 시사하지만[10], 우리는 GPT-4를 평가에 사용하는 것을 정당화하기 위해 인간 연구를 실시합니다(섹션 6.4). 우리는 GPT-4의 판단이 인간과 강하게 상관관계가 있음을 발견했으며, 인간과 GPT-4의 일치는 보통 인간 평가자 간의 일치도와 유사하거나 더 높았습니다.

### 방법

![](/assets/images/posts/169/img_22.png)

![](/assets/images/posts/169/img_23.png)

그림 2: 왼쪽. 예상 보상과 기준 정책에 대한 KL의 경계. DPO는 모든 KL 값에 대해 가장 높은 기대 보상을 제공하여 최적화의 품질을 보여줍니다. 오른쪽. GPT-4를 평가자로 사용한 TL;DR 요약의 승률과 사람이 직접 작성한 요약의 승률 비교. DPO는 요약에 대한 PPO의 최상의 성능을 능가하는 동시에 샘플링 온도 변화에 더 강력합니다

![](/assets/images/posts/169/img_24.png)

그림 3: 왼쪽. Anthropic-HH 한 단계 대화에 대해 GPT-4로 계산한 승리율; DPO는 Anthropic-HH 테스트 세트에서 선택한 요약보다 개선된 유일한 방법입니다. 맞습니다. 훈련 과정에서 다양한 샘플링 온도에 따른 승률. 데이터 세트 레이블에 대한 DPO의 개선은 다양한 샘플링 온도에 대한 훈련 과정에서 상당히 안정적입니다.

### 6.1 DPO가 RLHF 목표를 얼마나 잘 최적화할 수 있는가?

![](/assets/images/posts/169/img_25.png)

DPO의 보상/KL 트레이드 오프는 PPO를 엄격하게 지배합니다. 둘째, DPO는 PPO보다 더 나은 프론티어를 달성합니다, PPO가 기준 진실 보상(PPO-GT)에 접근할 수 있는 경우에도 마찬가지입니다.

### 6.2 DPO가 실제 선호 데이터셋에 확장할 수 있는가?

다음으로, 우리는 요약 및 단일 턴 대화에서 DPO의 미세 조정 성능을 평가합니다. 요약의 경우, ROUGE와 같은 자동 평가 지표는 인간의 선호와 잘 일치하지 않을 수 있으며 [38], 이전 연구에서는 PPO를 사용하여 인간 선호에 맞춰 언어 모델을 미세 조정하면 더 효과적인 요약을 제공할 수 있음을 발견했습니다. 우리는 TL;DR 요약 데이터셋의 테스트 스플릿에서 완성을 샘플링하고, 테스트 세트의 참조 완성에 대한 평균 승률을 계산하여 다양한 방법을 평가합니다. 모든 방법의 완성은 온도(temperature) 0.0에서 1.0까지 다양하게 샘플링되었으며, 승률은 그림 2(오른쪽)에 표시됩니다. DPO, PPO 및 Preferred-FT는 모두 동일한 GPT-J SFT 모델을 미세 조정합니다. 우리는 DPO가 온도 0.0에서 약 61%의 승률을 기록하여, 최적 샘플링 온도 0.0에서 57%의 승률을 기록한 PPO보다 뛰어남을 발견했습니다. DPO는 또한 N 기반 최선의 방법보다 더 높은 최대 승률을 달성했습니다. 우리는 DPO의 β 하이퍼파라미터를 의미 있게 튜닝하지 않았기 때문에, 이러한 결과는 DPO의 잠재력을 과소평가할 수 있습니다. 게다가, DPO는 PPO보다 샘플링 온도에 훨씬 더 강력하며, PPO는 높은 온도에서 기본 GPT-J 모델의 성능으로 저하될 수 있습니다. Preferred-FT는 SFT 모델보다 크게 개선되지 않았습니다. 우리는 섹션 6.4에서 DPO와 PPO를 인간 평가에서 직접 비교했으며, DPO 샘플(온도 0.25)이 PPO 샘플(온도 0)보다 58% 더 선호되었습니다.

단일 턴 대화에서는, 우리는 Anthropic HH 데이터셋 [1]의 테스트 스플릿의 하위 집합에서 다양한 방법을 평가합니다. GPT-4 평가는 테스트에서 선호된 완성을 참조하여 다양한 방법의 승률을 계산합니다. 이 작업에 대한 표준 SFT 모델이 없기 때문에, 우리는 사전 훈련된 Pythia-2.8B를 시작으로 하여, 선택된 완성에 대해 Preferred-FT를 사용하여 참조 모델을 훈련하여 모델의 분포 내에서 완성을 유지하고, 그런 다음 DPO를 사용하여 훈련합니다. 우리는 또한 128개의 Preferred-FT 완성 중 최고의 결과와 2-샷 프롬프트 버전의 Pythia-2.8B 기본 모델과 비교했으며, DPO가 각 방법의 최적 온도에서 동등하거나 더 나은 성능을 보임을 발견했습니다. 우리는 또한 Anthropic HH 데이터셋 [5]에서 PPO로 훈련된 RLHF 모델을 평가했지만, 기본 Pythia-2.8B 모델보다 나은 성능을 제공하는 프롬프트나 샘플링 온도를 찾지 못했습니다. TL;DR 결과와 두 방법이 동일한 보상 함수를 최적화한다는 사실에 기반하여, 우리는 128개의 최선 방법을 PPO 수준 성능의 대략적인 대리자로 간주합니다. 전체적으로, DPO는 Anthropic HH 데이터셋에서 선호된 완성보다 향상된 성능을 제공하고, 계산적으로 복잡한 128개의 최선 방법과 유사하거나 더 나은 성능을 제공하는 유일한 계산적으로 효율적인 방법입니다. 마지막으로, 그림 3은 DPO가 상대적으로 빠르게 최적 성능에 도달함을 보여줍니다.

![](/assets/images/posts/169/img_26.png)

### 6.3 새로운 입력 분포에 대한 일반화

분포 변화에 따른 PPO와 DPO의 성능을 비교하기 위해, 우리는 Reddit TL;DR 요약 실험에서 PPO와 DPO 정책을 CNN/DailyMail 데이터셋의 테스트 스플릿에 있는 뉴스 기사와 같은 다른 분포에서 평가합니다 [24]. TL;DR에서 사용한 최적 샘플링 온도(0과 0.25)를 사용하여 평가합니다. 결과는 표 1에 제시되어 있습니다. 우리는 GPT-4 승률을 데이터셋의 실제 요약본과 비교하여 계산했으며, Reddit TL;DR에서 사용한 동일한 GPT-4 (C) 프롬프트를 사용하되, "forum post"를 "news article"로 교체했습니다. 이 새로운 분포에서도, DPO는 PPO 정책을 상당한 차이로 능가합니다. 이 실험은 DPO 정책이 추가적인 레이블이 없는 Reddit TL;DR 프롬프트를 사용하지 않더라도 PPO 정책만큼 잘 일반화할 수 있음을 보여주는 초기 증거를 제공합니다.

![](/assets/images/posts/169/img_27.png)

표 2: TL;DR 요약 샘플에 대한 인간과 GPT-4의 승률 및 판결당 동의율 비교. 인간은 GPT-4와 서로 동의하는 정도만큼 동의합니다. 각 실험 은 명시된 방법의 요약과 온도가 0인 PPO의 요약을 비교합니다.

### 6.4 GPT-4 판단을 인간 판단으로 검증

우리는 TL;DR 요약 실험의 결과와 두 가지 다른 GPT-4 프롬프트를 사용하여 GPT-4의 판단 신뢰성을 검증하기 위해 인간 연구를 수행합니다. GPT-4 (S) (simple) 프롬프트는 단순히 어떤 요약이 게시물의 중요한 정보를 더 잘 요약하는지 묻습니다. GPT-4 (C) (concise) 프롬프트는 어떤 요약이 더 간결한지 묻습니다; 우리는 GPT-4 (S) 프롬프트로 GPT-4가 인간보다 더 길고 반복적인 요약을 선호하는 것을 발견했기 때문에 이 프롬프트를 평가합니다. 전체 프롬프트는 부록 C.2를 참조하십시오. 우리는 가장 높은 성능(DPO, 온도 0.25), 가장 낮은 성능(PPO, 온도 1.0) 및 중간 성능(SFT, 온도 0.25)의 방법을 사용하여 세 가지 비교를 수행하며, 이는 다양한 샘플 품질을 포괄하는 것을 목표로 합니다; 세 가지 방법 모두 최고 성능의 온도에서 탐욕적으로 샘플링된 PPO와 비교됩니다. 두 프롬프트 모두에서, GPT-4는 인간이 서로 동의하는 빈도만큼 자주 인간과 동의하는 경향이 있음을 발견했습니다. 이는 GPT-4가 인간 평가의 적절한 대리임을 시사합니다(제한된 인간 평가자로 인해, 우리는 DPO와 PPO-1 비교에 대해서만 다수의 인간 판단을 수집했습니다). 전체적으로, GPT-4 (C) 프롬프트는 일반적으로 인간의 대표성을 더 잘 제공합니다; 따라서 우리는 섹션 6.2의 주요 결과에 대해 이 프롬프트를 사용합니다. 인간 연구에 대한 추가 세부 사항, 평가자에게 제공된 웹 인터페이스 및 인간 자원 봉사자 목록은 부록 D.3을 참조하십시오.

### 7. 토론

선호도 학습은 능력 있고 일치하는 언어 모델을 훈련하기 위한 강력하고 확장 가능한 프레임워크입니다. 우리는 DPO를 소개했습니다. 이는 강화 학습 없이 선호도로부터 언어 모델을 훈련하는 간단한 학습 패러다임입니다. 선호도 학습 문제를 표준 강화 학습 설정으로 강제하여 기성품 RL 알고리즘을 사용하는 대신, DPO는 언어 모델 정책과 보상 함수 간의 매핑을 식별하여 강화 학습 없이도 언어 모델을 인간 선호에 맞출 수 있게 합니다. 사실상 하이퍼파라미터 튜닝 없이도, DPO는 PPO 기반 알고리즘을 포함한 기존의 RLHF 알고리즘과 비슷하거나 더 나은 성능을 보입니다. 따라서 DPO는 인간 선호도로 더 많은 언어 모델을 훈련하는 장벽을 의미 있게 줄입니다.

### 제한 사항 및 향후 연구

우리의 결과는 향후 연구를 위한 몇 가지 중요한 질문을 제기합니다. DPO 정책은 명시적 보상 함수로 학습하는 것과 비교하여 분포 밖에서 어떻게 일반화될까요? 초기 결과는 DPO 정책이 PPO 기반 모델과 유사하게 일반화될 수 있음을 시사하지만, 더 포괄적인 연구가 필요합니다. 예를 들어, DPO 정책의 자기 라벨링으로 훈련하여 레이블이 없는 프롬프트를 효과적으로 사용할 수 있을까요? 다른 측면에서는, 보상 과다 최적화가 직접 선호 최적화 설정에서 어떻게 나타나며, 그림 3-오른쪽의 성능 저하는 그것의 예시인가요? 추가적으로, 우리는 최대 60억 개의 매개변수를 가진 모델을 평가했지만, DPO를 몇 배 더 큰 최신 모델로 확장하는 탐구는 흥미로운 방향입니다. 평가와 관련하여, GPT-4로 계산된 승률은 프롬프트에 의해 영향을 받는다는 것을 발견했습니다. 향후 연구는 자동 시스템으로부터 고품질 판단을 이끌어내는 최선의 방법을 연구할 수 있습니다. 마지막으로, DPO는 인간 선호로부터 언어 모델을 훈련하는 것 외에도 다른 모달리티의 생성 모델을 훈련하는 등 많은 가능한 응용이 있습니다.

### 감사의 말

EM은 Knight-Hennessy 대학원 펠로우십의 자금 지원에 감사드립니다. CF와 CM은 CIFAR 펠로우입니다. 이 연구는 부분적으로 Stanford Accelerator for Learning (SAL)과 Stanford Institute for Human-Centered Artificial Intelligence (HAI) Generative AI for the Future of Learning 시드 그랜트 프로그램의 지원을 받았습니다. Stanford Center for Research on Foundation Models (CRFM)은 이 연구의 실험에 사용된 일부 컴퓨팅 자원을 제공했습니다. 이 연구는 ONR 그랜트 N00014-20-1-2675의 지원을 부분적으로 받았습니다.

[2305.18290v2.pdf

1.24MB](./file/2305.18290v2.pdf)
