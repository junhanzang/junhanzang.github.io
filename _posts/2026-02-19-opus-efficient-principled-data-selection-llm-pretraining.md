---
title: "OPUS: LLM 사전 학습에서 매 반복마다 효율적이고 원칙에 입각한 데이터 선택을 향하여"
date: 2026-02-19 12:00:00
categories:
  - 인공지능
tags:
  - LLM
  - 데이터 선택
  - Pre-training
  - AdamW
  - Muon
  - 논문 리뷰
---

<https://arxiv.org/abs/2602.05400>

**OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration**
Shaobo Wang, Xuan Ouyang, Tianyi Xu, Yuzheng Hu, Jialin Liu, Guo Chen, Tianyu Zhang, Junhao Zheng, Kexin Yang, Xingzhang Ren, Dayiheng Liu, Linfeng Zhang

EPIC Lab, SJTU / Qwen Team, Alibaba Group / UW–Madison / UIUC / Mila - Quebec AI Institute

---

## 초록

고품질 공개 텍스트가 고갈에 가까워지는 "데이터 장벽(Data Wall)" 상황에서, 사전 학습의 초점이 양(量)에서 질(質)로 이동하고 있다. 기존 접근법은 학습 동역학을 무시하는 정적 휴리스틱 필터, 또는 SGD 방식의 기하학을 가정하여 원시 그래디언트를 그대로 사용하는 동적 방법에 의존한다. 본 논문은 **OPUS(Optimizer-induced Projected Utility Selection)**를 제안한다. OPUS는 현대 옵티마이저(AdamW, Muon)가 형성한 효과적인 업데이트를 안정적인 인-분포(in-distribution) 프록시의 목표 방향으로 투영하여 후보 데이터의 유용성을 평가하는 동적 프레임워크이다.

OPUS는 Ghost 기법(CountSketch 포함)과 볼츠만 샘플링을 통해 단 4.7%의 추가 계산 비용만으로 다양성을 확보한다. 실험 결과, FineWeb 및 FineWeb-Edu에서 GPT-2 Large/XL을 300억 토큰으로 사전 학습하면 산업계 베이스라인을 능가하고 2,000억 토큰 전체 학습과 동등한 성능을 달성한다. 또한 SciencePedia에서 Qwen3-8B-Base를 지속 사전 학습(continued pre-training)할 때, 전체 학습(30억 토큰) 대비 5억 토큰만으로도 더 우수한 성능을 보인다.

---

## 1. 서론

LLM 사전 학습은 무제한적 데이터 확장에서 효율성 중심으로 전환하고 있다. 최근 전망에 따르면, 쉽게 구할 수 있는 고품질 공개 텍스트는 2026~2028년 사이에 고갈될 수 있다. 이러한 데이터 제약 상황에서 최적화의 핵심 질문은 "어떤 토큰을 수집할 수 있는가"에서 **"이 특정 옵티마이저 스텝에서 어떤 토큰이 학습을 이끌어야 하는가"**로 바뀌어야 한다.

### 기존 방법의 한계

**정적 방법(Static Methods):** FineWeb-Edu 분류기, DCLM 품질 필터링 등 고정된 학습 무관 휴리스틱에 의존한다. 모델이 진화함에 따라 샘플의 유용성이 변화한다는 사실을 고려하지 않는다.

**동적 방법(Dynamic Methods):** 원시 그래디언트 공간에서 후보를 평가하여, AdamW·Muon 등 적응형 옵티마이저가 그래디언트를 전처리하고 실질적인 업데이트 방향을 재형성한다는 현실과 근본적으로 불일치한다.

### 핵심 기여

1. **원칙에 입각한 옵티마이저 인식 유용성:** AdamW와 Muon의 효과적인 업데이트 방향에 대한 폐쇄형 근사(closed-form approximation)를 유도하여, 원시 그래디언트 공간이 아닌 실제 옵티마이저 유도 기하학에서 데이터를 평가한다.

2. **안정적인 인-분포 프록시 구성:** 사전 학습 코퍼스에서 벤치마크 정렬 샘플을 직접 검색하는 Bench-Proxy 절차를 제안하여, 유용성 추정을 안정화하는 신뢰할 수 있는 프록시 방향을 만든다.

3. **확장 가능한 유용성 추정:** Ghost 기법과 CountSketch 투영을 결합하여, 샘플별 그래디언트 완전 구현 없이 낮은 차원 공간에서 내적 계산을 수행한다.

4. **다양성을 위한 볼츠만 샘플링:** 결정론적 top-k 선택 대신 소프트 볼츠만 분포와 스텝 내 중복 패널티를 사용하는 확률적 샘플링으로 다양성 붕괴를 방지한다.

5. **강력한 실증 검증:** FineWeb 데이터셋에서 GPT-XL 기준 10개 벤치마크 평균 2.2% 정확도 향상, 8배 계산량 감소 달성.

---

## 2. 관련 연구

### 정적 사전 학습 데이터 선택

대규모 LLM 파이프라인 대부분은 학습 전에 정적 코퍼스 필터링을 적용한다.

- **FineWeb 및 FineWeb-Edu:** Common Crawl에서 문서 수준 중복 제거 및 품질 필터링
- **QuRating:** 쌍별 선호도로부터 품질 평점을 학습, 품질과 다양성의 균형 유지
- **DSIR:** 축소된 특성 공간에서 중요도 재샘플링을 통한 데이터셋 매칭을 형식화
- **DataComp-LM(DCLM):** 필터링 전략을 비교하는 표준화된 코퍼스 및 평가 스위트
- **UltraFineweb:** 고품질 데이터를 위한 효율적인 필터링 및 검증 메커니즘

이러한 방법들은 저품질 노이즈 제거에는 효과적이지만, 샘플의 유용성이 시간 불변이라고 가정하며 최적화 과정에서 모델의 진화하는 필요를 반영하지 못한다.

### 사전 학습 중 동적 데이터 선택

동적 선택은 추정된 학습 유용성을 기반으로 즉석에서 샘플을 선택한다.

- **휴리스틱 방법:** 온라인 배치 선택으로 높은 손실 또는 높은 퍼플렉시티 샘플 우선 선택
- **영향 함수 방법:** 검증 손실에 대한 학습 포인트 영향 추정; 고전적 방법은 헤시안 역행렬 필요(비용 과다)
- **GREATS:** Taylor 전개를 통한 샘플별 검증 손실 감소 근사
- **MATES 및 Group-MATES:** 경량 영향 모델 학습
- **퍼플렉시티 기반 프루닝:** 경쟁력 있는 단순 신호

OPUS는 옵티마이저 유도 업데이트와 유용성을 정렬하고, 투영된 평가와 소프트 샘플링을 사용함으로써 이 연구 흐름을 발전시킨다.

### 영향 함수와 데이터 귀속

- **고전적 영향 함수:** 헤시안 기반 민감도 분석으로 포인트 업가중 효과 근사
- **확장 가능한 대리 방법:** 실용적 대안으로 정확한 2차 계산 대체
- **1차 프록시:** 학습 초기에 정보적 부분집합 선택, 망각 이벤트 활용
- **그래디언트 매칭:** 그래디언트 매칭으로 부분집합 선택 최적화
- **샤플리 값:** 협력 게임 이론으로 데이터 가치 정량화

많은 방법들이 원시 그래디언트 기하학에서 작동하고 결정론적 top-k 검색을 채택하여, 빠르게 변화하는 학습 동역학 아래서 불안정해진다. OPUS는 이 한계를 확장성과 다양성을 보존하면서 해결한다.

---

## 3. 배경

### 3.1 LLM 사전 학습

파라미터 $\theta \in \mathbb{R}^d$로 매개변수화된 자기회귀 언어 모델 $f\_\theta$를 고려한다. 학습 샘플은 토큰 시퀀스 $z = (x\_1, \ldots, x\_L)$이며, 모델은 다음 토큰 분포 $p\_\theta(x\_i \mid x\_{<i})$를 정의한다. 시퀀스당 손실은 음의 로그 우도:

$$\mathcal{L}(z; \theta) = -\frac{1}{L} \sum_{i=1}^L \log p_\theta(x_i \mid x_{<i})$$

임의의 분포(또는 유한 집합) $\mathcal{Q}$에 대해 기대 손실을 $\mathcal{L}(\mathcal{Q}; \theta) := \mathbb{E}\_{z \sim \mathcal{Q}}[\mathcal{L}(z; \theta)]$로 정의한다.

전체 사전 학습 코퍼스 $\mathcal{D}$는 다음으로 분할된다:
- **학습 집합 $\mathcal{D}\_{tr}$:** 파라미터 업데이트에 사용
- **홀드아웃 검증 집합 $\mathcal{D}\_{val}$:** 선택 안내; $\mathcal{D}\_{val} \cap \mathcal{D}\_{tr} = \emptyset$

### 3.2 사전 학습에서의 데이터 선택

#### 정적 데이터 선택
학습 전에 전체 후보 풀 $\mathcal{D}\_{tr}$을 오프라인으로 필터링한다. 평가 함수 $S(z)$가 각 샘플 $z \in \mathcal{D}\_{tr}$에 품질 점수를 부여하며, 임계값 또는 top-k 선택으로 $\mathcal{D}\_{selected}$를 구성한다. 확장 가능하지만 학습 중 모델의 진화하는 상태 $\theta\_t$를 무시한다.

#### 동적 데이터 선택
매 스텝 $t$마다 현재 파라미터 $\theta\_t$와 옵티마이저 상태에 적응하며 데이터를 선택한다. 스텝 $t$에서 학습 스트림 $\mathcal{D}\_{tr}$으로부터 $N$개 시퀀스의 후보 버퍼 $\mathcal{B}\_t = \{z\_1, \ldots, z\_N\}$를 받고, 크기 $K = \lfloor\rho N\rfloor$ (선택 비율 $\rho \in (0,1]$)의 부분집합 $\hat{\mathcal{B}}\_t \subset \mathcal{B}\_t$을 선택한다:

$$\hat{\mathcal{B}}_t = \text{Select}(\mathcal{B}_t; s_t(\cdot), K)$$

### 3.3 대규모 사전 학습의 현대 옵티마이저

많은 동적 선택 방법들이 원시 그래디언트 $\nabla \mathcal{L}(z; \theta\_t)$를 사용해 SGD 방식의 기하학을 암묵적으로 가정한다. 그러나 현대 LLM 학습은 모멘텀, 적응형 전처리 등 상태를 사용하여 그래디언트를 변환하는 옵티마이저를 사용하므로, 실효 업데이트 방향이 달라진다.

스텝 $t$에서 전처리 연산자 $\mathbf{P}\_t$를 적용한 옵티마이저 유도 실효 업데이트:

$$\Delta\theta_t(\hat{\mathcal{B}}_t) = -\eta_t \sum_{z \in \hat{\mathcal{B}}_t} \mathbf{P}_t \nabla \mathcal{L}(z; \theta_t)$$

이는 데이터 선택 점수를 원시 그래디언트 공간이 아닌 옵티마이저 유도 기하학에서 정의할 필요성을 동기부여한다.

---

## 4. 옵티마이저 유도 전처리기

### 4.1 확률적 경사 하강법(SGD)

SGD는 균일한 스칼라 학습률만 적용하고 상태 기반 전처리 없이 작동하므로, 실효 업데이트 방향이 미니배치 그래디언트와 일치한다. 고정 스텝 $t$에서 SGD는 근사적으로 항등 업데이트 기하학을 유도한다: $\mathbf{P}\_t \approx \mathbf{I}$.

### 4.2 Muon 전처리기

Muon은 선형 가중치 행렬 $W\_\ell \in \mathbb{R}^{o \times i}$에 대해 미니배치 그래디언트의 EMA 모멘텀을 유지한다:

$$\mathbf{m}_{t+1,\ell}(S) = \mu \mathbf{m}_{t,\ell} + (1-\mu)\mathbf{g}_{t,\ell}(S)$$

실제로 Muon은 직교화기에 공급되는 "이중 평활화" 방향을 형성한다:

$$\mathbf{q}_{t+1,\ell}(S) = \mu^2 \mathbf{m}_{t,\ell}(S) + (1-\mu^2)\mathbf{g}_{t,\ell}(S)$$

파라미터 스텝:

$$\Delta W_{t,\ell}(S) = -\eta_t \mathcal{O}_{t,\ell}(\mathbf{q}_{t+1,\ell}(S))$$

#### 온라인 선택 관점

고정 스텝 $t$에서 Muon의 상태를 고정하고, Newton–Schulz 연산자를 참조 방향 $\bar{\mathbf{q}}\_{t,\ell}$으로부터 구성하여 동결시킨다. 이 근사 하에서 NS는 근사적으로 선형 좌측 곱셈 맵을 유도한다:

$$\mathcal{O}_{t,\ell}(Z) \approx \mathbf{S}_{t,\ell}Z, \quad \mathbf{S}_{t,\ell} = a\mathbf{I} + b\mathbf{A}_{t,\ell} + c\mathbf{A}_{t,\ell}^2$$

$$\mathbf{A}_{t,\ell} := \tilde{\bar{\mathbf{q}}}_{t,\ell}\tilde{\bar{\mathbf{q}}}_{t,\ell}^\top$$

여기서 $a, b, c$는 고정된 NS 다항식 계수이다. 선형화된 사전 탐색 업데이트를 대입하면:

$$\Delta W_{t,\ell}(S) \approx -\mathbf{P}^{\text{Muon}}_{t,\ell} \mathbf{g}_{t,\ell}(S) + \text{const}$$

$$\mathbf{P}^{\text{Muon}}_{t,\ell} := \kappa_t \mathbf{S}_{t,\ell}$$

Muon은 평가 전에 그래디언트 방향을 재형성하는 밀집하고 샘플 독립적인 좌측 전처리기를 유도한다.

### 4.3 AdamW 전처리기

스텝 $t$에서 부분집합 $S$에 적용된 분리된 AdamW 업데이트:

$$\mathbf{m}_t(S) = \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\mathbf{g}_t(S)$$
$$\mathbf{v}_t(S) = \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)\mathbf{g}_t(S)^{\odot 2}$$
$$\hat{\mathbf{m}}_t(S) = \mathbf{m}_t(S)/(1-\beta_1^t), \quad \hat{\mathbf{v}}_t(S) = \mathbf{v}_t(S)/(1-\beta_2^t)$$
$$\theta_{t+1}(S) = \theta_t - \alpha_t \hat{\mathbf{m}}_t(S)/(\sqrt{\hat{\mathbf{v}}_t(S)} + \varepsilon) - \alpha_t \lambda \theta_t$$

#### 온라인 선택 관점

고정 스텝 $t$에서 옵티마이저 상태를 상수로 취급한다. RMS 기하학을 동결하여 2차 모멘트 업데이트에서 $S$ 의존성을 제거하는 근사를 적용한다:

$$\sqrt{\hat{\mathbf{v}}_t(S)} + \varepsilon \approx \sqrt{\bar{\mathbf{v}}_t} + \varepsilon, \quad \bar{\mathbf{v}}_t := \beta_2 \mathbf{v}_{t-1}/(1-\beta_2^t)$$

이를 대입하면, OPUS가 고정 스텝 $t$에서 상대적 유용성으로 부분집합을 순위 매기므로, $S$에 무관한 항은 유용성에 가산 상수를 기여하고 순위에 영향을 주지 않는다. 효과적인 데이터 의존 업데이트:

$$\Delta\theta_t(S) \approx -\mathbf{P}^{\text{AdamW}}_t \mathbf{g}_t(S) + \text{const}$$

$$\mathbf{P}^{\text{AdamW}}_t := C_t \cdot \text{Diag}(1/(\sqrt{\hat{\mathbf{v}}_{t-1}} + \varepsilon))$$

$$C_t := \alpha_t(1-\beta_1)/(1-\beta_1^t)$$

AdamW는 유용성을 측정하기 전에 좌표를 재조정하는 근사적 대각 전처리기를 유도한다.

---

## 5. 방법론: OPUS

대규모 사전 학습에서의 동적 선택은 세 가지 요건을 충족해야 한다:

1. **원칙성(Principled):** 옵티마이저 유도 업데이트 기하학 하에서 홀드아웃 프록시의 개선을 측정하는 명시적 목적함수에서 도출된 점수
2. **효율성(Efficient):** 고차원 공간에서 샘플별 그래디언트를 구현하지 않는 평가
3. **확장성(Scalable):** 모델 차원 $m$이 증가해도 오버헤드가 적절히 유지되어 매 스텝 선택 가능

### 5.1 옵티마이저 유도 유용성 목적함수

후보 배치 $\mathcal{S}$의 유용성을 한 번의 최적화 스텝 후 검증 집합 $\mathcal{D}\_{val}$에서의 손실 감소로 정의한다:

$$U^{(t)}(\mathcal{S}) := \mathcal{L}(\mathcal{D}_{val}; \theta_t) - \mathcal{L}(\mathcal{D}_{val}; \theta_{t+1}(\mathcal{S}))$$

#### 한계 이득(Marginal Gain)

후보 $z \in \mathcal{B}\_t \setminus \hat{\mathcal{B}}\_t$를 추가하는 한계 유용성:

$$U^{(t)}_z := U^{(t)}(\hat{\mathcal{B}}_t \cup \{z\}) - U^{(t)}(\hat{\mathcal{B}}_t)$$

1차 Taylor 근사를 적용하면:

$$U^{(t)}_z \approx -\nabla_\theta \mathcal{L}(\mathcal{D}_{val}; \tilde{\theta}_t(\hat{\mathcal{B}}_t))^\top \Delta\theta_t(\{z\})$$

#### 옵티마이저 유도 기하학

현대 LLM 학습의 상태 의존 전처리기 $\mathbf{P}\_t$를 적용한 옵티마이저 유도 실효 업데이트 방향:

$$\mathbf{u}^{(t)}_z := \mathbf{P}_t \nabla_\theta \mathcal{L}(z; \theta_t)$$

단일 후보 $z$ 추가 시: $\Delta\theta\_t(\{z\}) = -\eta\_t \mathbf{u}^{(t)}\_z$. 대입하면:

$$U^{(t)}_z \approx \eta_t \langle \mathbf{u}^{(t)}_z, \nabla_\theta \mathcal{L}(\mathcal{D}_{val}; \tilde{\theta}_t(\hat{\mathcal{B}}_t)) \rangle$$

#### 가상 검증 그래디언트 근사

현재 선택된 부분집합 $\hat{\mathcal{B}}\_t$에서 가상 파라미터 $\tilde{\theta}\_t(\hat{\mathcal{B}}\_t)$에서 평가된 검증 그래디언트를 정확히 계산하는 것은 비용이 과다하다. 따라서 현재 파라미터 $\theta\_t$ 주변에서 그래디언트 함수를 선형화한다:

$$\nabla_\theta \mathcal{L}(\mathcal{D}_{val}; \tilde{\theta}_t(\hat{\mathcal{B}}_t)) \approx \mathbf{g}^{(t)}_{val} + \mathbf{H}^{(t)}_{val} \Delta\theta_t(\hat{\mathcal{B}}_t)$$

누적 업데이트를 대입하고 헤시안을 등방성으로 근사($\mathbf{H}\_{val} \approx \mathbf{I}$)하면, 누적 실효 방향 $\mathbf{G}^{(t)} := \sum\_{z\_j \in \hat{\mathcal{B}}\_t} \mathbf{u}^{(t)}\_{z\_j}$를 정의하여 실용적인 중복 조정 점수를 얻는다:

$$U^{(t)}_z \approx \eta_t \langle \mathbf{u}^{(t)}_z, \mathbf{g}_{val}^{(t)} \rangle - \eta_t^2 \langle \mathbf{u}^{(t)}_z, \mathbf{G}^{(t)} \rangle$$

$$\underbrace{\eta_t \langle \mathbf{u}^{(t)}_z, \mathbf{g}_{val}^{(t)} \rangle}_{\text{정렬(Alignment)}} - \underbrace{\eta_t^2 \langle \mathbf{u}^{(t)}_z, \mathbf{G}^{(t)} \rangle}_{\text{중복 패널티(Redundancy Penalty)}}$$

#### Bench-Proxy를 통한 안정적인 프록시 구성

프록시 방향 $\mathbf{g}^{(t)}\_{val}$의 품질이 원칙적인 선택에 매우 중요하다. 랜덤 홀드아웃은 낮은 분산 신호를 제공하지만 다운스트림 태스크 분포를 포착하지 못하는 경우가 많고, 원시 벤치마크 샘플을 직접 사용하면 분포 변화가 심하고 그래디언트 노이즈가 발생한다.

**Bench-Proxy**를 도입한다(그림 3(a)): 동결된 텍스트 인코더(Arctic-Embed-L v2)로 목표 벤치마크 검증 집합과 사전 학습 코퍼스의 후보 문서를 임베딩하여, 코사인 유사도가 가장 높은 상위 $M$개 사전 학습 문서를 검색하여 인-분포 프록시 풀 $\mathcal{D}\_{proxy}$를 구성한다.

이 접근법은 목표 태스크와 정렬되면서도 사전 학습 매니폴드 내에 있는 프록시를 제공하여, 유효한 그래디언트 추정을 보장한다. 스텝 $t$에서 프록시 미니배치를 추출하여 방향을 추정한다:

$$\mathbf{g}^{(t)}_{proxy} = \frac{1}{K} \sum_{k=1}^{K_{proxy}} \nabla_\theta \mathcal{L}(\tilde{z}_k; \theta_t)$$

### 5.2 확장 가능한 유용성 추정

#### Ghost 기법

선형 레이어에서 역전파된 그래디언트의 rank-1 외적 구조를 활용한다. 가중치 $W\_r$인 선형 레이어 $r$에 대해, 샘플 $z$의 입력 활성화 벡터 $\mathbf{a}^{(z)}\_r$과 출력 그래디언트 벡터 $\mathbf{b}^{(z)}\_r$로 샘플별 그래디언트를 인수분해한다:

$$\nabla_{W_r} \mathcal{L}(z; \theta_t) = \mathbf{a}^{(z)}_r \otimes \mathbf{b}^{(z)}_r$$

표준 순전파/역전파 중에 $\mathbf{a}^{(z)}\_r$과 $\mathbf{b}^{(z)}\_r$를 모두 이용할 수 있으므로, 고차원 행렬 그래디언트를 구현하지 않고 그래디언트 통계를 계산한다. 프록시 배치와 후보 배치를 단일 순전파/역전파 패스 내에서 연결하여 모든 샘플에 대한 $\{\mathbf{a}^{(z)}\_r, \mathbf{b}^{(z)}\_r\}$를 수집하고, 투영된 점수 계산에 필요한 모든 정보를 담아 레이어별로 저비용 메모리로 폐기한다.

#### CountSketch 투영

유용성 $U^{(t)}\_z$를 계산하려면 옵티마이저 전처리기 $\mathbf{P}\_t$를 적용해야 한다. 희소 CountSketch 맵 $\Pi: \mathbb{R}^d \to \mathbb{R}^m$ ($m \ll d$)을 사용하여 결과 실효 업데이트를 낮은 차원의 스케치 공간으로 투영한다.

입력 차원 $d\_{in} \times d\_{out}$인 선형 레이어 $r$에 대해, 샘플별 전처리된 스케치 특성 $\phi^{(t,r)}(z) \in \mathbb{R}^m$을 암묵적으로 계산한다:

$$\phi^{(t,r)}(z) = \Pi_r(\mathbf{P}_{t,r}(\mathbf{a}^{(z)}_r \otimes \mathbf{b}^{(z)}_r))$$

- **AdamW의 경우:** $\mathbf{P}\_{t,r}$이 대각(diagonal)이어서 외적 그래디언트의 좌표별 분리 가능 구조를 유지한다. CountSketch 투영을 전처리와 인터리브하여 적용하고, 대각 가중치를 즉석에서 적용함으로써 투영 비용이 $\mathcal{O}(d\_{in} + d\_{out})$이다.
- **밀집 전처리기(Muon 등)의 경우:** 좌표 혼합이 분리 가능성을 파괴하여 투영 비용이 $\mathcal{O}(d\_{in} d\_{out})$이다.

스케치 공간에서 정렬 및 중복 항을 레이어에 걸쳐 합산하여 근사한다:

$$U^{(t)}_z \approx \eta_t \sum_{r \in \mathcal{R}} \langle \phi^{(t,r)}(z), \psi^{(t,r)}_{proxy} \rangle - \eta_t^2 \sum_{r \in \mathcal{R}} \langle \phi^{(t,r)}(z), \mathbf{\Phi}^{(t,r)} \rangle$$

### 5.3 볼츠만 샘플링

다양성을 보존하기 위해 결정론적 greedy top-k를 확률적 샘플링으로 대체한다. 유용성 공식이 기하학적 중복을 명시적으로 패널티하지만, greedy 선택은 추정 노이즈에 취약하다. 따라서 볼츠만 샘플링을 채택하여 강건성을 개선한다:

$$p^{(t)}_z \propto \exp(U^{(t)}_z / \tau)$$

이를 통해 높은 유용성의 후보가 선호되면서도 보완적 후보들도 비제로 확률을 유지하여, 로컬 프록시 노이즈에 대한 과적합을 방지한다.

---

## 6. 알고리즘

![OPUS 파이프라인 개요](/assets/images/posts/opus-efficient-principled-data-selection-llm-pretraining/x3.png)

**알고리즘 1: OPUS - 옵티마이저 유도 투영 유용성 선택**

```
입력: 모델 f_θ; 학습 데이터 스트림 D_tr; 프록시 풀 D_proxy;
      옵티마이저 O; 선택 비율 ρ; 투영 차원 m

초기화: 해시 h:[d]→[m]와 부호 s:[d]→{-1,+1}로 CountSketch 연산자 Π 초기화

for t=0,1,… do

  1. 배치 샘플링: D_tr에서 후보 버퍼 B_t={z₁,…,z_N} 읽기

  2. 전처리기 계산: 스텝 t에서 O의 상태로부터 옵티마이저 유도 전처리기
     P_t = P(O_t) 구성

  3. 프록시 특성 생성: D_proxy에서 K_proxy개 샘플 추출, Ghost 인수 수집,
     레이어별 프록시 스케치 ψ^(t,r)_proxy 계산

  4. 후보 특성 생성: Ghost 인수로부터 레이어별 스케치 φ^(t,r)(z) ∈ R^m 암묵적 계산

  5. 소프트 샘플링 루프:
     목표 배치 크기 K=⌊ρN⌋, 선택 집합 B̂_t←∅ 초기화

     for j=1 to K do
       각 z에 대해 유용성 U^(t)_z 계산:
         U^(t)_z ← η_t Σ_r⟨φ^(t,r)(z), ψ^(t,r)_proxy⟩
                    - η²_t Σ_r⟨φ^(t,r)(z), Φ^(t,r)⟩

       Softmax로 z* 샘플링: p_t(z*) ∝ exp(U^(t)_z / τ)

       배치에 추가: B̂_t ← B̂_t ∪ {z*}

       이력 업데이트: Φ^(t,r) ← Φ^(t,r) + φ^(t,r)(z*), ∀r
     end for

  6. 업데이트: 옵티마이저 O로 배치 B̂_t를 사용하여 θ_{t+1} 학습

end for
```

---

## 7. 실험

### 7.1 실험 설정

#### 모델 및 학습 설정

300억 업데이트 토큰의 고정 최적화 예산 하에 GPT-2 Large와 GPT-2 XL을 처음부터 사전 학습한다:
- **GPT-2 Large:** 36층, 히든 크기 1280, ~7.74억 파라미터
- **GPT-2 XL:** 48층, 히든 크기 1600, ~15억 파라미터

Qwen3-8B-Base를 사용한 지속 사전 학습도 평가한다:
- 36층, 히든 크기 4096, ~80억 파라미터

#### 옵티마이저 설정

두 가지 옵티마이저 설정을 동일한 학습률 스케줄과 학습 레시피 하에 평가한다:

**Muon 설정:** 행렬 형태의 파라미터에 Muon 업데이트 적용, 나머지(바이어스, 정규화 파라미터)에는 AdamW 사용. 행렬 파라미터 학습률 $\eta\_{\text{muon}} = 10^{-2}$, 모멘텀 $\mu = 0.95$.

**AdamW 설정:** 모든 파라미터에 AdamW 사용. $\beta\_1 = 0.8$, $\beta\_2 = 0.95$, $\varepsilon = 10^{-8}$, 가중치 감쇠 $\lambda = 0$.

#### OPUS 평가 설정

- 후보 버퍼 크기: $N = 32$ (GPT-2), $N = 16 \times W$ (Qwen3-8B, GPU당 수집 후 전역 선택)
- 선택 비율 $\rho = 0.5$ (배치의 절반 선택)
- 프록시 배치 크기 8, 매 스텝 검증 집합 갱신
- 스케치 차원 $m = 8192$ (GPT-2 XL 기준 최대 행렬 그래디언트 ~10.24M 차원 → 약 1,250배 압축)

#### 벤치마크 평가 구성

**핵심 벤치마크 (인-도메인):**

| 벤치마크 | 도메인 | 선택지 수 | 평가 방식 | 지표 |
|----------|--------|-----------|-----------|------|
| MMLU | 지식 | 4 | LL | 정확도 |
| ANLI | 이해 | 3 | PPL | 정확도 |
| HellaSwag | 상식/추론 | 4 | PPL | 정확도 |
| PIQA | 상식/추론 | 2 | PPL | 정확도 |
| SIQA | 상식/추론 | 3 | PPL | 정확도 |
| WinoGrande | 언어 | 2 | LL | 정확도 |
| ARC-Easy | 과학/추론 | 4 | PPL | 정확도 |
| ARC-Challenge | 과학/추론 | 4 | PPL | 정확도 |
| CommonsenseQA | 상식/추론 | 5 | PPL | 정확도 |
| WSC | 언어 | 2 | PPL | 정확도 |

**기타 벤치마크 (아웃-오브-도메인):**

| 벤치마크 | 도메인 | 선택지 수 | 평가 방식 | 지표 |
|----------|--------|-----------|-----------|------|
| BBH | 추론(어려움) | – | 생성 | 정확 일치 |
| RACE-Middle | 이해 | 4 | PPL | 정확도 |
| RACE-High | 이해 | 4 | PPL | 정확도 |
| AX-b | 언어 | 2 | PPL | 정확도 |
| AX-g | 언어 | 2 | PPL | 정확도 |
| StoryCloze | 이해 | 2 | PPL | 정확도 |

#### 베이스라인 방법

**정적 베이스라인:**
1. QuRating
2. DSIR
3. DCLM-FastText
4. FineWeb-Edu 분류기
5. UltraFineweb 분류기

**동적 선택:**
1. High-PPL: 현재 모델 하에서 높은 손실 시퀀스 선택
2. GREATS: SGD 기반 프록시 방향과 그래디언트가 가장 잘 정렬된 샘플 선택

**참조:** 30B 및 60B 업데이트 토큰에서의 랜덤 선택

### 7.2 Bench-Proxy 구성

목표 벤치마크 검증 집합 $\mathcal{D}\_{val}$을 추정하는 Bench-Proxy 구성을 설명한다.

#### 유사도 평가

동결된 문장 임베딩 모델 Arctic-Embed-L v2로 각 벤치마크 샘플과 각 사전 학습 문서를 인코딩하여 공유 임베딩 공간에서 코사인 유사도를 계산한다. 모든 벤치마크 샘플에 대한 최대 유사도를 취하여 문서가 어떤 벤치마크 인스턴스와도 강하게 정렬되는지 포착한다.

#### 프록시 구성

점수 매겨진 코퍼스에서 점수를 내림차순으로 정렬하고 고정 토큰 예산(실험에서 3,000만 토큰)에 도달할 때까지 탐욕적으로 누적하여 콤팩트한 벤치마크 정렬 프록시 샤드를 생성한다.

### 7.3 처음부터 사전 학습 실험

#### FineWeb에서의 성능

![OPUS 성능 비교](/assets/images/posts/opus-efficient-principled-data-selection-llm-pretraining/x1.png)

**표 1: FineWeb 데이터셋에서 30B 토큰 학습 후 평가 결과**

**Muon 옵티마이저를 사용한 GPT-2 Large (30B 업데이트 토큰):**

| 방법 | MMLU | ANLI | HellaSwag | PIQA | SIQA | W.G. | ARC-E | ARC-C | C.QA | WSC | 평균 |
|------|------|------|-----------|------|------|------|-------|-------|------|-----|-----|
| Random | 28.46 | 32.93 | 42.71 | 69.70 | 40.07 | 49.17 | 37.57 | 28.14 | 31.94 | 36.54 | 39.72 |
| PPL | 28.40 | 33.24 | 42.69 | 70.13 | 40.17 | 48.38 | 36.16 | 23.05 | 31.86 | 36.54 | 39.06 |
| GREATS | 28.49 | 33.31 | 42.22 | 70.18 | 39.46 | 49.41 | 36.86 | 24.41 | 33.25 | 36.54 | 39.41 |
| QuRating | 31.53 | 34.12 | 39.47 | 66.38 | 39.82 | 50.59 | 40.92 | 30.51 | 30.22 | 38.46 | 40.20 |
| DSIR | 28.50 | 33.39 | 43.04 | 69.70 | 40.53 | 49.64 | 37.39 | 24.41 | 32.27 | 36.54 | 39.54 |
| DCLM-FastText | 29.36 | 33.17 | 44.26 | 71.16 | 39.82 | 49.96 | 37.92 | 24.75 | 32.02 | 36.54 | 39.90 |
| FineWeb-Edu | 28.83 | 32.67 | 43.09 | 70.02 | 40.28 | 47.75 | 39.15 | 24.75 | 33.66 | 38.46 | 39.87 |
| UltraFineweb | 29.00 | 32.99 | 44.38 | 71.11 | 40.17 | 48.78 | 37.57 | 25.08 | 33.91 | 38.46 | 40.15 |
| **OPUS (제안)** | **28.76** | **33.12** | **42.92** | **69.97** | **39.56** | **50.43** | **38.98** | **29.15** | **33.09** | **36.54** | **40.25** |
| Random (60B) | 28.70 | 33.23 | 45.20 | 71.16 | 40.79 | 49.41 | 39.68 | 25.42 | 31.12 | 36.54 | 40.13 |

**Muon 옵티마이저를 사용한 GPT-2 XL (30B 업데이트 토큰):**

| 방법 | MMLU | ANLI | HellaSwag | PIQA | SIQA | W.G. | ARC-E | ARC-C | C.QA | WSC | 평균 |
|------|------|------|-----------|------|------|------|-------|-------|------|-----|-----|
| Random | 28.73 | 33.98 | 48.01 | 70.46 | 39.61 | 47.91 | 38.98 | 25.42 | 33.25 | 36.54 | 40.29 |
| PPL | 29.35 | 33.42 | 47.87 | 71.55 | 40.69 | 45.86 | 38.45 | 24.07 | 30.38 | 36.54 | 39.82 |
| GREATS | 29.95 | 33.58 | 42.26 | 70.18 | 39.61 | 47.67 | 36.33 | 23.73 | 30.55 | 38.46 | 39.23 |
| QuRating | 33.28 | 33.19 | 48.62 | 70.95 | 41.20 | 48.70 | 37.04 | 26.78 | 30.88 | 36.54 | 40.72 |
| DSIR | 29.58 | 33.98 | 48.49 | 71.93 | 39.51 | 47.59 | 38.10 | 26.44 | 32.68 | 38.46 | 40.68 |
| DCLM-FastText | 30.40 | 34.08 | 44.07 | 71.38 | 41.97 | 48.38 | 38.80 | 29.49 | 30.88 | 36.54 | 40.60 |
| FineWeb-Edu | 29.66 | 33.12 | 48.45 | 71.71 | 41.25 | 46.17 | 39.19 | 28.14 | 31.29 | 38.46 | 40.74 |
| UltraFineweb | 29.95 | 33.31 | 43.11 | 70.57 | 40.79 | 47.51 | 36.51 | 26.44 | 31.70 | 36.54 | 39.64 |
| **OPUS (제안)** | **29.89** | **33.29** | **48.39** | **71.27** | **41.10** | **47.99** | **39.68** | **26.44** | **31.37** | **48.08** | **41.75** |
| Random (60B) | 30.24 | 33.84 | 51.10 | 72.25 | 40.89 | 48.78 | 41.98 | 23.05 | 32.35 | 38.46 | 41.29 |

**AdamW를 사용한 GPT-2 Large (30B 업데이트 토큰):**

| 방법 | MMLU | ANLI | HellaSwag | PIQA | SIQA | W.G. | ARC-E | ARC-C | C.QA | WSC | 평균 |
|------|------|------|-----------|------|------|------|-------|-------|------|-----|-----|
| Random | 28.19 | 32.91 | 42.65 | 69.37 | 40.79 | 50.12 | 37.21 | 25.08 | 30.06 | 36.54 | 39.29 |
| PPL | 28.69 | 33.44 | 42.23 | 68.77 | 40.43 | 47.36 | 36.68 | 22.37 | 32.84 | 36.54 | 38.94 |
| GREATS | 28.77 | 33.46 | 43.00 | 70.46 | 40.63 | 49.96 | 38.45 | 23.39 | 32.02 | 36.54 | 39.67 |
| QuRating | 31.87 | 33.08 | 43.22 | 70.24 | 40.74 | 49.88 | 37.21 | 24.75 | 33.58 | 36.54 | 40.11 |
| DSIR | 28.22 | 33.18 | 43.42 | 69.53 | 40.02 | 48.93 | 37.92 | 25.08 | 31.20 | 38.46 | 39.60 |
| DCLM-FastText | 29.11 | 33.05 | 43.60 | 70.67 | 39.41 | 47.51 | 39.33 | 25.08 | 33.42 | 36.54 | 39.77 |
| FineWeb-Edu | 29.03 | 35.41 | 42.82 | 70.29 | 40.38 | 47.51 | 39.51 | 27.12 | 31.86 | 38.46 | 40.24 |
| UltraFineweb | 29.05 | 33.51 | 43.51 | 70.67 | 40.38 | 48.62 | 41.62 | 25.76 | 34.15 | 36.54 | 40.38 |
| **OPUS (제안)** | **31.09** | **34.04** | **45.52** | **69.97** | **40.69** | **51.62** | **42.50** | **26.44** | **33.99** | **38.46** | **41.43** |
| Random (60B) | 29.08 | 33.08 | 44.40 | 70.89 | 41.15 | 48.70 | 37.74 | 22.03 | 32.43 | 36.54 | 39.60 |

**AdamW를 사용한 GPT-2 XL (30B 업데이트 토큰):**

| 방법 | MMLU | ANLI | HellaSwag | PIQA | SIQA | W.G. | ARC-E | ARC-C | C.QA | WSC | 평균 |
|------|------|------|-----------|------|------|------|-------|-------|------|-----|-----|
| Random | 28.76 | 33.56 | 46.63 | 70.35 | 42.37 | 49.19 | 39.15 | 24.41 | 32.68 | 36.54 | 40.36 |
| PPL | 29.32 | 33.67 | 45.31 | 70.08 | 41.71 | 49.72 | 39.68 | 24.75 | 31.29 | 38.46 | 40.02 |
| GREATS | 28.81 | 33.49 | 40.73 | 69.53 | 42.48 | 49.01 | 34.22 | 24.75 | 31.04 | 38.46 | 39.25 |
| QuRating | 32.24 | 32.61 | 34.66 | 66.65 | 38.54 | 50.43 | 36.86 | 24.75 | 28.42 | 36.54 | 38.71 |
| DSIR | 29.37 | 33.09 | 45.88 | 70.67 | 39.97 | 47.51 | 38.80 | 24.41 | 33.42 | 36.54 | 39.97 |
| DCLM-FastText | 29.43 | 34.47 | 42.45 | 69.91 | 41.86 | 47.59 | 36.33 | 24.41 | 31.53 | 36.54 | 39.45 |
| FineWeb-Edu | 29.71 | 33.51 | 46.62 | 71.93 | 41.91 | 46.88 | 40.04 | 25.08 | 32.10 | 36.54 | 40.43 |
| UltraFineweb | 29.25 | 33.51 | 41.76 | 69.21 | 41.40 | 49.57 | 37.92 | 24.07 | 32.76 | 36.54 | 39.60 |
| **OPUS (제안)** | **29.43** | **33.51** | **46.12** | **70.35** | **41.35** | **50.36** | **39.33** | **29.15** | **33.99** | **36.54** | **41.01** |
| Random (60B) | 29.55 | 33.57 | 48.75 | 72.09 | 41.10 | 48.78 | 40.92 | 27.12 | 34.48 | 36.54 | 41.29 |

FineWeb에서의 주요 발견:
- OPUS는 모든 모델/옵티마이저 조합에서 경쟁력 있거나 우수한 평균 정확도 달성
- 다양한 모델 규모(Large/XL)와 옵티마이저 유형(Muon/AdamW)에서 강건성 입증
- 정적 산업계 베이스라인과 동등하거나 그를 능가하는 성능
- 랜덤 30B 대비 랜덤 60B의 차이가 데이터 선택의 중요성을 보여주는 유의미한 참조값 제공

### 7.4 지속 사전 학습: Qwen3-8B on SciencePedia

![데이터 선택 방법 비교](/assets/images/posts/opus-efficient-principled-data-selection-llm-pretraining/x2.png)

SciencePedia(30억 토큰 풀)에서 Qwen3-8B-Base의 지속 사전 학습을 평가한다. OPUS는 전체 학습(30억 토큰) 대비 **단 5억 토큰만으로 더 우수한 성능**을 달성하는 6배의 데이터 효율성을 보여준다.

평가 벤치마크:
- **OlympicArena:** 다학문 인지 추론 벤치마크 (제로샷 프롬프팅)
- **SciAssess:** 과학 문헌 분석 LLM 능력 평가 (생물학, 화학, 재료공학, 의학 4개 하위 도메인, 3-샷)

### 7.5 제거 실험(Ablation Study)

주요 설계 선택을 검증한다:

- **옵티마이저 유도 전처리기:** 원시 그래디언트 공간 대신 옵티마이저 유도 기하학에서 평가하면 측정 가능한 개선 제공
- **Bench-Proxy 구성:** 검색 기반 인-분포 프록시 풀이 랜덤 검증 집합보다 우수
- **CountSketch 투영 차원:** 압축 비율과 순위 보존 간의 트레이드오프 탐색
- **볼츠만 샘플링 vs. greedy 선택:** 소프트 샘플링이 비정상적(non-stationary) 스트림에서 다양성 붕괴 방지

모든 주요 구성요소가 OPUS 성능 향상에 의미 있게 기여함을 확인한다.

### 7.6 효율성 분석

OPUS는 Ghost 기법과 CountSketch 투영을 통해 **단 4.7%의 추가 계산 비용**만으로 동작한다. GPT-2 XL 기준으로 랜덤 선택 대비 8배의 계산량 감소를 달성하면서도 성능을 크게 향상시킨다. 스케치 차원 $m = 8192$는 최대 행렬 그래디언트(~10.24M 차원)를 약 1,250배 압축한다.

---

## 8. 결론 및 향후 연구

본 논문은 현대 옵티마이저 기하학과 정렬된 원칙적이고 확장 가능한 LLM 사전 학습 동적 데이터 선택 프레임워크 OPUS를 제안하였다. 원시 그래디언트 공간이 아닌 옵티마이저 유도 업데이트 공간에서 유용성을 정의함으로써, 각 최적화 스텝에서 어떤 토큰이 학습을 이끌어야 하는지에 대한 이론적으로 근거 있는 접근법을 제공한다.

**핵심 기여:**
1. AdamW와 Muon에 대한 폐쇄형 근사를 사용한 옵티마이저 인식 유용성 목적함수
2. 인-분포 프록시 신호를 보장하는 안정적인 Bench-Proxy 구성
3. Ghost 기법과 CountSketch를 결합한 효율적인 구현
4. 비정상적 스트림 하에서 다양성을 보존하는 볼츠만 샘플링
5. 다양한 설정에서의 포괄적인 실증 검증

**향후 연구 방향:**
- 다른 현대 옵티마이저(AdamW 변형, 새로운 적응형 방법)로 옵티마이저 인식 평가 확장
- 학습 동역학에 반응하는 적응적 프록시 구성 메커니즘 개발
- OPUS와 다른 데이터 효율 기법(커리큘럼 학습, 코어셋 선택) 결합
- 더 큰 모델과 더 긴 학습 지평선으로 확장
- 수렴 및 근사 품질에 대한 이론적 보장 탐구
