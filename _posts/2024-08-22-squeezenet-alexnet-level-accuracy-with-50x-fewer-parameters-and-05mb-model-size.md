---
title: "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size"
date: 2024-08-22 21:55:03
categories:
  - 인공지능
---

<https://arxiv.org/abs/1602.07360>

[SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360)

**초록 (Abstract)**  
최근의 심층 컨볼루션 신경망(CNN)에 대한 연구는 주로 정확성을 향상시키는 데 중점을 두고 있다. 일정한 정확성 수준을 달성하는 여러 CNN 구조들이 존재하는데, 동일한 정확성을 가진다면 더 작은 CNN 구조는 다음과 같은 최소 세 가지 장점을 제공한다: (1) 더 작은 CNN은 분산 학습 시 서버 간의 통신을 줄여준다. (2) 더 작은 CNN은 자율주행차에 새로운 모델을 클라우드에서 내보낼 때 필요한 대역폭이 적다. (3) 더 작은 CNN은 메모리가 제한된 FPGA와 같은 하드웨어에 더 적합하다. 이러한 장점을 제공하기 위해 우리는 SqueezeNet이라는 소형 CNN 구조를 제안한다. SqueezeNet은 ImageNet에서 AlexNet 수준의 정확성을 달성하면서도 매개변수가 50배 더 적다. 추가로, 모델 압축 기법을 통해 SqueezeNet을 0.5MB 미만으로 압축할 수 있다(이는 AlexNet보다 510배 더 작은 크기다).

SqueezeNet 아키텍처는 이곳에서 다운로드할 수 있다: <https://github.com/DeepScale/SqueezeNet>

**1. 도입 및 동기 (Introduction and Motivation)**  
최근의 심층 CNN 연구는 주로 컴퓨터 비전 데이터셋에서 정확성을 높이는 데 초점을 맞추고 있다. 일정한 정확성을 달성하는 여러 CNN 구조가 존재할 수 있는데, 같은 정확성이라면 매개변수가 더 적은 CNN 구조는 여러 이점을 제공한다:

- **더 효율적인 분산 학습**: 서버 간의 통신이 분산 CNN 학습의 확장성에 한계가 된다. 분산 데이터 병렬 학습에서는 통신 오버헤드가 모델의 매개변수 수에 비례한다. 간단히 말해, 작은 모델은 통신이 적게 필요하기 때문에 더 빠르게 학습된다.
- **클라이언트에 새로운 모델을 내보낼 때 오버헤드 감소**: 자율주행 분야에서 Tesla와 같은 회사들은 정기적으로 서버에서 고객의 차량으로 새로운 모델을 전송한다. 이는 흔히 'OTA(Over-the-Air) 업데이트'라고 한다. 소비자 보고서에 따르면, Tesla의 자율주행 기능은 최근 OTA 업데이트로 점진적으로 안전성이 향상되었다고 한다. 하지만 현재의 일반적인 CNN/DNN 모델을 OTA 업데이트하는 데는 큰 데이터 전송이 필요하다. AlexNet의 경우, 이를 전송하는 데 240MB의 통신이 필요하다. 더 작은 모델은 통신 요구량이 적기 때문에 자주 업데이트하는 것이 더 가능해진다.
- **FPGA 및 임베디드 시스템에 적합한 배포 가능성**: FPGA는 종종 10MB 이하의 온칩 메모리만을 가지고 있으며 오프칩 메모리나 저장소를 제공하지 않는다. 추론 시, 충분히 작은 모델은 FPGA에 직접 저장될 수 있어 메모리 대역폭의 병목 없이 비디오 프레임이 실시간으로 FPGA를 통과할 수 있다. 또한, CNN을 ASIC(Application-Specific Integrated Circuits)에 배포할 때, 충분히 작은 모델은 칩에 직접 저장될 수 있으며, 더 작은 모델은 ASIC이 더 작은 다이에 맞출 수 있도록 한다.

이와 같이, 더 작은 CNN 구조에는 여러 이점이 있다. 이를 염두에 두고 우리는 기존 모델과 동일한 정확성을 유지하면서도 매개변수가 적은 CNN 구조를 식별하는 문제에 직접적으로 집중한다. 우리는 그러한 구조를 발견했고, 이를 SqueezeNet이라고 부른다. 또한, 새로운 CNN 구조를 탐색하기 위한 보다 체계적인 접근 방식을 제시한다.

논문의 나머지 부분은 다음과 같이 구성된다. 2장에서는 관련 연구를 검토하고, 3장과 4장에서는 SqueezeNet 구조를 설명하고 평가한다. 그 후 CNN 구조 설계 선택이 모델 크기와 정확성에 미치는 영향을 이해하는 데 초점을 맞춘다. 이를 위해 SqueezeNet과 유사한 구조들의 설계 공간을 탐색한다. 5장에서는 CNN 미세 구조를 탐색하는데, 이는 개별 레이어와 모듈의 조직 및 차원 구성으로 정의된다. 6장에서는 CNN 거시 구조를 탐색하는데, 이는 CNN의 레이어가 높은 수준에서 어떻게 조직되는지를 정의한다. 마지막으로 7장에서 결론을 내린다. 요약하자면, 3장과 4장은 CNN 연구자와 SqueezeNet을 새로운 응용 프로그램에 적용하고자 하는 실무자에게 유용하며, 나머지 장은 고급 연구자들이 자신만의 CNN 구조를 설계하고자 할 때 유용하다.

**2. 관련 연구 (Related Work)**  
**2.1 모델 압축 (Model Compression)**  
우리 연구의 궁극적인 목표는 정확성을 유지하면서 매우 적은 매개변수를 가진 모델을 식별하는 것이다. 이를 해결하기 위한 합리적인 접근 방식은 기존의 CNN 모델을 손실 방식으로 압축하는 것이다. 실제로 모델 압축을 주제로 한 연구 커뮤니티가 형성되었으며, 여러 가지 접근 방식이 보고되었다. Denton 등(Denton et al. 2014)의 비교적 간단한 접근 방식은 사전 학습된 CNN 모델에 대해 특이값 분해(SVD)를 적용하는 것이다. Han 등(Han et al. 2015b)은 '네트워크 가지치기(Network Pruning)'를 개발했으며, 이는 사전 학습된 모델에서 시작하여 특정 임계값 이하의 매개변수를 0으로 대체하여 희소 행렬을 만들고, 그 후 희소 CNN에 대해 몇 차례 추가 학습을 수행하는 방식이다. 최근 Han 등(Han et al. 2015a)은 네트워크 가지치기에 양자화(8비트 이하)와 허프만 인코딩을 결합한 '딥 압축(Deep Compression)'이라는 접근 방식을 제안했으며, 더 나아가 EIE(Han et al. 2016a)라는 하드웨어 가속기를 설계하여 압축된 모델에서 직접 작동하여 상당한 속도 향상과 에너지 절감을 달성했다.

**2.2 CNN 미세 구조 (CNN Microarchitecture)**  
컨볼루션(Convolutions)은 인공 신경망에서 최소 25년 동안 사용되어 왔다. LeCun 등(LeCun et al. 1989)은 1980년대 후반에 숫자 인식 응용 프로그램을 위해 CNN을 대중화하는 데 기여했다. 신경망에서 컨볼루션 필터는 일반적으로 높이, 너비, 채널이라는 세 가지 주요 차원으로 이루어진 3차원이다. 이미지에 적용될 때 CNN 필터는 첫 번째 레이어에서 일반적으로 3개의 채널을 가지며(RGB), 각 후속 레이어 L\_i​에서는 필터가 이전 레이어 L\_{i-1}​의 필터와 동일한 수의 채널을 가진다. LeCun 등(LeCun et al. 1989)의 초기 연구는 5x5x채널 필터를 사용했으며, 최근 VGG(Simonyan & Zisserman, 2014) 아키텍처는 3x3 필터를 광범위하게 사용한다. Network-in-Network(Lin et al., 2013) 및 GoogLeNet(Szegedy et al., 2014; Ioffe & Szegedy, 2015; Szegedy et al., 2015; 2016) 계열의 모델들은 일부 레이어에서 1x1 필터를 사용한다.

아주 깊은 CNN을 설계하는 추세가 이어지면서 각 레이어에 적합한 필터 차원을 수동으로 선택하는 것은 번거롭다. 이를 해결하기 위해 고정된 구조로 다수의 컨볼루션 레이어로 구성된 고차원적인 빌딩 블록 또는 모듈이 제안되었다. 예를 들어, GoogLeNet 논문들은 Inception 모듈을 제안했으며, 이 모듈은 일반적으로 1x1 및 3x3 필터를 포함하고, 때로는 5x5(Szegedy et al., 2014) 또는 1x3 및 3x1(Szegedy et al., 2015) 필터를 포함한다. 이러한 모듈들이 다수 결합되며, 때로는 추가적인 임시 레이어와 함께 완전한 네트워크를 형성한다. 우리는 개별 모듈의 조직 및 차원을 설명하는 데 'CNN 미세 구조(CNN microarchitecture)'라는 용어를 사용한다.

**2.3 CNN 거시 구조 (CNN Macroarchitecture)**  
CNN 미세 구조가 개별 레이어와 모듈을 의미하는 반면, 우리는 CNN 거시 구조를 여러 모듈을 결합하여 종단 간 CNN 아키텍처를 구성하는 시스템 수준의 조직으로 정의한다.

최근 문헌에서 가장 널리 연구된 CNN 거시 구조 주제는 네트워크 깊이(즉, 레이어 수)가 성능에 미치는 영향이다. Simonyan과 Zisserman은 12에서 19개의 레이어로 구성된 VGG 계열의 CNN을 제안하며, 더 깊은 네트워크가 ImageNet-1k 데이터셋에서 더 높은 정확도를 낸다고 보고했다(Deng et al., 2009; Simonyan & Zisserman, 2014). 또한, K. He 등은 최대 30개의 레이어를 가진 더 깊은 CNN을 제안하며, ImageNet에서 더 높은 정확도를 달성했다(He et al., 2015a).

여러 레이어 또는 모듈 간의 연결 방식을 선택하는 것도 최근에 떠오르는 CNN 거시 구조 연구 주제이다. 예를 들어, Residual Networks (ResNet) (He et al., 2015b)와 Highway Networks (Srivastava et al., 2015)는 다중 레이어를 건너뛰어 연결하는 방식을 제안한다. 예를 들어, 레이어 3의 활성화 값을 레이어 6의 활성화 값에 더하는 방식이다. 이러한 연결을 우리는 '우회 연결(bypass connections)'이라고 부른다. ResNet의 저자들은 우회 연결이 있는 34개의 레이어 CNN과 없는 CNN을 A/B 비교 실험하였고, 우회 연결을 추가하면 Top-5 ImageNet 정확도가 2%포인트 향상된다는 결과를 얻었다.

**2.4 신경망 설계 공간 탐색 (Neural Network Design Space Exploration)**  
신경망(심층 신경망과 컨볼루션 신경망 포함)은 미세 구조, 거시 구조, 해법(Solvers), 기타 하이퍼파라미터 등 여러 옵션을 가진 매우 넓은 설계 공간을 가진다. 이러한 요인들이 신경망의 정확성에 어떻게 영향을 미치는지에 대해 직관을 얻고자 하는 것은 자연스러운 일이다(즉, 설계 공간의 모양에 대한 이해). 신경망의 설계 공간 탐색(DSE) 관련 연구 대부분은 더 높은 정확성을 제공하는 신경망 구조를 찾기 위한 자동화된 접근 방식을 개발하는 데 중점을 두고 있다. 이러한 자동화된 DSE 접근 방식에는 베이지안 최적화(Snoek et al., 2012), 시뮬레이션된 어닐링(Ludermir et al., 2006), 랜덤화 탐색(Bergstra & Bengio, 2012), 그리고 유전 알고리즘(Stanley & Miikkulainen, 2002)이 포함된다. 각 논문은 제안된 DSE 접근 방식이 기존의 대표적인 기준 모델보다 더 높은 정확도를 달성하는 신경망 구조를 찾아냈다는 사례를 제공하고 있다. 그러나 이러한 논문들은 신경망 설계 공간의 모양에 대한 직관을 제공하려는 시도는 하지 않는다. 이 논문 후반부에서 우리는 자동화된 접근 방식을 배제하고, 대신 원칙적인 A/B 비교 실험을 통해 CNN 구조적 결정이 모델 크기와 정확성에 어떤 영향을 미치는지 조사하는 방법으로 CNN을 재구성한다.

다음 섹션에서는 모델 압축 유무에 따른 SqueezeNet 아키텍처를 먼저 제안하고 평가한 후, SqueezeNet과 유사한 CNN 구조에서 미세 구조 및 거시 구조의 설계 선택이 미치는 영향을 탐구한다.

**3. SqueezeNet: 적은 매개변수로 정확성 유지**

이 섹션에서는 매개변수가 적은 CNN 아키텍처에 대한 설계 전략을 개략적으로 설명하고, 이후 새로운 빌딩 블록인 Fire 모듈을 소개한다. 마지막으로, 우리는 이 설계 전략을 활용하여 주로 Fire 모듈로 구성된 SqueezeNet을 설계한다.

**3.1 아키텍처 설계 전략**

이 논문의 궁극적인 목표는 매개변수가 적으면서도 경쟁력 있는 정확성을 유지하는 CNN 아키텍처를 식별하는 것이다. 이를 달성하기 위해, CNN 아키텍처 설계 시 세 가지 주요 전략을 사용한다:

- **전략 1. 3x3 필터를 1x1 필터로 대체**  
  주어진 수의 컨볼루션 필터를 사용할 수 있는 예산이 있을 때, 대부분의 필터를 1x1 필터로 교체하는 것을 선택한다. 왜냐하면 1x1 필터는 3x3 필터에 비해 매개변수 수가 9배 적기 때문이다.
- **전략 2. 3x3 필터의 입력 채널 수를 줄인다**  
  3x3 필터로만 구성된 컨볼루션 레이어를 생각해보자. 이 레이어의 전체 매개변수 수는 (입력 채널 수) \* (필터 수) \* (3\*3)이다. 따라서 CNN의 전체 매개변수 수를 줄이기 위해서는 3x3 필터의 수를 줄이는 것뿐만 아니라, 3x3 필터로 들어가는 입력 채널 수를 줄이는 것도 중요하다. 우리는 다음 섹션에서 설명할 squeeze 레이어를 사용하여 3x3 필터의 입력 채널 수를 줄인다.
- **전략 3. 네트워크 후반부에서 다운샘플링을 수행하여 큰 활성화 맵을 유지한다**  
  컨볼루션 네트워크에서 각 컨볼루션 레이어는 최소 1x1 크기의 활성화 맵을 출력하며, 종종 더 큰 활성화 맵을 생성한다. 이 활성화 맵의 높이와 너비는 (1) 입력 데이터의 크기(예: 256x256 이미지)와 (2) CNN 아키텍처에서 다운샘플링이 이루어지는 레이어의 선택에 따라 결정된다. 보통, CNN 아키텍처는 일부 컨볼루션 또는 풀링 레이어에서 스트라이드(stride > 1)를 설정하여 다운샘플링을 수행한다(Szegedy et al., 2014; Simonyan & Zisserman, 2014; Krizhevsky et al., 2012). 네트워크의 초반 레이어에서 큰 스트라이드를 사용하면 대부분의 레이어가 작은 활성화 맵을 가진다. 반대로, 네트워크의 대부분의 레이어에서 스트라이드가 1로 설정되고, 스트라이드가 1보다 큰 레이어가 네트워크의 후반부에 집중되어 있으면, 네트워크의 많은 레이어가 큰 활성화 맵을 가진다. 우리의 직관은 다운샘플링을 지연시켜 큰 활성화 맵을 유지하면, 다른 조건이 동일할 때 더 높은 분류 정확성을 얻을 수 있다는 것이다. 실제로 K. He와 H. Sun은 다운샘플링을 지연시키는 방법을 네 가지 다른 CNN 아키텍처에 적용했으며, 각 경우에서 다운샘플링을 지연시키면 더 높은 분류 정확성을 얻을 수 있음을 보고했다(He & Sun, 2015).

![](/assets/images/posts/261/img.png)

그림 1: 미세 구조적 관점: Fire 모듈 내 컨볼루션 필터의 구성

![](/assets/images/posts/261/img_1.png)

전략 1과 2는 CNN에서 매개변수의 양을 신중하게 줄이면서도 정확성을 유지하려는 시도에 관한 것이다. 전략 3은 제한된 매개변수 예산 내에서 정확성을 최대화하는 데 중점을 둔다. 다음으로, 우리는 Fire 모듈을 설명하는데, 이 모듈은 CNN 아키텍처의 빌딩 블록으로, 우리가 전략 1, 2, 3을 성공적으로 적용할 수 있게 해준다.

![](/assets/images/posts/261/img_2.png)

![](/assets/images/posts/261/img_3.png)

![](/assets/images/posts/261/img_4.png)

**그림 2: SqueezeNet 아키텍처의 거시적 구조**  
왼쪽: SqueezeNet(3.3절),  
가운데: 단순 우회 연결이 포함된 SqueezeNet(6절),  
오른쪽: 복잡한 우회 연결이 포함된 SqueezeNet(6절).

**3.2 Fire 모듈**  
우리는 Fire 모듈을 다음과 같이 정의한다. Fire 모듈은 다음으로 구성된다: 첫째, 1x1 필터만을 가진 squeeze 컨볼루션 레이어, 그리고 둘째, 1x1과 3x3 필터를 혼합하여 가진 expand 레이어로 구성된다. 이는 그림 1에서 설명된다. Fire 모듈에서 자유롭게 1x1 필터를 사용하는 것은 3.1절에서 언급한 전략 1의 적용 사례이다. Fire 모듈에서는 세 가지 조정 가능한 차원(하이퍼파라미터)을 제공한다:

![](/assets/images/posts/261/img_5.png)

**3.3 SqueezeNet 아키텍처**

이제 SqueezeNet CNN 아키텍처를 설명하겠다. 그림 2에 나타난 바와 같이, SqueezeNet은 독립적인 컨볼루션 레이어(conv1)로 시작해 8개의 Fire 모듈(fire2-9)을 차례로 거친 후, 마지막 컨볼루션 레이어(conv10)로 끝난다. 네트워크의 초반부에서 후반부로 갈수록 Fire 모듈의 필터 수를 점진적으로 증가시킨다. SqueezeNet은 레이어 conv1, fire4, fire8, conv10 이후에 스트라이드 2로 최대 풀링(max-pooling)을 수행하는데, 이러한 상대적으로 늦은 풀링 적용은 3.1절의 전략 3에 따른 것이다. 전체 SqueezeNet 아키텍처는 표 1에 제시되어 있다.

**3.3.1 SqueezeNet의 기타 세부사항**

간결함을 위해 표 1과 그림 2에서는 SqueezeNet의 여러 세부사항과 설계 선택을 생략했다. 이러한 설계 선택은 아래와 같으며, 이들에 대한 직관은 인용된 논문들에서 찾을 수 있다.

- 1x1 필터와 3x3 필터의 출력 활성화 값이 동일한 높이와 너비를 가지도록, expand 모듈의 3x3 필터에 입력 데이터에 대해 1픽셀의 제로 패딩을 추가한다.
- ReLU 활성화 함수(Nair & Hinton, 2010)를 squeeze와 expand 레이어의 활성화 값에 적용한다.
- fire9 모듈 이후에 드롭아웃(dropout, Srivastava et al., 2014) 비율 50%를 적용한다.
- SqueezeNet에는 완전 연결층(fully-connected layer)이 없는데, 이 설계 선택은 NiN(Lin et al., 2013) 아키텍처에서 영감을 받았다.
- SqueezeNet 학습 시 초기 학습률을 0.04로 설정하고, 학습이 진행됨에 따라 학습률을 선형적으로 감소시킨다(Mishkin et al., 2016에서 설명된 바와 같다). 학습 프로토콜(예: 배치 크기, 학습률, 매개변수 초기화)에 대한 자세한 사항은 Caffe 호환 설정 파일을 참조하라: [SqueezeNet 설정 파일](https://github.com/DeepScale/SqueezeNet).
- Caffe 프레임워크는 여러 필터 해상도를 포함하는 컨볼루션 레이어(예: 1x1 및 3x3)를 기본적으로 지원하지 않는다(Jia et al., 2014). 이를 해결하기 위해, 우리는 두 개의 별도 컨볼루션 레이어(1x1 필터 레이어, 3x3 필터 레이어)로 expand 레이어를 구현하고, 그 결과를 채널 차원에서 결합한다. 이것은 1x1 및 3x3 필터를 모두 포함하는 하나의 레이어를 구현하는 것과 수치적으로 동일하다.

우리는 SqueezeNet 설정 파일을 Caffe CNN 프레임워크에서 정의된 형식으로 공개했지만, Caffe 외에도 여러 다른 CNN 프레임워크가 등장했다. 여기에는 MXNet(Chen et al., 2015a), Chainer(Tokui et al., 2015), Keras(Chollet, 2016), Torch(Collobert et al., 2011)가 포함되며, 각각 고유의 CNN 아키텍처 표현 형식을 사용한다. 하지만 이들 라이브러리는 대부분 cuDNN(Chetlur et al., 2014)이나 MKL-DNN(Das et al., 2016)과 같은 동일한 계산 백엔드를 사용한다. 연구 커뮤니티는 SqueezeNet CNN 아키텍처를 다른 여러 CNN 소프트웨어 프레임워크와 호환되도록 포팅했다:

- MXNet의 SqueezeNet 포팅: Haria (2016)
- Chainer의 SqueezeNet 포팅: Bell (2016)
- Keras의 SqueezeNet 포팅: DT42 (2016)
- Torch의 SqueezeNet Fire 모듈 포팅: Waghmare (2016)

**4. SqueezeNet 평가**

이제 SqueezeNet에 대한 평가로 초점을 옮기겠다. 2.1절에서 검토한 각 CNN 모델 압축 논문들의 목표는 ImageNet(Deng et al., 2009) (ILSVRC 2012) 데이터셋을 사용하여 이미지를 분류하도록 학습된 AlexNet(Krizhevsky et al., 2012) 모델을 압축하는 것이었다. 따라서 SqueezeNet을 평가할 때, 우리는 AlexNet과 관련된 모델 압축 결과를 비교 기준으로 사용한다.

**표 1: SqueezeNet 아키텍처의 차원**  
(이 표의 형식은 Inception2 논문(Ioffe & Szegedy, 2015)에서 영감을 받았다.)

![](/assets/images/posts/261/img_6.png)

**표 2에서 우리는 최근의 모델 압축 결과와의 비교를 통해 SqueezeNet을 검토한다.**  
SVD 기반 접근 방식은 사전 학습된 AlexNet 모델을 5배 압축할 수 있었지만, Top-1 정확도가 56.0%로 감소했다(Denton et al., 2014). 네트워크 가지치기(Network Pruning)는 모델 크기를 9배 줄이면서도 ImageNet에서 Top-1 정확도 57.2%와 Top-5 정확도 80.3%의 기준 성능을 유지했다(Han et al., 2015b). 딥 압축(Deep Compression)은 모델 크기를 35배 줄이면서도 기준 정확도 수준을 유지했다(Han et al., 2015a). SqueezeNet의 경우, AlexNet에 비해 모델 크기를 50배 줄이면서도 Top-1 및 Top-5 정확도에서 AlexNet과 동등하거나 그 이상의 성능을 달성했다. 이러한 결과들을 표 2에 요약하였다.

**표 2: SqueezeNet과 모델 압축 접근 방식의 비교**  
여기서 '모델 크기'는 학습된 모델의 모든 매개변수를 저장하는 데 필요한 바이트 수를 의미한다.

![](/assets/images/posts/261/img_7.png)

우리는 모델 압축 커뮤니티의 최신 성과를 능가한 것으로 보인다. 압축되지 않은 32비트 값을 사용하여 모델을 표현했을 때도, SqueezeNet은 모델 크기 면에서 모델 압축 커뮤니티의 최고 성과보다 1.4배 더 작으며, 기준 정확도를 유지하거나 이를 초과하고 있다. 지금까지 풀리지 않은 질문은 "작은 모델은 압축에 적합한가? 아니면 작은 모델이 밀도가 높은 부동 소수점 값이 제공하는 모든 표현력을 '필요로' 하는가?"였다. 이를 확인하기 위해, 우리는 Deep Compression(Han et al., 2015a)을 SqueezeNet에 적용했고, 33% 희소성6 및 8비트 양자화를 사용했다. 그 결과, 0.66MB 모델(32비트 AlexNet보다 363배 더 작음)이 AlexNet과 동일한 정확도를 유지했다. 또한, 6비트 양자화와 33% 희소성을 적용했을 때, 0.47MB 모델(32비트 AlexNet보다 510배 더 작음)을 동일한 정확도로 생성할 수 있었다. 이로써, 작은 모델도 압축에 적합하다는 것을 확인했다.

이 결과는 또한 Deep Compression(Han et al., 2015a)이 매개변수가 많은 CNN 아키텍처(AlexNet 및 VGG 등)에만 효과적인 것이 아니라, 이미 압축된 SqueezeNet 아키텍처에도 적용 가능하다는 것을 보여준다. Deep Compression은 SqueezeNet을 10배 압축하면서도 기준 정확도를 유지했다. 요약하자면, CNN 아키텍처 혁신(SqueezeNet)과 최신 압축 기술(Deep Compression)을 결합하여 기준 정확도를 유지하면서 모델 크기를 510배 줄이는 데 성공했다.

마지막으로, Deep Compression(Han et al., 2015b)은 CNN 매개변수를 6비트 또는 8비트로 양자화하기 위한 코드북을 사용한다. 따라서 대부분의 일반 프로세서에서, Deep Compression에서 제안된 방식으로 8비트 양자화 시 4배의 속도 향상 또는 6비트 양자화 시 5.3배의 속도 향상을 달성하는 것은 쉽지 않다. 하지만 Han 등은 이러한 코드북 양자화된 CNN을 더 효율적으로 계산할 수 있는 맞춤형 하드웨어인 Efficient Inference Engine(EIE)을 개발했다(Han et al., 2016a). 또한, 우리가 SqueezeNet을 공개한 이후 몇 개월 동안, P. Gysel은 SqueezeNet을 8비트로 선형 양자화하는 Ristretto 전략을 개발했다(Gysel, 2016). Ristretto는 8비트로 연산을 수행하며, 매개변수와 활성화 값을 8비트 데이터 유형으로 저장한다. SqueezeNet 추론에 Ristretto 전략을 사용했을 때, Gysel은 32비트 대신 8비트 데이터를 사용했을 때 정확도가 1% 미만으로 감소하는 것을 관찰했다.

**5. CNN 미세 구조 설계 공간 탐색**

지금까지 우리는 작은 모델을 위한 아키텍처 설계 전략을 제안했고, 이를 통해 SqueezeNet을 설계했으며, SqueezeNet이 AlexNet보다 50배 더 작으면서도 동일한 정확도를 유지한다는 것을 발견했다. 그러나 SqueezeNet과 다른 모델들은 CNN 아키텍처의 넓고 대부분 탐구되지 않은 설계 공간에 속해 있다. 이제 5장과 6장에서 우리는 이 설계 공간의 여러 측면을 탐구한다. 이 탐구는 두 가지 주요 주제로 나뉜다: 미세 구조 탐색(모듈당 레이어 차원 및 구성)과 거시 구조 탐색(모듈과 기타 레이어의 고수준 종단 간 조직).

이 섹션에서는 3.1절에서 제안한 설계 전략과 관련하여 미세 구조 설계 공간의 모양에 대한 직관을 제공하기 위한 실험을 설계하고 실행한다. 여기서 우리의 목표는 각 실험에서 정확도를 최대화하는 것이 아니라, CNN 아키텍처 선택이 모델 크기와 정확도에 미치는 영향을 이해하는 것이다.

**5.1 CNN 미세 구조 메타파라미터**

![](/assets/images/posts/261/img_8.png)

**5.2 Squeeze 비율(SR)**

3.1절에서 우리는 squeeze 레이어를 사용하여 3x3 필터가 처리하는 입력 채널 수를 줄임으로써 매개변수를 줄이는 방법을 제안했다. Squeeze 비율(SR)은 squeeze 레이어의 필터 수와 확장 레이어의 필터 수 간의 비율로 정의된다. 이제 우리는 squeeze 비율이 모델 크기와 정확도에 미치는 영향을 조사하는 실험을 설계한다.

![](/assets/images/posts/261/img_9.png)

![](/assets/images/posts/261/img_10.png)

**(a)** Squeeze 비율 (SR)이 모델 크기와 정확도에 미치는 영향을 탐구함.

![](/assets/images/posts/261/img_11.png)

**(b)** 확장 레이어에서 3x3 필터의 비율 (pct\_{3x3})이 모델 크기와 정확도에 미치는 영향을 탐구함.

**그림 3:** 미세 구조 설계 공간 탐색.

**5.3 1x1과 3x3 필터 간의 트레이드오프**

3.1절에서 우리는 CNN에서 일부 3x3 필터를 1x1 필터로 대체함으로써 매개변수를 줄이는 방법을 제안했다. 하지만 필터의 공간적 해상도가 CNN에서 얼마나 중요한지에 대한 의문이 남아 있다. VGG(Simonyan & Zisserman, 2014) 아키텍처는 대부분의 레이어에서 3x3 공간 해상도를 사용하며, GoogLeNet(Szegedy et al., 2014)과 Network-in-Network(NiN, Lin et al., 2013)은 일부 레이어에서 1x1 필터를 사용한다. GoogLeNet과 NiN에서는 1x1과 3x3 필터의 비율에 대한 구체적인 분석 없이 특정 양의 필터를 제안하고 있다. 여기에서는 1x1과 3x3 필터의 비율이 모델 크기와 정확도에 미치는 영향을 탐구하고자 한다.

이 실험에서는 다음과 같은 메타파라미터를 사용한다:

- basee=128
- incre=128
- freq=2
- SR=0.500  
  그리고 pct\_{3x3}​를 1%에서 99%까지 변경한다. 즉, 각 Fire 모듈의 확장 레이어는 1x1과 3x3 필터로 나뉘며, 이 비율을 "대부분 1x1"에서 "대부분 3x3"으로 조정한다. 이전 실험과 마찬가지로, 이 모델들은 그림 2와 동일한 레이어 구성을 가지는 8개의 Fire 모듈로 구성되어 있다. 그림 3(b)에서 이 실험의 결과를 보여준다. 그림 3(a)와 그림 3(b)의 13MB 모델들은 동일한 아키텍처를 가지고 있다: SR=0.500과 pct\_{3x3} = 50. 그림 3(b)에서 볼 수 있듯이, Top-5 정확도는 3x3 필터를 50% 사용할 때 85.6%에 도달하며, 그 이상의 3x3 필터 비율을 증가시키면 모델 크기만 커지고 ImageNet에서 정확도는 더 이상 향상되지 않는다.

**6. CNN 거시 구조 설계 공간 탐색**

지금까지 우리는 미세 구조 수준에서, 즉 CNN의 개별 모듈 내용에 대한 설계 공간을 탐구했다. 이제 우리는 Fire 모듈 간의 고수준 연결과 관련된 거시 구조 설계 결정을 탐구한다. ResNet(He et al., 2015b)에서 영감을 받아, 우리는 세 가지 다른 아키텍처를 탐구했다:

- 기존의 SqueezeNet(이전 섹션들에서 다룬 대로).
- 일부 Fire 모듈 사이에 단순 우회 연결이 있는 SqueezeNet(Srivastava et al., 2015; He et al., 2015b에서 영감을 받음).
- 나머지 Fire 모듈 사이에 복잡한 우회 연결이 있는 SqueezeNet.

이 세 가지 SqueezeNet 변형은 그림 2에 설명되어 있다.

단순 우회 아키텍처는 Fire 모듈 3, 5, 7, 9 주변에 우회 연결을 추가하여 이러한 모듈들이 입력과 출력 간의 잔여 함수를 학습하도록 요구한다. ResNet과 마찬가지로, Fire3 주변에 우회 연결을 구현하기 위해 Fire4의 입력을 (Fire2의 출력 + Fire3의 출력)으로 설정한다. 이는 이러한 Fire 모듈의 매개변수에 적용되는 정규화를 변경하며, ResNet의 경우처럼 모델 학습 성능 및 최종 정확도를 향상시킬 수 있다.

하나의 한계점은 입력 채널 수와 출력 채널 수가 동일해야 한다는 것이다. 따라서 단순한 우회 연결은 그림 2의 중간 다이어그램에서 볼 수 있듯이 Fire 모듈 중 절반에만 적용될 수 있다. 채널 수가 일치하지 않는 경우, 우리는 그림 2의 오른쪽에 설명된 것처럼 복잡한 우회 연결을 사용한다. 단순 우회 연결이 "단순한 와이어"에 불과한 반면, 복잡한 우회 연결은 필요한 출력 채널 수에 맞게 필터 수를 설정한 1x1 컨볼루션 레이어를 포함한다. 복잡한 우회 연결은 모델에 추가 매개변수를 더하지만, 단순 우회 연결은 그렇지 않다.

우리는 단순히 정규화를 변경하는 것 외에도, 우회 연결을 추가하면 squeeze 레이어에 의해 발생하는 표현적 병목 현상을 완화하는 데 도움이 될 것이라고 생각한다. SqueezeNet에서 squeeze 비율(SR)은 0.125로 설정되어 있어, 각 squeeze 레이어는 동반된 확장 레이어보다 8배 적은 출력 채널을 가지고 있다. 이러한 심각한 차원 감소로 인해 squeeze 레이어를 통해 전달되는 정보는 제한적이다. 하지만 SqueezeNet에 우회 연결을 추가함으로써 squeeze 레이어를 우회해 더 많은 정보가 흐를 수 있는 길을 열어준다.

**표 3: 서로 다른 거시 구조 구성에 따른 SqueezeNet의 정확도와 모델 크기**

![](/assets/images/posts/261/img_12.png)

우리는 그림 2에 제시된 세 가지 거시 구조로 SqueezeNet을 학습시켰고, 표 3에서 정확도와 모델 크기를 비교했다. 거시 구조 탐색 동안 미세 구조는 표 1에 설명된 SqueezeNet과 동일하게 유지했다. 복잡한 우회 연결과 단순 우회 연결 모두 기본 SqueezeNet 아키텍처에 비해 정확도가 향상되었다. 흥미롭게도, 단순 우회 연결이 복잡한 우회 연결보다 더 높은 정확도 향상을 가져왔다. 단순 우회 연결을 추가하면 모델 크기를 증가시키지 않으면서 Top-1 정확도는 2.9%포인트, Top-5 정확도는 2.2%포인트 증가했다.

**7. 결론**

이 논문에서는 CNN 설계 공간 탐색을 위한 보다 체계적인 접근 방식을 제안했다. 이를 위해 우리는 AlexNet보다 50배 적은 매개변수를 가지면서도 ImageNet에서 AlexNet 수준의 정확도를 유지하는 SqueezeNet 아키텍처를 소개했다. 또한, SqueezeNet을 0.5MB 이하로 압축해, 압축되지 않은 AlexNet보다 510배 더 작은 크기로 만들었다. 이 논문을 2016년에 기술 보고서로 발표한 이후, Song Han과 그의 동료들은 SqueezeNet과 모델 압축에 대해 추가 실험을 진행했다. Han 등은 Dense-Sparse-Dense(DSD)라는 새로운 접근 방식을 사용하여 학습 중 모델 압축을 정규화 기법으로 활용해 정확도를 더욱 향상시켰으며, 그 결과, ImageNet-1k에서 압축된 SqueezeNet 모델이 우리의 결과보다 1.2%포인트 더 높은 정확도를, 압축되지 않은 모델은 4.3%포인트 더 높은 정확도를 달성했다.

논문 초반에 언급했듯이, 작은 모델은 FPGA에서 온칩 구현에 더 적합하다. 우리가 SqueezeNet 모델을 공개한 이후, Gschwend는 SqueezeNet의 변형 모델을 개발하여 FPGA에 구현했다(Gschwend, 2016). 예상대로 Gschwend는 SqueezeNet 유사 모델의 매개변수를 FPGA 내부에 완전히 저장할 수 있었고, 모델 매개변수를 로드하기 위해 오프칩 메모리에 접근할 필요를 제거했다.

이 논문의 맥락에서 우리는 ImageNet을 대상 데이터셋으로 삼았다. 그러나 ImageNet으로 학습된 CNN 표현을 다양한 응용 프로그램에 적용하는 것이 일반적인 관행이 되었다. 예를 들어, 세밀한 객체 인식(Zhang et al., 2013; Donahue et al., 2013), 이미지 속 로고 식별(Iandola et al., 2015), 이미지에 대한 설명 문장 생성(Fang et al., 2015) 등에서 CNN이 사용되고 있다. ImageNet으로 학습된 CNN은 자율 주행 관련 응용 프로그램에도 적용되었으며, 여기에는 이미지에서의 보행자 및 차량 감지(Iandola et al., 2014; Girshick et al., 2015; Ashraf et al., 2016)와 비디오에서의 차량 및 보행자 감지(Chen et al., 2015b), 도로 형상 분할(Badrinarayanan et al., 2015) 등이 포함된다. 우리는 SqueezeNet이 작은 모델 크기가 중요한 다양한 응용 프로그램에 적합한 CNN 아키텍처 후보가 될 것이라고 생각한다.

SqueezeNet은 우리가 CNN 아키텍처의 설계 공간을 폭넓게 탐색하는 과정에서 발견한 여러 새로운 CNN 중 하나다. 우리는 SqueezeNet이 독자에게 CNN 아키텍처 설계 공간의 넓은 가능성을 고려하고 탐구하는 데 영감을 줄 수 있기를 바라며, 이 탐구를 보다 체계적인 방식으로 수행할 수 있기를 기대한다.

[1602.07360v4.pdf

0.88MB](./file/1602.07360v4.pdf)
