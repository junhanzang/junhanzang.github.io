---
title: "Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network"
date: 2025-05-19 21:14:09
categories:
  - 인공지능
tags:
  - sed
---

<https://arxiv.org/abs/1710.02998>

[Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network](https://arxiv.org/abs/1710.02998)

**초록**  
본 논문에서는 시간 정보 없이 오디오 내에 존재하는 소리 이벤트의 목록(약한 라벨, weak labels)만 주어졌을 때, 소리 이벤트의 시작 시점과 종료 시점(강한 라벨, strong labels)을 학습할 수 있는 신경망 아키텍처와 학습 방식을 제안한다. 제안된 방법은 컨볼루션 신경망(CNN)과 순환 신경망(RNN)을 순차적으로 쌓은 구조 위에 두 개의 예측 레이어(먼저 강한 라벨, 다음으로 약한 라벨)를 배치하여 구성된다.  
입력 오디오 특징으로는 프레임 단위의 로그 멜 밴드 에너지를 사용하고, 약한 라벨 예측 레이어에는 데이터셋에서 제공된 약한 라벨을 그대로 학습에 활용한다. 강한 라벨은 약한 라벨을 입력 오디오의 프레임 수만큼 반복하여 생성하며, 이는 학습 시 강한 라벨 예측 레이어의 학습에 사용된다. 또한, 두 예측 레이어에서 계산된 손실 값에 서로 다른 가중치를 부여함으로써 네트워크가 약한 라벨과 강한 라벨로부터 학습하는 정도를 조절할 수 있도록 설계하였다.  
제안된 방법은 17개의 소리 이벤트 클래스로 구성된 155시간 분량의 공개 데이터셋에서 평가되었으며, 보지 못한 테스트 세트에서 강한 라벨 기준 오류율 0.84, 약한 라벨 기준 F-score 43.3%를 기록하여 최고 성능을 달성하였다.

**색인어** — 소리 이벤트 검출(sound event detection), 약한 라벨(weak labels), 심층 신경망(deep neural network), CNN, GRU

## 1 서론

**소리 이벤트 검출**(Sound Event Detection, SED)은 오디오 녹음에서 소리 이벤트(sound event)와 그 시작 및 종료 시점을 인식하는 작업이다. 이러한 소리 이벤트와 그 시간 정보를 인식하는 것은 감시 시스템 [1, 2], 생물 다양성 모니터링 [3, 4], 질의 기반 멀티미디어 검색 [5] 등 다양한 응용 분야에서 유용하게 사용될 수 있다. 전통적으로 SED는 각 소리 이벤트에 대한 시간 정보가 주어진 데이터셋을 기반으로 수행되어 왔다 [6, 7]. 본 논문에서는 이러한 소리 이벤트의 시간 정보를 **강한 라벨**(strong labels)이라 정의한다.

인터넷에는 방대한 양의 오디오 데이터가 존재하며, Freesound[1](#user-content-fn-1), YouTube[2](#user-content-fn-2)와 같은 협업 기반 혹은 소셜 웹사이트는 사용자가 자막이나 태그 등의 메타데이터와 함께 멀티미디어를 업로드할 수 있도록 하고 있다. 이처럼 온라인 소스로부터 특정 태그와 연관된 오디오 데이터를 비교적 적은 시간과 노력으로 자동 수집할 수 있다. 최근 Gemekke 등 [8]은 YouTube에서 632개의 소리 이벤트 태그를 이용해 약 200만 개의 10초 오디오 클립을 수집하였다. 이러한 태그들은 해당 오디오에 특정 소리 이벤트가 존재함을 나타내지만, 해당 이벤트가 몇 번 발생했는지 혹은 언제 발생했는지에 대한 시간 정보는 포함되어 있지 않다. 본 논문에서는 이러한 시간 정보가 없는 태그를 **약한 라벨**(weak labels)이라고 한다. 오디오의 약한 라벨을 식별하는 작업은 문헌에서 **오디오 태깅(audio tagging)**으로도 언급된다 [9, 10].

강한 라벨을 가진 데이터셋을 수집하고 주석을 다는 것은 많은 인력과 시간이 필요한 작업이다. 반면, 약한 라벨 데이터는 이벤트 클래스만 표시하면 되므로 훨씬 적은 시간으로 주석 작업을 할 수 있다. 만약 약한 라벨만을 가지고도 강한 라벨을 학습할 수 있는 SED 모델을 만들 수 있다면, 보다 방대한 데이터로 학습시킬 수 있게 된다. 본 논문에서는 약한 라벨로부터 강한 라벨을 학습할 수 있는 SED 기법을 제안한다.

약한 라벨을 사용해 강한 라벨을 학습하려는 유사한 연구들은 음악 [11, 12], 조류 분류 [13, 14] 등의 인접 도메인에서도 진행되어 왔다. 예를 들어 Liu 등 [11]은 오디오 클립의 전체 레벨 정보만 주어졌을 때, 완전 합성곱 신경망(FCN)을 사용해 각 프레임별 악기와 템포를 인식하였다. 이후 이 네트워크는 SED 작업으로 확장되었고 [15], 공개 데이터셋에서 실험되었다. FCN의 장점은 어떤 길이의 오디오 입력도 처리할 수 있다는 점이다. 그러나 프레임 단위의 강한 라벨은 세그먼트 단위의 출력을 프레임 수만큼 복제하는 업스케일링 레이어를 통해 얻기 때문에 해상도에 제한이 있다. 이러한 FCN을 기반으로 한 또 다른 연구 [16]에서는 업스케일링 레이어 없이 1.5초 길이의 짧은 세그먼트 단위로 라벨을 예측하였다. 이 연구에서는 FCN과 VGG 유사 구조 [17]를 비교하였는데, 두 모델 모두 1.5초 단위로 라벨을 출력하였다. FCN은 전체 오디오와 약한 라벨을 입력으로 사용하여 학습하고, VGG 네트워크는 전체 오디오를 서브세그먼트로 나누어 각 서브세그먼트가 동일한 라벨을 공유한다고 가정하고 학습하였다. 그 결과 FCN이 VGG 기반 방법보다 더 우수한 SED 성능을 보였다. Kumar 등 [18]은 이 작업을 위해 다중 인스턴스 학습(Multiple Instance Learning, MIL) 기법 [19]을 제안하였지만, 후속 논문 [16]에서 저자들은 해당 접근 방식이 대규모 데이터셋에는 확장성이 떨어진다고 언급하였다.

실제 환경에서는 소리 이벤트들이 종종 서로 겹쳐 발생한다. 이러한 겹침(overlap)을 인식할 수 있는 SED 방법을 **다성 SED(polyphonic SED)** 방법이라 한다. 최근에는 강한 라벨로 학습된 다성 SED 모델로, 로그 멜 밴드 에너지 특징과 컨볼루션-순환 신경망(CNN-RNN)을 결합한 구조가 제안되었으며 [20], 여러 데이터셋에서 평가되었다. 이와 유사한 CNN-RNN 구조는 오디오 태깅에서도 최신 기법들을 능가하는 성능을 보여준 바 있다 [9, 10]. 본 논문에서는 이러한 방법에 기반하여, 오디오와 약한 라벨만 주어졌을 때 SED와 오디오 태깅을 동시에 수행하는 구조를 제안한다. 구체적으로는, 오디오에서 추출된 로그 멜 밴드 특징을 입력으로 사용하고, stacked CNN-RNN 네트워크를 확장하여 강한 라벨과 약한 라벨을 순차적으로 예측하도록 구성하였다. 학습을 위해서는 약한 라벨을 프레임 수만큼 복제한 더미 강한 라벨을 생성하여 활용하고, 약한/강한 예측 레이어의 손실(loss)에 서로 다른 가중치를 부여하여 학습 효과를 조절한다.

제안된 CNN-RNN 아키텍처와 유사한 네트워크는 현재 오디오 태깅 분야에서 최고 수준의 성능을 기록하고 있다 [9, 10]. 이는 해당 구조가 시간 도메인 상의 관련 정보를 학습하고 이를 활성 클래스에 효과적으로 매핑할 수 있음을 의미한다. 본 논문에서는 이러한 학습 구조가 중간 계층에서 학습한 시간 정보를 강한 라벨로 추출 가능함을 보인다. 또한 기존 연구 [15, 16]과 비교하여, 제안된 방법은 구조적으로 더 높은 시간 해상도의 강한 라벨 예측을 지원한다.

제안된 특징 추출 방식과 네트워크 구조는 **2장**에서 설명되며, 데이터셋과 평가 기준, 평가 절차는 **3장**에서 다룬다. 마지막으로, 실험 결과와 이에 대한 논의는 **4장**에서 제시한다.

![](/assets/images/posts/564/img.png)

**그림 1 설명:**  
약한 라벨로부터 강한 라벨을 학습하기 위한 스택 구조의 컨볼루션 및 순환 신경망 구조.

---

## Footnotes

1. <https://freesound.org/> [↩](#user-content-fnref-1)
2. <https://www.youtube.com/> [↩](#user-content-fnref-2)

---
