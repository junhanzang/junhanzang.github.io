---
title: "Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network"
date: 2025-05-19 21:14:09
categories:
  - 인공지능
tags:
  - sed
---

<https://arxiv.org/abs/1710.02998>

[Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network

This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We

arxiv.org](https://arxiv.org/abs/1710.02998)

**초록**  
본 논문에서는 시간 정보 없이 오디오 내에 존재하는 소리 이벤트의 목록(약한 라벨, weak labels)만 주어졌을 때, 소리 이벤트의 시작 시점과 종료 시점(강한 라벨, strong labels)을 학습할 수 있는 신경망 아키텍처와 학습 방식을 제안한다. 제안된 방법은 컨볼루션 신경망(CNN)과 순환 신경망(RNN)을 순차적으로 쌓은 구조 위에 두 개의 예측 레이어(먼저 강한 라벨, 다음으로 약한 라벨)를 배치하여 구성된다.  
입력 오디오 특징으로는 프레임 단위의 로그 멜 밴드 에너지를 사용하고, 약한 라벨 예측 레이어에는 데이터셋에서 제공된 약한 라벨을 그대로 학습에 활용한다. 강한 라벨은 약한 라벨을 입력 오디오의 프레임 수만큼 반복하여 생성하며, 이는 학습 시 강한 라벨 예측 레이어의 학습에 사용된다. 또한, 두 예측 레이어에서 계산된 손실 값에 서로 다른 가중치를 부여함으로써 네트워크가 약한 라벨과 강한 라벨로부터 학습하는 정도를 조절할 수 있도록 설계하였다.  
제안된 방법은 17개의 소리 이벤트 클래스로 구성된 155시간 분량의 공개 데이터셋에서 평가되었으며, 보지 못한 테스트 세트에서 강한 라벨 기준 오류율 0.84, 약한 라벨 기준 F-score 43.3%를 기록하여 최고 성능을 달성하였다.

**색인어** — 소리 이벤트 검출(sound event detection), 약한 라벨(weak labels), 심층 신경망(deep neural network), CNN, GRU

## 1 서론

**소리 이벤트 검출**(Sound Event Detection, SED)은 오디오 녹음에서 소리 이벤트(sound event)와 그 시작 및 종료 시점을 인식하는 작업이다. 이러한 소리 이벤트와 그 시간 정보를 인식하는 것은 감시 시스템 [1, 2], 생물 다양성 모니터링 [3, 4], 질의 기반 멀티미디어 검색 [5] 등 다양한 응용 분야에서 유용하게 사용될 수 있다. 전통적으로 SED는 각 소리 이벤트에 대한 시간 정보가 주어진 데이터셋을 기반으로 수행되어 왔다 [6, 7]. 본 논문에서는 이러한 소리 이벤트의 시간 정보를 **강한 라벨**(strong labels)이라 정의한다.

인터넷에는 방대한 양의 오디오 데이터가 존재하며, Freesound[1](#user-content-fn-1), YouTube[2](#user-content-fn-2)와 같은 협업 기반 혹은 소셜 웹사이트는 사용자가 자막이나 태그 등의 메타데이터와 함께 멀티미디어를 업로드할 수 있도록 하고 있다. 이처럼 온라인 소스로부터 특정 태그와 연관된 오디오 데이터를 비교적 적은 시간과 노력으로 자동 수집할 수 있다. 최근 Gemekke 등 [8]은 YouTube에서 632개의 소리 이벤트 태그를 이용해 약 200만 개의 10초 오디오 클립을 수집하였다. 이러한 태그들은 해당 오디오에 특정 소리 이벤트가 존재함을 나타내지만, 해당 이벤트가 몇 번 발생했는지 혹은 언제 발생했는지에 대한 시간 정보는 포함되어 있지 않다. 본 논문에서는 이러한 시간 정보가 없는 태그를 **약한 라벨**(weak labels)이라고 한다. 오디오의 약한 라벨을 식별하는 작업은 문헌에서 **오디오 태깅(audio tagging)**으로도 언급된다 [9, 10].

강한 라벨을 가진 데이터셋을 수집하고 주석을 다는 것은 많은 인력과 시간이 필요한 작업이다. 반면, 약한 라벨 데이터는 이벤트 클래스만 표시하면 되므로 훨씬 적은 시간으로 주석 작업을 할 수 있다. 만약 약한 라벨만을 가지고도 강한 라벨을 학습할 수 있는 SED 모델을 만들 수 있다면, 보다 방대한 데이터로 학습시킬 수 있게 된다. 본 논문에서는 약한 라벨로부터 강한 라벨을 학습할 수 있는 SED 기법을 제안한다.

약한 라벨을 사용해 강한 라벨을 학습하려는 유사한 연구들은 음악 [11, 12], 조류 분류 [13, 14] 등의 인접 도메인에서도 진행되어 왔다. 예를 들어 Liu 등 [11]은 오디오 클립의 전체 레벨 정보만 주어졌을 때, 완전 합성곱 신경망(FCN)을 사용해 각 프레임별 악기와 템포를 인식하였다. 이후 이 네트워크는 SED 작업으로 확장되었고 [15], 공개 데이터셋에서 실험되었다. FCN의 장점은 어떤 길이의 오디오 입력도 처리할 수 있다는 점이다. 그러나 프레임 단위의 강한 라벨은 세그먼트 단위의 출력을 프레임 수만큼 복제하는 업스케일링 레이어를 통해 얻기 때문에 해상도에 제한이 있다. 이러한 FCN을 기반으로 한 또 다른 연구 [16]에서는 업스케일링 레이어 없이 1.5초 길이의 짧은 세그먼트 단위로 라벨을 예측하였다. 이 연구에서는 FCN과 VGG 유사 구조 [17]를 비교하였는데, 두 모델 모두 1.5초 단위로 라벨을 출력하였다. FCN은 전체 오디오와 약한 라벨을 입력으로 사용하여 학습하고, VGG 네트워크는 전체 오디오를 서브세그먼트로 나누어 각 서브세그먼트가 동일한 라벨을 공유한다고 가정하고 학습하였다. 그 결과 FCN이 VGG 기반 방법보다 더 우수한 SED 성능을 보였다. Kumar 등 [18]은 이 작업을 위해 다중 인스턴스 학습(Multiple Instance Learning, MIL) 기법 [19]을 제안하였지만, 후속 논문 [16]에서 저자들은 해당 접근 방식이 대규모 데이터셋에는 확장성이 떨어진다고 언급하였다.

실제 환경에서는 소리 이벤트들이 종종 서로 겹쳐 발생한다. 이러한 겹침(overlap)을 인식할 수 있는 SED 방법을 **다성 SED(polyphonic SED)** 방법이라 한다. 최근에는 강한 라벨로 학습된 다성 SED 모델로, 로그 멜 밴드 에너지 특징과 컨볼루션-순환 신경망(CNN-RNN)을 결합한 구조가 제안되었으며 [20], 여러 데이터셋에서 평가되었다. 이와 유사한 CNN-RNN 구조는 오디오 태깅에서도 최신 기법들을 능가하는 성능을 보여준 바 있다 [9, 10]. 본 논문에서는 이러한 방법에 기반하여, 오디오와 약한 라벨만 주어졌을 때 SED와 오디오 태깅을 동시에 수행하는 구조를 제안한다. 구체적으로는, 오디오에서 추출된 로그 멜 밴드 특징을 입력으로 사용하고, stacked CNN-RNN 네트워크를 확장하여 강한 라벨과 약한 라벨을 순차적으로 예측하도록 구성하였다. 학습을 위해서는 약한 라벨을 프레임 수만큼 복제한 더미 강한 라벨을 생성하여 활용하고, 약한/강한 예측 레이어의 손실(loss)에 서로 다른 가중치를 부여하여 학습 효과를 조절한다.

제안된 CNN-RNN 아키텍처와 유사한 네트워크는 현재 오디오 태깅 분야에서 최고 수준의 성능을 기록하고 있다 [9, 10]. 이는 해당 구조가 시간 도메인 상의 관련 정보를 학습하고 이를 활성 클래스에 효과적으로 매핑할 수 있음을 의미한다. 본 논문에서는 이러한 학습 구조가 중간 계층에서 학습한 시간 정보를 강한 라벨로 추출 가능함을 보인다. 또한 기존 연구 [15, 16]과 비교하여, 제안된 방법은 구조적으로 더 높은 시간 해상도의 강한 라벨 예측을 지원한다.

제안된 특징 추출 방식과 네트워크 구조는 **2장**에서 설명되며, 데이터셋과 평가 기준, 평가 절차는 **3장**에서 다룬다. 마지막으로, 실험 결과와 이에 대한 논의는 **4장**에서 제시한다.

![](/assets/images/posts/564/img.png)

**그림 1 설명:**  
약한 라벨로부터 강한 라벨을 학습하기 위한 스택 구조의 컨볼루션 및 순환 신경망 구조.

---

## Footnotes

1. <https://freesound.org/> [↩](#user-content-fnref-1)
2. <https://www.youtube.com/> [↩](#user-content-fnref-2)
---

- **약한 라벨(Weak Labels)**:
  - 오디오 전체에 대해 **어떤 소리 이벤트가 포함되어 있는지만** 알려줍니다.
  - 예: "이 오디오에는 개 짖는 소리가 있어요."
  - **언제** 그 소리가 나는지는 모릅니다.
- **강한 라벨(Strong Labels)**:
  - **어떤 소리 이벤트가**
  - **언제부터 언제까지** 발생했는지를 알려줍니다.
  - 예: "개 짖는 소리가 3.2초부터 4.0초까지 발생했어요."

그래서 논문에서는 **약한 라벨만 있는 데이터로 강한 라벨을 예측할 수 있는 모델을 만드는 것**이 핵심 목표입니다.
---

## 2. 방법 (Method)

그림 1은 제안된 방법의 전체 블록 다이어그램을 보여준다. 오디오로부터 추출된 **로그 멜 밴드 에너지(log mel-band energy)** 특징은 스택 구조의 컨볼루션 및 순환 신경망(stacked convolutional and recurrent neural network)에 입력되며, 이 네트워크는 **강한 라벨(strong labels)**을 먼저 예측하고, 이어서 **약한 라벨(weak labels)**을 예측한다.

오디오 특징은 10초 길이의 입력 오디오에 대해 겹치는 윈도우(overlapping windows)를 사용하여 계산되며, 이로부터 **T개의 프레임**이 생성된다. 제안된 신경망은 이러한 특징들을 먼저 강한 라벨로 매핑하고, 이후 강한 라벨을 다시 약한 라벨로 매핑한다. 입력이 **T개의 프레임**이고, 데이터셋에 총 **C개의 소리 클래스**가 존재할 때, 네트워크는 강한 라벨 출력으로 각 프레임마다 **C개의 클래스 확률**을 예측하며, 약한 라벨 출력으로는 전체 오디오에 대해 **C개 클래스의 확률**만을 예측한다. 각 소리 클래스에 대한 예측 출력은 [0, 1] 범위의 연속적인 값이며, 1은 해당 소리 클래스의 존재를, 0은 부재를 의미한다.

특징 추출 및 신경망 구조의 세부 사항은 아래에 설명한다.

### 2.1 특징 추출 (Feature Extraction)

**로그 멜 밴드 에너지(log mel-band energy, mbe)**는 **40ms 길이의 해밍 윈도우(Hamming window)**를 사용하고, **50% 겹침(overlap)**을 적용하여 추출된다. 총 40개의 멜 밴드가 사용되며, 주파수 범위는 0~22050Hz이다. 10초 길이의 오디오 입력에 대해, 특징 추출 블록은 **500 × 40** 크기의 출력을 생성하며, 이때 **T = 500**이다.
---

여기서 **"500 × 40 크기의 출력"**은 바로 **신경망에 입력되는 오디오 특징 행렬의 크기**를 의미합니다.

구체적으로 설명하면:

- **500**: 시간 축(time axis)의 프레임 수 — 10초 오디오를 40ms 간격(50% 겹침)으로 나누면 약 500개 프레임이 생깁니다.
- **40**: 주파수 축(frequency axis)의 **mel-band** 개수 — 0~22050Hz 범위를 40개의 mel 필터로 나눈 것.

즉, 입력 오디오 10초짜리를 **프레임 × 주파수** 형태의 스펙트로그램으로 바꾸면  
→ **500 (time steps) × 40 (mel bands)**  
의 2D 특성 행렬이 만들어지고, 이것이 모델의 **입력**으로 사용됩니다.

### ✅ 1. **이 논문은 Mel-spectrogram을 "시계열 특징"으로 처리**

- 500×40은 사실상 T x F 형태, 즉 **시간-주파수 도메인 특징**입니다.
- CNN + RNN 구조는 여기서 **CNN으로 주파수 축 특징을 요약하고, RNN이 시간 축(T=500)을 따라 학습**하는 형태예요.
- 즉, 이 구조는 mel-spectrogram을 **이미지가 아닌 시계열로 인식**한 셈입니다.

### ✅ 2. **Conv2D → RNN으로 이어지는 설계는 '이미지' 처리와 다름**

- 현재는 ConvNet (예: EfficientNet, ConvNeXt)을 쓰기 위해 mel-spectrogram을 **이미지처럼** (channels, height, width)로 넣고 처리하는 방식이 일반적입니다.
  - 보통 n\_mels × time을 height × width로 보고 3채널 RGB 흉내 내기도 하죠.
- 하지만 이 논문은 **주파수 축(C=40)**에만 CNN을 적용하고, **시간 축(T=500)**은 RNN으로 처리하므로 "이미지 처리 모델"이 아니라 **시계열 특화 모델**에 가깝습니다.

### ✅ 3. **왜 이렇게 했을까? → 시간 정보 보존 + 다성(polyphonic) 고려**

- SED는 이벤트가 언제 발생하는지가 핵심이기 때문에, 시간 축을 **RNN이 직접 처리**하게 하면 타이밍 정보를 더 정교하게 다룰 수 있습니다.
- 특히 **overlapping events (polyphonic SED)**에서는 시간 축의 순서를 유지한 상태에서 누적 정보를 추적하는 게 중요했죠.

### ✅ 4. **기술 발전 당시의 한계와 선택**

- 2017년 당시에는 Vision 기반 백본 (EfficientNet, Swin 등)을 오디오에 쓰는 것이 아직 일반적이지 않았고, 멜스펙을 "이미지로" 다루는 아이디어가 막 확산되던 시점이었습니다.
- 대신 **CNN + RNN** 조합이 음성/오디오 처리의 주력 구조였기 때문에, 이 논문도 그 프레임 안에서 효율적인 라벨 예측 방법을 설계한 겁니다.

![](/assets/images/posts/564/img_1.png)
---

### 2.2 신경망 구조 (Neural Network)

제안된 네트워크의 입력은 그림 1에 나타난 바와 같이 **T×40** 크기의 **로그 멜 밴드 에너지(mbe)** 특징이다. 입력의 지역적인 변위에 불변한(local shift-invariant) 특징은 앞부분의 CNN 층을 통해 학습된다. 모든 CNN 레이어에서는 **3×3 수용 영역(receptive field)**을 사용하고, 출력 크기를 입력과 동일하게 유지하기 위해 **제로 패딩(zero padding)**을 적용한다.

CNN 레이어마다 **주파수 축(frequency axis)**을 따라 **최대 풀링(max-pooling)**을 수행하여 차원을 **T × 1 × N**으로 축소한다. 여기서 **N**은 마지막 CNN 레이어의 필터 개수다. 반면, **시간 축(time axis)**에 대해서는 **시간 해상도 보존(time resolution preservation)**을 위해 풀링을 적용하지 않는다.

CNN의 활성화 값은 이후 **양방향 GRU (bi-directional Gated Recurrent Units)**에 전달되며, 이 GRU는 tanh 활성화 함수를 사용하여 소리 이벤트의 **장기 시계열 구조(long-term temporal structure)**를 학습한다. 이후 **시간 분산 완전 연결층(time-distributed dense layer)**이 연결되어 특징 차원을 줄인다. GRU와 dense 레이어 모두에서 시간 해상도 **T 프레임**은 유지된다.

여러 개의 클래스 라벨을 동시에 예측해야 하므로, 마지막 dense 레이어에서는 **시그모이드(sigmoid)** 활성화를 사용한다. 이 예측 레이어는 입력 오디오 내에 존재하는 강한 라벨(strong labels)을 출력하며, 이후 본문에서는 이를 **강한 출력(strong output)**이라 부른다. 강한 라벨의 출력 차원은 **T × C**이고, 여기서 **C**는 소리 클래스의 수이다. 이 출력에 대해 강한 라벨 손실(strong label loss)을 계산한다.

또한, 특징 차원을 줄이고 프레이밍 정보를 제거하기 위해 추가 dense 레이어를 적용하여, 오디오 전체에 존재하는 **C개의 약한 라벨(weak labels)**로 매핑한다. 이 예측 레이어는 이후 **약한 출력(weak output)**이라 부르며, 이 출력에 대해 약한 라벨 손실(weak label loss)을 계산한다. 네트워크의 최종 손실(total loss)은 강한 손실과 약한 손실의 **가중합(weighted sum)**으로 계산된다.

학습 과정에서는, **강한 출력과 약한 출력 각각에 대한 손실의 가중치(weight)를 달리 적용**하여, 한 출력을 더 우선적으로 학습할 수 있도록 설계하였다. 다시 말해, 학습 중에는 약한 라벨과 그에 따른 가중치 조정 방식이 **강한 라벨 학습을 조절하는 역할**을 한다. 반면, 테스트 시에는 강한 출력으로부터 약한 라벨이 유도된다(예측된 strong labels를 기반으로 weak labels 추론).

모든 CNN 레이어의 활성화 값에 대해 **배치 정규화(Batch Normalization)** [21]를 적용하였다. 손실 함수로는 **이진 크로스 엔트로피(Binary Cross-Entropy)**를 사용하며, 옵티마이저는 **Adam** [22]을 사용하여 총 **1000 에폭(epoch)** 동안 학습을 수행하였다.

**Early stopping** 기법을 적용하여 과적합(overfitting)을 방지하였다. 구체적으로, **강한 라벨의 오류율(error rate)**과 **약한 라벨의 F-score**를 합산한 값(이후 **학습 지표(training metric)**라 칭함)이 **100 에폭 이상 개선되지 않으면 학습을 중단**하였다.

또한, 네트워크의 각 레이어 뒤에는 **드롭아웃(dropout)** [23]을 적용하여 정규화 효과를 주고, 일반화 성능을 높여 **보지 못한 데이터(unseen data)**에서도 잘 작동하도록 하였다.

모델은 **Keras** [24]를 사용하여 구현되었으며, 백엔드 프레임워크로는 **Theano** [25]를 사용하였다.

## 3 평가 (Evaluation)

### 3.1 데이터셋 (Dataset)

제안된 방법은 Google이 최근 공개한 **AudioSet** 데이터의 하위 집합을 사용하여 평가되었다 [8]. 이 하위 데이터셋은 **DCASE(Detection and Classification of Acoustic Scenes and Events)** 챌린지의 일환으로 구성된 것이다 [26].

이 데이터셋은 **학습용(train), 테스트용(test), 평가용(evaluation)** 세 가지 분할로 구성되어 있다.

- 학습 세트는 **총 51,172개의 녹음**으로 구성되어 있으며,
- 테스트 세트는 **488개의 녹음**으로 구성되어 있다.
- 모든 녹음은 **10초 길이**, **모노 채널(mono channel)**이며, **샘플링 레이트는 44,100Hz**이다.

이 모든 녹음은 [8]에서 설명된 바와 같이 **공개된 유튜브 영상**으로부터 수집되었다. 이 학습 및 테스트 세트로 학습된 다양한 방법들은, **DCASE 2017 챌린지**에서 **보지 못한 평가 세트(1,103개의 녹음)**를 통해 성능이 비교되었다 [26].

이 데이터셋은 총 **17개의 라벨**을 포함하며, 하나의 녹음에 **여러 개의 라벨이 동시에 존재할 수 있다**. **강한 라벨(strong labels)**은 **테스트 세트에만 제공**되고, **약한 라벨(weak labels)**은 **학습 세트와 테스트 세트 모두에 제공**된다.

그러나 우리의 신경망을 학습시키기 위해서는 **학습 데이터에도 강한 라벨이 필요**하다. 이를 위해, 우리는 각 오디오의 **모든 프레임에 약한 라벨을 복제하여 인위적인 강한 라벨을 생성**하고, 이를 학습에 사용하였다.
---

![](/assets/images/posts/564/img_2.png)
---

### 3.2 평가 지표 (Metric)

우리의 방법은 DCASE 챌린지 [26]에서 사용된 방식과 유사하게 평가된다. **평가는 약한 라벨 예측과 강한 라벨 예측에 대해 각각 독립적으로 수행**된다.

#### ? 약한 라벨 평가 (Weak Label Evaluation)

약한 라벨은 **재현율(Recall, R)**, **정밀도(Precision, P)**, 그리고 이들로부터 계산되는 **F-score**로 평가된다. 계산식은 다음과 같다:

- **정밀도 (P)** = TP / (TP + FP)
- **재현율 (R)** = TP / (TP + FN)
- **F-score** = 2 × P × R / (P + R)

여기서:

- **TP (True Positive)**: 예측한 라벨이 실제 정답 라벨과 일치한 횟수
- **FP (False Positive)**: 예측한 라벨이 실제 라벨에는 없었던 경우
- **FN (False Negative)**: 실제 정답 라벨이 있었지만 예측하지 못한 경우

#### ? 강한 라벨 평가 (Strong Label Evaluation)

강한 라벨은 [27]에서 제안된 **세그먼트 기반(segment-based) F-score** 및 **오류율(Error Rate, ER)**로 평가된다.

**F-score 계산식**:

![](/assets/images/posts/564/img_3.png)

여기서:

- **TP(k), FP(k), FN(k)**: 각 세그먼트 k에 대해 계산된 정/오 예측 횟수
- **K**: 전체 세그먼트 수

**오류율 (Error Rate, ER) 계산식**:

![](/assets/images/posts/564/img_4.png)

여기서:

- **N(k)**: 세그먼트 k에 실제로 존재하는 라벨의 개수
- **S(k)**: 치환(Substitution) 수 = min(FN(k), FP(k))
- **D(k)**: 삭제(Deletion) 수 = max(0, FN(k) - FP(k))
- **I(k)**: 삽입(Insertion) 수 = max(0, FP(k) - FN(k))

우리는 강한 라벨 평가에서 **1초 길이의 세그먼트**를 사용한다. 이상적인 경우는 **F-score가 100**, **ER이 0**이 되는 것이다.
---

**오류율(Error Rate, ER)**은 강한 라벨(시간 정보 포함된 예측)이 **얼마나 정확하지 못했는지**를 나타내는 지표인데요, 개념은 간단합니다. **예측에서 몇 번 틀렸는지를 전체 실제 이벤트 수로 나눈 것**입니다.

![](/assets/images/posts/564/img_5.png)

![](/assets/images/posts/564/img_6.png)
---

### 3.3 베이스라인 (Baseline)

이 데이터셋의 베이스라인 방법은 [26]에서 제공된다. 이는 해당 데이터셋을 사용할 다른 방법들과의 **비교 기준점(reference point)**을 제공하기 위한 **기본적인 방법(basic method)**이다.

이 베이스라인은 오디오 특징으로 **로그 멜 밴드 에너지(mbe)**를 사용한다. 사용된 신경망은 **완전 연결형 네트워크(fully-connected network)**로,

- 은닉층(hidden layer)은 **두 개**,
- 각 층은 **50개의 유닛**과 **20% 드롭아웃(dropout)**을 갖고 있다.
- 그 뒤에는, 데이터셋의 클래스 수만큼 **시그모이드(sigmoid)** 유닛을 가진 예측층이 위치한다.

학습 시에는 오디오 특징에서 **5프레임의 문맥(context)**을 사용하며, **이진 크로스 엔트로피 손실(binary cross-entropy loss)**과 **Adam 옵티마이저**를 사용해 네트워크를 학습한다.

베이스라인의 평가 지표(metric) 점수는 **표 1(Table 1)**에 제시되어 있다. 이 베이스라인 네트워크는 입력 오디오 특징의 프레임 수만큼 **약한 라벨을 복제**하여 학습한다. 테스트 시에는, 예측된 **강한 라벨(strong labels)**로부터 **활성화된 소리 이벤트를 식별하여 약한 라벨을 외부에서 추정**한다.

### 3.4 평가 절차 (Evaluation Procedure)

스택 구조의 컨볼루션 및 순환 신경망(stacked convolutional and recurrent neural network)은 다음과 같은 구성으로 학습된다:

- 입력: **로그 멜 밴드 에너지(mbe)**
- 약한 출력(weak output): 데이터셋에서 제공된 **약한 라벨**
- 강한 출력(strong output): **각 시간 프레임마다 약한 라벨을 복제**하여 생성한 **강한 라벨**

데이터의 규모가 크고 하드웨어 메모리에도 제약이 있기 때문에, 학습 시간은 상당히 오래 걸린다 (우리의 하드웨어 기준으로 **약 1800초/에폭**). 따라서 **광범위한 하이퍼파라미터 탐색(hyperparameter search)**을 수행할 수 없어, [7]에서 사용된 네트워크 구성과 유사한 설정으로 시작한 후, **레이어별 유닛/필터 수를 무작위로 조정하는 랜덤 서치(random search)** [28]를 수행하였다. 그 결과, **과적합(overfitting)이나 과소적합(underfitting)이 없고**, 학습 지표가 강하게 나오는 구성을 선택하였다.

이 데이터셋은 매우 크고, 다양한 사용자가 업로드한 자료로 구성되어 있어, **데이터 자체에 충분한 다양성(variability)**이 존재한다고 판단하여 **정규화 기법(regularizer)은 사용하지 않았다**. 가장 높은 학습 지표를 기록한 최적 구성은 **그림 1**에 나타나 있으며, **총 약 218,000개의 파라미터**를 포함한다. 이보다 더 많은 파라미터(최대 2,000,000)를 가진 구성들도 실험했지만, 선택된 구성보다 **유의미한 성능 향상은 없었다**.

최종 네트워크 구성을 확정한 후, **데이터 다양성만으로는 과적합을 방지하기 어렵다는 가정을 검증하기 위해**, 각 레이어에 **드롭아웃(dropout)**을 정규화 기법으로 적용해보았으며, 드롭아웃 비율은 {0.05, 0.15, 0.25, 0.5, 0.75} 범위로 설정하여 실험하였다. 본 실험에서는 **모든 레이어에 동일한 드롭아웃 값을 적용**하였다.

또한, **두 예측 레이어(약한/강한 출력)에 적용되는 손실 가중치(weight)** 역시 여러 조합으로 실험하였으며, 가중치 후보는 로그 스케일 기반의 {0.002, 0.02, 0.2, 1} 집합으로 구성되었다. 이 중 0.002는 약한 라벨의 총 시간 프레임(1)에 대해 강한 라벨의 총 프레임 수(500) 비율에 근거하여 설정한 값이다.

![](/assets/images/posts/564/img_7.png)
---

### ✅ 1. **Dropout 변화에 따른 성능 급격한 차이: 자연스러운가?**

표에서 보면:

![](/assets/images/posts/564/img_8.png)

→ 0.25 → 0.5로 가면서 성능이 **절반 이상 급감**합니다.

#### ? 문제점

- Dropout은 보통 **0.3~0.5** 사이가 실용적인 범위인데, 여기선 **0.5부터 모델 붕괴 수준**입니다.
- 특히 **weak label F-score가 19.2까지 떨어지는 것**은 **모델이 거의 예측을 못했다는 의미**로, 일반적이지 않습니다.
- 반대로 0.05~0.15 구간은 너무 좋은데, 사실 **0.05에서도 과적합이 느껴진다**면 **데이터 양이나 모델 capacity 조절이 더 중요**한 상황입니다.

#### ? 가능성

- 모델이 너무 작아서 (218K 파라미터) dropout 영향이 **과도하게 클 수 있음**
- 입력 차원이나 batch size가 작아서 dropout의 랜덤성에 더 취약했을 수 있음
- 아니면, 단순히 **한 번의 실험 결과**로 평가 지표를 보고 있는 걸 수도 있습니다 (→ **평균 없이 단건 실험**)

### ✅ 2. **랜덤 서치인데 이렇게 확연한 결과 차이?**

논문에서는 이렇게 말했죠:

> we perform a random search by varying the number of units/filters... until no under or over-fitting is observed while having a strong training metric

#### ? 문제점

- 랜덤 서치는 **다수의 조합을 시도**해야 의미가 있습니다. 그런데 결과에는 **드롭아웃 하나만 변화시키는 실험만 제시**됨.
- 그렇다면 **seed 고정 없이 한 번씩만 실험**했을 가능성이 높고,  
  → 그럴 경우 **결과의 분산(variance)**이 상당할 수 있습니다.
- 특히 dropout과 같이 stochastic한 요소는 **seed에 따라 성능 편차가 매우 큽니다.**

### ✅ 3. 과적합 여부를 약한 라벨 F-score로 판단했나?

> “...until no under or over-fitting is observed while having a strong training metric.”

여기서 "training metric"이 **약한/강한 라벨의 F-score + ER 합산값**이라면,  
→ **validation set 없이 training score만으로 튜닝**한 걸 수도 있어요.

#### ? 문제점

- 그럴 경우 드롭아웃이 낮으면 **훈련 성능은 높아도 과적합**, 드롭아웃이 높으면 **훈련 자체를 어렵게 만들어 성능 저하**.
- 이건 **과적합을 판단하는 방식 자체가 부정확한 실험 설계**라는 의미입니다.

![](/assets/images/posts/564/img_9.png)

### ? 결론 & 제안

- 이 결과를 그대로 받아들이기보다는, **seed 고정 + k-fold validation + 여러 실험 평균** 기반의 재현성 검증이 꼭 필요합니다.
- 특히 Dropout을 실험할 때는 **5회 이상 평균**을 보고 비교하지 않으면 **랜덤성에 의한 착시 효과**가 큽니다.
- 또한 weak/strong loss 비율을 적절히 튜닝하지 않으면 training이 weak label만 따라가서 **pseudo strong label 학습이 의미 없게 될 수 있음**.

다시 정리하면, 이런 점들이 특히 “초기 논문 느낌”을 줍니다:
---

### ✅ 1. **데이터셋 분할과 과적합 판단이 모호함**

- validation set을 명확히 언급하지 않고, **training metric**만으로 과적합 여부를 판단.
- → 지금은 standard하게 **train/val/test 분할과 조기 종료(Early stopping) 기준을 명확히 분리**하죠.
---

### ✅ 2. **Dropout만 바꿨는데 성능이 급변**

- Dropout은 보조적 regularizer인데, 모델 성능이 너무 민감하게 반응함.
- 재현성 있는 실험 없이 **단일 실행 결과만 보고 성능을 판단**한 것으로 보임.
---

### ✅ 3. **Strong label을 weak label 복제로 생성**

- 지금은 이런 경우 pseudo-labeling이나 MIL 기반의 더 정교한 방식이 쓰이지만,
- 이 논문은 **강한 라벨이 없는 걸 단순 복제**해서 생성해버림 → 구조적 한계.
---

### ✅ 4. **Random Search가 실제로 Random했는지 의심**

- seed 고정, 반복 실험, 평균 성능 등 전혀 언급 없음 → 실험 결과 신뢰도 떨어짐
---

### ✅ 5. **모델 구조 자체가 간단하고 parameter tuning이 소극적**

- 파라미터 수가 21만 개 정도 → 지금 관점에선 매우 작고 shallow한 모델
- Conv+GRU 구조가 당시엔 신선했지만, 지금은 CNN 또는 Transformer 기반 구조들이 훨씬 효율적임
---

## 4 결과 및 논의 (Results and Discussion)

다양한 드롭아웃 값에 대해 수행한 약한/강한 라벨의 평가 지표는 **표 1**에서 비교된다.  
제안된 네트워크에서 드롭아웃 값을 **0.15**로 설정했을 때,

- **약한 라벨(weak labels)**: F-score 43.3%
- **강한 라벨(strong labels)**: 오류율(Error Rate, ER) 0.84  
  라는 최고의 학습 지표가 달성되었다.

이 실험은 **약한 출력과 강한 출력에 동일한 가중치(1)를 부여**한 상태에서 수행되었다. 베이스라인 [26]의 성능인

- **약한 라벨 F-score 13.1%**,
- **강한 라벨 ER 1.02**와 비교할 때,  
  이는 **유의미한 성능 향상**이다.

이후, 드롭아웃 값을 **0.15**로 고정한 상태에서, **약한 출력과 강한 출력에 서로 다른 가중치를 부여했을 때 네트워크가 어떻게 학습되는지를 분석**하였으며, 결과는 **표 2**에 제시하였다. 예를 들어 표의 첫 번째 행에서는, **강한 출력 손실에는 0.002의 가중치**, **약한 출력 손실에는 1.0의 가중치**를 적용하였다.

강한 라벨은 학습을 위해 약한 라벨을 프레임 수만큼 복제하여 생성한 것이므로, 실제로는 **정확한 진짜 레이블이 아니며**, 직관적으로 **약한 라벨의 손실에 더 높은 가중치를 부여하는 것이 성능에 유리할 것**이라 예상했다. 하지만 실험 결과, **약한 출력과 강한 출력 모두에 동일한 가중치(1)를 부여한 경우 가장 높은 학습 성능을 보였다.**

또한 흥미로운 점은,

- 비록 학습에 사용된 강한 라벨은 ‘약한 품질(weak)’임에도 불구하고,
- **강한 출력의 손실에 더 높은 가중치를 줄 경우, 실제로 강한 라벨의 F-score와 ER이 더 좋아지는 경향을 보였다.**

다만, 이 경우에는 **약한 라벨의 성능(F-score)은 저하**되었다.

우리는 가장 높은 학습 지표를 달성한 **(약한 출력과 강한 출력에 동일 가중치 1 적용)** 구성에서의 예측 결과를 분석하였다. 약한 라벨 중에서는 다음과 같은 소리 이벤트들이 **60% 이상의 F-score**로 특히 잘 예측되었다:

- 차량 관련 소리: **기차(train), 스케이트보드(skateboard)**
- 경고음: **소방차 사이렌(fire engine siren), 민방위 경보(civil defense siren)**

반면, 다음 소리 이벤트들은 **F-score가 0**으로 나타나 예측에 완전히 실패했다:

- **구급차 사이렌(ambulance siren)**
- **자동차 경보(car alarm)**
- **자동차 지나감(car passing)**
- **후진 경고음(reverse beeps)**
- **기차 경적(train horn)**

이러한 추세는 **강한 라벨에서도 동일하게 관찰**되었다.

우리가 제안한 방법이 실제로 무엇을 학습하고 있는지를 이해하기 위해, **네트워크 첫 번째 CNN 레이어의 활성화(activation)를 시각화**해보았다. 이 시각화는 **saliency map** [29] 기법을 **keras-vis** [30]를 통해 구현하였다. Saliency map은 출력 클래스에 대해 입력 특징이 얼마나 영향을 미치는지를 나타내는 **기울기 기반 시각화**이다.

그 예시는 **그림 2**에 나타나 있으며, 테스트 데이터셋의 "–jc0NAxK8M\_30.000\_40.000"이라는 녹음에 대해 수행되었다.

- 위쪽 서브플롯: 소리 클래스 **‘자동차(car)’**에 대한 **강한 출력의 활성화 맵**
- 중앙 서브플롯: 동일 클래스에 대한 **약한 출력의 활성화 맵**
- 아래 서브플롯: 입력 mbe 특징 위에 실제 정답 영역을 **빨간 점선**으로 표시한 것

이 시각화로부터, **강한 출력과 약한 출력 모두 해당 소리 이벤트가 발생한 정확한 시간 구간에 대해 활성화되고 있음**을 확인할 수 있었다.

![](/assets/images/posts/564/img_10.png)

![](/assets/images/posts/564/img_11.png)

**그림 2 설명:**  
테스트 녹음 "–jc0NAxK8M\_30.000\_40.000"에서 소리 클래스 **‘자동차(car)’**에 대해

- 위: 강한 출력의 CNN 1층 활성화
- 가운데: 약한 출력의 CNN 1층 활성화
- 아래: 입력 mbe 특징과 정답 구간(빨간 점선)  
  → 두 활성화 맵 모두 정답 시간 영역에 초점이 맞춰져 있음을 보여준다.
---

### ✅ 1. **모델 설계가 단순하고, 세부 분류에 취약**

- F-score = 0인 클래스들이 다수 존재 → 특정 클래스는 **전혀 학습이 안 됨**.
- 예: ambulance siren, car alarm, reverse beeps 등
- 이는 단순히 모델 capacity 문제가 아니라, **데이터 표현이나 분산(distribution)에 제대로 적응하지 못한 구조**임을 암시.
---

### ✅ 2. **학습 라벨의 품질 문제 (약한 라벨을 강한 라벨로 복제)**

- 학습용 strong label을 단순 복제로 만든 구조는,  
  → \*\*"언제 나는지" 정보 없이 "존재만 아는" 라벨을 "프레임 단위로 복제"\*\*한 것이기 때문에,  
  → 시간적으로 엉뚱한 프레임에서 존재한다고 착각하게 만들 가능성 높음.
- 이런 상황에서 GRU가 아무리 temporal한 특징을 보더라도, **입력 시점 자체가 잘못된 정보로 유도**됨.
---

### ✅ 3. **클래스 간 불균형 및 도메인 불일치**

- F-score가 높은 클래스들을 보면:
  - **train, skateboard, fire engine siren** 등 매우 독립적이고 뚜렷한 음색을 가진 클래스
- 반대로 성능이 낮은 클래스들은:
  - **ambulance siren, car passing, car horn** 등 **다른 클래스와 음색이 겹치거나, 환경 소음과 섞이기 쉬운 소리**
- 즉, 이 구조는 **명확한 고유 패턴이 있는 소리엔 강하지만, 주변 맥락이나 유사 클래스 구분이 필요한 경우는 약함**
---

### ✅ 4. **데이터 문제 + 모델 취약성이 시너지로 작용**

- 해당 데이터셋은 YouTube에서 수집된 약한 라벨 기반 클립
  - 영상 업로더의 라벨링 실수
  - 불완전한 타이밍 정보
  - 다양한 마이크 품질
- 이런 라벨링 오차나 도메인 편차가 존재하는 상태에서, 구조적으로 강인하지 못한 모델은 **"예쁘게 보이기 위한 학습"만 할 가능성**이 높음 → overfitting to noise
---

### ✅ 5. **결국 이 실험에서 얻은 건 ‘부분적으로만 학습된’ 모델**

- 몇몇 클래스에 대해서만 잘 작동하고,
- 많은 클래스에서 학습이 되지 않으며,
- strong label 학습이 실제로 의미 있었는지도 의문
- 특히 dropout과 loss weight 변화에 따라 **weak/strong label 성능이 반비례로 움직이는 경향**은 구조적인 제약을 암시함

![](/assets/images/posts/564/img_12.png)
---

### 4.1 DCASE 2017 챌린지 결과

DCASE 2017 챌린지 [26]의 평가용 분할(evaluation split)에서 제안된 방법의 결과는 **표 2**에 제시되어 있다. 테스트 세트에서의 성능을 기반으로 서로 다른 강한/약한 출력 가중치를 가진 **네 가지 시스템**이 선택되었다. 강한 출력에 더 높은 가중치를 줄수록 **강한 라벨 성능이 더 좋아지는 경향**은 평가 데이터에서도 동일하게 나타났으며, **최소 오류율 ER\_ch = 0.79**를 기록하였다.

참고로, 다른 시스템들과의 비교 결과:

- [31]은 **약한 라벨 기준 최고 F-score 55.6%**를 달성하였고,
- [32]는 **강한 라벨 기준 최저 오류율 0.66**을 기록하였다.

### 5 결론 (Conclusion)

본 논문에서는, **시간 정보 없이 소리 이벤트의 존재만 주어진 오디오 녹음으로부터 소리 이벤트의 시간적 정보(시작 시점과 종료 시점)를 학습하는 문제**를 다루었다. 이를 위해 **두 개의 예측 출력 레이어를 갖는 스택형 컨볼루션 및 순환 신경망 구조**와 그에 따른 **학습 방식**을 제안하였다.

제안된 네트워크는 **두 예측 레이어의 손실에 서로 다른 가중치를 적용하여 학습**되었다. 비록 학습에 사용된 강한 라벨은 **단순히 약한 라벨을 프레임 수만큼 반복한 것**에 불과했지만, **두 예측 레이어에 동일한 가중치(1)를 부여했을 때**, 네트워크가 **의미 있는 강한 라벨을 정확히 학습할 수 있었음**이 관찰되었다.

해당 평가 실험은 **총 155시간 분량의 공개 오디오 데이터셋**을 활용하여 수행되었으며, 테스트 데이터에서 다음과 같은 성능을 달성하였다:

- **강한 라벨**: 오류율(Error Rate) **0.84**
- **약한 라벨**: F-score **43.3%**
