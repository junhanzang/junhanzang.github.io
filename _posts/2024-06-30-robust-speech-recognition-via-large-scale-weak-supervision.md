---
title: "Robust Speech Recognition via Large-Scale Weak Supervision"
date: 2024-06-30 15:37:55
categories:
  - 인공지능
---

<https://arxiv.org/abs/2212.04356>

[Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)

### 요약

우리는 인터넷에 있는 대량의 오디오 전사를 예측하도록 훈련된 음성 처리 시스템의 능력을 연구합니다. 68만 시간의 다국어 및 다중 작업 감독으로 확장된 결과 모델은 표준 벤치마크에서 잘 일반화되며, 종종 기존의 완전 감독된 결과와 경쟁하지만, 미세 조정 없이 제로샷 전이 설정에서 이를 수행합니다. 인간과 비교했을 때, 모델은 정확성과 견고성에서 인간에 근접합니다. 우리는 견고한 음성 처리를 위한 기초로서 모델 및 추론 코드를 공개합니다.

### 1. 소개

음성 인식의 발전은 Wav2Vec 2.0(Baevski et al., 2020)으로 대표되는 비지도 사전 훈련 기술의 개발에 의해 촉진되었습니다. 이러한 방법은 인간의 라벨이 필요 없이 원시 오디오에서 직접 학습하기 때문에 대규모의 라벨 없는 음성 데이터셋을 효과적으로 사용할 수 있으며, 100만 시간의 훈련 데이터로 빠르게 확장되었습니다(Zhang et al., 2021). 이는 학계의 감독된 데이터셋에서 일반적으로 사용하는 약 1,000시간을 훨씬 초과하는 양입니다. 표준 벤치마크에서 미세 조정할 때, 이러한 접근 방식은 특히 저데이터 환경에서 최첨단 성능을 향상시켰습니다.

이러한 사전 훈련된 오디오 인코더는 고품질의 음성 표현을 학습하지만, 순전히 비지도 학습으로 인해 이러한 표현을 사용 가능한 출력으로 매핑하는 등가의 성능을 가진 디코더가 없으므로 실제 작업(예: 음성 인식)을 수행하려면 미세 조정이 필요합니다. 이는 미세 조정이 여전히 복잡한 과정이 될 수 있기 때문에 유용성과 영향력을 제한합니다. 미세 조정을 요구하는 것에는 추가적인 위험이 있습니다. 머신러닝 방법은 훈련 데이터셋 내에서 패턴을 찾아내어 성능을 향상시키는 데 매우 능숙하지만, 이러한 패턴 중 일부는 약하고 잘못된 것이어서 다른 데이터셋 및 분포에 일반화되지 않습니다. Radford et al. (2021)은 ImageNet 데이터셋(Russakovsky et al., 2015)에서 컴퓨터 비전 모델을 미세 조정할 때 객체 분류 정확도가 9.2% 증가했지만, 다른 7개의 자연 이미지 데이터셋에서 같은 객체를 분류할 때 평균 정확도에는 향상이 없음을 문서화했습니다.

이것은 비지도 사전 훈련이 오디오 인코더의 품질을 크게 향상시켰지만, 동등한 품질의 사전 훈련된 디코더가 부족하고, 데이터셋 특정 미세 조정 프로토콜이 추천되는 것이 유용성과 견고성을 제한하는 중요한 약점임을 시사합니다. 음성 인식 시스템의 목표는 광범위한 환경에서 “상자에서 꺼내 바로” 신뢰할 수 있게 작동하는 것이어야 하며, 모든 배포 분포에 대해 디코더를 감독적으로 미세 조정할 필요가 없어야 합니다.

Narayanan et al. (2018), Likhomanenko et al. (2020), Chan et al. (2021)에 의해 입증된 바와 같이, 많은 데이터셋/도메인에 걸쳐 감독된 방식으로 사전 훈련된 음성 인식 시스템은 단일 소스에서 훈련된 모델보다 더 높은 견고성을 보이며, 보류된 데이터셋에 더 효과적으로 일반화됩니다. 이 작업들은 가능한 한 많은 기존의 고품질 음성 인식 데이터셋을 결합하여 이를 달성합니다. 그러나 쉽게 이용할 수 있는 이러한 데이터는 여전히 적습니다. SpeechStew(Chan et al., 2021)는 기존의 7개 데이터셋을 합쳐 총 5,140시간의 감독을 제공합니다. 이는 무시할 수 없는 양이지만, Zhang et al. (2021)에서 사용된 100만 시간의 라벨 없는 음성 데이터에 비하면 여전히 적은 양입니다.

기존의 고품질 감독된 데이터셋의 제한된 크기를 인식하여, 최근 노력은 음성 인식을 위한 더 큰 데이터셋을 만들었습니다. Chen et al. (2021) 및 Galvez et al. (2021)은 고품질 인간 검증 전사의 요구를 완화함으로써 10,000 및 30,000시간의 더 많은 훈련 데이터를 제공하는 자동화된 파이프라인을 사용하여 약하게 감독된 음성 인식을 확장했습니다. 이러한 품질과 양의 균형은 종종 올바른 선택입니다. 음성 인식에서는 아직 많이 연구되지 않았지만, 컴퓨터 비전의 최근 연구는 ImageNet(Russakovsky et al., 2015)과 같은 금 표준 크라우드소싱 데이터셋을 넘어 훨씬 크지만 약하게 감독된 데이터셋으로 이동하면 모델의 견고성과 일반화가 크게 향상됨을 보여주었습니다(Mahajan et al., 2018; Kolesnikov et al., 2020).

그러나 이러한 새로운 데이터셋은 기존의 고품질 데이터셋의 합보다 몇 배 더 크지만, 이전의 비지도 작업보다는 여전히 훨씬 작습니다. 이 작업에서는 약하게 감독된 음성 인식을 68만 시간의 라벨된 오디오 데이터로 확장하여 그 격차를 해소합니다. 우리는 우리의 접근 방식을 Whisper2라고 부릅니다. 우리는 이 규모로 훈련된 모델이 제로샷으로 기존 데이터셋에 잘 전이되며, 고품질의 결과를 얻기 위해 데이터셋 특정 미세 조정이 필요 없음을 입증합니다.

또한, 우리의 작업은 영어만을 대상으로 하는 음성 인식을 넘어 다국어 및 다중 작업 약하게 감독된 사전 훈련의 범위를 넓히는 데 중점을 둡니다. 68만 시간의 오디오 중 117,000시간은 96개의 다른 언어를 포함합니다. 데이터셋에는 125,000시간의 X!en 번역 데이터도 포함됩니다. 우리는 충분히 큰 모델의 경우 다국어 및 다중 작업 훈련에 대한 단점이 없으며, 오히려 이점이 있음을 발견했습니다.

우리의 연구는 약하게 감독된 사전 훈련의 단순한 확장이 음성 인식에 대해 아직 과소평가되었음을 시사합니다. 우리는 최근 대규모 음성 인식 작업의 주류가 된 자기 감독 또는 자기 훈련 기술이 필요 없이 이러한 결과를 달성합니다. 견고한 음성 인식에 대한 추가 연구의 기초로서, 우리는 다음 URL에서 추론 코드와 모델을 공개합니다: <https://github.com/openai/whisper>.

### 2. 접근 방법

#### 2.1 데이터 처리

최근 웹 규모의 텍스트를 활용하여 머신러닝 시스템을 훈련시키는 연구 경향을 따르면서, 우리는 데이터 전처리에 최소한의 접근 방식을 취합니다. 많은 음성 인식 작업과는 대조적으로, 우리는 전사의 원시 텍스트를 예측하도록 Whisper 모델을 훈련시킵니다. 이는 발화와 그 전사된 형태 간의 매핑을 학습하기 위해 시퀀스-투-시퀀스 모델의 표현력을 활용하는 것으로, 이는 자연스러운 전사를 생성하기 위해 별도의 텍스트 역정규화 단계를 제거하므로 음성 인식 파이프라인을 단순화합니다.

우리는 인터넷에 있는 오디오와 전사가 쌍을 이루는 데이터를 구성합니다. 이는 다양한 환경, 녹음 설정, 화자, 언어에서의 오디오를 아우르는 매우 다양한 데이터셋을 만듭니다. 오디오 품질의 다양성은 모델이 견고해지도록 돕지만, 전사 품질의 다양성은 그와 같은 이점을 주지 않습니다. 초기 검사에서는 원시 데이터셋에 품질이 낮은 전사가 많이 포함되어 있음을 보여주었습니다. 이를 해결하기 위해 전사 품질을 향상시키기 위한 여러 자동 필터링 방법을 개발했습니다.

인터넷의 많은 전사들은 실제로 사람이 생성한 것이 아니라 기존의 ASR 시스템의 출력물입니다. 최근 연구는 인간과 기계가 혼합된 데이터셋으로 훈련하면 번역 시스템의 성능이 크게 저하될 수 있음을 보여주었습니다(Ghorbani et al., 2021). "전사체"를 학습하지 않도록 하기 위해, 우리는 훈련 데이터셋에서 기계 생성 전사를 감지하고 제거하는 여러 휴리스틱을 개발했습니다. 많은 기존 ASR 시스템은 복잡한 구두점(느낌표, 쉼표, 물음표)이나 단락과 같은 공백 형식, 대문자와 같은 스타일적 측면을 예측하기 어려운 요소들을 제거하거나 표준화하여 제한된 서면 언어만을 출력합니다. 모든 대문자 또는 소문자 전사는 인간이 생성했을 가능성이 매우 낮습니다. 많은 ASR 시스템에는 어느 정도의 텍스트 역정규화가 포함되어 있지만, 이는 종종 단순하거나 규칙 기반이며 쉼표를 포함하지 않는 등의 다른 처리되지 않은 측면에서 여전히 감지될 수 있습니다.

우리는 CLD2에 따라 발화된 언어가 전사의 언어와 일치하는지 확인하기 위해, VoxLingua107(Valk & Alum¨ae, 2021)에서 데이터셋의 프로토타입 버전으로 훈련된 프로토타입 모델을 미세 조정하여 만든 오디오 언어 감지기를 사용합니다. 두 언어가 일치하지 않으면, 우리는 (오디오, 전사) 쌍을 데이터셋의 음성 인식 훈련 예제로 포함하지 않습니다. 전사 언어가 영어인 경우 예외를 두고 이러한 쌍을 X!en 음성 번역 훈련 예제로 데이터셋에 추가합니다. 우리는 훈련 데이터셋의 중복 및 자동 생성 콘텐츠 양을 줄이기 위해 전사 텍스트의 퍼지 중복 제거를 사용합니다.

우리는 오디오 파일을 30초 구간으로 나누고, 그 시간 구간 내에 발생하는 전사의 일부와 짝을 이룹니다. 우리는 음성이 없는 구간을 포함한 모든 오디오로 훈련하며(확률적으로 서브 샘플링하여), 이러한 구간을 음성 활동 감지를 위한 훈련 데이터로 사용합니다.

추가 필터링을 위해 초기 모델을 훈련한 후, 우리는 훈련 데이터 소스에 대한 오류율 정보를 수집하고, 오류율이 높고 데이터 소스 크기가 큰 순서로 수동 검사를 수행하여 효율적으로 저품질 소스를 제거했습니다. 이 검사는 부분적으로만 전사되었거나 잘못 정렬된 전사와 필터링 휴리스틱이 감지하지 못한 저품질 기계 생성 캡션이 많이 남아 있음을 보여주었습니다.

오염을 피하기 위해, 우리는 훈련 데이터셋과 TED-LIUM 3(Hernandez et al., 2018)과 같이 중복 위험이 높은 평가 데이터셋 간에 전사 수준의 중복 제거를 수행합니다.

![](/assets/images/posts/185/img.png)

그림 1. 접근 방식 개요. 시퀀스 투 시퀀스 트랜스포머 모델은 다국어 음성 인식, 음성 번역, 음성 언어 식별, 음성 활동 감지 등 다양한 음성 처리 작업에 대해 학습됩니다. 이러한 모든 작업은 디코더가 예측할 토큰 시퀀스로 공동으로 표현되므로 단일 모델이 기존 음성 처리 파이프라인의 여러 단계를 대체할 수 있습니다. 멀티태스크 트레이닝 형식은 2.3절에서 자세히 설명한 대로 작업 지정자 또는 분류 대상 역할을 하는 특수 토큰 세트를 사용합니다.

### 2. 접근 방법

#### 2.2 모델

우리의 연구는 대규모 감독된 사전 훈련의 음성 인식 능력을 연구하는 데 초점을 맞추고 있으므로 모델 개선으로 인해 결과가 혼란스러워지는 것을 피하기 위해 기존의 아키텍처를 사용합니다. 우리는 검증된 신뢰할 수 있는 확장성을 가진 인코더-디코더 Transformer(Vaswani et al., 2017)를 선택했습니다. 모든 오디오는 16,000 Hz로 재샘플링되며, 10밀리초 간격으로 25밀리초 창에서 80채널 로그 크기 Mel 스펙트로그램 표현이 계산됩니다. 특성 정규화를 위해, 우리는 사전 훈련 데이터셋 전반에 걸쳐 입력을 -1에서 1 사이로 전역적으로 스케일링하고, 평균을 0에 가깝게 맞춥니다. 인코더는 필터 너비가 3인 두 개의 컨볼루션 레이어와 GELU 활성화 함수(Hendrycks & Gimpel, 2016)로 구성된 작은 줄기로 이 입력 표현을 처리합니다. 두 번째 컨볼루션 레이어는 스트라이드가 2입니다. 그런 다음 사인 위치 임베딩이 줄기의 출력에 추가된 후 인코더 Transformer 블록이 적용됩니다. Transformer는 사전 활성화 잔여 블록(Child et al., 2019)을 사용하며, 인코더 출력에 최종 레이어 정규화가 적용됩니다. 디코더는 학습된 위치 임베딩과 입력-출력 토큰 표현을 공유합니다(Press & Wolf, 2017). 인코더와 디코더는 같은 너비와 Transformer 블록 수를 가집니다. 그림 1은 모델 아키텍처를 요약합니다.

우리는 영어 전용 모델에 대해 GPT-2에서 사용된 바이트 수준 BPE 텍스트 토크나이저(Sennrich et al., 2015; Radford et al., 2019)를 사용하며, 다국어 모델의 경우 다른 언어에서 과도한 단편화를 피하기 위해 어휘를 재조정하되 크기는 동일하게 유지합니다.

#### 2.3 다중 작업 형식

주어진 오디오 스니펫에서 어떤 단어가 말해졌는지 예측하는 것은 전체 음성 인식 문제의 핵심 부분이며 연구에서 광범위하게 다루어졌지만, 그것이 전부는 아닙니다. 완전한 기능을 갖춘 음성 인식 시스템은 음성 활동 감지, 화자 분리, 텍스트 역정규화와 같은 많은 추가 구성 요소를 포함할 수 있습니다. 이러한 구성 요소는 종종 별도로 처리되며, 이는 핵심 음성 인식 모델 주위에 상대적으로 복잡한 시스템을 초래합니다. 이 복잡성을 줄이기 위해, 우리는 단일 모델이 전체 음성 처리 파이프라인을 수행하도록 하고 싶습니다. 여기서 중요한 고려 사항은 모델의 인터페이스입니다. 동일한 입력 오디오 신호에서 수행할 수 있는 많은 다른 작업이 있습니다: 전사, 번역, 음성 활동 감지, 정렬, 언어 식별 등이 그 예입니다.

이러한 일대다 매핑이 단일 모델과 함께 작동하려면, 어떤 형태의 작업 지정이 필요합니다. 우리는 모든 작업과 조건 정보를 디코더에 대한 입력 토큰 시퀀스로 지정하는 간단한 형식을 사용합니다. 우리의 디코더는 오디오 조건부 언어 모델이므로, 전사된 텍스트의 이전 기록에 조건부로 학습하도록 훈련하여 더 긴 범위의 텍스트 컨텍스트를 사용하여 모호한 오디오를 해결할 수 있기를 기대합니다. 구체적으로는, 일정한 확률로 현재 오디오 세그먼트 이전의 전사 텍스트를 디코더의 컨텍스트에 추가합니다. 예측의 시작은 토큰으로 표시합니다.

먼저, 우리는 훈련 세트에서 각 언어를 나타내는 고유 토큰으로 표현되는 언어를 예측합니다(총 99개). 오디오 세그먼트에 음성이 없을 경우, 모델은 이를 나타내는 토큰을 예측하도록 훈련됩니다. 다음 토큰은 또는 토큰으로 작업(전사 또는 번역)을 지정합니다. 그 후, 타임스탬프를 예측할지 여부를 토큰으로 지정합니다. 이 시점에서 작업과 원하는 형식이 완전히 지정되며, 출력이 시작됩니다. 타임스탬프 예측의 경우, 현재 오디오 세그먼트와 관련된 시간을 예측하며, 모든 시간을 20밀리초 단위로 양자화합니다. 이는 Whisper 모델의 기본 시간 해상도와 일치하며, 이를 위해 각 시간에 대해 추가 토큰을 어휘에 추가합니다. 우리는 캡션 토큰과 함께 이를 예측합니다: 시작 시간 토큰은 각 캡션의 텍스트 앞에 예측되고, 종료 시간 토큰은 후에 예측됩니다. 최종 전사 세그먼트가 현재 30초 오디오 청크에 부분적으로만 포함될 때, 타임스탬프 모드에서 해당 세그먼트에 대해 시작 시간 토큰만 예측하여 이후 디코딩이 그 시간에 맞춰진 오디오 창에서 수행되도록 합니다. 그렇지 않으면 해당 세그먼트를 포함하지 않도록 오디오를 잘라냅니다. 마지막으로 토큰을 추가합니다. 우리는 이전 컨텍스트 텍스트에 대한 훈련 손실만 마스킹하고, 나머지 모든 토큰을 예측하도록 모델을 훈련시킵니다. 우리의 형식과 훈련 설정에 대한 개요는 그림 1을 참조하십시오.

### 2.4 훈련 세부 사항

우리는 Whisper의 확장 특성을 연구하기 위해 다양한 크기의 모델군을 훈련합니다. 개요는 표 1을 참조하십시오. 우리는 FP16을 사용하여 가속기 간 데이터 병렬 처리와 동적 손실 스케일링 및 활성화 체크포인팅(Griewank & Walther, 2000; Chen et al., 2016)을 통해 훈련합니다. 모델은 AdamW(Loshchilov & Hutter, 2017)와 그라디언트 노름 클리핑(Pascanu et al., 2013)으로 훈련되며, 처음 2048번 업데이트 동안 워밍업한 후 선형 학습률 감쇠를 통해 0으로 감소합니다. 배치 크기는 256개 세그먼트로 설정되며, 데이터셋을 두세 번 반복하여 220번 업데이트 동안 모델을 훈련합니다. 몇 번의 에포크만 훈련하기 때문에 과적합은 큰 문제가 되지 않으며, 데이터 증강이나 정규화는 사용하지 않고 대규모 데이터셋 내의 다양성에 의존하여 일반화와 견고성을 촉진합니다. 전체 훈련 하이퍼파라미터는 부록 F를 참조하십시오.

초기 개발 및 평가 동안 Whisper 모델이 화자의 이름을 추측하는 경향이 있다는 것을 관찰했습니다. 이는 사전 훈련 데이터셋의 많은 전사문에 발화자의 이름이 포함되어 있어 모델이 이를 예측하려 하지만, 이러한 정보는 최근 30초 오디오 컨텍스트만으로는 거의 추론할 수 없기 때문입니다. 이를 피하기 위해, 우리는 발화자 주석이 포함되지 않은 전사문의 하위 집합에서 Whisper 모델을 간단히 미세 조정하여 이러한 행동을 제거합니다.

![](/assets/images/posts/185/img_1.png)

표 1. Whisper 모델군의 아키텍처 세부 사항.

### 3. 실험

#### 3.1 제로샷 평가

Whisper의 목표는 데이터셋 특정 미세 조정 없이 특정 분포에서 고품질 결과를 얻기 위해 신뢰할 수 있는 단일 견고한 음성 처리 시스템을 개발하는 것입니다. 이 기능을 연구하기 위해, 우리는 다양한 기존 음성 처리 데이터셋을 재사용하여 Whisper가 도메인, 작업, 언어 전반에 걸쳐 잘 일반화할 수 있는지 확인합니다. 이러한 데이터셋의 표준 평가 프로토콜(훈련 및 테스트 분할 포함)을 사용하는 대신, 각 데이터셋의 훈련 데이터를 사용하지 않고 제로샷 설정에서 Whisper를 평가하여 광범위한 일반화를 측정합니다.

### 3.2 평가 지표

음성 인식 연구는 일반적으로 단어 오류율(WER) 지표를 기반으로 시스템을 평가하고 비교합니다. 그러나 문자열 편집 거리 기반인 WER은 모델의 출력과 참조 전사 간의 모든 차이를 페널티로 간주하여, 전사 스타일에서의 사소한 차이까지도 포함합니다. 그 결과, 인간이 정확하다고 판단할 전사도 사소한 형식 차이로 인해 높은 WER을 가질 수 있습니다. 이러한 문제는 모든 전사자에게 발생할 수 있지만, 특정 데이터셋 전사 형식을 관찰하지 않은 Whisper와 같은 제로샷 모델에는 특히 심각합니다.

이는 새로운 관찰이 아니며, 인간 판단과 더 잘 연관된 평가 지표 개발은 활발한 연구 분야입니다. 몇 가지 유망한 방법이 있지만, 아직 음성 인식을 위한 광범위한 채택은 이루어지지 않았습니다. 우리는 비의미적 차이의 페널티를 최소화하기 위해 WER 계산 전에 텍스트를 광범위하게 표준화하여 이 문제를 해결하고자 합니다. 우리의 텍스트 정규화 도구는 Whisper 모델이 사소한 차이로 인해 페널티를 받은 일반적인 패턴을 식별하기 위해 반복적인 수동 검사를 통해 개발되었습니다. 부록 C에는 자세한 내용이 포함되어 있습니다. 몇 가지 데이터셋에 대해, 우리는 데이터셋의 참조 전사가 공백으로 단어와 수축형을 분리하는 특이성으로 인해 WER이 최대 50%까지 감소하는 것을 관찰했습니다. 이 개발 절차는 Whisper 모델의 전사 스타일에 과적합될 위험이 있음을 주의하며, 이는 4.4절에서 조사합니다. 우리는 비교를 쉽게 하고 분포 외 설정에서 음성 인식 시스템의 성능을 연구하는 데 도움이 되도록 텍스트 정규화 도구의 코드를 공개합니다.

### 3.3 영어 음성 인식

2015년, Deep Speech 2(Amodei et al., 2015)는 LibriSpeech 테스트-클린 분할을 전사할 때 인간 수준의 성능을 맞춘 음성 인식 시스템을 보고했습니다. 그들의 분석의 일환으로, "이 결과를 고려할 때, 우리는 추가적인 도메인 적응 없이 깨끗한 읽기 음성에서 일반 음성 시스템이 더 개선될 여지가 거의 없다고 의심합니다."라고 결론지었습니다. 그러나 7년 후, LibriSpeech 테스트-클린에서 SOTA WER은 그들의 5.3%에서 1.4%(Zhang et al., 2021)로 또 73% 감소했으며, 이는 그들이 보고한 인간 수준 오류율 5.8%보다 훨씬 낮습니다. 분포 내 데이터에서의 성능이 이렇게 크게 개선되었음에도 불구하고, LibriSpeech에서 훈련된 음성 인식 모델은 다른 설정에서 사용될 때 여전히 인간 오류율보다 훨씬 높습니다. 분포 내에서 초인적인 성능을 보고하면서도 분포 외에서 인간 이하의 성능을 보이는 이 격차는 무엇 때문일까요?

우리는 인간과 기계 성능의 차이는 테스트 세트에서 인간과 기계 성능으로 측정되는 다양한 능력의 혼동 때문이라고 의심합니다. 이 주장은 처음에는 혼란스러울 수 있습니다. 인간과 기계가 같은 테스트를 치르고 있다면 어떻게 다른 능력이 테스트될 수 있을까요? 차이는 테스트가 아니라 훈련 방식에 있습니다. 인간은 종종 특정 데이터 분포에 대한 감독이 거의 또는 전혀 없는 상태에서 작업을 수행하도록 요청됩니다. 따라서 인간의 성능은 분포 외 일반화를 측정합니다. 하지만 머신러닝 모델은 일반적으로 평가 분포로부터 많은 양의 감독을 받은 후 평가되므로, 기계 성능은 대신 분포 내 일반화를 측정합니다. 인간과 기계가 동일한 테스트 데이터를 평가받고 있지만, 훈련 데이터의 차이로 인해 두 가지 매우 다른 능력이 측정됩니다.

Whisper 모델은 광범위하고 다양한 분포의 오디오에서 훈련되고 제로샷 설정에서 평가되므로 기존 시스템보다 인간 행동을 훨씬 더 잘 맞출 수 있습니다. 이것이 사실인지(또는 기계와 인간 성능의 차이가 아직 이해되지 않은 요인 때문인지) 연구하기 위해, 우리는 Whisper 모델을 인간 성능과 표준 미세 조정된 머신러닝 모델과 비교하여 어느 쪽에 더 가깝게 일치하는지 확인할 수 있습니다.

이 차이를 정량화하기 위해, 우리는 여러 분포/데이터셋에 걸친 평균 성능인 전체 견고성과 Taori et al. (2020)이 도입한 유효 견고성, 즉 기준 데이터셋(보통은 분포 내 데이터셋)과 하나 이상의 분포 외 데이터셋 간의 예상 성능 차이를 측정합니다. 유효 견고성이 높은 모델은 기준 데이터셋에서의 성능을 기반으로 분포 외 데이터셋에서 예상보다 더 나은 성능을 보이며, 모든 데이터셋에서 동일한 성능을 내는 이상적인 상태에 가까워집니다. 우리의 분석을 위해 LibriSpeech를 기준 데이터셋으로 사용합니다. 이는 현대 음성 인식 연구에서 중심적인 역할을 하며, 이를 기반으로 훈련된 많은 모델들이 공개되어 있어 견고성 행동을 특성화할 수 있기 때문입니다. 우리는 분포 외 행동을 연구하기 위해 12개의 다른 학술 음성 인식 데이터셋을 사용합니다. 이러한 데이터셋에 대한 전체 세부 사항은 부록 A에 나와 있습니다.

![](/assets/images/posts/185/img_2.png)

### 그림 2. 제로샷 Whisper 모델은 인간 견고성과의 격차를 좁힙니다.

LibriSpeech dev-clean에서 인간과 맞먹거나 능가함에도 불구하고, 감독된 LibriSpeech 모델은 다른 데이터셋에서 인간보다 약 두 배 더 많은 오류를 발생시켜 그 취약성과 견고성 부족을 보여줍니다. 그러나 제로샷 Whisper 모델의 추정 견고성 경계는 이 특정 인간의 95% 신뢰 구간을 포함합니다.

![](/assets/images/posts/185/img_3.png)

### 표 2. 다양한 데이터셋에 걸친 유효 견고성의 상세 비교.

두 모델 모두 LibriSpeech에서의 성능이 0.1% 이내로 비슷하지만, 제로샷 Whisper 모델은 LibriSpeech 성능에 비해 다른 데이터셋에서 훨씬 더 나은 성능을 보이며, 평균적으로 55.2% 적은 오류를 발생시킵니다. 텍스트 정규화를 적용한 후 두 모델의 단어 오류율(WER)로 보고된 결과입니다.

우리의 주요 결과는 그림 2와 표 2에 요약되어 있습니다. 최고 성능의 제로샷 Whisper 모델은 LibriSpeech 클린 테스트에서 2.5의 비교적 평범한 WER을 기록했으며, 이는 현대 감독된 기준 또는 2019년 중반의 최첨단 성능과 비슷합니다. 그러나 제로샷 Whisper 모델은 감독된 LibriSpeech 모델과는 매우 다른 견고성 특성을 가지고 있으며, 다른 데이터셋에서 모든 벤치마크된 LibriSpeech 모델을 크게 능가합니다. 3,900만 개의 파라미터를 가진 가장 작은 제로샷 Whisper 모델도 LibriSpeech 테스트 클린에서 6.7의 WER을 기록했으며, 다른 데이터셋에서 평가할 때 최고 감독된 LibriSpeech 모델과 거의 경쟁할 만한 수준입니다. 그림 2에서 인간과 비교했을 때, 최고 성능의 제로샷 Whisper 모델은 대략 인간의 정확성과 견고성에 맞먹습니다. 이러한 견고성의 큰 개선에 대한 자세한 분석은 표 2에서 제로샷 Whisper 모델과 LibriSpeech 테스트 클린에서 가장 가까운 성능을 가진 감독된 LibriSpeech 모델의 성능을 비교합니다. 기준 분포에서의 성능은 매우 유사하지만, 제로샷 Whisper 모델은 다른 음성 인식 데이터셋에서 평가할 때 평균 상대 오류 감소율이 55.2%에 달합니다.

이 결과는 모델의 성능을 인간과 비교할 때, 제로샷 및 분포 외 평가를 강조하여 머신러닝 시스템의 능력을 과장하는 잘못된 비교를 피해야 함을 시사합니다.

![](/assets/images/posts/185/img_4.png)

### 그림 3. 사전 훈련 감독 양과 다운스트림 음성 인식 성능 간의 상관 관계.

특정 언어에 대한 사전 훈련 음성 인식 데이터 양은 해당 언어의 Fleurs에서의 제로샷 성능을 매우 잘 예측합니다.

![](/assets/images/posts/185/img_5.png)

표 3. 다국어 음성 인식 성능. 제로샷 Whisper는 Multilingual LibriSpeech(MLS)의 성능을 향상시키지만, VoxPopuli에서는 Maestro, XLS-R 및 mSLAM에 비해 여전히 크게 뒤처집니다.

![](/assets/images/posts/185/img_6.png)

### 그림 4. 사전 훈련 감독 양과 다운스트림 번역 성능 간의 상관 관계.

특정 언어에 대한 사전 훈련 번역 데이터 양은 Whisper의 해당 언어에서의 제로샷 성능을 적당히 예측할 뿐입니다.

### 3.4 다국어 음성 인식

다국어 음성 인식에 대한 이전 연구와 비교하기 위해, 우리는 두 개의 저데이터 벤치마크, Multilingual LibriSpeech (MLS) (Pratap et al., 2020b)와 VoxPopuli (Wang et al., 2021)에서의 결과를 표 3에 보고합니다.

Whisper는 Multilingual LibriSpeech에서 뛰어난 성능을 보여주며, 제로샷 설정에서 XLS-R (Babu et al., 2021), mSLAM (Bapna et al., 2022), 그리고 Maestro (Chen et al., 2022b)를 능가합니다. 그러나 이 결과에 대해 간단한 텍스트 표준화 도구를 사용하였으므로, 직접적인 비교나 SOTA 성능 주장에는 주의가 필요합니다. 반면, VoxPopuli에서는 Whisper가 이전 연구보다 현저히 낮은 성능을 보이며, 원래 논문에서의 VP-10K+FT 기준만을 능가합니다. 우리는 Whisper 모델이 VoxPopuli에서 낮은 성능을 보이는 이유가 다른 모델들이 이 분포를 비지도 사전 훈련 데이터의 주요 소스로 포함하고 있으며, 이 데이터셋이 미세 조정에 유리한 상당히 많은 감독 데이터를 포함하고 있기 때문이라고 의심합니다. MLS는 언어당 10시간의 훈련 데이터를 갖고 있지만, VoxPopuli는 언어당 평균 훈련 데이터 양이 대략 10배 더 많습니다.

이 두 벤치마크는 다소 한정적입니다. 왜냐하면 15개의 고유 언어만 포함하고 있으며, 대부분이 인도-유럽 어족에 속하고 많은 언어들이 자원이 풍부한 언어이기 때문입니다. 이러한 벤치마크는 Whisper 모델의 다국어 능력을 연구하는 데 제한된 범위와 공간을 제공하며, 이는 75개 언어에 대한 음성 인식 훈련 데이터를 포함합니다. Whisper의 성능을 더 넓게 연구하기 위해, 우리는 Fleurs 데이터셋(Conneau et al., 2022)에서의 성능도 보고합니다. 특히, 우리는 특정 언어에 대해 훈련 데이터 양과 해당 언어에 대한 제로샷 성능 간의 관계를 연구하는 데 관심이 있었습니다.

그림 3에서 이 관계를 시각화했습니다. 언어당 훈련 데이터 양의 로그와 단어 오류율(WER)의 로그 사이에 0.83의 강한 제곱 상관 계수를 발견했습니다. 이러한 로그-로그 값에 대한 선형 적합의 회귀 계수를 확인한 결과, 훈련 데이터가 16배 증가할 때마다 WER이 절반으로 줄어드는 추정치를 얻었습니다. 또한, 이 추세에 따라 예상보다 성능이 낮은 주요 언어들은 고유한 스크립트를 가지고 있으며 훈련 데이터셋의 대부분을 차지하는 인도-유럽 어족과 더 먼 관계를 가진 언어들로, 히브리어(HE), 텔루구어(TE), 중국어(ZH), 한국어(KO)와 같은 언어들이었습니다. 이러한 차이는 언어적 거리로 인한 전이 부족, 이러한 언어에 적합하지 않은 바이트 수준 BPE 토크나이저, 또는 데이터 품질의 차이 때문일 수 있습니다.

![](/assets/images/posts/185/img_7.png)

### 표 4. X!en 음성 번역 성능

제로샷 Whisper는 CoVoST2에서 전반적으로, 중간 자원 및 낮은 자원 설정에서 기존 모델을 능가하지만, 자원이 풍부한 언어에서는 이전의 감독된 작업에 비해 여전히 약간 뒤처집니다.

![](/assets/images/posts/185/img_8.png)

### 표 5. 언어 식별 성능

제로샷 Whisper의 언어 식별 정확도는 Fleurs에서 이전 감독된 결과와 비교하여 경쟁력이 없습니다. 이는 부분적으로 Whisper가 Fleurs 언어 중 20개 언어에 대한 훈련 데이터가 없어 큰 페널티를 받기 때문입니다.

![](/assets/images/posts/185/img_9.png)

### 그림 5. SNR에 따른 LibriSpeech 테스트-클린에서의 WER

추가 백색 소음(왼쪽) 및 펍 소음(오른쪽) 하에서 LibriSpeech 훈련된 모델의 정확도는 Whisper 모델(F)보다 더 빠르게 저하됩니다. NVIDIA STT 모델()은 낮은 소음에서 최고의 성능을 발휘하지만, 높은 소음(SNR < 10 dB)에서는 Whisper에 뒤처집니다. 낮은 소음에서 두 번째로 좋은 모델(H)은 LibriSpeech에서만 미세 조정된 것이며, 더 빠르게 성능이 저하됩니다.

### 3.5 번역

Whisper 모델의 번역 능력을 연구하기 위해 CoVoST2(Wang et al., 2020b)의 X!en 하위 집합에서 성능을 측정했습니다. 우리는 Maestro, mSLAM, XLS-R과 비교했으며, 이들은 이전에 최고 성능을 기록한 모델들입니다. 우리는 CoVoST2 훈련 데이터를 전혀 사용하지 않고도 제로샷으로 29.1 BLEU의 새로운 최고 성능을 달성했습니다. 이는 사전 훈련 데이터셋에서 68,000시간의 X!en 번역 데이터를 사용한 덕분입니다. 이 데이터는 비록 잡음이 많지만, CoVoST2의 861시간의 X!en 번역 훈련 데이터보다 훨씬 방대합니다. Whisper 평가가 제로샷이기 때문에, CoVoST2의 가장 낮은 자원 그룹에서 특히 잘 수행하여 mSLAM보다 6.7 BLEU 개선되었습니다. 반면, 최고 성능의 Whisper 모델은 자원이 풍부한 언어에서는 Maestro와 mSLAM에 비해 평균적으로 성능이 개선되지 않았습니다.

더 넓은 언어 집합에 대한 추가 분석을 위해, 우리는 Fleurs라는 음성 인식 데이터셋을 번역 데이터셋으로 재활용했습니다. 동일한 문장이 모든 언어에 대해 전사되므로, 영어 전사를 참조 번역으로 사용했습니다. 그림 4에서는 언어당 번역 훈련 데이터 양과 Fleurs에서의 제로샷 BLEU 점수 간의 상관 관계를 시각화했습니다. 훈련 데이터 양이 증가함에 따라 성능이 향상되는 명확한 경향이 있지만, 음성 인식에서 관찰된 0.83보다 낮은 0.24의 제곱 상관 계수를 보였습니다. 이는 부분적으로 오디오 언어 식별 오류로 인한 훈련 데이터의 잡음이 원인일 수 있습니다. 예를 들어, 웨일스어(CY)는 번역 데이터가 9,000시간 있다고 하지만, 예상보다 성능이 크게 떨어져 13 BLEU에 불과했습니다. 이 많은 웨일스어 번역 데이터는 놀라운 것으로, 세계에서 가장 많이 사용되는 언어인 프랑스어, 스페인어, 러시아어 등을 앞서는 수준입니다. 조사 결과, 웨일스어 번역 데이터의 대부분이 실제로는 영어 오디오에 영어 자막이 포함된 것이었으며, 언어 식별 시스템에 의해 영어 오디오가 웨일스어로 잘못 분류되어 전사 데이터가 아닌 번역 훈련 데이터로 포함되었습니다.

### 3.6 언어 식별

언어 식별을 평가하기 위해 Fleurs 데이터셋(Conneau et al., 2022)을 사용했습니다. Whisper의 제로샷 성능은 이전의 감독된 작업과 비교하여 경쟁력이 없으며, 감독된 SOTA보다 13.6% 낮은 성능을 보였습니다. 그러나 Whisper는 Fleurs에서 언어 식별에 불리합니다. 왜냐하면 Whisper 데이터셋에는 Fleurs의 102개 언어 중 20개 언어에 대한 훈련 데이터가 없기 때문에 정확도가 최대 80.4%로 제한됩니다. 겹치는 82개 언어에서 최고의 Whisper 모델은 80.3%의 정확도를 달성했습니다.

### 3.7 추가 소음에 대한 견고성

우리는 Whisper 모델과 14개의 LibriSpeech 훈련 모델의 소음 견고성을 테스트하여 오디오에 백색 소음이나 Audio Degradation Toolbox(Mauch & Ewert, 2013)에서 가져온 펍 소음을 추가했을 때 WER을 측정했습니다. 펍 소음은 붐비는 레스토랑이나 펍에서 흔히 들리는 배경 소음과 분명치 않은 대화로 더 자연스러운 소음 환경을 나타냅니다. 14개 모델 중 12개는 LibriSpeech에서 사전 훈련 및/또는 미세 조정되었으며, 나머지 두 개는 LibriSpeech를 포함한 혼합 데이터셋에서 훈련된 NVIDIA STT 모델입니다. 주어진 신호 대 잡음비(SNR)에 해당하는 추가 소음 수준은 개별 예제의 신호 세기를 기반으로 계산되었습니다. 그림 5는 추가 소음이 강해짐에 따라 ASR 성능이 어떻게 저하되는지를 보여줍니다. 소음이 낮을 때(40 dB SNR) 우리의 제로샷 성능을 능가하는 많은 모델이 있지만, 이는 대부분의 모델이 주로 LibriSpeech에서 훈련되었기 때문에 놀랍지 않습니다. 그러나 소음이 강해질수록 모든 모델의 성능이 빠르게 저하되어 SNR이 10 dB 이하인 추가 펍 소음에서는 Whisper 모델보다 성능이 떨어집니다. 이는 특히 펍 소음과 같은 더 자연스러운 분포 변화에서 Whisper의 소음에 대한 견고성을 보여줍니다.

![](/assets/images/posts/185/img_10.png)

### 그림 6. Whisper는 장기 전사에서 최첨단 상용 및 오픈 소스 ASR 시스템과 경쟁합니다.

여섯 개의 ASR 시스템에서 일곱 개의 장기 데이터셋에 대한 단어 오류율 분포를 비교하며, 입력 길이는 몇 분에서 몇 시간까지 다양합니다. 박스는 예제별 WER의 사분위를 나타내며, 각 박스에 데이터셋별 집계 WER이 주석으로 표시되어 있습니다. 우리의 모델은 모든 데이터셋에서 최고의 오픈 소스 모델(NVIDIA STT)을 능가하며, 대부분의 경우 상용 ASR 시스템보다도 우수한 성능을 보입니다.

### 3.8 장기 전사

Whisper 모델은 30초 오디오 청크로 훈련되었으며, 한 번에 더 긴 오디오 입력을 처리할 수 없습니다. 이는 짧은 발화로 구성된 대부분의 학술 데이터셋에서는 문제가 되지 않지만, 현실 세계의 응용에서는 몇 분 또는 몇 시간 동안의 오디오를 전사해야 하는 경우에 문제가 됩니다. 우리는 30초 오디오 세그먼트를 연속적으로 전사하고 모델이 예측한 타임스탬프에 따라 창을 이동시키는 방식으로 장기 오디오를 버퍼링하여 전사하는 전략을 개발했습니다. 장기 오디오를 신뢰성 있게 전사하려면 빔 서치와 모델 예측의 반복성 및 로그 확률에 기반한 온도 조절이 중요하다는 것을 관찰했습니다. 전체 절차는 4.5절에 설명되어 있습니다.

우리는 가능한 한 다양한 데이터 분포를 다루기 위해 다양한 길이와 녹음 조건의 음성 녹음을 포함하는 7개의 데이터셋에서 장기 전사 성능을 평가합니다. 여기에는 각 예제가 전체 TED 강연인 TED-LIUM3(Hernandez et al., 2018)의 장기 적응, The Late Show with Stephen Colbert (Meanwhile)에서 가져온 용어가 많은 세그먼트 모음, 온라인 블로그에서 ASR 벤치마크로 사용된 비디오/팟캐스트 세트(Rev16 및 Kincaid46), 수익 전화 회의 녹음(Del Rio et al., 2021), 그리고 지역 아프리카계 미국인 언어 말뭉치(CORAAL)(Gunter et al., 2021)의 전체 인터뷰 등이 포함됩니다. 장기 데이터셋에 대한 전체 세부 사항은 부록 A에 나와 있습니다.

우리는 오픈 소스 모델뿐만 아니라 4개의 상용 ASR 서비스와 성능을 비교합니다. 결과는 그림 6에 요약되어 있으며, Whisper와 4개의 상용 ASR 서비스의 단어 오류율 분포와 오픈 소스 모델 중 가장 성능이 좋은 NVIDIA STT Conformer-CTC Large 모델(NeMo 툴킷에서 제공)도 포함되어 있습니다(Kuchaiev et al., 2019). 모든 상용 ASR 서비스는 2022년 9월 1일 기준 기본 영어 전사 설정을 사용하여 쿼리되었으며, NVIDIA STT 모델의 경우 장기 전사를 가능하게 하기 위해 FrameBatchASR 클래스의 버퍼링 추론 구현을 사용했습니다. 결과는 Whisper가 대부분의 데이터셋에서 비교 모델보다 더 나은 성능을 보이며, 특히 드문 단어가 많은 Meanwhile 데이터셋에서 두드러집니다. 또한 일부 상용 ASR 시스템이 이러한 공개 데이터셋 중 일부를 훈련에 사용했을 가능성이 있어, 이러한 결과가 시스템의 상대적 견고성을 정확하게 반영하지 않을 수도 있음을 주목합니다.

![](/assets/images/posts/185/img_11.png)

### 그림 7. Whisper의 성능은 전문 인간 전사자와 비슷합니다.

이 도표는 Whisper, 그림 6의 동일한 4개 상용 ASR 시스템(A-D), 컴퓨터 보조 인간 전사 서비스(E), 그리고 4개의 인간 전사 서비스(F-I)에서 전사된 Kincaid46 데이터셋의 25개 녹음에 대한 WER 분포를 보여줍니다. 박스 플롯에 예제별 WER을 나타내는 점이 겹쳐 있으며, 25개 녹음에 대한 집계 WER이 각 박스에 주석으로 표시되어 있습니다.

### 3.9 인간 성능과의 비교

모호하거나 분명하지 않은 발음 및 라벨링 오류 때문에 각 데이터셋에는 다양한 수준의 불가피한 오류가 존재합니다. ASR 시스템의 WER 지표만으로는 각 데이터셋에서 얼마나 개선 여지가 있는지 파악하기 어렵습니다. Whisper의 성능이 인간 성능에 얼마나 가까운지를 정량화하기 위해, 우리는 Kincaid46 데이터셋에서 25개의 녹음을 선택하고 전문 전사자가 작성한 전사를 얻기 위해 5개의 서비스를 사용했습니다. 이 중 하나는 컴퓨터 보조 전사를 제공하고 나머지 네 개는 전적으로 인간에 의해 전사되었습니다. 오디오 선택은 방송된 스크립트와 비스크립트, 전화 및 VoIP 통화, 회의 등 다양한 녹음 조건을 포함합니다. 그림 7은 25개의 녹음에 대한 예제별 WER 분포와 집계 WER을 보여주며, 컴퓨터 보조 서비스는 Whisper보다 1.15%포인트 낮은 집계 WER을 가지고 있으며, 순수 인간 성능은 Whisper보다 소수점 차이로 더 좋습니다. 이러한 결과는 Whisper의 영어 ASR 성능이 완벽하지는 않지만 인간 수준의 정확도에 매우 가깝다는 것을 나타냅니다.

![](/assets/images/posts/185/img_12.png)

### 그림 8. 제로샷 Whisper 성능은 모델 크기가 증가함에 따라 작업 및 언어 전반에서 안정적으로 확장됩니다.

옅은 음영의 선은 개별 데이터셋 또는 언어를 나타내며, 집계 성능의 부드러운 추세보다 성능이 더 다양함을 보여줍니다. 큰 V2는 이 분석의 작은 모델에 존재하지 않는 몇 가지 변경 사항을 포함하므로 주황색 점선으로 구별됩니다

![](/assets/images/posts/185/img_13.png)

### 표 6. 데이터셋 크기가 증가함에 따라 성능이 향상됩니다.

영어 음성 인식 성능은 12개 데이터셋의 평균을 나타내며, 다국어 음성 인식은 Fleurs와 X!en 번역에서 중복되는 언어의 성능을 보고하고, CoVoST2에서 평균 BLEU를 보고합니다. 데이터셋 크기는 시간 단위로 보고되었습니다.

### 4. 분석 및 소거

#### 4.1 모델 확장

약하게 감독된 훈련 접근 방식에서 큰 가능성은 기존의 감독 학습보다 훨씬 더 큰 데이터셋을 사용할 수 있는 잠재력입니다. 그러나 이는 데이터가 금 표준 감독보다 훨씬 더 시끄럽고 품질이 낮을 가능성이 있다는 대가를 수반합니다. 이러한 접근 방식의 문제는 처음에는 유망해 보이지만, 이러한 데이터로 훈련된 모델의 성능이 데이터셋의 내재된 품질 수준에서 포화될 수 있으며, 이는 인간 수준보다 훨씬 낮을 수 있다는 점입니다. 관련된 문제는 데이터셋에서 훈련에 사용되는 용량과 컴퓨팅이 증가함에 따라 모델이 데이터셋의 특이성을 악용할 수 있으며, 분포 외 데이터에 대한 견고한 일반화 능력이 저하될 수 있다는 점입니다.

이러한 경우가 맞는지 확인하기 위해, 우리는 모델 크기의 함수로 Whisper 모델의 제로샷 일반화를 연구했습니다. 우리의 분석은 그림 8에 요약되어 있습니다. 영어 음성 인식을 제외하고는, 다국어 음성 인식, 음성 번역 및 언어 식별에서 모델 크기가 증가함에 따라 성능이 계속 증가합니다. 영어 음성 인식의 수익 감소는 3.9절에서의 분석이 시사하듯 인간 수준 성능에 접근하는 포화 효과 때문일 수 있습니다.

#### 4.2 데이터셋 확장

68만 시간의 라벨된 오디오를 포함하는 Whisper 데이터셋은 감독된 음성 인식에서 만들어진 가장 큰 데이터셋 중 하나입니다. Whisper의 성능에 데이터셋 크기가 얼마나 중요한지 정확히 알아보기 위해, 우리는 전체 데이터셋 크기의 0.5%, 1%, 2%, 4%, 8%로 서브샘플된 버전에서 중간 크기 모델 시리즈를 훈련하고, 전체 데이터셋에서 훈련된 동일한 중간 크기 모델과 성능을 비교했습니다. 검증 손실을 기반으로 조기 종료하여 각 데이터셋 크기에 대한 모델 체크포인트를 선택했습니다. 평가는 매개변수의 지수 이동 평균 추정치(Polyak & Juditsky, 1992)를 사용하여 수행했으며, 학습률이 조기 종료로 인해 서브샘플된 데이터셋에서 훈련된 모델에 대해 완전히 0으로 감소하지 않는 영향을 줄이기 위해 0.9999의 스무딩률을 사용했습니다. 영어 및 다국어 음성 인식과 X!en 번역 성능은 표 6에 보고되었습니다.

모든 작업에서 데이터셋 크기가 증가하면 성능이 향상되지만, 작업 및 크기별로 개선 속도에 상당한 변동성이 나타납니다. 영어 음성 인식에서는 3,000시간에서 13,000시간으로 급격히 성능이 향상되었으나, 13,000시간에서 54,000시간 사이에서는 개선 속도가 눈에 띄게 느려집니다. 전체 데이터셋을 사용하면 크기가 12.5배 증가하지만 WER은 추가로 1포인트만 감소합니다. 이는 영어 음성 인식을 위한 모델 크기 확장 시 관찰된 수익 감소와 유사하며, 인간 수준의 성능에 접근할 때 나타나는 포화 효과로 설명될 수 있습니다.

다국어 음성 인식에서는 54,000시간까지 WER 개선이 멱법칙 추세를 따르다가 전체 데이터셋 크기로 확장할 때 7포인트만 추가로 개선됩니다. X!en 번역에서는 7,000시간 이하의 오디오로 훈련할 때 성능이 거의 제로에 가깝고, 54,000시간까지는 대략 로그-선형 개선 추세를 따르다가 전체 데이터셋 크기로 확장할 때 수익 감소를 보입니다.

54,000시간에서 전체 데이터셋 크기인 680,000시간으로 이동할 때 모든 작업에서 수익 감소가 나타나는 일반적인 추세는 현재 최고의 Whisper 모델이 데이터셋 크기에 비해 덜 훈련되었음을 시사할 수 있으며, 더 긴 훈련과 더 큰 모델의 조합으로 성능을 더욱 향상시킬 수 있습니다. 또한, 음성 인식을 위한 데이터셋 크기 확장에서 성능 개선이 끝나가고 있음을 시사할 수도 있습니다. 이러한 설명을 구분하기 위해 추가 분석이 필요합니다.

![](/assets/images/posts/185/img_14.png)

### 그림 9. 스케일에 따라 다중 작업 및 다국어 전이가 개선됩니다.

작은 모델의 경우, 다중 작업 및 다국어 설정에서 공동으로 훈련할 때 영어 음성 인식 성능이 저하됩니다. 그러나 다국어 및 다중 작업 모델은 스케일의 이점을 더 많이 얻으며, 결국 영어 데이터만으로 훈련된 모델을 능가합니다. 95% 부트스트랩 추정 신뢰 구간이 표시됩니다

![](/assets/images/posts/185/img_15.png)

### 그림 10. 대부분의 데이터셋에서 우리의 텍스트 정규화 도구는 Whisper 모델과 다른 오픈 소스 모델에서 WER 감소에 유사한 효과를 보입니다.

각 데이터셋에 대해, 상자 그림은 우리의 평가 스위트에서 다양한 모델 간의 상대적 WER 감소 분포를 보여주며, 우리의 텍스트 정규화 도구를 사용하면 일반적으로 FairSpeech의 정규화 도구보다 낮은 WER을 달성합니다. 일부 데이터셋에서는 우리의 정규화 도구가 WER을 상당히 더 많이 감소시키며, 이는 CallHome과 Switchboard와 같이 실제에 많은 축약형이 있는 데이터셋과 많은 숫자 표현이 포함된 WSJ에서 특히 그렇습니다.

### 4.3 다중 작업 및 다국어 전이

여러 작업과 언어에서 단일 모델을 공동 훈련할 때 발생할 수 있는 잠재적 문제는 여러 작업의 학습 간 간섭으로 인해 단일 작업이나 언어에서 훈련했을 때보다 성능이 떨어지는 부정적 전이입니다. 이러한 현상이 발생하는지 조사하기 위해, 우리는 영어 음성 인식만 훈련된 모델과 표준 다중 작업 및 다국어 훈련 설정에서 훈련된 모델의 성능을 비교하고, 제로샷 영어 음성 인식 벤치마크에서의 평균 성능을 측정했습니다. 영어 음성 인식 작업에 사용된 FLOP 양을 조정했으며, 공동 훈련 설정에서는 전체 계산의 65%만이 이 작업에 사용되므로 동일 크기의 영어 전용 모델과 비교할 때 과소 훈련으로 인해 분석이 혼란스러워지는 것을 방지했습니다.

그림 9에 시각화된 결과는 적당한 양의 계산으로 훈련된 작은 모델의 경우, 작업과 언어 간에 실제로 부정적 전이가 발생하며, 공동 모델이 동일한 계산량으로 훈련된 영어 전용 모델보다 성능이 떨어진다는 것을 보여줍니다. 그러나 다중 작업 및 다국어 모델은 더 잘 확장되며, 우리의 가장 큰 실험에서는 다른 작업에서의 긍정적 전이를 보여 영어 전용 모델보다 더 나은 성능을 보입니다. 우리의 가장 큰 실험에서는 공동 모델이 작업당 사용된 계산량을 조정하지 않아도 영어 전용 모델보다 약간 더 나은 성능을 보입니다.

### 4.4 텍스트 정규화

우리는 무해한 단어 오류를 제외하기 위해 Whisper와 함께 텍스트 정규화를 개발했기 때문에, 우리의 정규화 도구가 일반적인 전사 변형을 다루기보다는 Whisper의 특이성을 수정하는 데 과적합될 위험이 있습니다. 이를 확인하기 위해, 우리는 Whisper에 우리의 정규화 도구를 사용하는 것과 FairSpeech 프로젝트(Koenecke et al., 2020)에서 독립적으로 개발된 정규화 도구를 사용하는 것을 비교했습니다. 그림 10에서 그 차이를 시각화했습니다. 대부분의 데이터셋에서 두 정규화 도구는 비슷한 성능을 보였으며, Whisper와 비교된 오픈 소스 모델 간의 WER 감소에서 큰 차이가 없었습니다. 그러나 WSJ, CallHome, Switchboard 데이터셋에서는 우리의 정규화 도구가 Whisper 모델의 WER을 훨씬 더 많이 감소시켰습니다. 감소의 차이는 진실 데이터에서 사용된 다른 형식과 두 정규화 도구가 이를 어떻게 페널티를 주는지에서 비롯됩니다. 예를 들어, CallHome과 Switchboard에서는 "you’re"와 "you are"와 같은 일반적인 영어 축약형의 차이에 대해 우리 표준화 도구는 페널티를 주지 않았으며, WSJ에서는 우리의 정규화 도구가 숫자 및 금액 표현의 서면 및 구어 형태를 표준화했습니다. 예를 들어, "sixty-eight million dollars"와 "$68 million"의 차이를 표준화했습니다.

![](/assets/images/posts/185/img_16.png)

### 표 7. 추가 디코딩 휴리스틱이 적용될 때마다 장기 전사 성능이 점진적으로 향상됩니다.

각 개입에 대한 세부 사항은 4.5절에 설명되어 있습니다.

### 4.5 신뢰할 수 있는 장기 전사 전략

Whisper를 사용하여 장기 오디오를 전사하는 것은 타임스탬프 토큰의 정확한 예측에 의존하여 모델의 30초 오디오 컨텍스트 창을 이동할 양을 결정합니다. 한 창에서의 부정확한 전사는 다음 창에서의 전사에 부정적인 영향을 미칠 수 있습니다. 우리는 3.8절과 3.9절에서 보고된 결과에 적용된 장기 전사의 실패 사례를 피하기 위한 일련의 휴리스틱을 개발했습니다. 첫째, 로그 확률을 점수 함수로 사용하여 5개의 빔으로 빔 서치를 사용하여 탐욕적 디코딩에서 더 자주 발생하는 반복 루핑을 줄입니다. 생성된 토큰의 평균 로그 확률이 -1보다 낮거나 생성된 텍스트의 gzip 압축률이 2.4보다 높은 경우 온도를 0.2씩 최대 1.0까지 증가시킵니다. 적용된 온도가 0.5 이하일 때 이전 텍스트 조건으로 이전 창에서 전사된 텍스트를 제공하면 성능이 더욱 향상됩니다. 토큰의 확률만으로는 음성이 없는 세그먼트를 구별하기에 충분하지 않지만, 0.6의 무음 확률 임계값과 -1의 평균 로그 확률 임계값을 결합하면 Whisper의 음성 활동 감지가 더 신뢰할 수 있게 됩니다. 마지막으로, 모델이 입력의 처음 몇 단어를 무시하는 실패 모드를 피하기 위해 초기 타임스탬프 토큰을 0.0초에서 1.0초 사이로 제한했습니다. 표 7은 위의 각 개입을 추가함에 따라 전체적으로 WER이 점진적으로 감소하지만, 데이터셋 전체에 걸쳐 균등하게 감소하지 않는다는 것을 보여줍니다. 이러한 휴리스틱은 모델의 잡음 있는 예측을 해결하기 위한 임시방편으로 작동하며, 장기 디코딩의 신뢰성을 더욱 향상시키기 위해 추가 연구가 필요합니다.

### 5. 관련 연구

#### 음성 인식 확장

음성 인식 연구에서 일관된 주제는 컴퓨팅, 모델 및 데이터셋의 확장의 이점을 문서화하는 것입니다. 음성 인식에 딥러닝을 적용한 초기 연구는 모델의 깊이와 크기가 성능을 향상시키며, GPU 가속을 활용하여 더 큰 모델의 훈련을 가능하게 한다는 것을 발견했습니다(Mohamed et al., 2009). 추가 연구에서는 데이터셋 크기가 증가함에 따라 딥러닝 접근 방식의 음성 인식 성능이 향상되었음을 보여주었습니다. 예를 들어, TIMIT 훈련 데이터를 3시간만 사용했을 때는 이전의 GMM-HMM 시스템과 경쟁할 수 있었지만, 2,000시간의 Switchboard 데이터셋으로 훈련했을 때는 단어 오류율이 30% 감소했습니다(Seide et al., 2011). Liao et al. (2013)은 딥러닝 기반 음성 인식 데이터셋의 크기를 1,000시간 이상 증가시키기 위해 약하게 감독된 학습을 활용한 초기 예입니다.

이러한 경향은 Deep Speech 2(Amodei et al., 2015)가 16개의 GPU를 통해 고처리량 분산 훈련을 개발하고 12,000시간의 훈련 데이터로 확장하면서 계속되었습니다. 반감독 사전 훈련을 활용한 Narayanan et al. (2018)은 데이터셋 크기를 훨씬 더 키우고 162,000시간의 라벨된 오디오로 훈련을 연구했습니다. 최근 연구에서는 억 개의 파라미터 모델(Zhang et al., 2020)과 최대 100만 시간의 훈련 데이터를 사용하는(Zhang et al., 2021) 연구가 이루어졌습니다.

#### 다중 작업 학습

다중 작업 학습(Caruana, 1997)은 오랫동안 연구되어 왔습니다. 음성 인식에서는 10년 이상 다국어 모델이 탐구되었습니다(Schultz & Kirchhoff, 2006). 단일 모델을 사용한 다중 작업 학습을 탐구한 NLP의 영감적이고 기초적인 연구는 Collobert et al. (2011)입니다. 여러 인코더와 디코더를 사용하는 시퀀스-투-시퀀스 프레임워크의 다중 작업 학습은 Luong et al. (2015)에서 조사되었습니다. 공유 인코더/디코더 아키텍처와 언어 코드를 사용하는 방법은 Johnson et al. (2017)에서 처음으로 기계 번역에 대해 시연되어 별도의 인코더와 디코더가 필요하지 않게 했습니다. 이 접근 방식은 McCann et al. (2018)의 “텍스트-투-텍스트” 프레임워크로 더욱 단순화되었으며, Radford et al. (2019)와 Raffel et al. (2020)의 대형 트랜스포머 언어 모델에서의 성공으로 인기를 얻었습니다. Toshniwal et al. (2018)은 단일 모델로 여러 언어에서 현대 딥러닝 음성 인식 시스템을 공동으로 훈련했으며, Pratap et al. (2020a)는 이 연구를 억 개의 파라미터 모델로 50개 언어로 확장했습니다. MUTE(Wang et al., 2020c)와 mSLAM(Bapna et al., 2022)은 텍스트와 음성 언어 작업을 공동 훈련하여 작업 간 전이를 시연했습니다.

#### 견고성

모델이 얼마나 효과적으로 전이되고 분포 변화 및 기타 유형의 변형에 얼마나 견고한지에 대한 문제는 오랫동안 연구되어 왔으며, 머신러닝의 여러 분야에서 활발히 연구되고 있습니다. Torralba & Efros (2011)은 10년 전에 데이터셋 간의 머신러닝 모델의 일반화 부족을 강조했습니다. 많은 다른 연구들도 IID 테스트 세트에서 높은 성능을 보이더라도, 약간 다른 설정에서 평가될 때 머신러닝 모델이 여전히 많은 실수를 할 수 있음을 반복적으로 보여주었습니다(Lake et al., 2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al., 2019; Recht et al., 2019). 최근 Taori et al. (2020)은 이미지 분류 모델의 견고성을 연구했으며, Miller et al. (2020)은 질문-응답 모델에 대해 이를 조사했습니다. 주요 발견 중 하나는 여러 도메인 훈련이 견고성과 일반화를 증가시킨다는 것이며, 이는 서론에서 논의된 바 있습니다. 이러한 발견은 NLP(Hendrycks et al., 2020) 및 컴퓨터 비전(Radford et al., 2021)을 포함하여 음성 인식뿐만 아니라 여러 분야에서 재현되었습니다.

### 6. 한계와 향후 연구

우리의 실험 결과, 분석 및 제거 작업에서 여러 한계와 향후 연구 과제를 발견했습니다.

#### 개선된 디코딩 전략

Whisper를 확장하면서, 더 큰 모델들이 비슷한 발음의 단어를 혼동하는 등의 인식 관련 오류를 줄이는 데 있어 꾸준하고 신뢰할 수 있는 진전을 이루었음을 관찰했습니다. 그러나 특히 장기 전사에서 남아있는 많은 오류는 성격상 더 고집스럽고 인간/지각적이지 않습니다. 이러한 오류는 시퀀스-투-시퀀스 모델, 언어 모델 및 텍스트-오디오 정렬의 실패 모드가 결합된 것이며, 반복 루프에 빠지거나, 오디오 세그먼트의 처음 또는 마지막 몇 단어를 전사하지 않거나, 실제 오디오와 전혀 관련이 없는 전사를 출력하는 등의 문제를 포함합니다. 4.5절에서 논의한 디코딩 세부 사항이 상당한 도움을 주지만, Whisper 모델을 고품질의 감독된 데이터셋에서 미세 조정하거나 디코딩 성능을 보다 직접적으로 최적화하기 위해 강화 학습을 사용하는 것이 이러한 오류를 더욱 줄이는 데 도움이 될 수 있다고 생각합니다.

#### 저자원 언어의 훈련 데이터 증가

그림 3에서 보듯이, Whisper의 음성 인식 성능은 많은 언어에서 여전히 매우 낮습니다. 동일한 분석은 언어의 성능이 해당 언어의 훈련 데이터 양에 의해 매우 잘 예측되므로 개선의 명확한 경로를 제시합니다. 우리의 사전 훈련 데이터셋은 주로 영어 중심의 인터넷에서 수집되었기 때문에 영어에 편향되어 있어 대부분의 언어는 1000시간 미만의 훈련 데이터를 가지고 있습니다. 이러한 희귀 언어에 대한 데이터 양을 증가시키기 위한 목표를 세우면 전체 훈련 데이터셋 크기를 약간만 증가시키더라도 평균 음성 인식 성능을 크게 향상시킬 수 있습니다.

#### 미세 조정 연구

이 연구에서는 음성 처리 시스템의 견고성 특성에 중점을 두고 Whisper의 제로샷 전이 성능만을 연구했습니다. 이는 일반적인 신뢰성을 나타내기 때문에 중요한 설정이지만, 고품질의 감독된 음성 데이터가 존재하는 많은 도메인에서는 미세 조정을 통해 결과를 더욱 개선할 수 있을 것입니다. 미세 조정을 연구하는 추가적인 이점은 훨씬 더 일반적인 평가 설정이기 때문에 이전 연구와의 직접적인 비교가 가능하다는 것입니다.

#### 언어 모델이 견고성에 미치는 영향 연구

서론에서 주장했듯이, Whisper의 견고성은 부분적으로 오디오 조건부 언어 모델인 강력한 디코더 덕분이라고 생각됩니다. Whisper의 이점이 인코더, 디코더 또는 둘 다에서 비롯되는 정도는 현재 불명확합니다. 이는 Whisper의 다양한 설계 요소를 제거하여 연구하거나, wav2vec 2.0과 같은 기존 음성 인식 인코더가 언어 모델과 함께 사용될 때 성능이 어떻게 변화하는지 연구함으로써 연구할 수 있습니다.

#### 보조 훈련 목표 추가

Whisper는 비지도 사전 훈련이나 자가 학습 방법이 없기 때문에 대부분의 최신 최첨단 음성 인식 시스템과 눈에 띄게 다릅니다. 우리는 이러한 방법이 좋은 성능을 달성하는 데 필요하지 않다고 생각했지만, 이를 통합함으로써 결과를 더욱 개선할 수 있을 가능성이 있습니다.

### 7. 결론

Whisper는 약하게 감독된 사전 훈련의 확장이 음성 인식 연구에서 지금까지 과소평가되어 왔음을 시사합니다. 우리는 최근 대규모 음성 인식 작업의 주류가 된 자가 감독 및 자가 학습 기술이 필요 없이, 큰 규모와 다양한 감독된 데이터셋에서 단순히 훈련하고 제로샷 전이에 중점을 둠으로써 음성 인식 시스템의 견고성을 크게 향상시킬 수 있음을 입증했습니다.

### 감사의 말

Whisper에서 사용된 데이터를 만든 수백만 명의 사람들에게 감사드립니다. 이 프로젝트에 영감을 준 폭포 하이킹 대화에서 Nick Ryder, Will Zhuk, Andrew Carr에게도 감사드립니다. 이 프로젝트에서 소프트웨어 및 하드웨어 인프라 작업을 수행한 OpenAI의 가속화 및 슈퍼컴퓨팅 팀에게도 감사드립니다. 마지막으로, 프로젝트를 정책 관점에서 조언해 준 Pamela Mishkin에게도 감사드립니다. 이 프로젝트 전반에 걸쳐 사용된 많은 소프트웨어 패키지의 개발자들에게도 감사드립니다. 여기에는 Numpy(Harris et al., 2020), SciPy(Virtanen et al., 2020), ftfy(Speer, 2019), PyTorch(Paszke et al., 2019), pandas(pandas development team, 2020), scikit-learn(Pedregosa et al., 2011) 등이 포함되지만 이에 국한되지는 않습니다.

[2212.04356v1.pdf

0.96MB](./file/2212.04356v1.pdf)
