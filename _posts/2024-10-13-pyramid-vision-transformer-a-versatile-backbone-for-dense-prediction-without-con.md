---
title: "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"
date: 2024-10-13 23:57:04
categories:
  - 인공지능
---

프로젝트 때문에, 너무 바빠서 잘 못하고 있다.... 오랜만에 하나...

<https://arxiv.org/abs/2102.12122>

[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)

**초록**  
비록 컨볼루션 신경망(CNN)이 컴퓨터 비전에서 큰 성공을 거두었지만, 본 연구에서는 다양한 밀집 예측 작업에 유용한, 보다 간단하고 컨볼루션이 없는 백본 네트워크를 조사합니다. 최근 제안된 비전 트랜스포머(ViT)가 이미지 분류에 특화되어 설계된 것과 달리, 우리는 다양한 밀집 예측 작업에 트랜스포머를 적용하는 데 어려움을 극복하는 \*\*피라미드 비전 트랜스포머(PVT)\*\*를 소개합니다. PVT는 현재의 최신 기술과 비교하여 몇 가지 장점을 가지고 있습니다.

1. 일반적으로 저해상도 출력과 높은 계산 및 메모리 비용을 초래하는 ViT와 달리, PVT는 이미지의 밀집 분할을 통해 훈련될 수 있어 밀집 예측에 중요한 높은 출력 해상도를 달성할 수 있을 뿐 아니라, 큰 특징 맵의 계산을 줄이기 위해 점진적으로 축소되는 피라미드를 사용합니다.
2. PVT는 CNN과 트랜스포머의 장점을 모두 상속받아, 다양한 비전 작업을 위한 통합 백본으로 사용될 수 있으며, 컨볼루션 없이도 CNN 백본을 대체할 수 있습니다.
3. 우리는 PVT를 광범위한 실험을 통해 검증하였으며, 객체 탐지, 인스턴스 및 의미론적 분할을 포함한 많은 하위 작업의 성능을 향상시키는 것을 확인했습니다. 예를 들어, 비슷한 수의 파라미터로 PVT와 RetinaNet을 결합한 모델은 COCO 데이터셋에서 40.4 AP를 달성하여, ResNet50+RetinaNet(36.3 AP)을 4.1 절대 AP만큼 능가합니다(Figure 2 참조). 우리는 PVT가 픽셀 수준의 예측을 위한 대안적이고 유용한 백본으로 사용되고, 향후 연구를 촉진하는 데 기여할 수 있기를 바랍니다.

![](/assets/images/posts/284/img.png)

(a) CNNs: VGG [54], ResNet [22] 등

![](/assets/images/posts/284/img_1.png)

(b) 비전 트랜스포머 [13]

![](/assets/images/posts/284/img_2.png)

(c) 피라미드 비전 트랜스포머 (우리의 모델)

**그림 1: 다양한 아키텍처 비교**  
여기서 "Conv"와 "TF-E"는 각각 "컨볼루션"과 "트랜스포머 인코더"를 의미합니다.

(a) 많은 CNN 백본들은 객체 탐지(DET), 인스턴스 및 의미론적 분할(SEG)과 같은 밀집 예측 작업을 위해 피라미드 구조를 사용합니다.  
(b) 최근 제안된 비전 트랜스포머(ViT) [13]는 이미지 분류(CLS)에 특화된 "칼럼형" 구조입니다.  
(c) CNN의 피라미드 구조를 통합함으로써, 우리는 \*\*피라미드 비전 트랜스포머(PVT)\*\*를 제안합니다. 이는 많은 컴퓨터 비전 작업에 활용될 수 있는 다용도의 백본으로 사용될 수 있으며, ViT의 적용 범위와 영향을 확장합니다. 또한, 우리의 실험은 PVT가 DETR [6]과 쉽게 결합되어 컨볼루션 없이도 종단 간 객체 탐지 시스템을 구축할 수 있음을 보여줍니다.

**1. 서론**  
컨볼루션 신경망(CNN)은 컴퓨터 비전에서 놀라운 성공을 거두며 거의 모든 작업에 다재다능하고 지배적인 접근 방식으로 자리 잡았습니다 [54, 22, 73, 49, 21, 39, 9, 32]. 그럼에도 불구하고, 본 연구는 CNN을 넘어 대체 백본 네트워크를 탐구하고자 하며, 이는 이미지 분류 [12]뿐만 아니라 객체 탐지 [40, 14], 의미론적 분할 [83] 및 인스턴스 분할 [40]과 같은 밀집 예측 작업에도 사용할 수 있습니다.

![](/assets/images/posts/284/img_3.png)

![](/assets/images/posts/284/img_4.png)

**그림 2: COCO val2017에서 객체 탐지를 위해 RetinaNet을 사용하는 다양한 백본들의 성능 비교**  
여기서 "T", "S", "M", "L"은 각각 크기가 작은(tiny), 작은(small), 중간(medium), 큰(large) 우리 PVT 모델을 나타냅니다. 그림에서 알 수 있듯이, 모델의 파라미터 수가 비슷할 때, PVT 변종들은 ResNet(R) [22], ResNeXt(X) [73], ViT [13] 등의 대응 모델들보다 성능이 현저히 우수합니다.

트랜스포머 [64]가 자연어 처리에서 성공을 거둔 것에 영감을 받아, 많은 연구자들이 이를 컴퓨터 비전에 적용하고자 노력해왔습니다. 예를 들어, 일부 연구 [6, 85, 72, 56, 24, 42]는 비전 작업을 학습 가능한 쿼리를 사용한 사전 조회 문제로 모델링하고, 트랜스포머 디코더를 CNN 백본 위에 태스크-특화된 헤드로 사용합니다. 일부 기존 연구들도 CNN에 주의(attention) 모듈 [70, 48, 80]을 결합했지만, 우리가 아는 한, 밀집 예측 작업을 해결하기 위해 컨볼루션 없는 깨끗한 트랜스포머 백본을 탐구한 연구는 드뭅니다.

최근에 Dosovitskiy 등 [13]은 이미지 분류를 위한 비전 트랜스포머(ViT)를 도입했습니다. 이는 CNN 백본을 컨볼루션 없는 모델로 대체하려는 흥미롭고 의미 있는 시도입니다. 그림 1 (b)에 나와 있듯이, ViT는 거친 이미지 패치를 입력으로 하는 칼럼형 구조를 가지고 있습니다. 자원 제약으로 인해, ViT는 세밀한 이미지 패치(예: 패치당 4×4 픽셀)를 사용할 수 없고, 대신 거친 패치(예: 패치당 32×32 픽셀)만을 입력으로 받아들여 낮은 출력 해상도(예: 32-스트라이드)를 초래합니다. ViT는 이미지 분류에 적용 가능하지만, 객체 탐지 및 분할과 같은 픽셀 수준의 밀집 예측에 바로 적용하기에는 어려움이 있습니다. 그 이유는 (1) 출력 특징 맵이 단일 스케일이며 저해상도이고, (2) 일반적인 입력 이미지 크기(예: COCO 벤치마크에서 짧은 모서리가 800 픽셀)에서도 계산 및 메모리 비용이 비교적 높기 때문입니다.

위의 한계를 해결하기 위해, 본 연구에서는 \*\*피라미드 비전 트랜스포머(PVT)\*\*라는 순수 트랜스포머 백본을 제안합니다. 이는 이미지 수준 예측뿐만 아니라 픽셀 수준의 밀집 예측을 포함한 여러 다운스트림 작업에서 CNN 백본의 대안으로 사용될 수 있습니다. 특히, 그림 1 (c)에 설명된 바와 같이, 우리의 PVT는 다음과 같은 방식으로 기존 트랜스포머의 어려움을 극복합니다. (1) 밀집 예측 작업에 필수적인 고해상도 표현을 학습하기 위해 세밀한 이미지 패치(즉, 패치당 4×4 픽셀)를 입력으로 받아들입니다. (2) 네트워크가 깊어질수록 트랜스포머의 시퀀스 길이를 줄이기 위해 점진적으로 축소되는 피라미드를 도입하여 계산 비용을 크게 줄입니다. (3) 고해상도 특징을 학습할 때 자원 소비를 줄이기 위해 공간 축소 주의(SRA) 계층을 채택합니다.

전체적으로, 제안된 PVT는 다음과 같은 장점을 가지고 있습니다. 첫째, 기존의 CNN 백본(그림 1 (a) 참조)은 네트워크 깊이에 따라 증가하는 지역 수용 영역을 가지고 있는 반면, 우리 PVT는 항상 전역 수용 영역을 생성하여 탐지 및 분할 작업에 더 적합합니다. 둘째, ViT(그림 1 (b) 참조)와 비교할 때, 진보된 피라미드 구조 덕분에 우리의 방법은 RetinaNet [39] 및 Mask R-CNN [21]과 같은 대표적인 밀집 예측 파이프라인에 더 쉽게 통합될 수 있습니다. 셋째, 우리는 PVT와 다른 태스크-특화 트랜스포머 디코더를 결합하여 PVT+DETR [6]과 같은 컨볼루션 없는 객체 탐지 파이프라인을 구축할 수 있습니다. 우리가 아는 한, 이는 최초의 완전한 컨볼루션 없는 객체 탐지 파이프라인입니다.

우리의 주요 기여는 다음과 같습니다:

1. 우리는 \*\*피라미드 비전 트랜스포머(PVT)\*\*를 제안하며, 이는 다양한 픽셀 수준의 밀집 예측 작업을 위해 설계된 최초의 순수 트랜스포머 백본입니다. PVT와 DETR을 결합함으로써, 우리는 밀집 앵커나 비최대 억제(NMS)와 같은 수작업 구성 요소 없이 종단 간 객체 탐지 시스템을 구축할 수 있습니다.
2. 우리는 점진적인 축소 피라미드와 공간 축소 주의(SRA)를 설계하여 트랜스포머를 밀집 예측에 포팅할 때 발생하는 많은 어려움을 극복했습니다. 이를 통해 트랜스포머의 자원 소비를 줄이고, PVT가 멀티스케일 및 고해상도 특징을 학습하는 데 유연해지도록 했습니다.
3. 우리는 제안된 PVT를 이미지 분류, 객체 탐지, 인스턴스 및 의미론적 분할 등 여러 다른 작업에 대해 평가하고, 이를 인기 있는 ResNet [22] 및 ResNeXt [73]과 비교했습니다. 그림 2에 제시된 바와 같이, 우리 PVT는 파라미터 규모가 다르더라도 기존 기술보다 일관되게 성능을 개선했습니다. 예를 들어, 유사한 파라미터 수를 사용하여 RetinaNet [39]으로 객체 탐지를 수행할 때, PVT-Small은 COCO val2017에서 40.4 AP를 달성하며 ResNet50보다 4.1 포인트 더 높은 성능을 보였습니다 (40.4 vs. 36.3). 또한, PVT-Large는 ResNeXt101-64x4d보다 1.6 포인트 더 높은 42.6 AP를 달성했으며, 파라미터는 30% 적습니다.

**2. 관련 연구**  
**2.1 CNN 백본**  
CNN은 시각 인식 분야에서 딥 뉴럴 네트워크의 주축 역할을 합니다. 표준 CNN은 처음에는 손글씨 숫자를 구분하기 위해 [34]에서 도입되었습니다. 이 모델에는 특정 수용 영역을 가진 컨볼루션 커널이 포함되어 있어 유리한 시각적 컨텍스트를 포착합니다. 변환 불변성을 제공하기 위해, 컨볼루션 커널의 가중치는 전체 이미지 공간에 대해 공유됩니다. 최근에는 계산 자원의 급속한 발전(예: GPU)과 대규모 이미지 분류 데이터셋(예: ImageNet [51])에서 스택된 컨볼루션 블록을 성공적으로 훈련하는 것이 가능해졌습니다 [33, 54]. 예를 들어, GoogLeNet [59]은 여러 커널 경로를 포함한 컨볼루션 연산자가 매우 경쟁력 있는 성능을 달성할 수 있음을 보여주었습니다. 멀티 경로 컨볼루션 블록의 효과는 Inception 시리즈 [60, 58], ResNeXt [73], DPN [10], MixNet [65], SKNet [36]에서 추가로 입증되었습니다. 또한, ResNet [22]은 컨볼루션 블록에 스킵 연결을 도입하여 매우 깊은 네트워크를 생성/훈련하는 것이 가능해졌고, 컴퓨터 비전 분야에서 인상적인 결과를 얻었습니다. DenseNet [25]은 각 컨볼루션 블록을 모든 이전 블록과 연결하는 밀집 연결 토폴로지를 도입했습니다. 더 최근의 발전은 리뷰 논문 [31, 53]에서 확인할 수 있습니다.

완전히 성숙한 CNN과 달리, 비전 트랜스포머 백본은 아직 초기 개발 단계에 있습니다. 본 연구에서는 대부분의 비전 작업에 적합한 새로운 다용도 트랜스포머 백본을 설계함으로써 비전 트랜스포머의 범위를 확장하고자 합니다.

**2.2 밀집 예측 작업**  
**기초 개념**: 밀집 예측 작업은 특징 맵에서 픽셀 수준의 분류 또는 회귀를 수행하는 것을 목표로 합니다. 객체 탐지와 의미론적 분할은 대표적인 밀집 예측 작업입니다.

**객체 탐지**: 딥 러닝 시대에서 CNN [34]은 객체 탐지를 위한 주요 프레임워크로 자리 잡았으며, 여기에는 단일 단계 탐지기(예: SSD [43], RetinaNet [39], FCOS [62], GFL [37, 35], PolarMask [71], OneNet [55])와 다중 단계 탐지기(Faster R-CNN [49], Mask R-CNN [21], Cascade R-CNN [4], Sparse R-CNN [57])가 포함됩니다. 이러한 객체 탐지기들은 대부분 높은 해상도 또는 다중 스케일 특징 맵을 기반으로 좋은 탐지 성능을 얻습니다. 최근에는 DETR [6]과 변형된 DETR [85]이 CNN 백본과 트랜스포머 디코더를 결합하여 종단 간 객체 탐지기를 구축했습니다. 이들 역시 정확한 객체 탐지를 위해 높은 해상도 또는 다중 스케일 특징 맵을 필요로 합니다.

**의미론적 분할**: CNN은 의미론적 분할에서도 중요한 역할을 합니다. 초기에는 FCN [44]이 완전 컨볼루션 아키텍처를 도입하여 임의의 크기의 이미지에 대해 공간적 분할 맵을 생성했습니다. 이후 Noh 등 [47]은 디컨볼루션 연산을 도입하여 PASCAL VOC 2012 데이터셋 [52]에서 인상적인 성능을 달성했습니다. FCN에서 영감을 받아, U-Net [50]이 의료 이미지 분할 분야에서 도입되었으며, 동일한 공간 크기의 저수준 및 고수준 특징 맵 간의 정보 흐름을 연결했습니다. 보다 풍부한 전역 컨텍스트 표현을 탐구하기 위해 Zhao 등 [81]은 다양한 풀링 스케일에 대한 피라미드 풀링 모듈을 설계했으며, Kirillov 등 [32]은 FPN [38]을 기반으로 한 경량의 분할 헤드인 Semantic FPN을 개발했습니다. 마지막으로, DeepLab 시리즈 [8, 41]은 수용 영역을 확장하면서도 특징 맵 해상도를 유지하기 위해 확장 컨볼루션을 적용했습니다. 객체 탐지 방법과 유사하게, 의미론적 분할 모델도 높은 해상도 또는 다중 스케일 특징 맵에 의존합니다.

![](/assets/images/posts/284/img_5.png)

**그림 3**: **피라미드 비전 트랜스포머(PVT)의 전체 아키텍처**. 전체 모델은 네 개의 스테이지로 나뉘며, 각 스테이지는 패치 임베딩 레이어와 Li-층 트랜스포머 인코더로 구성됩니다. 피라미드 구조를 따르며, 네 개의 스테이지의 출력 해상도는 높은 (4-스트라이드)에서 낮은 (32-스트라이드)으로 점진적으로 축소됩니다.

**2.3 시각에서의 셀프 어텐션과 트랜스포머**  
컨볼루션 필터의 가중치는 일반적으로 훈련 후 고정되기 때문에, 서로 다른 입력에 동적으로 적응할 수 없습니다. 이러한 문제를 완화하기 위해 **동적 필터** [30] 또는 **셀프 어텐션** [64]을 사용하는 다양한 방법이 제안되었습니다. **논-로컬 블록** [70]은 공간과 시간에서 장거리 의존성을 모델링하려고 시도했으며, 이는 정확한 비디오 분류에 유리하다는 것이 입증되었습니다. 그러나 논-로컬 연산자는 높은 계산 및 메모리 비용 문제를 겪습니다. **크리스-크로스(Criss-cross)** [26]는 크리스-크로스 경로를 통해 희소한 어텐션 맵을 생성함으로써 복잡성을 줄였습니다. **Ramachandran 등** [48]은 **독립적인 셀프 어텐션**을 제안하여 컨볼루션 층을 지역 셀프 어텐션 유닛으로 대체했습니다. **AANet** [3]은 셀프 어텐션과 컨볼루션 연산을 결합할 때 경쟁력 있는 결과를 달성했습니다. **LambdaNetworks** [2]는 효율적인 셀프 어텐션인 **람다 레이어**를 사용하여 CNN의 컨볼루션을 대체했습니다. **DETR** [6]은 트랜스포머 디코더를 사용해 객체 탐지를 학습 가능한 쿼리를 통한 종단 간 사전 조회 문제로 모델링하여, \*\*비최대 억제(NMS)\*\*와 같은 수작업 프로세스의 필요성을 성공적으로 제거했습니다. **DETR** 기반으로, **변형된 DETR** [85]은 변형 어텐션 레이어를 추가하여 희소한 컨텍스트 요소들에 집중함으로써 더 빠른 수렴과 더 나은 성능을 얻었습니다. 최근에는 **비전 트랜스포머(ViT)** [13]이 이미지를 패치들의 시퀀스로 취급하여 이미지 분류를 위해 순수한 트랜스포머 [64] 모델을 사용했습니다. **DeiT** [63]은 **ViT**를 새로운 **증류(distillation)** 방법을 사용해 확장했습니다. 이전 모델들과 달리, 본 연구에서는 트랜스포머에 **피라미드 구조**를 도입하여, 특정 작업의 헤드나 이미지 분류 모델이 아닌 밀집 예측 작업을 위한 순수한 트랜스포머 백본을 제시합니다.

**3. 피라미드 비전 트랜스포머 (PVT)**  
**3.1 전체 아키텍처**  
우리의 목표는 피라미드 구조를 트랜스포머 프레임워크에 도입하여, 밀집 예측 작업(예: 객체 탐지 및 의미론적 분할)을 위한 다중 스케일 특징 맵을 생성할 수 있도록 하는 것입니다. PVT의 개요는 그림 3에 나와 있습니다. CNN 백본 [22]과 유사하게, 우리 방법은 서로 다른 스케일의 특징 맵을 생성하는 네 개의 스테이지로 구성됩니다. 모든 스테이지는 패치 임베딩 레이어와 L\_i 트랜스포머 인코더 레이어로 구성된 유사한 아키텍처를 공유합니다.

첫 번째 스테이지에서, 크기가 H x W x 3인 입력 이미지를 먼저 H x W / 4^2 패치로 나눕니다. (ResNet에서처럼, 출력 특징 맵의 가장 높은 해상도를 4-스트라이드로 유지합니다.) 각 패치의 크기는 4 x 4 x 3입니다. 그런 다음, 평탄화된 패치를 선형 투영에 전달하여 크기가 H x W / 4^2 x C\_1인 임베딩된 패치를 얻습니다. 이후, 위치 임베딩과 함께 임베딩된 패치들을 L\_1 레이어를 가진 트랜스포머 인코더에 통과시키며, 출력은 H/4 x W/4 x C\_1 크기의 특징 맵 F\_1으로 재구성됩니다. 같은 방식으로, 이전 스테이지의 특징 맵을 입력으로 사용하여, F\_2, F\_3, F\_4라는 특징 맵을 얻습니다. 이들의 스트라이드는 입력 이미지에 대해 각각 8, 16, 32 픽셀입니다. 특징 피라미드 {F\_1, F\_2, F\_3, F\_4}와 함께, 우리 방법은 이미지 분류, 객체 탐지, 의미론적 분할을 포함한 대부분의 다운스트림 작업에 쉽게 적용될 수 있습니다.

**3.2 트랜스포머를 위한 특징 피라미드**  
다중 스케일 특징 맵을 얻기 위해 서로 다른 컨볼루션 스트라이드를 사용하는 CNN 백본 네트워크 [54, 22]와 달리, 우리 PVT는 패치 임베딩 레이어를 통해 특징 맵의 스케일을 제어하는 점진적 축소 전략을 사용합니다.

여기서 i번째 스테이지의 패치 크기를 P\_i라고 표시합니다. 스테이지 i의 시작에서, 입력 특징 맵 F\_{i-1} ∈ {R}^{H\_{i-1} x W\_{i-1} x C\_{i-1}}을 H\_{i-1} W\_{i-1} / P\_i^2 개의 패치로 균등하게 나누고, 각 패치는 평탄화되어 C\_i차원의 임베딩으로 투영됩니다. 선형 투영 후, 임베딩된 패치의 형태는 H\_{i-1}/P\_i x W\_{i-1}/P\_i x C\_i로 볼 수 있으며, 높이와 너비는 입력보다 P\_i배 작아집니다.

이와 같은 방식으로 각 스테이지에서 특징 맵의 스케일을 유연하게 조정할 수 있으며, 이를 통해 트랜스포머를 위한 특징 피라미드를 구성할 수 있습니다.

**3.3 트랜스포머 인코더**

![](/assets/images/posts/284/img_6.png)

**그림 4**: **다중 헤드 어텐션(MHA) vs. 공간 축소 어텐션(SRA)**. 공간 축소 연산을 통해, 우리의 SRA의 계산/메모리 비용은 MHA보다 훨씬 낮습니다.

스테이지 i의 트랜스포머 인코더는 L\_i개의 인코더 레이어로 구성되며, 각 레이어는 어텐션 레이어와 피드포워드 레이어 [64]로 이루어져 있습니다. PVT는 높은 해상도(예: 4-스트라이드)의 특징 맵을 처리해야 하므로, 우리는 인코더에서 전통적인 다중 헤드 어텐션(MHA) 레이어 [64]를 대체하기 위해 **공간 축소 어텐션(SRA)** 레이어를 제안합니다.

MHA와 유사하게, 우리의 SRA는 쿼리 Q, 키 K, 값 V를 입력으로 받고, 세밀화된 특징을 출력합니다. 차이점은 SRA가 어텐션 연산 전에 K와 V의 공간 스케일을 줄인다는 것입니다(그림 4 참조), 이는 계산/메모리 오버헤드를 크게 줄입니다. 스테이지 i에서 SRA의 세부 사항은 다음과 같이 공식화될 수 있습니다:

![](/assets/images/posts/284/img_7.png)

여기서 Concat(⋅)은 [64]에서처럼 결합(concatenation) 연산을 나타냅니다. W^Q\_j ∈  {R}^{C\_i x d\_{head}}, W^K\_j ∈  {R}^{C\_i x d\_{head}}, W^V\_j ∈  {R}^{C\_i x d\_{head}}, 그리고 W\_O ∈  {R}^{C\_i x C\_i}는 선형 투영 매개변수입니다. N\_i는 스테이지 i의 어텐션 레이어에서의 헤드 수입니다. 따라서 각 헤드의 차원(d\_{head})은 C\_i / N\_i입니다. SR(⋅)은 입력 시퀀스(예: K 또는 V)의 공간 차원을 줄이는 연산으로, 다음과 같이 표현됩니다:

![](/assets/images/posts/284/img_8.png)

여기서 {x} ∈ {R}^{(H\_i W\_i) x C\_i}는 입력 시퀀스를 나타내며, R\_i는 스테이지 i의 어텐션 레이어의 축소 비율을 나타냅니다. ({x}, R\_i)$는 입력 시퀀스 {x}를 크기 H\_i W\_i / R\_i^2 x (R\_i^2 C\_i)로 재구성하는 연산입니다. W\_S ∈ {R}^{(R\_i^2 C\_i) x C\_i}$는 입력 시퀀스의 차원을 C\_i로 줄이는 선형 투영입니다. Norm(⋅)은 레이어 정규화 [1]을 의미합니다. 원래 트랜스포머 [64]에서와 같이, 우리의 어텐션 연산 Attention(⋅)은 다음과 같이 계산됩니다:

![](/assets/images/posts/284/img_9.png)

이러한 수식을 통해, 우리의 어텐션 연산의 계산/메모리 비용이 MHA보다 R\_i^2배 낮다는 것을 알 수 있으며, 따라서 SRA는 제한된 자원으로 더 큰 입력 특징 맵/시퀀스를 처리할 수 있습니다.

**3.4 모델 세부 사항**  
요약하면, 우리의 방법의 하이퍼파라미터는 다음과 같습니다:

- P\_i: 스테이지 i의 패치 크기;
- C\_i: 스테이지 i의 출력 채널 수;
- L\_i: 스테이지 i의 인코더 레이어 수;
- R\_i: 스테이지 i의 SRA의 축소 비율;
- N\_i: 스테이지 i의 SRA의 헤드 수;
- E\_i: 스테이지 i의 피드포워드 레이어 [64]의 확장 비율;

ResNet [22]의 설계 규칙을 따라, 우리는 (1) 얕은 스테이지에서는 작은 출력 채널 수를 사용하고, (2) 주요 계산 자원을 중간 스테이지에 집중시킵니다.

논의의 예시를 제공하기 위해, 우리는 서로 다른 크기의 PVT 모델 시리즈인 PVT-Tiny, -Small, -Medium, -Large를 표 1에서 설명하며, 이들의 파라미터 수는 각각 ResNet18, 50, 101, 152와 비슷합니다. 이러한 모델들을 특정 다운스트림 작업에 사용하는 더 자세한 내용은 섹션 4에서 소개될 것입니다.

![](/assets/images/posts/284/img_10.png)

**표 1**: PVT 시리즈의 세부 설정. 설계는 ResNet [22]의 두 가지 규칙을 따릅니다: (1) 네트워크 깊이가 깊어짐에 따라 숨겨진 차원이 점진적으로 증가하고 출력 해상도가 점진적으로 축소됨; (2) 주요 계산 자원이 스테이지 3에 집중됨.

**3.5 논의**  
우리 모델과 가장 관련된 연구는 ViT [13]입니다. 여기서 두 모델 간의 관계와 차이점을 논의합니다. 먼저, PVT와 ViT 모두 컨볼루션 없이 순수한 트랜스포머 모델입니다. 이들의 주요 차이점은 **피라미드 구조**에 있습니다. 전통적인 트랜스포머 [64]와 유사하게, ViT의 출력 시퀀스 길이는 입력과 동일하며, 이는 ViT의 출력이 **단일 스케일**임을 의미합니다(그림 1 (b) 참조). 또한, 자원 제약으로 인해 ViT의 입력은 \*\*거친 그레인(예: 패치 크기 16 또는 32 픽셀)\*\*이므로 출력 해상도도 상대적으로 낮습니다(예: 16-스트라이드 또는 32-스트라이드). 결과적으로, ViT를 높은 해상도 또는 다중 스케일 특징 맵이 필요한 **밀집 예측 작업**에 직접 적용하기는 어렵습니다.

우리의 PVT는 **점진적 축소 피라미드**를 도입하여 트랜스포머의 전통을 깹니다. 이는 전통적인 CNN 백본과 마찬가지로 **다중 스케일 특징 맵**을 생성할 수 있습니다. 추가로, 우리는 **고해상도 특징 맵을 처리**하고 계산/메모리 비용을 줄이기 위해 간단하지만 효과적인 어텐션 레이어—SRA를 설계했습니다. 이러한 설계의 이점 덕분에, 우리의 방법은 ViT에 비해 다음과 같은 장점을 가집니다: 1) **더 유연**함—다양한 스테이지에서 서로 다른 스케일/채널의 특징 맵을 생성할 수 있음; 2) **더 다재다능**함—대부분의 다운스트림 작업 모델에 쉽게 플러그 앤 플레이 가능; 3) **계산/메모리 효율성**—더 높은 해상도의 특징 맵이나 더 긴 시퀀스를 처리할 수 있음.

**4. 다운스트림 작업에의 응용**  
**4.1 이미지 수준 예측**  
이미지 분류는 이미지 수준 예측의 가장 고전적인 작업입니다. 논의를 위해, 우리는 서로 다른 크기의 PVT 모델 시리즈(PVT-Tiny, -Small, -Medium, -Large)를 설계하며, 이들의 파라미터 수는 각각 ResNet18, 50, 101, 152와 비슷합니다. PVT 시리즈의 자세한 하이퍼파라미터 설정은 추가 자료(SM)에 제공됩니다.

이미지 분류의 경우, 우리는 ViT [13]와 DeiT [63]를 따라 **학습 가능한 분류 토큰**을 마지막 스테이지의 입력에 추가한 후, 완전 연결(FC) 층을 사용하여 해당 토큰에 대해 분류를 수행합니다.

**4.2 픽셀 수준 밀집 예측**  
이미지 수준 예측 외에도 특징 맵에서 픽셀 수준의 분류 또는 회귀가 필요한 **밀집 예측**도 다운스트림 작업에서 자주 볼 수 있습니다. 여기서는 두 가지 대표적인 작업인 **객체 탐지**와 **의미론적 분할**에 대해 논의합니다.

우리는 PVT 모델을 세 가지 대표적인 밀집 예측 방법에 적용했습니다. 이 방법들은 **RetinaNet** [39], **Mask R-CNN** [21], **Semantic FPN** [32]입니다. **RetinaNet**은 널리 사용되는 단일 단계 탐지기이며, **Mask R-CNN**은 가장 인기 있는 두 단계 인스턴스 분할 프레임워크입니다. **Semantic FPN**은 특수한 연산(예: 팽창 컨볼루션) 없이 기본적인 의미론적 분할 방법입니다. 이러한 방법들을 기준으로 사용하여 서로 다른 백본의 효과를 충분히 검증할 수 있었습니다.

구현 세부 사항은 다음과 같습니다:  
(1) **ResNet**처럼, 우리는 PVT 백본을 **ImageNet**에서 사전 훈련된 가중치로 초기화합니다.  
(2) **출력 특징 피라미드** {F\_1, F\_2, F\_3, F\_4}를 **FPN** [38]의 입력으로 사용한 후, 세밀화된 특징 맵을 이후의 탐지/분할 헤드에 전달합니다.  
(3) **탐지/분할 모델**을 훈련할 때, PVT의 어느 레이어도 동결하지 않습니다.  
(4) **탐지/분할**의 입력은 임의의 모양일 수 있기 때문에, **ImageNet**에서 사전 훈련된 위치 임베딩이 더 이상 의미가 없을 수 있습니다. 따라서, 우리는 입력 해상도에 따라 사전 훈련된 위치 임베딩에 **양선형 보간법**을 수행합니다.

----------------------

양선형 보간법(Bilinear Interpolation)은 이미지나 2차원 데이터의 **크기를 조정**할 때 사용하는 **보간 기법** 중 하나입니다. 주로 이미지의 크기를 늘리거나 줄이는 과정에서 **새로운 픽셀 값을 계산**하기 위해 사용됩니다. 양선형 보간법은 주변 4개의 픽셀 값을 사용하여 새로운 픽셀 값을 **선형으로 보간**하는 방식으로 동작합니다.

----------------------

**5. 실험**  
우리는 PVT를 가장 대표적인 두 개의 CNN 백본인 **ResNet** [22]과 **ResNeXt** [73]와 비교했으며, 이들은 많은 다운스트림 작업의 벤치마크에서 널리 사용됩니다.

**5.1 이미지 분류**  
**설정**. **이미지 분류** 실험은 **ImageNet 2012** 데이터셋 [51]에서 수행되며, 이 데이터셋은 1,000개의 카테고리에서 128만 개의 훈련 이미지와 5만 개의 검증 이미지를 포함합니다. 공정한 비교를 위해 모든 모델은 **훈련 세트**에서 훈련되고, 검증 세트에서 **top-1 오류율**을 보고합니다. 우리는 **DeiT** [63]을 따르며 **랜덤 크로핑**, **랜덤 수평 뒤집기** [59], **레이블 스무딩 정규화** [60], **Mixup** [78], **CutMix** [76], **랜덤 지우기** [82] 등의 데이터 증강을 적용합니다. 훈련 중에는 **AdamW** [46] 옵티마이저를 사용하며, **모멘텀**은 0.9, **미니 배치 크기**는 128, **가중치 감소**는 $5 x 10^{-2}$입니다. 초기 학습률은 $1 x 10^{-3}$로 설정하고, **코사인 스케줄** [45]을 따라 감소시킵니다. 모든 모델은 8개의 V100 GPU에서 처음부터 300 에포크 동안 훈련됩니다. 벤치마크를 위해, 검증 세트에서 **중앙 크롭**을 적용하여 **224×224 패치**를 잘라 분류 정확도를 평가합니다.

![](/assets/images/posts/284/img_11.png)

**표 2: ImageNet 검증 세트에서의 이미지 분류 성능**  
"#Param"은 파라미터의 수를 나타냅니다. "GFLOPs"는 입력 크기 $224 x 224$에서 계산됩니다. "\*"는 원 논문의 전략으로 훈련된 방법의 성능을 나타냅니다.

**결과**. 표 2에서 알 수 있듯이, 우리의 PVT 모델은 유사한 파라미터 수와 계산 자원 하에서 기존의 CNN 백본보다 우수한 성능을 보입니다. 예를 들어, GFLOPs가 대략 유사할 때 PVT-Small의 top-1 오류율은 20.2로 ResNet50 [22]보다 1.3 포인트 낮습니다 (20.2 vs. 21.5). 한편, 유사하거나 더 낮은 복잡도에서도 PVT 모델은 최근에 제안된 트랜스포머 기반 모델(ViT [13]와 DeiT [63])과 비교할 만한 성능을 보입니다 (PVT-Large: 18.3 vs. ViT(DeiT)-Base/16: 18.3). 여기서 우리는 이러한 결과가 우리의 기대 내에 있다고 명확히 합니다. 왜냐하면 피라미드 구조는 밀집 예측 작업에 유리하지만, 이미지 분류에는 큰 개선을 가져오지 않기 때문입니다.

참고로 ViT와 DeiT는 분류 작업에 특화되어 설계되었기 때문에 밀집 예측 작업에는 적합하지 않으며, 밀집 예측 작업에서는 일반적으로 **효과적인 특징 피라미드**가 필요합니다.

**5.2 객체 탐지**  
**설정**. 객체 탐지 실험은 **COCO** 벤치마크 [40]에서 수행되었습니다. 모든 모델은 **COCO train2017** (118k 이미지)에서 훈련되었고, **val2017** (5k 이미지)에서 평가되었습니다. 우리는 두 가지 표준 탐지기인 **RetinaNet** [39]과 **Mask R-CNN** [21]을 사용하여 PVT 백본의 효과를 검증합니다. 훈련 전에, 백본은 **ImageNet**에서 사전 훈련된 가중치로 초기화하고, 새로 추가된 레이어는 **Xavier** [18] 방법으로 초기화합니다. 우리의 모델은 **8개의 V100 GPU**에서 배치 크기 16으로 훈련되며, **AdamW** [46] 옵티마이저를 사용하여 초기 학습률 $1 x 10^{-4}$로 최적화됩니다. 일반적인 관례 [39, 21, 7]를 따라, 모든 탐지 모델을 훈련하기 위해 **1×** 또는 **3×** 훈련 스케줄(즉, 12 또는 36 에포크)을 채택합니다. 훈련 이미지는 짧은 쪽이 800 픽셀로 조정되며, 긴 쪽은 1,333 픽셀을 넘지 않습니다. **3×** 훈련 스케줄을 사용할 때는 입력 이미지의 짧은 쪽을 **[640, 800]** 범위 내에서 무작위로 크기 조정합니다. 테스트 단계에서는 입력 이미지의 짧은 쪽을 800 픽셀로 고정합니다.

![](/assets/images/posts/284/img_12.png)

**표 3**: COCO val2017에서의 객체 탐지 성능. "MS"는 **다중 스케일 훈련** [39, 21]이 사용되었음을 의미합니다.

![](/assets/images/posts/284/img_13.png)

**표 4**: COCO val2017에서의 객체 탐지 및 인스턴스 분할 성능. **APb**와 **APm**은 각각 **경계 상자 AP**와 **마스크 AP**를 나타냅니다.

**결과**. **표 3**에서 알 수 있듯이, **RetinaNet**을 사용한 객체 탐지에서 유사한 파라미터 수 하에서 PVT 기반 모델은 그 대응 모델들을 크게 능가합니다. 예를 들어, **1×** 훈련 스케줄에서 **PVT-Tiny**의 AP는 **ResNet18**보다 4.9 포인트 높습니다 (36.7 vs. 31.8). 또한, **3×** 훈련 스케줄과 다중 스케일 훈련을 사용했을 때 **PVT-Large**는 **43.4**의 최고 AP를 기록하여 **ResNeXt101-64x4d**를 능가합니다 (43.4 vs. 41.8), 이때 파라미터 수는 30% 적습니다. 이러한 결과는 PVT가 객체 탐지에서 CNN 백본의 좋은 대안이 될 수 있음을 나타냅니다.

비슷한 결과가 **Mask R-CNN**을 기반으로 한 인스턴스 분할 실험에서도 나타났습니다(표 4 참조). **1×** 훈련 스케줄에서 **PVT-Tiny**는 **35.1**의 마스크 AP(**APm**)를 달성했으며, 이는 **ResNet18**보다 **3.9** 포인트 높고 (35.1 vs. 31.2), **ResNet50**보다도 **0.7** 포인트 더 높습니다 (35.1 vs. 34.4). **PVT-Large**가 달성한 최고 **APm**은 **40.7**로, **ResNeXt101-64x4d**보다 **1.0** 포인트 높으며 (40.7 vs. 39.7), 파라미터 수는 20% 적습니다.

**5.3 의미론적 분할**

![](/assets/images/posts/284/img_14.png)

**표 5**: **ADE20K 검증 세트에서의 서로 다른 백본의 의미론적 분할 성능**. "GFLOPs"는 입력 크기 512 x 512에서 계산됩니다. "\*"는 320K 반복 훈련과 다중 스케일 뒤집기 테스트를 나타냅니다.

**설정**. 우리는 의미론적 분할 성능을 벤치마크하기 위해 **ADE20K** [83]라는 도전적인 장면 파싱 데이터셋을 선택했습니다. ADE20K는 150개의 세밀한 의미론적 카테고리를 포함하며, 훈련용 20,210개, 검증용 2,000개, 테스트용 3,352개의 이미지를 제공합니다. 우리는 **팽창 컨볼루션** [74]이 없는 간단한 분할 방법인 **Semantic FPN** [32]을 기반으로 PVT 백본을 평가합니다. 훈련 단계에서 백본은 **ImageNet** [12]에서 사전 훈련된 가중치로 초기화되며, 새로 추가된 다른 레이어는 **Xavier** [18]로 초기화됩니다. 우리의 모델은 **AdamW** [46] 옵티마이저를 사용해 초기 학습률 1e-4로 최적화됩니다. 일반적인 관례 [32, 8]를 따라, 우리는 **4개의 V100 GPU**에서 배치 크기 16으로 **80k 반복** 동안 모델을 훈련합니다. 학습률은 지수 0.9의 다항식 감소 스케줄을 따라 감소합니다. 우리는 훈련을 위해 이미지를 무작위로 크기 조정하고 \*\*512 x 512\*\*로 크롭하며, 테스트 시 짧은 쪽을 **512 픽셀**로 재조정합니다.

**5.4 순수 트랜스포머 탐지 및 분할**

![](/assets/images/posts/284/img_15.png)

**표 6**: 순수 트랜스포머 객체 탐지 파이프라인의 성능. 우리는 **PVT**와 **DETR** [6]을 결합하여 순수 트랜스포머 탐지기를 구축했으며, AP가 **ResNet50** [22] 기반 원본 DETR보다 **2.4** 포인트 높습니다.

**PVT+DETR**. 컨볼루션이 없는 한계를 달성하기 위해, 우리는 PVT와 트랜스포머 기반 탐지 헤드인 **DETR** [6]을 간단히 결합하여 **순수 트랜스포머 객체 탐지 파이프라인**을 구축했습니다. 우리는 **COCO train2017**에서 모델을 **50 에포크** 동안 훈련하며, 초기 학습률은 \*\*$1 x 10^{-4}$\*\*입니다. 학습률은 **33번째 에포크**에서 10으로 나뉩니다. 우리는 데이터 증강으로 **랜덤 뒤집기**와 **다중 스케일 훈련**을 사용했습니다. 다른 모든 실험 설정은 **섹션 5.2**와 동일합니다. **표 6**에 보고된 바와 같이, PVT 기반 DETR은 **COCO val2017**에서 **34.7 AP**를 달성하여, ResNet50 기반 원본 DETR보다 **2.4** 포인트 높은 성능을 보였습니다 (34.7 vs. 32.3). 이러한 결과는 순수 트랜스포머 탐지기가 객체 탐지 작업에서도 잘 작동할 수 있음을 증명합니다.

![](/assets/images/posts/284/img_16.png)

**표 7**: 순수 트랜스포머 의미론적 분할 파이프라인의 성능. 우리는 **PVT**와 **Trans2Seg** [72]를 결합하여 순수 트랜스포머 탐지기를 구축했으며, **ResNet50-d16+Trans2Seg**보다 **2.9%**, \*\*ResNet50-d8+DeeplabV3+\*\*보다 **1.1%** 높은 성능을 보였으며, **GFLOPs**는 더 낮았습니다. "d8"과 "d16"은 각각 **팽창률 8**과 **16**을 의미합니다.

**PVT+Trans2Seg**. 우리는 PVT와 **Trans2Seg** [72]라는 트랜스포머 기반 분할 헤드를 결합하여 **순수 트랜스포머 의미론적 분할 모델**을 구축했습니다. **섹션 5.3**의 실험 설정에 따라, **ADE20K** [83]에서 **40k 반복** 훈련, 단일 스케일 테스트를 수행하고, **표 7**에서 **ResNet50+Trans2Seg** [72] 및 **DeeplabV3+** [9]와 비교했습니다. 우리는 **PVT-Small+Trans2Seg**가 **42.6 mIoU**를 달성하여 **ResNet50-d8+DeeplabV3+** (41.5)보다 우수한 성능을 보였음을 발견했습니다. 참고로 \*\*ResNet50-d8+DeeplabV3+\*\*는 팽창 컨볼루션의 높은 계산 비용으로 인해 **120.5 GFLOPs**를 사용하지만, 우리의 방법은 **31.6 GFLOPs**로 **4배 적습니다**. 또한, **PVT-Small+Trans2Seg**는 **ResNet50-d16+Trans2Seg**보다 더 나은 성능을 보였으며 (mIoU: 42.6 vs. 39.7, GFLOPs: 31.6 vs. 79.3), 이는 순수 트랜스포머 분할 네트워크가 실현 가능함을 증명합니다.

**5.5 소거 연구**

**설정**. 우리는 **ImageNet** [12]과 **COCO** [40] 데이터셋에서 소거 연구를 수행했습니다. ImageNet에 대한 실험 설정은 **섹션 5.1**의 설정과 동일합니다. COCO의 경우, 모든 모델은 **1× 훈련 스케줄** (즉, 12 에포크)로 훈련되었으며, 다중 스케일 훈련은 사용하지 않았습니다. 다른 설정은 **섹션 5.2**를 따릅니다.

![](/assets/images/posts/284/img_17.png)

**표 8**: **RetinaNet**을 사용한 객체 탐지에서 **ViT**와 **우리 PVT**의 성능 비교. **ViT-Small/4**는 작은 패치 크기(즉, 패치당 $4 x 4$)로 인해 GPU 메모리가 부족합니다. **ViT-Small/32**는 **COCO val2017**에서 **31.7 AP**를 얻었으며, 이는 PVT-Small보다 **8.7 포인트 낮습니다**.

**피라미드 구조**. 트랜스포머를 밀집 예측 작업에 적용할 때 **피라미드 구조**는 매우 중요합니다. **ViT** (그림 1 (b) 참조)는 칼럼형 프레임워크로, 출력이 단일 스케일입니다. 이는 거친 이미지 패치(예: 패치당 $32 x 32$ 픽셀)를 입력으로 사용할 때 출력 특징 맵의 해상도가 낮아져, 탐지 성능이 낮아지는 결과를 초래합니다(예: COCO val2017에서 **31.7 AP**, **표 8** 참조). **RetinaNet**에 ViT를 적용하기 위해 우리는 **ViT-Small/32**의 **2, 4, 6, 8번째 레이어**에서 특징을 추출하고, 이를 서로 다른 스케일로 보간했습니다. 그러나 **PVT**와 같이 세밀한 이미지 패치(예: 패치당 $4 x 4$ 픽셀)를 입력으로 사용할 때 **ViT**는 \*\*GPU 메모리(32G)\*\*가 부족해집니다. 우리 방법은 **점진적 축소 피라미드**를 통해 이 문제를 피합니다. 구체적으로, 우리의 모델은 얕은 스테이지에서 고해상도 특징 맵을 처리하고, 깊은 스테이지에서 저해상도 특징 맵을 처리할 수 있습니다. 따라서, **COCO val2017**에서 **40.4 AP**라는 유망한 성능을 얻었으며, 이는 **ViT-Small/32**보다 **8.7 포인트** 높습니다 (40.4 vs. 31.7).

![](/assets/images/posts/284/img_18.png)

**그림 5**: COCO val2017에서 서로 다른 백본 설정 하에서 **RetinaNet**의 AP 곡선. 상단: **ImageNet**에서 사전 훈련된 가중치 사용 vs. 랜덤 초기화. 하단: **PVT-S** vs. **R50** [22].

![](/assets/images/posts/284/img_19.png)

**표 9**: **더 깊게 vs. 더 넓게**. "Top-1"은 ImageNet 검증 세트에서의 top-1 오류율을 나타냅니다. "AP"는 COCO val2017에서의 경계 상자 AP를 나타냅니다. 깊은 모델(즉, **PVT-Medium**)은 유사한 파라미터 수 하에서 넓은 모델(즉, **PVT-Small-Wide**)보다 더 나은 성능을 보입니다.

**더 깊게 vs. 더 넓게**. CNN 백본이 더 깊어야 할지 더 넓어야 할지에 대한 문제는 이전 연구 [22, 77]에서 광범위하게 논의되었습니다. 여기서 우리는 이 문제를 **PVT**에서도 탐구합니다. 공정한 비교를 위해, **PVT-Small**의 숨겨진 차원 {C\_1, C\_2, C\_3, C\_4}을 스케일 팩터 **1.4**로 곱해 깊은 모델(즉, **PVT-Medium**)과 유사한 파라미터 수를 갖도록 했습니다. **표 9**에 나와 있듯이, 깊은 모델(즉, **PVT-Medium**)은 **ImageNet**과 **COCO**에서 모두 넓은 모델(즉, **PVT-Small-Wide**)보다 일관되게 더 좋은 성능을 보입니다. 따라서 PVT의 설계에서 더 깊게 가는 것이 더 넓게 가는 것보다 더 효과적입니다. 이러한 관찰을 바탕으로 **표 1**에서는 모델의 깊이를 증가시켜 다양한 크기의 **PVT 모델**을 개발했습니다.

**사전 훈련된 가중치**. 대부분의 밀집 예측 모델(예: **RetinaNet** [39])은 **ImageNet**에서 사전 훈련된 백본 가중치에 의존합니다. 우리는 이 문제를 **PVT**에서도 논의했습니다. **그림 5** 상단에서, 우리는 사전 훈련된 가중치가 있는(빨간색 곡선)과 없는(파란색 곡선) **RetinaNet-PVT-Small**의 검증 AP 곡선을 그렸습니다. 우리는 사전 훈련된 가중치가 있는 모델이 없는 모델보다 더 잘 수렴하며, 최종 AP 간의 차이가 **1×** 훈련 스케줄에서는 **13.8**, **3×** 훈련 스케줄 및 다중 스케일 훈련에서는 **8.4**에 달하는 것을 발견했습니다. 따라서 **CNN 기반 모델**처럼, 사전 훈련된 가중치는 **PVT 기반 모델**이 더 빠르고 잘 수렴하도록 도울 수 있습니다. 또한, **그림 5** 하단에서 **PVT 기반 모델**(빨간색 곡선)의 수렴 속도가 **ResNet 기반 모델**(초록색 곡선)보다 더 빠른 것도 확인할 수 있습니다.

![](/assets/images/posts/284/img_20.png)

**표 10**: **PVT vs. CNN w/ 논-로컬**. **APm**은 마스크 AP를 나타냅니다. 유사한 파라미터 수와 GFLOPs에서, 우리 **PVT**는 **논-로컬이 포함된 CNN 백본**(ResNet50+GC r4)보다 **1.6 APm** 더 높습니다 (37.8 vs. 36.2).

**PVT vs. "논-로컬이 포함된 CNN"**. **전역 수용 영역**을 얻기 위해, **GCNet** [5]와 같은 잘 설계된 CNN 백본은 **논-로컬 블록**을 CNN 프레임워크에 통합합니다. 여기서는 **인스턴스 분할**을 위해 **Mask R-CNN**을 사용하여 우리의 \*\*PVT(순수 트랜스포머)\*\*와 \*\*GCNet(CNN w/ 논-로컬)\*\*의 성능을 비교합니다. **표 10**에 보고된 바와 같이, **PVT-Small**은 **ResNet50+GC r4** [5]보다 **APm**에서 **1.6** 포인트 (37.8 vs. 36.2), **APm75**에서 **2.0** 포인트 (40.3 vs. 38.3) 더 높은 성능을 기록했습니다. 이에 대한 두 가지 가능한 이유는 다음과 같습니다:

(1) 비록 단일 전역 어텐션 레이어(예: 논-로컬 [70] 또는 다중 헤드 어텐션(MHA) [64])가 **전역 수용 영역 특징**을 획득할 수 있지만, 모델 성능은 모델이 깊어질수록 계속해서 향상됩니다. 이는 **다중 MHA를 스택하여** 특징의 표현 능력을 더욱 향상시킬 수 있음을 나타냅니다. 따라서, **더 많은 전역 어텐션 레이어**를 가진 순수 트랜스포머 백본인 **PVT**는 논-로컬 블록이 장착된 CNN 백본(예: GCNet)보다 더 나은 성능을 보이는 경향이 있습니다.

(2) **일반 컨볼루션**은 **공간 어텐션 메커니즘** [84]의 특별한 인스턴스 형태로 간주될 수 있습니다. 다시 말해, **MHA의 형식**이 일반 컨볼루션보다 더 유연합니다. 예를 들어, 다른 입력에 대해 **컨볼루션의 가중치**는 고정되지만, **MHA의 어텐션 가중치**는 입력에 따라 **동적으로 변화**합니다. 따라서, MHA 레이어로 가득 찬 순수 트랜스포머 백본이 학습하는 특징은 더 유연하고 표현력이 풍부할 수 있습니다.

**계산 오버헤드**

![](/assets/images/posts/284/img_21.png)

**표 11**: 서로 다른 입력 크기에서의 지연 시간과 AP. "Scale"과 "Time"은 입력 크기와 이미지당 시간 비용을 나타냅니다. 짧은 쪽이 640 픽셀일 때, **PVT-Small+RetinaNet**은 **ResNet50+RetinaNet**보다 낮은 **GFLOPs**와 시간 비용(V100 GPU 기준)을 가지면서도 **2.4 포인트 높은 AP**를 얻습니다 (38.7 vs. 36.3).

입력 크기가 증가함에 따라, **PVT**의 GFLOPs 증가율은 **ResNet** [22]보다 크지만 **ViT** [13]보다는 낮습니다(그림 6 참조). 그러나 입력 크기가 **640×640 픽셀**을 초과하지 않을 경우, **PVT-Small**과 **ResNet50**의 GFLOPs는 비슷합니다. 이는 **PVT**가 중간 해상도 입력을 사용하는 작업에 더 적합함을 의미합니다.

**COCO**에서 입력 이미지의 짧은 쪽은 **800 픽셀**입니다. 이 조건에서 **PVT-Small**을 기반으로 한 **RetinaNet**의 추론 속도는 **ResNet50** 기반 모델보다 느리다는 것이 **표 11**에 보고되었습니다. (1) 이 문제에 대한 직접적인 해결책은 **입력 크기를 줄이는 것**입니다. 입력 이미지의 짧은 쪽을 **640 픽셀**로 줄일 경우, **PVT-Small** 기반 모델은 **ResNet50** 기반 모델보다 빠르게 실행되며(51.7ms vs. 55.9ms), **2.4** 더 높은 **AP**를 기록합니다 (38.7 vs. 36.3). (2) 또 다른 해결책은 **계산 복잡도가 낮은 셀프 어텐션 레이어를 개발하는 것**입니다. 이는 탐구할 가치가 있는 방향이며, 최근에 **PVTv2** [67]라는 해결책을 제안했습니다.

**탐지 및 분할 결과**. **그림 7**에서는 **COCO val2017** [40]에서의 객체 탐지 및 인스턴스 분할 결과와 **ADE20K** [83]에서의 의미론적 분할 결과를 일부 제시합니다. 이러한 결과는 **컨볼루션 없는 순수 트랜스포머 백본**(즉, PVT)이 밀집 예측 모델(예: **RetinaNet** [39], **Mask R-CNN** [21], **Semantic FPN** [32])에 쉽게 통합될 수 있으며, **고품질 결과**를 얻을 수 있음을 나타냅니다.

![](/assets/images/posts/284/img_22.png)

**그림 6**: 서로 다른 입력 크기에서 모델의 **GFLOPs**. **GFLOPs 증가율**: **ViT-Small/16** [13] > **ViT-Small/32** [13] > **PVT-Small** (우리의 모델) > **ResNet50** [22]. 입력 크기가 **640×640**보다 작을 때, **PVT-Small**과 **ResNet50** [22]의 GFLOPs는 비슷합니다.

![](/assets/images/posts/284/img_23.png)

**그림 7**: **COCO val2017** [40]에서의 객체 탐지 및 인스턴스 분할 결과, 그리고 **ADE20K** [83]에서의 의미론적 분할 결과. 결과는 (왼쪽부터 오른쪽 순으로) **PVT-Small** 기반의 **RetinaNet** [39], **Mask R-CNN** [21], **Semantic FPN** [32]에 의해 각각 생성되었습니다.

**6. 결론 및 미래 작업**  
우리는 객체 탐지 및 의미론적 분할과 같은 밀집 예측 작업을 위한 순수 트랜스포머 백본인 **PVT**를 소개합니다. 우리는 **점진적 축소 피라미드**와 **공간 축소 어텐션 레이어**를 개발하여 제한된 계산/메모리 자원 하에서 고해상도 및 다중 스케일 특징 맵을 얻습니다. 객체 탐지 및 의미론적 분할 벤치마크에서 수행한 광범위한 실험은 유사한 파라미터 수에서 **PVT**가 잘 설계된 CNN 백본보다 강력함을 검증합니다.

PVT가 **CNN 백본**(예: ResNet, ResNeXt)의 대안이 될 수 있지만, **SE** [23], **SK** [36], **팽창 컨볼루션** [74], **모델 가지치기** [20], **NAS** [61]와 같은 CNN용으로 설계된 특정 모듈과 연산은 이 작업에서 고려되지 않았습니다. 또한, 수년간의 급속한 발전을 통해 **Res2Net** [17], **EfficientNet** [61], **ResNeSt** [79]와 같은 많은 잘 설계된 CNN 백본들이 존재하게 되었습니다. 반면, **컴퓨터 비전에서의 트랜스포머 기반 모델**은 아직 초기 개발 단계에 있습니다. 따라서 우리는 **OCR** [68, 66, 69], **3D** [28, 11, 27], **의료 영상 분석** [15, 16, 29]과 같은 많은 잠재적인 기술 및 응용이 미래에 탐구될 수 있다고 믿으며, **PVT**가 좋은 출발점이 되기를 바랍니다.

**감사의 말**  
이 연구는 중국 자연과학재단(Grant 61672273, Grant 61832008), 장쑤성 유망 청년 과학 재단(Grant BK20160021), 중국 박사 후 혁신 인재 지원 프로그램(Grant BX20200168, 2020M681608), 홍콩 일반 연구 기금(No. 27208720)의 지원을 받았습니다.

[2102.12122v2.pdf

1.04MB](./file/2102.12122v2.pdf)
