---
title: "FastVLM: Efficient Vision Encoding for Vision Language Models"
date: 2025-07-30 16:44:18
categories:
  - 인공지능
tags:
  - FastVLM
---

<https://arxiv.org/abs/2412.13303>

[FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/abs/2412.13303)

초록  
입력 이미지 해상도를 확장하는 것은 Vision Language Model(VLM)의 성능을 향상시키는 데 필수적이며, 특히 텍스트가 풍부한 이미지 이해 과제에서 더욱 중요하다. 그러나 ViT(Vision Transformer)와 같은 널리 사용되는 비전 인코더는 토큰 수가 급증하고 인코딩 지연(latency)이 커지기 때문에 고해상도에서 비효율적이다. VLM의 비전 인코더는 다양한 해상도 환경에서 두 가지 축을 따라 최적화될 수 있다: 인코딩 지연을 줄이거나 LLM에 전달되는 비전 토큰 수를 최소화해 전체 지연을 낮추는 것이다. 우리는 이미지 해상도, 비전 지연, 토큰 수, LLM 크기 간 상호작용에 대한 종합적인 효율성 분석을 바탕으로, 해상도, 지연, 정확도 간 최적의 균형을 달성하는 모델 **FastVLM**을 제안한다.

FastVLM은 고해상도 이미지에서 토큰 수를 줄이고 인코딩 시간을 크게 단축하는 새로운 하이브리드 비전 인코더 **FastViTHD**를 통합한다. 기존 방법과 달리 FastVLM은 추가적인 토큰 프루닝(token pruning) 없이 입력 이미지 스케일링만으로 비전 토큰 수와 이미지 해상도 간의 최적 균형을 달성하며, 이로써 모델 설계를 단순화한다. LLaVA-1.5 환경에서 FastVLM은 이전 연구와 비교해 VLM 벤치마크에서 유사한 성능을 유지하면서 **time-to-first-token(TTFT)**을 3.2배 개선했다. 또한 가장 높은 해상도(1152×1152)에서 LLaVa-OneVision과 비교 시, 동일한 0.5B LLM을 사용하면서도 SeedBench, MMMU, DocVQA 등의 주요 벤치마크에서 더 나은 성능을 보이며 TTFT는 85배 빠르고 비전 인코더 크기는 3.4배 더 작다.

코드와 모델은 <https://github.com/apple/ml-fastvlm>에서 제공된다.

### 1 서론

![](/assets/images/posts/587/img.png)

(a) Qwen2-0.5B

![](/assets/images/posts/587/img_1.png)

(b) Vicuna-7B

**그림 1:** FastVLM은 기존 연구보다 3배 이상 빠르다. VLM(Vision Language Model)에서 일반적으로 사용되는 비전 인코더를 (a) Qwen2 [86] 0.5B LLM과 (b) Vicuna 7B [98] LLM을 사용해 비교하였다. 모든 비전 인코더는 CLIP [69]로 사전학습(pretrained)되었다. 공정한 비교를 위해 모든 모델은 LLaVA-1.5 [53] 환경에서 동일하게 학습되었으며, 비전 인코더는 해상도 적응을 위해 학습 가능하게 설정하였다(자세한 내용은 4절 참조). 각 모델의 마커 크기는 비전 인코더의 파라미터 수를 나타낸다. **x축은 비전 인코더 지연(latency)과 LLM 사전 채움(prefilling) 시간의 합**을 나타낸다. 모든 모델은 M1 MacBook Pro에서 벤치마크되었다.

비전-언어 모델(Vision Language Models, VLM)은 시각적 입력과 텍스트 입력을 동시에 이해할 수 있게 한다. VLM은 일반적으로 사전학습된 비전 백본(vision backbone)에서 생성된 비전 토큰을 프로젝션 레이어를 통해 사전학습된 대규모 언어 모델(LLM)에 전달하여 구축된다. 이전 연구들 [54, 53]은 비전 백본, 프로젝션 레이어, 그리고 일반적으로 디코더 전용 트랜스포머 [84]인 LLM이라는 세 가지 구성 요소의 학습 및 미세조정 전략을 다양하게 탐구했다.

여러 연구 [61, 28, 66]는 이미지 해상도가 VLM 성능의 핵심 요소임을 강조하며, 특히 텍스트나 차트가 많은 데이터에서 중요하다고 보고한다. 그러나 이미지 해상도를 높이면 여러 가지 문제가 발생한다. 첫째, 사전학습된 비전 인코더는 고해상도를 지원하지 않을 수 있으며, 이는 사전학습 자체를 비효율적으로 만든다. 이를 해결하기 위한 한 가지 방법은 비전 백본을 고해상도에 적응하도록 지속적으로 사전학습(pretraining)하는 것이다 [6]. 또 다른 방법은 Sphinx [52], S2 [72], AnyRes [53]와 같은 타일링(tiling) 전략으로, 이미지를 여러 하위 영역으로 나누고 각 하위 영역을 독립적으로 백본에 입력하는 방식이다.

추가적인 문제는 고해상도 추론에서의 실행 시간(runtime) 계산 비용이다. 단일 고해상도 추론이든 저해상도 다중 추론(타일링 전략)이든 비전 토큰을 생성할 때 상당한 지연(latency)이 발생한다. 게다가 고해상도 이미지는 더 많은 토큰을 생성하므로 LLM의 사전 채움(prefilling) 시간(비전 토큰을 포함한 전체 컨텍스트 토큰에 대한 LLM의 forward pass 시간)이 증가해 **time-to-first-token (TTFT)**, 즉 비전 인코더 지연과 LLM 사전 채움 시간의 합이 더 길어진다.

본 연구에서는 런타임 효율성 관점에서 VLM 설계와 학습을 탐구한다. 해상도가 증가함에 따라 정확도와 지연 간의 트레이드오프를 개선하기 위해 최적화 공간을 분석하며, 지연은 비전 인코더 추론 시간과 LLM 사전 채움 시간을 모두 포함한다. 다양한 LLM 크기와 해상도로 광범위한 실험을 수행하여 특정 비전 백본에 대한 파레토 최적 곡선을 구축하고, 해상도와 LLM 크기 선택에 따른 주어진 런타임 예산(TTFT) 내에서 달성 가능한 최고의 정확도를 보여준다.

우리는 먼저 MobileCLIP [83]으로 사전학습된 하이브리드 컨볼루션-트랜스포머 아키텍처인 **FastViT [82]**를 VLM의 비전 백본으로 사용하는 방법을 탐구한다(3.1절). 이 하이브리드 백본은 ViT 모델보다 4배 이상 빠르게 비전 토큰을 생성하면서 멀티스케일 특징을 활용해 더 높은 VLM 정확도를 달성할 수 있음을 보여준다(3.1.1절).

그러나 MobileCLIP 사전학습 FastViT처럼 임베딩 생성을 목표로 하는 것이 아니라, 고해상도 VLM을 주 목표로 할 경우 추가적인 구조 최적화가 가능하다. 이를 위해 우리는 고해상도 이미지에서 효율적인 VLM 성능을 발휘하도록 특별히 설계된 새로운 하이브리드 비전 인코더 **FastViTHD**를 도입하고(3.2절), 이를 비전 백본으로 활용해 시각적 지시 튜닝을 통해 **FastVLM**을 얻는다. FastVLM은 ViT 기반 VLM, 컨볼루션 기반 VLM, 그리고 이전에 언급한 하이브리드 FastViT 기반 VLM보다 다양한 입력 해상도와 LLM 크기에서 정확도-지연 트레이드오프가 크게 개선됨을 보여준다(그림 1(a), 그림 1(b), 그림 4). 특히, FastVLM은 더 작고 빠르며 적은 데이터로 학습되었음에도 여러 기존 연구를 능가한다(표 6). 가장 높은 해상도(1152×1152)에서 동작하는 LLaVA-OneVision [45]과 비교했을 때, 동일한 0.5B LLM을 사용하면서도 85배 빠른 TTFT와 3.4배 더 작은 비전 인코더를 통해 동등한 성능을 달성한다.

**본 연구의 기여 요약은 다음과 같다:**

- **하이브리드 비전 백본이 ViT를 능가함을 보이며**, 멀티스케일 비전 특징과 같은 추가 아키텍처적 개입을 통해 효율성을 유지하면서 VLM 성능을 더욱 향상시킴을 입증하였다.
- **FastViTHD**라는 새로운 하이브리드 아키텍처를 설계 및 사전학습하여, FastVLM에서 고해상도 입력 기반 효율적 VLM 성능을 최적화했다. 동일한 실험 환경에서 비전 백본만 변경했을 때, FastViTHD는 ViT 기반 및 컨볼루션 기반 모델보다 우수한 성능을 보였으며, **SigLIP-SO400M [94] 대비 3.2배 빠른 TTFT와 3.6배 작은 모델 크기**, **ConvNeXT [28] 대비 2.3배 빠른 TTFT와 1.7배 작은 모델 크기**를 달성했다. 또한 FastVLM은 시각 지시 튜닝 데이터가 늘어날수록 효과적으로 확장됨을 보여준다.
- 실제 하드웨어 벤치마크에서 비전 백본 지연과 LLM 사전 채움 시간을 모두 고려하여 VLM의 정확도-지연 트레이드오프를 체계적으로 연구했다. 이를 통해 FastVLM이 해상도-지연-정확도 간의 균형을 장치 기반 측정에서 개선했음을 입증했다.

### 2 관련 연구

#### 대규모 멀티모달 모델

대규모 언어 모델(LLM) [79, 86, 77, 98, 68]과 웹 스케일 이미지-텍스트 데이터셋으로 학습된 CLIP [69]과 같은 대규모 사전학습 비전 모델의 등장으로, 이미지와 LLM을 정렬(aligned)하여 시각 신호 해석을 가능하게 하는 다양한 멀티모달 아키텍처가 제안되었다. 초기 연구인 **Frozen** [80]과 **Florence** [1, 2]는 이미지 임베딩과 텍스트 임베딩을 LLM의 중간 레이어에서 교차 어텐션(cross-attention) 메커니즘으로 융합하였다. 이후에는 이미지 임베딩을 텍스트와 함께 LLM 입력으로 제공하는 **오토리그레시브(auto-regressive)** 아키텍처가 주류로 떠올랐다. 이 아키텍처를 사용하는 대표적인 연구에는 **LLaVA** [54, 53, 55], **mPLUG-Owl** [89, 90, 88], **InstructBLIP** [20], **BLIP-3** [85], **SPHINX** [52], **MiniGPT-4** [99], **VILA** [50], **MM1** [66], **Qwen-VL** [4], **InternVL** [15, 16], **Cambrian-1** [78] 등이 있다. 최근 **Fuyu** [5]와 **EVE** [23]는 원시(raw) 이미지를 LLM 디코더에 직접 전달하는 단순화된 아키텍처를 제안했다. 또한 **Chameleon** [76]은 사전학습된 코드북으로 이미지를 토크나이즈하는 초기 융합(early fusion) 혼합 모달 모델을 도입했다. 이러한 방식은 이미지 인코더를 생략하는 흥미로운 접근이지만, 사전학습된 이미지 인코더를 사용하는 기존 아키텍처 대비 성능은 여전히 뒤처진다.

#### 효율적인 이미지 인코딩

CLIP [69]으로 사전학습된 비전 트랜스포머(ViT) [24]는 VLM에서 이미지 인코딩을 위해 널리 사용되며, 대표적인 모델로 **SigLIP** [94], **EVA-CLIP** [75], **InternViT** [15], **DFN-CLIP** [26] 등이 있다. 성능 향상을 위해 최근 연구들 [36, 78, 73]은 서로 다른 목적 함수로 학습된 비전 인코더들을 앙상블하는 방식을 사용한다. 이러한 연구들은 효율적인 비전 인코더를 앙상블 내 일부로 활용할 수 있으므로 본 연구와는 직교적인 방향에 있다.

ViT 기반 아키텍처는 VLM에서 인기가 높지만, 인코더가 출력하는 시각 토큰 수가 많아 비효율성이 발생한다. 이를 해결하기 위해 **LLaVA-PruMerge** [70]와 **Matryoshka 기반 토큰 샘플링** [32, 7]은 동적으로 토큰을 프루닝(pruning)한다. 다른 접근법으로는 **Perceiver 스타일 리샘플러(resampler)** 또는 **풀링(pooling)** 기법을 사용해 토큰을 줄이는 방법이 있다 [20, 9, 18, 19].

ViT처럼 등방성(isotropic) 아키텍처를 사용한 뒤 별도의 리샘플러와 프로젝터를 설계하는 대신, **계층적(hierarchical) 아키텍처**가 더 단순한 설계 선택지가 될 수 있다. 예를 들어 **ConvNeXT** [57]와 **FastViT** [82]와 같은 계층적 백본은 각 연산 단계에서 입력 텐서를 다운샘플링하므로 토큰 수가 적다. 최근에는 순수 컨볼루션 비전 인코더를 사용해 VLM을 구성한 **ConvLLaVA** [28]가 소개되었다. 본 연구에서는 VLM을 위해 개선된 **컨볼루션-트랜스포머 하이브리드 아키텍처**를 제안하고, 이 아키텍처를 고해상도 입력으로 확장했을 때의 **파레토 최적(Pareto optimal) 운용 지점**을 논의한다.

### 3 아키텍처

![](/assets/images/posts/587/img_2.png)

**그림 2:** FastVLM 아키텍처 개요. FastVLM은 새로운 비전 인코더인 **FastViTHD**로 구성되며, LLaVA와 동일한 설정으로 학습된다. FastViTHD 아키텍처는 고해상도 환경에서 낮은 지연(latency)을 목표로 설계되었으며, 추가적인 셀프 어텐션(self-attention) 레이어를 활용하고 다운샘플링을 통해 **FastViT 대비 4배 적은 토큰**, 그리고 **ViT-L/14 대비 16배 적은 토큰**을 해상도 336에서 생성한다.

본 장에서는 먼저 **FastViT** 하이브리드 비전 인코더를 비전-언어 모델링에 도입하는 방법을 탐구한다. 이어서 VLM 과제에서의 성능 향상을 위한 아키텍처적 개입을 소개한다. 이를 기반으로 고해상도 VLM 효율성을 위해 설계된 새로운 하이브리드 비전 인코더 **FastViTHD**를 제안한다. 또한 FastViTHD가 FastViT 및 기존 방법 대비 다양한 LLM과 입력 해상도에서 최적임을 입증하는 종합적인 **어블레이션(ablation)** 실험을 제공한다.

그림 2는 FastVLM과 FastViTHD의 전체 아키텍처를 보여준다. 본 장의 모든 결과는 별도의 언급이 없는 한 Vicuna-7B [98]를 LLM 디코더로 사용하고 LLaVA-1.5 [53]와 동일한 설정으로 학습되었다. 자세한 내용은 4절에서 설명한다.

### 3.1 VLM 이미지 인코더로서의 FastViT

LLaVA와 같은 VLM은 크게 세 가지 구성 요소로 이루어진다: **이미지 인코더**, **비전-언어 프로젝터(vision-language projector)**, 그리고 **대규모 언어 모델(LLM)**. VLM의 성능과 런타임 효율성은 비전 백본(vision backbone)에 크게 의존한다. 특히 **고해상도에서 이미지를 인코딩하는 것**은 다양한 VLM 벤치마크에서 강력한 성능을 달성하는 데 필수적이며, 텍스트가 풍부한 과제에서 그 중요성이 더욱 두드러진다. 따라서 해상도를 확장할 수 있는 비전 인코더는 VLM에 특히 유리하다.

우리는 **하이브리드 비전 인코더(컨볼루션 레이어 + 트랜스포머 블록)**를 VLM에 이상적인 후보로 식별했다. 컨볼루션 레이어는 해상도 스케일링을 자연스럽게 지원하며, 트랜스포머 블록은 LLM에서 활용할 고품질 비전 토큰을 정제(refine)한다.

본 연구에서는 **MobileCLIP [83]의 MCi2 이미지 인코더**를 사용했으며, 이는 35.7M 파라미터를 가진 FastViT 아키텍처 기반의 CLIP 사전학습 하이브리드 비전 인코더이다. 편의상 본 논문에서는 이를 “**FastViT**”으로 지칭한다.

**표 1**에서 보이듯이 CLIP 사전학습 해상도(256×256)만으로 FastViT를 사용할 경우 VLM 성능은 충분히 강력하지 않다. 그러나 하이브리드 인코더인 FastViT의 주요 장점은 **우수한 이미지 해상도 스케일링 특성**에 있다. FastViT는 패치 크기 14를 사용하는 ViT 아키텍처 대비 **토큰을 5.2배 적게 생성**한다. 이러한 토큰 수 감소는 LLM 디코더의 **prefilling 시간 및 TTFT(time-to-first-token)** 감소로 이어져 VLM에 큰 이점을 제공한다.

예를 들어, **FastViT의 입력 해상도를 768×768로 확장하면**, ViT-L/14의 입력 해상도 336×336과 동일한 수의 비전 토큰을 생성하면서도 VLM 벤치마크에서 더 나은 성능을 달성한다. 이 성능 차이는 특히 TextVQA, DocVQA와 같이 텍스트 중심 벤치마크에서 더욱 두드러지며, 두 아키텍처가 동일한 토큰 수를 생성하더라도 **효율적인 컨볼루션 레이어 덕분에** FastViT는 훨씬 더 빠르게 이미지를 인코딩할 수 있다.

![](/assets/images/posts/587/img_3.png)

**표 1:** FastViT는 ViT-L/14보다 **약 4배 낮은 지연(latency)**에서 더 높은 정확도를 달성한다. 해상도를 768까지 확장하기 위해 LLaVA-1.5 학습 설정의 Stage-2에서 FastViT를 학습 가능 상태로 전환하였다. †공정한 비교를 위해 ViT-L/14도 동일한 Stage-2 학습에서 파인튜닝한 결과를 보고한다. 모든 지연은 밀리초(ms) 단위로 측정되었다. (자세한 내용은 4절 참조)

![](/assets/images/posts/587/img_4.png)

**표 2:** 멀티스케일 특징 및 풀링 전략을 적용하여 FastViT VLM 성능을 향상시킨 결과. 이 수정은 FastViT의 성능을 소폭 개선한다. 학습 설정은 Vicuna 7B를 사용한 LLaVA-1.5 환경이다.

### 3.1.1 멀티스케일 특징(Multi-Scale Features)

일반적인 컨볼루션 및 하이브리드 아키텍처는 연산을 4개의 구분된 스테이지로 나누며, 각 스테이지 사이에는 다운샘플링 연산이 존재한다. VLM은 일반적으로 마지막 전 단계(펜얼티메이트, penultimate) 레이어의 특징(feature)에 의존하지만, 네트워크의 초기 스테이지에서 추출되는 특징들은 서로 다른 세분화 수준(granularity)의 정보를 담고 있다. **여러 스케일의 정보를 집계(aggregate)하면 마지막 전 단계 특징을 보완**할 수 있다.

멀티스케일 특징 추출을 위한 아키텍처는 **그림 2**에 나타나 있다. 우리는 여러 스테이지의 특징을 풀링(pooling)하는 두 가지 설계를 비교(ablation)했다: **AvgPooling**과 **2D Depthwise Convolution**. **표 2**의 결과에 따르면, Depthwise Convolution을 사용하는 것이 더 나은 성능을 보였다.

![](/assets/images/posts/587/img_5.png)

**그림 3:** FastViTHD의 새로운 스케일링 전략은 다양한 이미지 해상도에서 지연(latency)을 줄인다. 단순히 FastViT 아키텍처를 확장한 **FastViT-Naive**와 본 논문에서 제안하는 **FastViTHD**는 동일한 파라미터 수를 가진다. 참고를 위해 **ConvNeXt-L** 결과도 포함하였다. 모든 모델은 **M1 MacBook Pro**에서 벤치마크되었으며, **LLaVA-1.5 환경과 Vicuna 7B**로 학습되었다. 참고로, **y축은 로그 스케일(log scale)**이다.

### 3.2 FastViTHD: VLM을 위한 고해상도 인코더

앞서 제안한 모델 개입을 포함한 **FastViT**는 ViT-L/14보다 **8.7배 작으면서도** 이미지 인코더로서 우수한 성능을 보인다. 그러나 이전 연구들 [15, 47]은 **이미지 인코더의 스케일을 키우면 일반화 능력이 향상된다**는 점을 보여주었다.

하이브리드 아키텍처 [17, 92]는 일반적으로 **4단계 구조에서 셀프 어텐션 레이어와 너비(width)를 확장**하는 방식으로 스케일링하지만, 이 방법에는 단점이 있다. 그림 3에서 보듯이, 기존 연구처럼 FastViT의 3단계와 4단계에서 단순히 셀프 어텐션 레이어 수를 늘리는 것은 **ConvNeXT-L보다도 느리고 비최적**임을 확인했다. 이를 해결하기 위해 우리는 **추가 다운샘플링 레이어가 포함된 새로운 스테이지를 도입**하여, 셀프 어텐션이 ViTamin과 같은 최신 모델에서 사용되는 16배 다운샘플링이 아닌 **32배 다운샘플링된 텐서**에서 동작하도록 설계했다(그림 2 참조). 단순 스케일링 접근법에 대한 자세한 내용은 부록 B에 설명되어 있다.

이 설계는 이미지 인코딩 지연(latency)을 줄이고, 연산 집약적인 LLM 디코더로 전달되는 **비전 토큰 수를 4배 감소**시켜 **TTFT(time-to-first-token)** 또한 단축한다. 아키텍처 개략도는 그림 2에 있으며, 이 모델을 **FastViTHD**라 부른다.

#### 모델 구조

모델은 그림 2와 같이 총 **5단계(stage)**로 구성된다.

- 첫 세 단계는 **RepMixer [82] 블록**을 사용하고,
- 마지막 두 단계는 **멀티헤드 셀프 어텐션(MHSA) [24] 블록**을 사용한다.
- 각 단계의 깊이(depth)는 **[2, 12, 24, 4, 2]**이며, 임베딩 차원은 **[96, 192, 384, 768, 1536]**이다.
- ConvFFN 레이어의 MLP 확장 비율은 **4.0**으로 설정되었다.
- 모델은 총 **125.1M 파라미터**를 가지며, 이는 MobileCLIP의 가장 큰 FastViT 변형보다 **3.5배 크지만 여전히 ViT 계열 모델보다 작다.**

![](/assets/images/posts/587/img_6.png)

**표 3:** FastViTHD는 CLIP 벤치마크에서 **상당히 낮은 지연**으로 경쟁력 있는 성능을 달성한다. 우리는 [12]에서 설명한 설정을 따라 평균 검색(retrieval) 성능을, [27]에서 설명한 설정을 따라 38개 작업에서의 평균 성능을 보고한다. 모든 모델은 **M1 MacBook Pro**에서 벤치마크되었다.

FastViTHD는 FastVLM 학습에 사용되기 전, **DataCompDR-1B 데이터셋**을 활용해 [83]과 동일한 CLIP 사전학습 설정으로 학습된다. **표 3**에 따르면 FastViTHD는 ViT-L/14보다 **2.4배 작고 6.9배 빠르면서도** 38개 멀티모달 제로샷 작업 [27]에서 유사한 평균 성능을 보인다. 또한 VLM을 위해 설계된 하이브리드 트랜스포머 아키텍처인 **ViTamin [12]**과 비교했을 때, FastViTHD는 **2.7배 작고 5.6배 빠르면서도 더 우수한 평균 검색 성능**을 달성한다.

**표 4**에서는 LLaVA-1.5 학습 후 VLM 과제에서 FastViTHD와 다른 CLIP 사전학습 계층적 백본(ConvNeXT-L 및 ConvNeXT-XXL)을 비교했다. FastViTHD는 ConvNeXT-XXL과 **동일한 정확도**를 보이면서도 **6.8배 작고 3.3배 빠르다.**

![](/assets/images/posts/587/img_7.png)

**표 4:** FastViTHD는 더 높은 해상도에서 ConvNeXT보다 **낮은 지연으로 더 높은 정확도**를 달성한다. 모델은 LLM이 처리할 **비전 토큰 수**를 기준으로 그룹화되어 있다. “C.N”은 ConvNeXT를 의미한다. 학습 설정은 **Vicuna 7B**를 사용한 **LLaVA-1.5** 환경이다.

### 3.2.1 비전 인코더 – 언어 디코더 상호작용

![](/assets/images/posts/587/img_8.png)

**그림 4:** FastViTHD는 FastViT와 비교했을 때 **정확도 대 TTFT(time-to-first-token)**의 파레토 최적 곡선을 개선한다. Qwen2 [86] 패밀리(채팅 변형) LLM의 다양한 크기와 서로 다른 이미지 해상도(각 포인트에 주석 표시)와 결합한 FastViT 및 FastViTHD 백본을 비교하였다. 파레토 최적 곡선은 두 비전 백본 각각에 대해 강조 표시되었다. 학습 설정은 LLaVA-1.5이며, **x축은 로그 스케일**이다.

![](/assets/images/posts/587/img_9.png)

**그림 5:** 고해상도에서 **비전 인코더 지연(vision latency)**이 TTFT의 주요 지배 요소가 된다. FastViTHD 비전 인코더와 Qwen2-1.5B LLM을 사용해 다양한 이미지 해상도에서 FastVLM의 TTFT를 분해(breakdown)한 결과를 보여준다.

![](/assets/images/posts/587/img_10.png)

**그림 6:** **동적 입력 해상도(AnyRes)**는 타일 수가 적을 때(2×2)만 최고 해상도에서 최적 성능을 보인다. 비전 인코더는 FastViTHD이며, 타일 그리드 크기는 괄호 안에 표시된다. 학습 설정은 Vicuna 7B를 사용한 LLaVA-1.5이며, **x축은 로그 스케일**이다.

VLM의 **정확도-지연(latency) 트레이드오프**는 여러 요인에 의해 결정된다.  
한편으로 VLM의 전체 성능은 다음에 의존한다:

1. 입력 이미지 해상도,
2. 비전 토큰의 수와 품질,
3. LLM의 성능(capability).

다른 한편으로 VLM의 총 지연(첫 번째 토큰 생성까지의 시간, TTFT)은 다음에 의해 결정된다:

1. 비전 인코더의 지연,
2. LLM의 **prefilling 시간**.

후자는 비전 인코더가 생성하는 토큰 수와 LLM의 크기에 영향을 받는다.

VLM의 최적화 공간이 복잡하기 때문에, 특정 비전 인코더의 최적성을 주장하려면 **(해상도, LLM)** 쌍에 대해 전반적인 검증이 필요하다. 본 연구에서는 FastViTHD가 FastViT 대비 최적임을 **실험적으로 입증**한다. 각 비전 인코더에 대해 **Qwen2 [86]-0.5B / 1.5B / 7B** 세 가지 LLM과 다양한 입력 이미지 해상도를 고려했다. 각 (해상도, LLM) 조합에 대해 **LLaVA-1.5 [53] 사전학습 및 시각 지시 튜닝**을 수행한 뒤, 다양한 작업에서 모델을 평가했다. 결과는 그림 4에 제시되어 있다.

첫째, 비전 인코더의 **파레토 최적 곡선(Fig. 4 강조 표시)**은 주어진 런타임 예산(TTFT)에서 달성 가능한 최대 성능을 나타내며, **서로 다른 크기의 LLM 조합**으로 구성된다. 특히, **작은 LLM과 고해상도를 조합하는 것은 비효율적**인데, 작은 LLM은 많은 토큰을 효과적으로 활용할 수 없고, TTFT는 비전 인코더 지연에 의해 지배되기 때문이다(그림 5 참조).

둘째, 그림 4에서 **FastViTHD의 파레토 최적 곡선은 FastViT 대비 명확하게 우수**하다. 주어진 런타임 예산 내에서 가능한 모든 (해상도, LLM) 조합을 고려했을 때, **FastViTHD는 평균 5개 지표(Average-5 metric)에서 2.5포인트 이상 향상된 성능**을 보였다. 또한 FastViTHD는 목표 VLM 성능에 **최대 3배 더 빠르게 도달**할 수 있다.

이전 섹션에서 이미 **FastViT 기반 VLM이 ViT 기반 VLM 대비 큰 개선**임을 보여주었지만, FastViTHD는 FastViT 대비 **추가적인 성능 향상**을 제공한다는 점이 중요하다.

### 3.2.2 정적(Static) vs. 동적(Dynamic) 입력 해상도

입력 해상도를 확장하는 방법은 두 가지가 있다.

1. 모델의 입력 해상도를 직접 조정하는 방법,
2. 이미지를 타일링(tiling)하고 인코더의 해상도를 타일 크기로 설정하는 방법이다.

타일 기반 추론(AnyRes)은 이전 연구 [52, 54]에서 ViT 모델이 고해상도 이미지를 처리할 수 있도록 제안되었다. 그러나 FastViTHD는 **고해상도 입력에서 효율적으로 추론하도록 설계되었기 때문에**, 두 전략을 사용했을 때 다양한 해상도에서 최적 동작점을 분석하였다.

그림 6에서 확인할 수 있듯이, **모델의 입력 해상도를 직접 원하는 해상도로 설정하는 것이 정확도-지연 트레이드오프 측면에서 가장 우수**하며, 동적 해상도 방식은 **1536×1536과 같은 극단적 해상도**에서 메모리 대역폭 제한으로 인해 일부 이점을 보인다. 만약 동적 해상도가 필요한 경우, **타일 수가 적은 설정(예: 2×2)**이 정확도-지연 트레이드오프 측면에서 더 우수한 성능을 낸다. 이에 대한 추가 논의는 부록 C.1에서 다룬다.

### 3.2.3 토큰 프루닝(Token Pruning) 및 다운샘플링 비교

우리는 다양한 해상도에서 동작하는 FastViTHD의 성능을 문헌에서 제안된 **토큰 프루닝 기법**과 비교하였다. **표 5**의 결과에서, VLM은 ViT와 같은 등방성(isotropic) 아키텍처에 토큰 프루닝 기법을 적용하는 것보다 **계층적 백본(hierarchical backbone)**을 사용하는 것이 **정확도-지연 트레이드오프** 측면에서 더 우수함을 알 수 있다.

단순히 VLM을 **더 낮은 입력 해상도에서 학습**하는 것만으로도 FastViTHD는 **비전 토큰 수를 16개까지 줄일 수 있으며**, 최근 제안된 토큰 프루닝 기법보다도 더 나은 성능을 보였다. 흥미롭게도, [7, 32, 33, 87]에서 제안된 가장 효과적인 토큰 프루닝 기법조차도, **입력 해상도 256×256에서 학습된 FastViTHD보다 성능이 낮았다**.

![](/assets/images/posts/587/img_11.png)

**표 5:** FastViTHD는 토큰 프루닝 기법보다 더 효과적으로 토큰 수를 줄인다. 모델은 **전체 비전 토큰 수**를 기준으로 그룹화되었다. “-”는 해당 논문에서 성능이 보고되지 않았음을 의미한다. 모든 모델은 Vicuna 7B를 사용한 LLaVA-1.5 설정으로 학습되었다. “‡”는 [87]에서 추가 파인튜닝이 수행되었음을 나타낸다. “I”는 **비전 전용 희소화(vision only sparsification)**, “I|T”는 **비전-언어 희소화(vision-language sparsification)**를 의미하며, 이는 [33]에서 보고되었다.

![](/assets/images/posts/587/img_12.png)

**표 6:** VLM 평가 및 최신 방법과의 비교. 모델은 **전체 비전 토큰 수**를 기준으로 그룹화되었다. “-”는 해당 논문에서 사전학습(PT) 또는 지시 튜닝(IT)에 사용된 데이터셋 크기가 명시되지 않았음을 의미한다. 2단계 이상의 학습을 사용하는 방법의 경우, 모든 사전학습 단계에서 사용된 샘플 수를 합산하여 PT로 보고했다.

- **TTFT**는 비전 인코더 지연과 LLM 프리필(prefill) 시간의 합을 의미하며, MLX [31]에 적합한 형식으로 공개된 모델만 지연을 보고했다.
- “Vic.”는 **Vicuna [98]**, “Qw.2”는 **Qwen2 [86]**, “Qw.”는 **Qwen [3]**, “L-2”는 **LLaMA-2**, “L-3”는 **LLaMA-3**, “ML.”은 **MobileLLaMA [18, 19]**, “DS.”는 **DeepSeek LLM [21]**을 의미한다.
- 입력 해상도와 비전 토큰 수는 모델별로 지원되는 **최대 해상도**를 기준으로 보고했으며, 일부 모델(LLaVA-OneVision [45], MM1 [66])은 **동적 입력 해상도**를 사용한다.
- FastVLM의 동적 해상도 모델은 **2×2 그리드**를 사용하며, 타일 크기는 1024로 설정되었다.
- † 표기된 성능 수치는 [78]에서 보고된 값이다.
- 여러 비전 인코더를 사용하는 VLM의 경우, 각 인코더의 크기를 별도로 명시하고 TTFT는 각 인코더의 지연을 합산하여 계산하였다.

### 4 실험(Experiments)

#### 학습 설정(Training Setup)

섹션 3에서 제시된 모든 어블레이션(ablations) 실험은 특별한 언급이 없는 한 **Vicuna-7B [98]**를 LLM 디코더로 사용하여 **LLaVA-1.5 [53]**에서 설명된 2단계 학습 설정을 따른다.

- **1단계(Stage 1):** LLaVA-558K alignment 데이터셋을 사용하여 프로젝터만 1 epoch 학습하며, 배치 크기는 256이고 학습률은 10⁻³이다. 이 단계에서 입력 이미지 해상도는 백본 사전학습 해상도(예: FastViT는 256, FastViTHD는 224)와 동일하다.
- **2단계(Stage 2):** LLaVA-665K 감독형 파인튜닝(supervised finetuning) 데이터셋을 사용해 모델을 1 epoch 학습하며, 비전 인코더, 프로젝터, LLM **모든 모듈을 튜닝**한다. 이 단계에서는 입력 이미지 해상도를 목표 해상도로 설정한다.

섹션 4에서는 다른 LLM 디코더(Qwen2-0.5B / 1.5B / 7B [86] 및 Vicuna-7B [98])를 사용한 결과를 제시한다. 두 가지 학습 설정을 보고한다:

1. **LLaVA-1.5의 2단계 학습 설정**
2. 최근 문헌 [43, 66]의 경향을 따른 **3단계 학습 설정**
   - **Stage 1:** 커넥터 학습
   - **Stage 1.5:** 해상도 스케일링
   - **Stage 2:** 시각 지시 튜닝(visual instruction tuning)  
     데이터셋 정보는 섹션 D에서 확인 가능하다. Stage 1은 백본 사전학습 해상도를 사용하고, 이후 두 단계는 목표 해상도로 조정된다. 두 설정 모두 Stage 1에서만 비전 인코더와 LLM을 동결(freeze)하며, 나머지 단계에서는 모든 모듈을 파인튜닝한다.

최적의 설정에서는 Stage 2 이후 **체인 오브 소트(chain-of-thought) reasoning**이 포함된 고품질 지시 튜닝 데이터셋 [30]으로 추가 파인튜닝(Stage 3)을 진행한다. 자세한 내용은 섹션 A 및 섹션 D에 설명된다. Stage 2 학습에서 **R4, R12, R41 체크포인트**를, Stage 3 학습에서 **R5, R13, R42 체크포인트**를 공개 소스 코드로 배포한다.

모든 FastVLM 모델은 **8× NVIDIA H100-80GB GPU**가 장착된 단일 노드에서 학습되었다. Qwen2-7B 디코더를 사용하는 Stage 1 학습은 약 30분이 소요된다. Stage 1.5 및 Stage 2 학습 시간은 입력 해상도에 따라 다르다. 예를 들어, **1024×1024 입력 해상도**에서 Stage 1.5는 77시간, Stage 2는 8시간이 소요된다. 해당 시간은 Stage 1.5에서 1,500만 샘플, Stage 2에서 110만 샘플을 사용한 기준이다.

#### 평가(Evaluation)

모델은 다음 주요 벤치마크에서 평가된다:

- **GQA [34]**, **ScienceQA [59]**, **TextVQA [74]**, **POPE [48]**, **LLaVA-in-the-wild [54]**, **VQAv2 [29]**, **MMVet [91]**, **MMMU [93]**, **DocVQA [65]**, **SeedBench [42]**
- GQA, ScienceQA, TextVQA, POPE, LLaVA-in-the-wild 벤치마크는 LLaVA [54]의 공식 평가를 사용한다.
- 나머지 벤치마크는 **lmms-eval [96] v0.2.2** 라이브러리를 사용하며, GPT를 평가자로 사용하는 경우 0613 버전 GPT가 기본으로 사용된다.

섹션 3에서 제시한 어블레이션 실험에는 **GQA, TextVQA, POPE, DocVQA, SeedBench**를 사용한다.

- GQA와 SeedBench는 **일반 지식 벤치마크**
- DocVQA와 TextVQA는 **텍스트 중심 벤치마크**
- POPE는 **환각(hallucination) 벤치마크**다.

이 벤치마크들은 다양성을 제공하며, 확률적 디코딩 환경에서도 초기화 간 분산이 낮아 빠른 평가가 가능하다. 섹션 D.3에 다양한 초기화 조건에서의 분산을 보고했으며, 5개 지표의 표준편차는 **0.5 이하**다. 이 5개 벤치마크의 평균을 **Avg-5**라 부르며, 분석의 신뢰 지표로 사용한다. Avg-5의 경험적 표준편차는 **0.1**이다.

#### 벤치마킹(Benchmarking)

모든 모델은 **M1 Max 칩과 32GB RAM을 탑재한 MacBook Pro**에서 벤치마크되었다.

- 이미지 인코더는 **coremltools v7.2**를 사용해 Core ML 패키지 파일로 변환 후, **XCode 15.4 (15F31d)**의 뉴럴 엔진에서 벤치마크되었다.
- LLM은 MacBook Pro의 GPU에서 **MLX [31]**로 벤치마크되었다. 모델은 먼저 **mlx\_lm.convert** 도구를 통해 Hugging Face 포맷에서 MLX 포맷으로 변환되며, 텐서는 FP16으로 캐스팅된다.
- 프리필링(prefilling) 지연은 **mlx\_lm.cache\_prompt** 도구 [31]를 통해 추정되며, **TTFT(Time-To-First-Token)**는 특정 해상도의 이미지 인코딩 지연과 해당 비전 토큰에 대한 LLM 프리필링 지연의 합으로 계산된다.

### 4.1 최신(state-of-the-art) 기법과의 비교

**표 6**에서는 FastVLM과 최근에 발표된 방법들을 비교한다. 연구마다 학습 설정이 크게 다르기 때문에, 공정한 비교를 위해 각 모델의 LLM 디코더, 지시 튜닝(instruction tuning) 데이터셋과 사전학습(pretraining) 데이터셋의 규모를 함께 보고한다.

#### 계층적 백본(Hierarchical Backbones) 비교

동일한 LLM과 유사한 학습 데이터 크기에서 **FastVLM (R20)**은 **ConvLLaVA [28] (R18)** 대비 TextVQA에서 **+8.4%**, DocVQA에서 **+12.5%** 높은 성능을 보이면서도 **22% 더 빠른 속도**를 기록한다. 해상도가 높아질수록 이 격차는 더 커지며, 동일한 LLM 디코더를 사용했을 때 **FastVLM (R28, R29)**는 **ConvLLaVA (R26)**보다 다양한 벤치마크에서 더 우수한 성능을 달성하면서 **2배 더 빠르다.**

#### 데이터셋 스케일링(Dataset Scaling)

해상도 스케일링을 위한 **중간 사전학습 단계(15M 샘플)**를 포함해 사전학습 데이터셋을 확장하면, **FastVLM (R21)**은 MM1 [66] (R38)과 비교해 다양한 벤치마크에서 동등하거나 더 우수한 성능을 낸다. 주목할 점은 FastVLM이 **5배 적은 비전 토큰**을 생성하면서도 이 성능을 달성했다는 것이다.

입력 해상도를 **1024×1024**로 올리고 **1,250만 샘플 규모의 지시 튜닝 데이터셋**을 사용하면, **FastVLM (R41)**은 TextVQA, DocVQA 등 해상도와 비전 토큰 수에 민감한 텍스트 중심 벤치마크를 포함해 MM1 (R38)과 LLaVA-NeXT (R39)를 능가한다. 입력 해상도를 **AnyRes** 방식으로 추가 확장하면 격차가 더 커지며, 자세한 내용은 섹션 C.1에 제시된다. 데이터셋 분할 정보는 섹션 D에 제공된다.

#### 다중 비전 인코더(Multiple Vision Encoders) 비교

최근 **MiniGemini [49]**와 **Cambrian-1 [78]**은 **여러 비전 인코더**에 의존하는 모델을 제안했다. 표 6에서 FastVLM (R40)은 단일 비전 인코더를 사용하면서도, 비슷한 규모의 시각 지시 튜닝 데이터셋으로 학습된 다중 인코더 모델들과 비교된다.

Cambrian-1 [78] (R44)에서는 **비전 인코딩 지연이 전체 TTFT(약 5초)의 3.2배를 차지**하며, 세부 분해는 표 10에 제시되어 있다. FastVLM (R40)은 유사한 데이터셋으로 학습했음에도 **Cambrian-1 (R44)보다 7.9배 빠르고** 성능도 우수하다. 지시 튜닝 데이터셋을 **1,250만 샘플**로 확장하면, **FastVLM (R41)**은 Cambrian-1 (R44)보다 **2.3배 적은 비전 토큰**으로도 텍스트 중심 평가(표 11 참조)에서 더 뛰어난 성능을 달성한다.

#### 디코더 효과(Effect of Decoder)

이전 연구 [44]에서 보이듯, VLM 성능은 LLM의 품질에도 크게 의존한다. **Vicuna-7B (R21, R29)**에서 **Qwen2 [86, 77] (R22, R30)**로 전환하면 모든 벤치마크에서 성능이 향상되며, 특히 **MMVet, LLaVA-in-the-wild, MMMU** 벤치마크에서 향상이 두드러진다.

**Qwen2-0.5B**를 LLM 디코더로 사용할 때, **FastVLM (R4)**는 **LLaVA-OneVision [45] (R2)**보다 **85배 빠른 속도**를 기록하면서도 성능에서 앞선다. 두 모델이 동일한 LLM 디코더를 사용한다는 점을 고려할 때, 이 결과는 **FastViTHD의 비전 인코더 품질이 높음**을 보여준다. 또한 FastViTHD는 SigLIP-SO400M [94]보다 **3.4배 더 작다.**

![](/assets/images/posts/587/img_13.png)

표 7: 동시대 연구와의 비교

모델은 **비전 토큰 수** 기준으로 내림차순 정렬되었으며, 각 모델의 비전 토큰 수는 해당 모델이 지원하는 최대 입력 해상도 기준으로 산정되었다.

- SmolVLM2의 비전 토큰 수는 논문에 명시되지 않아 모델 설정 기반으로 추정했다.
- † 표시는 **VLMEvalKit [25]**으로 평가된 모델이며, 나머지 모델은 **lmms-eval [96] 라이브러리**를 사용했다.
- 표 7의 FastVLM은 표 6의 **R5 및 R13** 모델이다.

#### 동시대 연구(Concurrent Works) 비교

표 7에서 **FastVLM**은 **SmolVLM2 [62]**, **FlorenceVL [11]** 등과 비교된다.

- **FastVLM (R7)**은 SmolVLM2 (R3)와 비교해 **ChartQA, TextVQA 등 텍스트 중심 평가에서 경쟁력 있는 성능**을 보이며, **DocVQA 및 InfoVQA**에서는 이를 능가한다. FastVLM (R7)은 **SmolVLM2 (R3)보다 8.2배 적은 비전 토큰**으로 이를 달성한다.
- 또한 FastVLM (R7)은 **FlorenceVL (R6)** 대비 TextVQA, DocVQA 벤치마크에서 더 높은 성능을 내며, **2.3배 적은 비전 토큰**과 **6.2배 더 작은 비전 인코더**를 사용한다.
- AI2D 및 ScienceQA와 같은 **지식 기반 평가**에서도 FastVLM (R7)은 모든 경쟁 모델을 능가한다.
- 가장 작은 스케일에서 **FastVLM (R2)**는 유사한 규모의 SmolVLM2 (R1)보다 **4.3배 적은 비전 토큰**을 사용하면서도 다양한 벤치마크에서 우수한 성능을 기록한다.

### 5 결론(Conclusion)

본 연구에서는 **고해상도 입력의 효율적 인코딩**을 위해 **FastViTHD 비전 백본**을 활용한 **FastVLM**을 제안하였다. FastViTHD는 **하이브리드 아키텍처**를 기반으로 하며, 강화된 이미지-텍스트 데이터로 사전학습되었고, **정확도 손실을 최소화하면서 비전 토큰 수를 크게 줄이는** 특성을 가진다.

FastVLM은 다양한 VLM 벤치마크에서 기존 연구들과 경쟁력 있는 성능을 보이며, **time-to-first-token**과 **비전 백본 파라미터 수** 모두에서 효율성을 향상시켰다. 특히 **M1 MacBook Pro**에서 수행한 엄격한 벤치마크 결과, FastVLM은 기존 연구와 비교해 **해상도-지연-정확도 간의 트레이드오프에서 최첨단 성능**을 달성했음을 확인했다.

### 감사의 글(Acknowledgement)

데모 앱 프로토타이핑에 귀중한 기여를 해준 **David Koski, Christopher Webb, Matt Biddulph, Nick Henderson, Megan Maher Welsh, Jamie Cheng, Jerremy Holland**에게 감사드린다. 또한 벤치마킹에 대한 통찰력 있는 피드백과 제안을 제공한 **Angelos Katharopoulos**와 **Awni Hannun**에게 감사를 표한다. 더불어, 유용한 피드백과 조언을 제공한 **Jen-Hao Rick Chang**과 **Vishwanath Sindagi**에게도 감사의 뜻을 전한다.
