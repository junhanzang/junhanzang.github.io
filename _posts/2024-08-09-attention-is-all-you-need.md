---
title: "Attention Is All You Need"
date: 2024-08-09 22:14:28
categories:
  - 인공지능
---

<https://arxiv.org/abs/1706.03762>

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

**요약**

기존의 시퀀스 변환 모델들은 인코더와 디코더를 포함하는 복잡한 순환 신경망(RNN) 또는 합성곱 신경망(CNN)에 기반하고 있습니다. 최고 성능의 모델들은 또한 주의 메커니즘(attention mechanism)을 통해 인코더와 디코더를 연결합니다. 우리는 전적으로 주의 메커니즘에 기반한 새로운 단순한 네트워크 아키텍처인 트랜스포머(Transformer)를 제안하며, 이는 순환과 합성곱을 완전히 배제합니다. 두 가지 기계 번역 작업에 대한 실험에서 이 모델은 더 높은 품질을 보이면서도 병렬화가 더 용이하고, 학습 시간이 상당히 줄어드는 것을 보여주었습니다. 우리 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU 점수를 기록했으며, 기존 최고 성과를 보인 모델들(앙상블 포함)을 2 BLEU 이상으로 개선했습니다. 또한 WMT 2014 영어-프랑스어 번역 작업에서는 8개의 GPU를 사용해 3.5일 동안 학습한 후 41.8 BLEU의 새로운 단일 모델 최고 성과를 기록했습니다. 이는 기존 문헌에서 가장 뛰어난 모델들의 학습 비용의 일부에 불과합니다. 우리는 트랜스포머가 큰 데이터셋과 제한된 데이터셋 모두에서 성공적으로 영어 문법 분석 작업에 적용될 수 있음을 보여줌으로써, 이 모델이 다른 작업에도 잘 일반화된다는 것을 입증했습니다.

**1. 서론**

순환 신경망(RNN), 특히 장단기 메모리(LSTM)와 게이트 순환 신경망(GRU)은 시퀀스 모델링과 변환 문제(예: 언어 모델링 및 기계 번역)에서 최첨단 접근 방식으로 확립되었습니다. 그 후로 많은 연구들이 순환 언어 모델과 인코더-디코더 아키텍처의 한계를 계속해서 넓혀왔습니다.

순환 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 나눕니다. 이 위치들을 계산 시간의 단계와 맞추면서, 이전 숨겨진 상태와 현재 위치의 입력을 함수로 하여 숨겨진 상태들의 시퀀스를 생성합니다. 이러한 본질적인 순차적 특성 때문에, 긴 시퀀스 길이에서는 병렬화가 어려워지고 메모리 제약으로 인해 예제 간의 배칭(batch)이 제한됩니다. 최근 연구에서는 팩터화 기법과 조건부 계산을 통해 계산 효율성을 크게 향상시키고, 후자의 경우 모델 성능도 개선했지만, 순차적 계산의 근본적인 제약은 여전히 남아 있습니다.

주의 메커니즘은 다양한 작업에서 시퀀스 모델링 및 변환 모델의 중요한 요소가 되었으며, 입력 또는 출력 시퀀스의 거리와 상관없이 의존성을 모델링할 수 있습니다. 하지만 대부분의 경우 이러한 주의 메커니즘은 순환 네트워크와 함께 사용됩니다.

이 연구에서는 순환을 배제하고 대신 주의 메커니즘만을 사용하여 입력과 출력 간의 전역 의존성을 도출하는 트랜스포머라는 모델 아키텍처를 제안합니다. 트랜스포머는 훨씬 더 많은 병렬화를 가능하게 하며, 8개의 P100 GPU에서 12시간 정도만 학습해도 새로운 번역 품질의 최고 성과를 달성할 수 있습니다.

**2. 배경**

순차적인 계산을 줄이려는 목표는 Extended Neural GPU [16], ByteNet [18], ConvS2S [9]와 같은 모델들의 기초를 이루고 있습니다. 이들 모두는 합성곱 신경망(CNN)을 기본 구성 요소로 사용하여 모든 입력 및 출력 위치에 대해 숨겨진 표현(hidden representations)을 병렬로 계산합니다. 이러한 모델들에서는 임의의 입력 또는 출력 위치 간의 신호를 연결하는 데 필요한 연산의 수가 위치 간의 거리에 따라 증가하는데, ConvS2S의 경우 선형적으로, ByteNet의 경우 로그적으로 증가합니다. 이는 먼 위치 간의 의존성을 학습하는 것을 더 어렵게 만듭니다. 트랜스포머에서는 이러한 연산 수를 일정하게 줄였지만, 주의 가중치를 평균화하는 위치로 인해 효과적인 해상도가 감소하는 대가를 치러야 했습니다. 이 효과는 3.2절에서 설명하는 멀티헤드 어텐션(Multi-Head Attention)으로 상쇄됩니다.

자기 주의(self-attention) 또는 내부 주의(intra-attention)는 시퀀스의 다른 위치들을 연결하여 시퀀스의 표현을 계산하는 주의 메커니즘입니다. 자기 주의는 독해, 요약 생성, 텍스트 간의 함의 추론, 그리고 작업에 독립적인 문장 표현 학습과 같은 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22].

엔드투엔드 메모리 네트워크(end-to-end memory networks)는 시퀀스에 맞춘 순환 대신 순환 주의 메커니즘에 기반을 두며, 단순 언어 질문 응답 및 언어 모델링 작업에서 우수한 성능을 보인 바 있습니다 [34].

하지만 우리가 아는 한, 트랜스포머는 시퀀스에 맞춘 RNN이나 합성곱을 사용하지 않고 자기 주의만으로 입력과 출력의 표현을 계산하는 첫 번째 변환 모델입니다. 다음 섹션에서는 트랜스포머를 설명하고, 자기 주의의 동기를 부여하며, [17, 18] 및 [9]와 같은 모델들에 비해 트랜스포머의 장점을 논의하겠습니다.

3. 모델 아키텍처

![](/assets/images/posts/245/img.png)

Figure 1은 트랜스포머 모델 아키텍처를 보여줍니다.

![](/assets/images/posts/245/img_1.png)

트랜스포머는 인코더와 디코더 모두에서 쌓아 올린 자기 주의 메커니즘과 위치별(point-wise) 완전 연결 계층을 사용하며, 이는 각각 그림 1의 왼쪽과 오른쪽 절반에 나타나 있습니다.

**3.1 인코더와 디코더 스택**

**인코더:** 인코더는 N=6개의 동일한 계층으로 이루어진 스택으로 구성됩니다. 각 계층에는 두 개의 하위 계층이 있습니다. 첫 번째는 멀티헤드 자기 주의 메커니즘이고, 두 번째는 간단한 위치별 완전 연결 피드포워드 네트워크입니다. 우리는 각각의 두 하위 계층 주위에 잔차 연결(residual connection) [11]을 적용하고, 그 후 층 정규화(layer normalization) [1]를 수행합니다. 즉, 각 하위 계층의 출력은 LayerNorm(x+Sublayer(x))이며, 여기서 Sublayer(x)는 해당 하위 계층이 구현하는 함수입니다. 이러한 잔차 연결을 용이하게 하기 위해, 모델 내의 모든 하위 계층과 임베딩 계층은 d\_model=512의 차원으로 출력을 생성합니다.

**디코더:** 디코더도 N=6개의 동일한 계층으로 이루어진 스택으로 구성됩니다. 각 인코더 계층의 두 하위 계층 외에도, 디코더는 인코더 스택의 출력에 대해 멀티헤드 어텐션을 수행하는 세 번째 하위 계층을 추가합니다. 인코더와 마찬가지로, 우리는 각 하위 계층 주위에 잔차 연결을 적용하고 그 후에 층 정규화를 수행합니다. 또한 디코더 스택에서 자기 주의 하위 계층을 수정하여 특정 위치가 이후의 위치에 주의를 기울이지 않도록 합니다. 이 마스킹(masking)은 출력 임베딩이 하나의 위치로 오프셋되는 것과 결합되어, 위치 i에 대한 예측이 위치 i보다 작은 위치에서 알려진 출력에만 의존하도록 보장합니다.

**3.2 주의 메커니즘(Attention)**

주의 메커니즘(Attention Function)은 쿼리(query)와 키-값(key-value) 쌍을 출력으로 매핑하는 함수로 설명할 수 있습니다. 여기서 쿼리, 키, 값, 출력 모두 벡터입니다. 출력은 값들의 가중합으로 계산되며, 각 값에 할당된 가중치는 쿼리와 해당 키의 호환성 함수에 의해 계산됩니다.

**3.2.1 스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)**

우리는 우리의 주의 메커니즘을 "스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)"이라고 부릅니다(그림 2 참조). 입력은 차원 d\_k​를 가진 쿼리와 키, 그리고 차원 d\_v​를 가진 값들로 구성됩니다. 우리는 쿼리와 모든 키들 간의 점곱(dot product)을 계산하고, 이를 d\_k​로 나눈 후 소프트맥스(softmax) 함수를 적용하여 값들의 가중치를 얻습니다.

실제로는, 여러 쿼리들을 동시에 처리하기 위해 이들을 하나의 행렬 Q로 묶어서 계산합니다. 키와 값들도 각각 K와 V 행렬로 묶습니다. 출력 행렬은 다음과 같이 계산됩니다:

![](/assets/images/posts/245/img_2.png)

가장 일반적으로 사용되는 두 가지 주의 메커니즘은 가산 주의(Additive Attention) [2]와 닷-프로덕트(곱셈적) 주의(Dot-Product Attention)입니다. 닷-프로덕트 주의는 스케일링 인자를 제외하고는 우리의 알고리즘과 동일합니다. 가산 주의는 단일 은닉층을 가진 피드포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 둘은 이론적으로 유사한 복잡도를 가지지만, 닷-프로덕트 주의는 고도로 최적화된 행렬 곱셈 코드를 사용하여 훨씬 빠르고 공간 효율적입니다.

![](/assets/images/posts/245/img_3.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/245/img_4.png)

![](/assets/images/posts/245/img_5.png)

![](/assets/images/posts/245/img_6.png)

![](/assets/images/posts/245/img_7.png)

![](/assets/images/posts/245/img_8.png)

![](/assets/images/posts/245/img_9.png)

q가 들어오는거고 k가 학습에 따라 계속 업데이트되서 v값이 q에따라 바뀐다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**3.2.2 멀티헤드 어텐션(Multi-Head Attention)**

![](/assets/images/posts/245/img_10.png)

![](/assets/images/posts/245/img_11.png)

그림 2: (위) 스케일드 닷-프로덕트 어텐션. (아래) 멀티헤드 어텐션은 여러 개의 어텐션 레이어가 병렬로 실행되는 구조를 보여줍니다.

![](/assets/images/posts/245/img_12.png)

멀티헤드 어텐션은 모델이 서로 다른 위치에서 서로 다른 표현 하위 공간(representation subspaces)에서 정보를 동시에 주목할 수 있도록 합니다. 단일 어텐션 헤드를 사용하면 평균화가 이를 방해합니다.

![](/assets/images/posts/245/img_13.png)

![](/assets/images/posts/245/img_14.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

q 차원은 k차원과 같고 q의 부족한 입력은 0이던 다른값이던 채워서 진행될 것이다

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**3.2.3 모델에서 어텐션의 적용**

트랜스포머는 세 가지 방법으로 멀티헤드 어텐션을 사용합니다:

- **"인코더-디코더 어텐션" 레이어**: 쿼리는 이전 디코더 레이어에서 나오고, 메모리 키와 값은 인코더의 출력에서 가져옵니다. 이는 디코더의 모든 위치가 입력 시퀀스의 모든 위치에 주목할 수 있게 합니다. 이는 [38, 2, 9]와 같은 시퀀스-투-시퀀스 모델에서의 일반적인 인코더-디코더 어텐션 메커니즘을 모방합니다.
- **인코더 내의 자기 어텐션 레이어**: 자기 어텐션 레이어에서는 모든 키, 값, 쿼리가 동일한 곳, 즉 인코더의 이전 레이어 출력에서 나옵니다. 인코더의 각 위치는 인코더의 이전 레이어의 모든 위치에 주목할 수 있습니다.
- **디코더 내의 자기 어텐션 레이어**: 디코더의 자기 어텐션 레이어는 디코더의 각 위치가 해당 위치까지의 모든 위치에 주목할 수 있도록 합니다. 우리는 디코더에서 자기 회귀(auto-regressive) 특성을 유지하기 위해 왼쪽으로의 정보 흐름을 방지해야 합니다. 이를 위해, 스케일드 닷-프로덕트 어텐션 내에서 소프트맥스의 입력 중 불법적인 연결에 해당하는 모든 값을 마스킹(즉, −∞-\infty−∞로 설정)하여 이를 구현합니다. 이는 그림 2에 나와 있습니다.

**3.3 위치별 피드포워드 네트워크(Position-wise Feed-Forward Networks)**

어텐션 하위 레이어 외에도, 인코더와 디코더의 각 레이어에는 각각의 위치에 동일하게 적용되는 완전 연결 피드포워드 네트워크가 포함됩니다. 이는 두 개의 선형 변환 사이에 ReLU 활성화를 가지는 구조입니다.

피드포워드 네트워크(FFN) 계산 공식:

![](/assets/images/posts/245/img_15.png)

![](/assets/images/posts/245/img_16.png)

3.4 임베딩과 소프트맥스(Embeddings and Softmax)

![](/assets/images/posts/245/img_17.png)

3.5 위치 인코딩(Positional Encoding)

![](/assets/images/posts/245/img_18.png)

![](/assets/images/posts/245/img_19.png)

우리는 또한 학습된 위치 임베딩을 사용하는 실험을 진행했으며, 두 버전이 거의 동일한 결과를 도출한다는 것을 발견했습니다(표 3, 행 (E) 참조). 우리는 모델이 훈련 중에 접한 것보다 더 긴 시퀀스 길이로도 외삽(extrapolate)할 수 있을 가능성이 있어서, 사인 함수 기반의 버전을 선택했습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

기억나는 요세 쓰는 대안적 방법들

### 1. **Relative Positional Encoding**

- **개요**: 기존의 Positional Encoding은 절대 위치 정보를 제공하지만, **Relative Positional Encoding**은 토큰들 간의 상대적인 위치 정보를 강조합니다. 이는 시퀀스 내에서 토큰 간의 관계를 더 유연하게 표현할 수 있습니다.
- **장점**: 상대적인 위치 정보는 시퀀스 길이에 덜 의존적이며, 다양한 시퀀스 길이에서 더 일반화될 수 있습니다. 특히 문장 구조가 중요한 자연어 처리 작업에서 효과적입니다.
- **사용 예시**: Transformer-XL, T5 모델 등이 상대적 위치 인코딩을 사용합니다.

### 2. **Learned Positional Embeddings**

- **개요**: 위치 정보를 학습된 임베딩으로 표현하는 방법입니다. 모델이 학습 과정에서 최적의 위치 임베딩을 자동으로 학습합니다.
- **장점**: 고정된 수학적 함수로 인코딩하는 것보다 더 유연하고 데이터에 특화된 위치 정보를 학습할 수 있습니다.
- **사용 예시**: BERT와 같은 모델들이 학습된 위치 임베딩을 사용합니다.

### 3. **Rotary Positional Embedding (RoPE)**

- **개요**: RoPE는 입력 벡터를 회전시키는 방식으로 위치 정보를 인코딩합니다. 이 방식은 고정된 주파수 함수로 위치 정보를 인코딩하면서도, 상대적 위치 정보를 더 잘 반영합니다.
- **장점**: 상대적 위치 정보를 효율적으로 반영할 수 있으며, 특히 GPT-3와 같은 대규모 언어 모델에서 사용되고 있습니다.
- **사용 예시**: GPT-3 모델에 적용된 기술로 잘 알려져 있습니다.

### 4. **Performer**

- **개요**: Performer는 Fast Attention 기반의 모델로, 위치 인코딩을 하지 않거나, 입력을 효율적으로 처리할 수 있도록 특화된 위치 인코딩 방법을 사용합니다.
- **장점**: 높은 효율성과 낮은 계산 비용을 유지하면서도 효과적인 위치 정보를 제공합니다.
- **사용 예시**: Performer 모델 자체는 효율적인 어텐션 메커니즘으로 잘 알려져 있습니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

**4 자기 주의(Self-Attention)의 이유**

이 섹션에서는 자기 주의 레이어와, 주로 시퀀스 변환 인코더 또는 디코더의 숨겨진 레이어에서 가변 길이의 기호 표현 시퀀스 (x1,…,xn)를 같은 길이의 다른 시퀀스로 매핑하기 위해 사용되는 순환 및 합성곱 레이어의 다양한 측면을 비교합니다. 자기 주의를 사용하는 이유를 설명하기 위해 세 가지 바람직한 기준을 고려합니다.

첫 번째는 레이어당 전체 계산 복잡도입니다. 두 번째는 병렬화할 수 있는 계산의 양으로, 이는 필요한 최소 순차 연산 수로 측정됩니다.세 번째는 네트워크에서 장거리 의존성 간의 경로 길이입니다. 장거리 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 중요한 도전 과제입니다. 이러한 의존성을 학습하는 능력에 영향을 미치는 주요 요소 중 하나는 네트워크에서 앞뒤 신호가 이동해야 하는 경로의 길이입니다. 입력 및 출력 시퀀스의 임의의 위치 조합 간의 경로가 짧을수록 장거리 의존성을 학습하기가 더 쉬워집니다 [12]. 따라서, 우리는 서로 다른 레이어 유형으로 구성된 네트워크에서 임의의 두 입력 및 출력 위치 간의 최대 경로 길이도 비교합니다.

![](/assets/images/posts/245/img_20.png)

표 1: 다양한 레이어 유형에 대한 레이어당 최대 경로 길이, 계산 복잡도 및 최소 순차 연산 수. 여기서 n은 시퀀스 길이, d는 표현 차원, k는 합성곱의 커널 크기, r는 제한된 자기 주의에서의 이웃 크기를 나타냅니다.

![](/assets/images/posts/245/img_21.png)

![](/assets/images/posts/245/img_22.png)

추가적인 이점으로, 자기 주의는 더 해석 가능한 모델을 제공할 수 있습니다. 우리는 모델에서 나온 어텐션 분포를 조사하고, 부록에서 예시를 제시하고 논의합니다. 개별 어텐션 헤드가 명확히 다른 작업을 수행하도록 학습될 뿐만 아니라, 많은 경우 문장의 구문 및 의미 구조와 관련된 행동을 보이는 것처럼 보입니다.

**5. 훈련(Training)**

이 섹션에서는 우리 모델의 훈련 방식을 설명합니다.

**5.1 훈련 데이터와 배칭(Batching)**

우리는 약 450만 개의 문장 쌍으로 구성된 표준 WMT 2014 영어-독일어 데이터셋에서 모델을 훈련했습니다. 문장은 바이트-페어 인코딩(byte-pair encoding, BPE) [3]을 사용해 인코딩되었으며, 약 37,000개의 토큰으로 구성된 공유된 소스-타겟 어휘를 가지고 있습니다. 영어-프랑스어의 경우, 3600만 개의 문장으로 구성된 훨씬 더 큰 WMT 2014 영어-프랑스어 데이터셋을 사용했으며, 32,000개의 단어 조각(word-piece) 어휘로 토큰을 분할했습니다 [38]. 문장 쌍은 대략적인 시퀀스 길이에 따라 배칭되었습니다. 각 훈련 배치에는 약 25,000개의 소스 토큰과 25,000개의 타겟 토큰을 포함한 문장 쌍 집합이 포함되었습니다.

**5.2 하드웨어와 일정(Schedule)**

우리는 8개의 NVIDIA P100 GPU가 장착된 한 대의 머신에서 모델을 훈련했습니다. 논문 전반에 걸쳐 설명된 하이퍼파라미터를 사용하는 기본(base) 모델의 경우, 각 훈련 단계는 약 0.4초가 소요되었습니다. 우리는 기본 모델을 총 100,000단계, 즉 12시간 동안 훈련했습니다. 큰(big) 모델(표 3의 하단 행에 설명된 모델)의 경우, 각 단계는 1.0초가 소요되었습니다. 큰 모델은 300,000단계(3.5일) 동안 훈련되었습니다.

**5.3 옵티마이저(Optimizer)**

![](/assets/images/posts/245/img_23.png)

**5.4 정규화(Regularization)**

우리는 훈련 중 세 가지 유형의 정규화를 적용했습니다:

![](/assets/images/posts/245/img_24.png)

**6. 결과(Results)**

**6.1 기계 번역(Machine Translation)**

![](/assets/images/posts/245/img_25.png)

**표 2**: 트랜스포머 모델은 영어-독일어 및 영어-프랑스어 newstest2014 테스트에서 이전 최첨단 모델들보다 더 나은 BLEU 점수를 기록했으며, 훈련 비용은 훨씬 적었습니다.

WMT 2014 영어-독일어 번역 작업에서, 트랜스포머의 큰(big) 모델(표 2에서 Transformer (big))은 이전에 보고된 최고 모델들(앙상블 포함)을 2.0 BLEU 이상 초과하여 28.4라는 새로운 최고 BLEU 점수를 기록했습니다. 이 모델의 구성은 표 3의 하단에 나와 있습니다. 훈련은 8개의 P100 GPU에서 3.5일이 소요되었습니다. 심지어 기본(base) 모델도 모든 이전에 발표된 모델들과 앙상블을 초과했으며, 경쟁 모델들의 훈련 비용의 일부만으로도 뛰어난 성능을 발휘했습니다.

![](/assets/images/posts/245/img_26.png)

**표 2**는 우리의 결과를 요약하고, 다른 문헌의 모델 아키텍처와 비교하여 번역 품질과 훈련 비용을 비교합니다. 우리는 모델을 훈련하는 데 사용된 부동 소수점 연산의 수를 추정하기 위해 훈련 시간, 사용된 GPU 수, 그리고 각 GPU의 지속적인 단일 정밀도 부동 소수점 성능을 곱하여 계산했습니다 .

**6.2 모델 변형(Model Variations)**

![](/assets/images/posts/245/img_27.png)

**표 3**: 트랜스포머 아키텍처의 변형. 나열되지 않은 값들은 기본(base) 모델과 동일합니다. 모든 메트릭은 영어-독일어 번역 개발 세트인 newstest2013에서 측정되었습니다. 나열된 퍼플렉시티(perplexity)는 바이트-페어 인코딩에 따른 워드피스(word-piece) 단위로 계산되었으며, 단어 단위 퍼플렉시티와 비교해서는 안 됩니다.

트랜스포머의 다양한 구성 요소의 중요성을 평가하기 위해, 우리는 기본 모델을 여러 가지 방식으로 변형하여, 개발 세트인 newstest2013에서 영어-독일어 번역 성능의 변화를 측정했습니다. 이전 섹션에서 설명한 대로 빔 서치를 사용했지만, 체크포인트 평균화는 사용하지 않았습니다. 이러한 결과를 표 3에 제시합니다.

**표 3의 (A) 행**에서는 계산량을 일정하게 유지하면서 주의 헤드(attention head) 수와 주의 키(key) 및 값(value)의 차원을 다양하게 조정했습니다(섹션 3.2.2 참조). 단일 헤드 어텐션(single-head attention)은 최적 설정보다 0.9 BLEU 낮은 성능을 보였으며, 너무 많은 헤드를 사용할 경우에도 성능이 떨어졌습니다.

**표 3의 (B) 행**에서는 주의 키 크기 d\_k​를 줄이면 모델 품질이 저하된다는 것을 관찰했습니다. 이는 호환성을 결정하는 것이 쉽지 않으며, 점곱(dot product)보다 더 정교한 호환성 함수가 유리할 수 있음을 시사합니다. **표 3의 (C)와 (D) 행**에서는 예상대로 더 큰 모델이 더 우수하며, 드롭아웃이 과적합을 피하는 데 매우 도움이 된다는 것을 확인했습니다. **표 3의 (E) 행**에서는 사인 함수 기반의 위치 인코딩을 학습된 위치 임베딩으로 대체했으며, 기본 모델과 거의 동일한 결과를 얻었습니다.

**6.3 영어 구문 분석(English Constituency Parsing)**

![](/assets/images/posts/245/img_28.png)

**표 4**: 트랜스포머는 영어 구문 분석에 잘 일반화됩니다(결과는 WSJ의 23 섹션에서 측정되었습니다).

트랜스포머가 다른 작업에도 잘 일반화할 수 있는지 평가하기 위해, 우리는 영어 구문 분석에 대한 실험을 수행했습니다. 이 작업은 특정한 도전 과제를 제시합니다. 출력은 강력한 구조적 제약을 받으며, 입력보다 상당히 길어집니다. 더욱이, RNN 시퀀스-투-시퀀스 모델들은 소규모 데이터 환경에서 최첨단 성과를 달성하지 못했습니다 [37].

![](/assets/images/posts/245/img_29.png)

**표 4**의 결과는 작업별 튜닝이 부족함에도 불구하고, 우리의 모델이 놀랍도록 잘 수행되며, Recurrent Neural Network Grammar [8]를 제외한 모든 이전에 보고된 모델들보다 더 나은 결과를 제공한다는 것을 보여줍니다.

RNN 시퀀스-투-시퀀스 모델과 달리 [37], 트랜스포머는 WSJ 훈련 세트의 4만 개 문장만으로 훈련했을 때도 BerkeleyParser [29]보다 더 우수한 성능을 발휘했습니다.

**7. 결론(Conclusion)**

이 연구에서 우리는 트랜스포머를 소개했습니다. 트랜스포머는 전적으로 어텐션에 기반한 최초의 시퀀스 변환 모델로, 인코더-디코더 아키텍처에서 가장 일반적으로 사용되던 순환 레이어를 멀티헤드 자기 어텐션으로 대체했습니다.

번역 작업의 경우, 트랜스포머는 순환 또는 합성곱 레이어 기반 아키텍처보다 훨씬 빠르게 훈련될 수 있습니다. WMT 2014 영어-독일어 및 WMT 2014 영어-프랑스어 번역 작업에서 모두 새로운 최첨단 성능을 달성했습니다. 전자의 작업에서는, 우리의 최고 모델이 이전에 보고된 모든 앙상블보다도 우수한 성과를 보였습니다.

우리는 어텐션 기반 모델의 미래에 대해 매우 기대하고 있으며, 이를 다른 작업에도 적용할 계획입니다. 우리는 트랜스포머를 텍스트 이외의 입력 및 출력 모달리티를 포함하는 문제로 확장하고, 이미지, 오디오, 비디오와 같은 대규모 입력 및 출력을 효율적으로 처리하기 위해 국소적이고 제한된 어텐션 메커니즘을 조사할 계획입니다. 생성 과정을 덜 순차적으로 만드는 것도 우리의 연구 목표 중 하나입니다.

우리가 모델을 훈련하고 평가하는 데 사용한 코드는 <https://github.com/tensorflow/tensor2tensor> 에서 사용할 수 있습니다.

**감사의 말(Acknowledgements)**

Nal Kalchbrenner와 Stephan Gouws에게 그들의 유익한 의견, 수정 및 영감에 대해 감사드립니다.

[1706.03762v7.pdf

2.11MB](./file/1706.03762v7.pdf)

오랜만에 봐도 항상 잘쓴것 같다
