---
title: "ToonCrafter: Generative Cartoon Interpolation"
date: 2024-06-15 23:02:43
categories:
  - 인공지능
---

<https://doubiiu.github.io/projects/ToonCrafter>

[ToonCrafter: Generative Cartoon Interpolation](https://doubiiu.github.io/projects/ToonCrafter)

요약 우리는 ToonCrafter를 소개한다. 이는 전통적인 대응 기반 만화 비디오 보간을 넘어 생성적 보간으로 나아가는 새로운 접근법이다. 전통적인 방법들은 종종 선형적인 움직임과 디소클루전(disocclusion)과 같은 복잡한 현상의 부재를 암묵적으로 가정하기 때문에, 만화에서 흔히 발생하는 과장된 비선형 및 큰 움직임과 가림(occlusion) 현상에서 부자연스럽거나 실패한 보간 결과를 초래한다. 이러한 한계를 극복하기 위해 우리는 라이브 액션 비디오 우선순위를 생성적 프레임워크 내에서 만화 보간에 더 적합하도록 조정하는 가능성을 탐구한다. ToonCrafter는 라이브 액션 비디오 움직임 우선순위를 생성적 만화 보간에 적용할 때 직면하는 문제들을 효과적으로 해결한다. 먼저, 우리는 라이브 액션 비디오 우선순위를 만화 영역에 원활하게 적응시키는 만화 교정 학습 전략을 설계하여 도메인 간 격차와 콘텐츠 유출 문제를 해결한다. 다음으로, 우리는 손실된 세부 사항을 보상하기 위해 이중 참조 기반 3D 생성 보간 디코더를 도입하여 압축된 잠재 우선 공간으로 인해 세부 사항을 유지한다. 마지막으로, 우리는 사용자에게 보간 결과에 대한 상호작용 제어를 가능하게 하는 유연한 스케치 인코더를 설계한다. 실험 결과는 제안된 방법이 시각적으로 설득력 있고 자연스러운 역학을 제공할 뿐만 아니라 디소클루전을 효과적으로 처리함을 보여준다. 비교 평가를 통해 기존 경쟁자들보다 우리의 접근법이 현저히 우수하다는 것이 입증되었다. 코드는 출판 후 제공될 예정이다.

1. 서론

만화 애니메이션은 인기 있는 매체이자 예술 형식이지만, 프레임마다 수작업으로 그리는 특성 때문에 여전히 많은 노동을 필요로 한다. 최근 실사 비디오 프레임 보간 방법들은 어느 정도 자동화를 가능하게 하지만, 만화 프레임 보간에는 만족스럽지 않다. 만화 애니메이션과 실사 비디오의 차이는 두 가지 측면에 있다: 프레임의 "희소성"과 텍스처의 풍부함이다. 실사 비디오 프레임은 카메라로 밀집하게 촬영할 수 있는 반면, 만화 프레임은 높은 제작 비용 때문에 시간적으로 희소하다(따라서 큰 움직임이 발생). 이러한 비용은 또한 만화에서 텍스처가 없는 컬러 영역이 실사 비디오보다 더 많이 발생하게 한다. 이 두 가지 특성은 만화 프레임 보간을 매우 어렵게 만든다.

지난 몇 년간, 심층 신경망에 의해 만화 비디오 보간이 상당히 발전했으나, 이러한 발전은 주로 기저 움직임이 단순하다고 가정하여 선형 보간에 초점을 맞추었다. 이들의 일반적인 아이디어는 먼저 두 프레임 간의 대응 관계를 식별하고(e.g. 옵티컬 플로우), 그런 다음 프레임을 선형적으로 보간하는 것이다. 그림 1(상단)에 보이는 예제는 걷는 사람을 특징으로 하는데, 명백히 선형 보간은 올바른 걷는 시퀀스 대신 "떠다니는" 사람을 생성할 뿐이다. 그림 1(중간)의 디소클루전 예제에서는 상황이 더욱 복잡해진다. 즉, 일상 생활에서 관찰되는 명백한 움직임을 보간하는 데 선형 움직임 가정은 크게 불충분하다.

이 논문에서 우리는 주어진 프레임의 정보에만 의존하는 대신, 복잡한 비선형 움직임이나 현상의 프레임을 합성하기 위해 생성적 만화 보간의 중요성을 지적한다. 최근 이미지 기반 확산 텍스트-비디오 모델의 발전은 이러한 대규모 데이터 학습 모델이 이미지로부터 다양한 타당한 비디오를 합성할 수 있음을 시사한다. 이는 우리가 생성적 만화 보간을 위해 이러한 비디오 모델에서 학습된 풍부한 움직임 우선순위를 활용할 가능성을 조사하게 한다.

불행히도 기존 모델을 만화 보간에 직접 적용하는 것은 세 가지 요인 때문에 만족스럽지 않다. 첫째, 모델이 주로 실사 비디오 콘텐츠로 학습되었기 때문에 도메인 간 격차가 존재한다. 비만화 콘텐츠가 우발적으로 합성될 수 있으며, 모델이 애니메이션 도메인 콘텐츠를 오해하여 적절한 움직임을 생성하지 못할 수 있다. 둘째, 계산 비용을 줄이기 위해 현재 비디오 확산 모델은 고도로 압축된 잠재 공간에 기반을 두고 있어, 세부 사항의 손실과 품질 저하가 발생한다. 이러한 저하는 고대비 영역, 세밀한 구조적 윤곽, 움직임 블러의 부재로 인해 더욱 두드러진다. 마지막으로, 생성 모델은 다소 무작위적이며 제어가 부족하다. 생성된 움직임에 대한 효과적인 제어가 만화 보간에 필요하다.

이 논문에서는 사전 학습된 비디오 확산 모델을 적응시키고 위의 세 가지 문제를 해결하기 위해 효과적이고 제어 가능한 생성적 프레임워크인 ToonCrafter를 제안한다. ToonCrafter는 세 가지 기능적 기술로 구성된다: 만화 교정 학습, 디코딩에서의 세부 사항 주입 및 전파, 스케치 기반 제어 가능한 생성. 구체적으로, 만화 교정 학습 전략은 수집된 만화 데이터에 대한 기저 이미지 조건 비디오 생성 모델의 공간 관련 맥락 이해 및 콘텐츠 생성 레이어를 세밀하게 조정하는 것을 포함한다. 이 접근법은 정확한 맥락 이해를 제공하고 만화 애니메이션 도메인에 적응하면서 풍부한 움직임 우선순위를 유지한다. 세부 손실과 품질 저하 문제를 해결하기 위해, 우리는 입력 이미지의 세부 정보를 생성된 프레임 잠재 변수로 주입하는 교차 주의 메커니즘을 사용하여 얕은 디코딩 레이어와 잔여 학습을 깊은 레이어에 고려한 이중 참조 기반 3D 디코더를 도입한다. 또한, 디코더는 시간적 일관성을 촉진하고 개선하기 위해 의사 3D 컨볼루션을 갖추고 있다. 마지막으로, 우리는 사용자에게 시간적으로 희소하거나 밀집된 움직임 구조 안내로 보간 결과를 유연하고 상호작용적으로 생성하거나 수정할 수 있는 프레임 독립 스케치 인코더를 제안한다.

그림 1에 나타난 바와 같이, ToonCrafter는 큰 비선형 움직임과 디소클루전이 있는 매우 어려운 경우에도 고품질의 중간 프레임을 생성할 수 있다. 또한 사용자가 희소한 스케치 입력을 통해 생성된 움직임을 효과적으로 제어할 수 있게 한다. 우리는 제안된 ToonCrafter를 평가하기 위해 광범위한 실험을 수행했으며, 기존 경쟁자들에 비해 상당한 우월성을 입증했다. 우리의 기여는 다음과 같이 요약된다:

- 생성적 만화 보간의 개념을 지적하고, 실사 비디오 우선순위를 활용하여 기존 경쟁자들을 크게 능가하는 혁신적인 솔루션을 도입했다.
- 실사 움직임 우선순위를 애니메이션 도메인에 효과적으로 적응시키는 만화 교정 학습 전략을 제시했다.
- 압축된 잠재 공간으로 인해 손실된 세부 사항을 보상하기 위한 이중 참조 기반 3D 디코더를 제안했다.
- 사용자에게 유연하고 제어 가능한 방식으로 보간 결과를 생성하거나 수정할 수 있는 시스템을 제공했다.

### 2. 관련 연구

#### 2.1. 비디오 프레임 보간

비디오 프레임 보간은 원본 비디오의 인접한 두 프레임 사이에 여러 프레임을 합성하는 것을 목표로 하며, 이는 최근 몇 년 동안 많이 연구되어 왔다. 기존의 딥러닝을 사용하는 작업은 위상 기반 방법 [32, 33], 커널 기반 방법 [10, 21, 29, 36, 37, 57, 61], 그리고 광학/특징 흐름 기반 방법 [4, 20, 22, 35, 40, 50, 52, 55]의 세 가지 범주로 나눌 수 있다. 최신의 최첨단 기술은 흐름 추정 [30, 38, 46]의 최근 발전 덕분에 더 많은 광학 흐름 기반 방법을 포함하게 되었다. 일반적인 접근법은 먼저 흐름을 사용하여 두 프레임 간의 대응 관계를 식별한 다음, 왜곡 [13] 및 융합을 수행하는 것이다. 독자들은 포괄적인 설문 조사 [11]를 참조하는 것이 좋다.

이러한 방법들은 실사 비디오를 보간하는 데 큰 성공을 거두었지만, 일반적으로 만화의 큰 비선형 움직임과 텍스처가 없는 영역을 처리하는 데 실패한다. 기존 연구들은 위의 문제 중 일부를 해결하려고 한다. Zhu 등 [62]은 만화 영역 대응을 네트워크 흐름 최적화 문제로 공식화하였다. AnimeInterp [24]는 색상 조각 매칭을 기반으로 한 세그먼트 유도 매칭 모듈을 도입하여 대응 식별을 강화하였다. 더 나아가, EISAI [8]는 도메인 특화 인지 손실을 사용하여 단색 영역의 이상을 제거하여 인지 품질을 향상시킨다. 가장 최근에, Li 등 [25]은 큰 움직임 문제를 해결하기 위해 중간 스케치 지침을 도입하였지만, 이는 손으로 그려야 하는 필요성 때문에 항상 사용 가능하지는 않다.

이러한 방법들은 만화 비디오 보간을 크게 발전시켰지만, 여전히 대응 관계의 명시적 식별과 선형 또는 단순한 움직임 가정에 의존한다. 이들은 만화에서의 복잡한 비선형 움직임이나 디소클루전 현상을 모델링하는 데 실패한다. 반면, 우리는 실사 비디오 생성 우선순위를 활용하여 새로운 생성적 만화 보간 패러다임을 통해 이러한 문제를 해결하고자 한다.

#### 2.2. 이미지 조건 비디오 확산 모델

최근에는 대규모 텍스트-비디오 (T2V) 모델 [6, 7, 16, 17, 31, 43, 49, 53]을 대규모 데이터셋에서 확산 모델(DMs) [18]을 사용하여 훈련하는 데 상당한 노력이 기울여지고 있다. 또한, 이러한 T2V 모델에 추가적인 이미지 조건을 도입하여 이미지-비디오 (I2V) 합성을 위한 연구도 잘 진행되고 있다 [5, 14, 48, 54, 60]. 구체적으로, SEINE [9]는 두 입력 프레임을 노이즈가 있는 비디오 잠재 변수와 결합하여 두 다른 장면을 연결하는 창의적인 전환 비디오 세그먼트를 생성하기 위해 처음으로 제안되었다. 마찬가지로, 단일 이미지-비디오 확산 모델인 DynamiCrafter [54], SparseCtrl [15], 그리고 PixelDance [56]도 두 입력 프레임을 노이즈가 있는 프레임 잠재 변수와 결합하거나 [54, 56], 보조 프레임 인코더 [15]를 사용하여 비디오 보간/전환의 하위 응용 프로그램에 대한 확장성을 보여준다.

불행히도, 만화 보간에 적용할 때 이러한 모델들은 독특한 만화의 도전 과제로 인해 안정적이지 않다. 이 논문에서는 실사 비디오에서 학습한 I2V 확산 모델의 풍부한 움직임 생성 우선순위를 활용하고 이를 생성적 만화 보간을 위해 적응시키고자 한다.

![](/assets/images/posts/162/img.png)

![](/assets/images/posts/162/img_1.png)

### 3.2. 만화 교정 학습

만화와 실사 비디오는 시각적 스타일, 과장된 표현, 단순화된 텍스처 등의 요인으로 인해 도메인 격차를 보인다. 이러한 도메인 격차는 기존의 비디오 생성 우선 모델(주로 실사 비디오로 훈련된)을 만화 애니메이션에 적용하는 데 어려움을 준다. 잠재적인 문제로는 비의도적으로 비만화 콘텐츠가 합성되는 것(Figure 5-I)과 모델이 애니메이션 콘텐츠를 정확하게 이해하지 못해 부적절한 움직임을 생성하는 경우가 있다. 이를 해결하기 위해, 우리는 먼저 만화 비디오 데이터셋을 수집하고 정교하게 설계된 파인 튜닝을 통해 운동 우선 모델을 만화 도메인에 적응시킨다.

#### 만화 비디오 데이터셋 구축

우리는 일련의 원시 만화 비디오를 수집한 후, 해상도와 주관적 품질을 기준으로 고품질 비디오를 수동으로 선택한다. 선택된 비디오의 총 길이는 500시간 이상이다. 우리는 PySceneDetect [1]를 사용하여 샷을 감지하고 분할한다. 평균 광학 흐름 [46] 크기가 낮은 비디오를 제거하여 정지 샷을 필터링한다. 또한, CRAFT [2]를 적용하여 많은 양의 텍스트가 포함된 클립을 제거한다. LAION [42] 회귀 모델을 사용하여 미적 점수를 계산하고 저미적 샘플을 제거하여 품질을 보장한다. 다음으로, BLIP-2 [23]를 사용하여 각 클립에 합성 캡션을 달아준다. 마지막으로, CLIP [39] 임베딩을 사용하여 각 비디오 클립의 첫 번째, 중간, 마지막 프레임을 주석 처리하고, 이를 통해 텍스트-비디오 정렬을 측정하여 불일치 샘플을 필터링한다. 최종적으로, 우리는 271K개의 고품질 만화 비디오 클립을 얻었고, 이를 두 세트로 무작위 분할하였다. 훈련 세트는 270K개의 클립을, 평가 세트는 1K개의 클립을 포함한다.

#### 교정 학습

수집된 만화 텍스트-비디오 데이터를 사용하여, 우리는 실사 비디오로 훈련된 DynamiCrafter 보간 모델(DCinterp)을 만화 보간에 적응시킬 수 있다. 그러나 DCinterp의 소거 네트워크를 우리의 데이터에 직접 파인 튜닝하면, 우리 만화 비디오 데이터(270K 비디오 클립)와 DCinterp의 원래 훈련 데이터(WebVid-10M [3], 10M) 간의 불균형 규모로 인해 파국적 망각이 발생하여 운동 우선순위가 악화된다(Sec. 4.5에서 증명됨). 하나의 직관적인 해결책은 DCinterp의 원래 훈련 데이터를 우리의 파인 튜닝 데이터와 함께 조직적으로 할당하여 훈련 세트를 확장하는 것이지만, 이 접근법은 상당히 더 많은 훈련 계산을 필요로 한다.

이 문제를 해결하기 위해, 우리는 대규모 실사 비디오에서 얻은 견고한 운동 우선순위를 손상시키지 않으면서 소규모 만화 데이터셋을 사용하여 기본 모델을 파인 튜닝할 수 있는 효율적인 교정 학습 전략을 설계한다. DCinterp 모델은 이미지-컨텍스트 프로젝터, 공간 레이어(StableDiffusion v2.1과 동일한 아키텍처 공유), 시간 레이어의 세 가지 주요 구성 요소로 구성된다. 실험 결과(Sec. 4.5)에서, 우리는 다음과 같은 관찰을 하였다: 이미지-컨텍스트 프로젝터는 DCinterp 모델이 입력 프레임의 컨텍스트를 소화하도록 돕는다; 공간 레이어는 비디오 프레임의 외관 분포를 학습하는 데 책임이 있다; 시간 레이어는 비디오 프레임 간의 운동 역학을 포착한다. 즉, 시간 레이어는 파인 튜닝 중에 고정되어야 실제 운동 우선순위를 보존할 수 있다(Figure 2 참조). 반면에, 이미지-컨텍스트 프로젝터는 더 나은 의미 정렬을 달성하고 모델이 만화 장면 컨텍스트를 더 잘 소화하도록 파인 튜닝할 수 있다. 동시에, 공간 레이어도 외관 교정을 위해 조정되어 중간 프레임에서 실사 비디오 콘텐츠의 생성을 방지해야 한다. 만화 애니메이션이 약간 다를 수 있는 움직임(가능한 한 엄격하지 않음)을 나타낼 수 있지만, 고수준의 운동 개념은 여전히 동일하다(그렇지 않으면 인간 관찰자가 움직임을 인식할 수 없음), 이는 외관이 만화 애니메이션 도메인 적응에서 지배적인 요소임을 의미한다.

요약하자면, 우리의 만화 교정 학습 전략은 시간 레이어를 고정하여 실제 운동 우선순위를 보존하고, 수집한 만화 데이터만으로 이미지-컨텍스트 프로젝터와 공간 레이어를 파인 튜닝하여 효과적인 도메인 적응을 달성하는 것에 초점을 맞춘다.

### 3.3. 디코딩에서의 디테일 주입 및 전파

현재의 대부분의 비디오 확산 모델 [6, 48], 우리가 기반으로 한 DynamiCrafter [54]를 포함하여, 고도로 압축된 잠재 공간 [41]에서 비디오를 생성하는 법을 학습한다. 이는 일반적으로 벡터 양자화 자동 인코딩 (VQ-VAE) [12]을 통해 얻어진다. 잠재 비디오 확산 패러다임은 계산 요구를 효과적으로 줄인다. 그러나 이는 세부 사항의 상당한 손실과 허용할 수 없는 품질 저하(깜박임 및 왜곡 아티팩트 포함)를 초래한다. 불행히도 이러한 저하는 만화 애니메이션에서 더 두드러지는데, 이는 고대비 영역, 세밀한 구조 윤곽, 그리고 모션 블러의 부재로 인해 발생한다(실사 비디오의 모션 블러는 품질 저하를 효과적으로 "숨긴다"). 따라서, 만화 보간에 잠재 비디오 확산 모델을 직접 적용하면 구조와 텍스처가 아티팩트를 포함하거나 원래 입력 프레임과 일치하지 않는 결과를 초래할 가능성이 높아져, 허용할 수 없는 결과를 초래할 수 있다(그림 6의 네 번째 행).

몇 가지 해결책이 디코딩 품질을 개선하려고 시도하지만, 이들은 완벽하게 정렬된 참조가 필요하거나 [27,28], 단일 프레임 생성에만 작동하거나 [10], 결과에서 고주파 구성 요소를 희생한다 [5].

![](/assets/images/posts/162/img_2.png)

![](/assets/images/posts/162/img_3.png)

### 3.4. 스케치 기반 제어 가능한 생성

우리 프레임워크의 생성적 특성은 비선형 움직임 보간을 가능하게 하지만, 생성 결과에 변동을 불가피하게 도입한다. 이러한 변동은 일부 사용자에게 필요할 수 있지만, 다른 사용자들은 프레임 보간에 대해 더 나은 제어를 선호할 수 있다. 실제 제작 환경에서 우리의 프레임워크를 더 제어 가능하게 만들기 위해, 우리는 산업적 실천 [25]을 따르고 스케치 기반 생성 지침을 도입한다. 우리는 프레임 독립적인 스케치 인코더 SSS를 제안하여 사용자가 희소한 스케치 지침을 사용하여 생성된 움직임을 제어할 수 있게 한다. 어댑터 메커니즘 [58]을 기반으로 구축된 우리의 스케치 인코더는 비디오 확산 모델을 스케치 조건 생성 모델로 효과적으로 변환한다.

![](/assets/images/posts/162/img_4.png)

![](/assets/images/posts/162/img_5.png)

### 4. 실험

#### 4.1. 구현 세부사항

우리의 구현은 주로 이미지-비디오 모델인 DynamiCrafter [54](보간 변형 512x320 해상도 기반)에 기반한다. 만화 교정 학습에서, 우리는 학습률(lr) 1x10^-6과 미니 배치 크기 32로 50K 스텝 동안 공간 레이어와 PPP를 훈련했다. 우리는 이중 참조 기반 3D 디코더를 lr=4.5x10^-6과 배치 크기 16으로 60K 스텝 동안 훈련했다. 스케치 인코더는 lr=5x10^-5와 배치 크기 32로 50K 스텝 동안 훈련되었다. 우리의 ToonCrafter는 수집된 만화 비디오 데이터셋을 사용하여 훈련되었다.

### 표 1. 만화 테스트 세트(1K)에 대한 최신 비디오 보간 방법과의 정량적 비교

MetricAnimeInterpEISAIFILMSEINEOurs

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| FVD ↓ | 196.66 | 146.65 | 189.88 | 98.96 | 43.92 |
| KVD ↓ | 8.844 | 5.558 | 8.001 | 2.93 | 1.52 |
| LPIPS ↓ | 0.1890 | 0.1702 | 0.2519 | 0.1733 | 0.1733 |
| CLIP\_img ↑ | 0.8866 | 0.8983 | 0.9008 | 0.8531 | 0.9221 |
| CLIP\_txt ↑ | 0.3096 | 0.3947 | 0.3638 | 0.4262 | 0.4129 |
| CPBD ↑ | 0.5974 | 0.6443 | 0.6317 | 0.6403 | 0.6723 |

우리는 배치 내에서 동적 FPS로 512x320 해상도의 16 프레임을 샘플링하여 테스트했다. 추론 시에는, 우리는 DDIM 샘플러 [45]를 classifier-free guidance [19]와 함께 사용했다.

### 4.2. 정량적 비교

#### 지표와 데이터셋

생성된 비디오의 공간 및 시간 도메인에서 품질과 시간적 움직임 역학을 평가하기 위해, 우리는 Fréchet Video Distance(FVD) [47]와 Kernel Video Distance(KVD)를 보고한다. 추가로, 우리는 이미지 수준 평가도 실시한다. [8]에 따라, 우리는 LPIPS [59]를 사용하여 생성된 비디오의 지각적 유사성을 그라운드 트루스 비디오와 비교하여 측정한다. 생성된 비디오 프레임의 의미적 정확성을 평가하기 위해, 생성된 프레임과 그라운드 트루스 프레임 및 텍스트 프롬프트의 CLIP [39] 특징 간의 코사인 유사성을 계산하고, 이를 각각 CLIP\_img 및 CLIP\_txt로 표시한다. 우리는 CPBD [34]를 사용하여 선명도를 평가한다.

이 지표들은 만화 비디오 데이터셋의 1K 평가 세트(512x320 해상도의 16 프레임)에서 평가되었다. 우리는 우리의 방법을 다양한 최신 기법들과 비교했다: 만화 비디오 보간 방법(AnimeInterp [24]와 EISAI [8]), 큰 움직임을 처리하기 위한 일반 비디오 보간 방법(FILM [40]), 그리고 생성적 비디오 전환(SEINE [9]). 이들은 공정한 비교를 위해 우리의 데이터셋에 맞게 파인 튜닝되었다. 정량적 결과는 표 1에 제시되어 있다.

결과에 따르면, 제안된 방법은 거의 모든 지표에서 이전 접근 방법을 현저히 능가했다(LPIPS를 제외하고). 그러나 우리는 LPIPS가 전체 참조 지표이며 픽셀 수준의 정렬을 요구하기 때문에, 생성적 맥락에서는 덜 적합하다고 주장한다. 특히, 그라운드 트루스 중간 프레임이 유일한 정답이 아닐 수 있고, 특히 다중 유효 결과가 가능한 모호한 움직임에서는 더욱 그렇다.

### 4.3. 정성적 비교

대표적인 보간 결과의 시각적 비교는 그림 9에 나와 있다. 비교된 모든 방법 중에서 우리의 접근 방식은 비선형의 타당한 움직임을 생성한다. 반면, 전통적인 대응 기반 방법들(즉, AnimeInterp, EISAI, FILM)은 디소클루전(그림 9의 'man' 케이스에서 손과 팔이 '사라지는' 문제)을 처리하지 못하여 자연스러운 움직임을 생성하는 데 어려움을 겪는다. 이러한 방법들은 또한 복잡한 비선형 움직임을 합성하는 데에도 어려움을 겪는다(그림 9의 'car' 케이스에서 형태 변형). SEINE은 괜찮은 움직임을 보이는 비디오를 생성할 수 있지만, 구조와 텍스처에서 심각한 왜곡이 발생한다. 더 나은 비교를 위해 추가 비디오는 보충 자료에 제공된다.

### 표 2. 사용자 연구 통계: Motion Quality(M.Q.), Temporal Coherence(T.C.), Frame Fidelity(F.F.)에 대한 선호도 비율

PropertyAnimeInterpEISAIFILMSEINEOurs

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| M.Q. ↑ | 3.24% | 6.94% | 6.02% | 14.81% | 68.98% |
| T.C. ↑ | 6.94% | 14.81% | 13.43% | 15.14% | 49.07% |
| F.F. ↑ | 6.48% | 11.57% | 12.50% | 18.00% | 51.39% |

### 4.5. 소거 연구 (Ablation Study)

#### 만화 교정 학습

우리는 도메인 적응 전략의 효과를 조사하기 위해 다음과 같은 기준을 구성한다:

1. 사전 학습된 DCinterp [54] 기본 모델을 직접 사용.
2. 이미지 컨텍스트 프로젝터(ICP)와 전체 소거 U-Net(공간+시간 레이어)을 파인 튜닝.
3. 시간 레이어를 유지하면서 ICP와 공간 레이어를 파인 튜닝.
4. 시간 레이어를 우회하여 공간 레이어만 파인 튜닝.
5. ICP만 파인 튜닝.

정량적 비교는 표 3에 나와 있다. 어떠한 파인 튜닝 없이 DCinterp를 사용하는 경우(기준 1)는 예기치 않은 실사 콘텐츠 생성을 초래한다(그림 5의 2번째 행). 모든 레이어를 파인 튜닝하는 경우(기준 2)는 실사 콘텐츠 생성을 어느 정도 적응시키지만, 생성된 콘텐츠의 일관성 문제로 인해 시간 우선 순위를 악화시킨다(그림 5의 3번째 행). 시간 레이어를 우회하는 경우(기준 3)는 불일치된 배포로 인해 품질이 저하된다. 기준 1, 2, 4를 비교하면, ICP와 공간 레이어를 파인 튜닝한 경우(기준 4)가 FVD와 CLIP\_img 모두에서 개선된 성능을 보인다. ICP만 파인 튜닝한 경우(기준 5)는 만화 컨텍스트를 더 잘 이해하여 의미적으로 올바른 콘텐츠를 생성하는 데 도움이 된다.

### 표 3. 다양한 교정 학습 전략의 소거 연구

이 모든 변형은 제안된 이중 참조 기반 3D 디코더 없이 평가되어 소거 네트워크의 원래 성능을 보여준다.

변형ICP공간시간시간 우회FVD ↓CLIP\_img ↑

|  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- |
| I (DCinterp) |  |  | ✔ |  | 86.62 | 0.8437 |
| II | ✔ | ✔ | ✔ |  | 70.73 | 0.8978 |
| III | ✔ | ✔ |  | ✔ | 291.45 | 0.7997 |
| IV (Ours) | ✔ | ✔ |  |  | 52.73 | 0.9096 |
| V | ✔ |  |  |  | 81.45 | 0.8875 |

#### 이중 참조 기반 3D VAE 디코더

제안된 이중 참조 기반 3D 디코더의 다양한 모듈의 효과를 평가하기 위해, 의사 3D 컨볼루션(P3D)을 제거한 변형(Ours\_no P3D)을 먼저 구성한다. 그런 다음, 하이브리드 주의-잔여(HAR) 모듈을 제거하여 두 번째 변형(Ours\_no HAR & P3D)을 얻는다. 이는 대부분의 확산 기반 이미지/비디오 생성 모델에서 사용되는 이미지 디코더와 동일하다. 우리는 비디오 재구성 작업에서 전체 방법을 평가하고 결과를 표 4에 보고한다. Ours\_no P3D의 성능은 디테일 전파가 감소하여 세부 정보의 손실을 초래한다(그림 6의 3번째 열). HAR과 P3D를 제거하면 성능이 크게 저하되며, 이는 참조 없이 VAE 디코더만으로는 압축된 잠재 변수에서 세부 정보를 복원하기 어렵기 때문이다(그림 6의 4번째 열). 우리의 전체 방법은 이중 참조 기반 디테일 주입 및 전파 메커니즘을 통해 이러한 세부 정보 손실을 효과적으로 보상한다.

### 표 4. 1K 만화 비디오 평가 세트에서 다양한 디코더의 재구성 결과에 대한 정량적 비교 (256x256).

HAR: 하이브리드 주의-잔여 학습. P3D: 의사 3D 컨볼루션.

변형참조시간PSNR ↑SSIM ↑LPIPS ↓

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| Ours | ✔ | ✔ | 33.83 | 0.9450 | 0.0204 |
| Ours\_no P3D | ✔ | ✔ | 32.51 | 0.9270 | 0.0326 |
| Ours\_no HAR & P3D | ✔ | X | 29.49 | 0.8670 | 0.0426 |

![](/assets/images/posts/162/img_6.png)

### 5. 응용

만화 스케치 보간은 색상과 텍스처가 없는 극도로 희소한 구조 때문에 매우 도전적이다. 그럼에도 불구하고, 우리 ToonCrafter는 강력한 일반화 능력 덕분에 이러한 보이지 않는 입력에서도 괜찮은 결과를 생성할 수 있다(그림 10 상단). ToonCrafter는 또한 1개 또는 2개의 참조 이미지와 프레임별 스케치를 제공하여 참조 기반 스케치 색상화도 지원할 수 있다. 이러한 응용의 시각적 결과는 그림 10에 제시되어 있다.

### 6. 결론

우리는 생성적 만화 보간을 최초로 시도한 혁신적인 프레임워크인 ToonCrafter를 소개하였다. 우리는 실사 움직임 우선순위를 유지하면서 도메인 격차를 극복하기 위해 만화 교정 학습을 제안하였고, 이중 참조 기반 3D 디코더를 통해 시각적 세부 사항을 보존하였다. 사용자가 보간을 제어할 수 있도록 프레임 독립적인 스케치 인코더를 설계하였다. 정량적 및 정성적 실험 결과는 기존 경쟁자들에 비해 우리의 방법의 효과성과 우수성을 입증한다. 마지막으로, 우리는 다양한 응용에서 우리의 접근법의 다재다능함을 보여준다.

[2405.17933v1.pdf

10.10MB](./file/2405.17933v1.pdf)

쉽게 생각하면 iuput, out을 받으면 보간 대신에 noise와 특징점 보간을 하여 이를 프레임 단위로 나누어서 만드는 것이다. 추가적으로 controlnet처럼 input 축에 이를 encoder 형식으로 가져오는 방식이 추가되어 단단하게 만드는 형식
