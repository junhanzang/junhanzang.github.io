---
title: "Bolmo: Byteifying the Next Generation of Language Models"
date: 2025-12-28 21:49:25
categories:
  - 인공지능
---

<https://arxiv.org/abs/2512.15586>

[Bolmo: Byteifying the Next Generation of Language Models](https://arxiv.org/abs/2512.15586)

초록  
우리는 10억(1B) 및 70억(7B) 파라미터 규모에서 경쟁력 있는 **완전 공개 바이트 수준 언어 모델(Language Model, LM)** 계열인 **Bolmo**를 소개한다. 기존의 바이트 수준 언어 모델 연구가 주로 처음부터 모델을 학습하는 방식에 초점을 맞춰온 것과 달리, 우리는 기존의 서브워드(subword) 수준 언어 모델을 **바이트화(byteification)** 하는 방식으로 Bolmo를 학습한다.

바이트화는 고정된 서브워드 어휘로 인해 발생하는 비효율성이나 문자 단위 이해 부족과 같은 서브워드 토크나이제이션의 한계를 극복하면서도, 최상위 서브워드 수준 언어 모델과 동등한 성능을 달성할 수 있게 해준다. Bolmo는 이러한 바이트화를 염두에 두고 특별히 설계되었다. 제안하는 아키텍처는 기존 바이트 수준 아키텍처의 표현력과 서브워드 수준 언어 모델 간의 불일치를 해소하며, 이를 통해 Bolmo와 원본 서브워드 모델 사이에 **효과적인 정확(distilled) 증류 목적 함수(exact distillation objective)** 를 적용할 수 있다.

이로써 일반적인 사전학습 토큰 예산의 1% 미만만을 투자하여 서브워드 수준 언어 모델을 바이트 수준 언어 모델로 변환하는 것이 가능해진다. Bolmo는 동일한 규모의 기존 모든 바이트 수준 언어 모델을 크게 능가하는 성능을 보이며, 문자 이해(character understanding)와 일부 코딩 과제에서는 원본 서브워드 수준 언어 모델보다도 더 뛰어난 성능을 달성한다. 동시에 다른 과제들에서도 원본 언어 모델의 성능에 근접한 결과를 유지한다.

더 나아가, 우리는 더 높은 토큰 압축 비율(token compression ratio)로 학습함으로써 Bolmo가 서브워드 수준 언어 모델과 경쟁력 있는 추론 속도를 달성할 수 있음을 보인다. 또한, 원본 서브워드 수준 언어 모델을 중심으로 형성된 기존 생태계를 활용함으로써, Bolmo를 저비용으로 효과적으로 사후 학습(post-training)할 수 있음을 입증한다. 이러한 결과는 바이트 수준 언어 모델을 다양한 활용 사례 전반에서 서브워드 수준 언어 모델과 경쟁 가능한 실질적인 선택지로 만든다.

## 1 서론 (Introduction)

현대의 언어 모델(Language Model, LM)은 대부분 **서브워드 토크나이제이션(subword tokenization)** 기법(Sennrich et al., 2016; Kudo, 2018)을 사용하여 텍스트를 고정된 토큰 어휘 집합으로 분할한다. 그러나 이러한 방식은 여러 문제를 야기한다. 대표적으로 **문자 단위 이해(character understanding)의 부족**(Edman et al., 2024; Cosma et al., 2025; Uzan and Pinter, 2025), **토크나이제이션 편향(tokenization bias)** (Phan et al., 2024; Hayase et al., 2025; Vieira et al., 2025)¹, 고정된 어휘로 인해 충분히 많은 단어(예를 들어 다국어 환경에서의 단어들)를 포함하지 못하는 문제, 즉 **어휘 병목(vocabulary bottleneck)** (Liang et al., 2023), 그리고 **비최적의 연산 자원 할당 가능성**(Hwang et al., 2025; Pagnoni et al., 2025) 등이 있다. 이러한 문제들은 서브워드 토크나이제이션의 대안에 대한 폭넓은 연구를 촉발했으며, 그중 가장 일반적인 접근은 UTF-8 바이트(byte) 단위로 전환하는 것이다.²

기존의 많은 바이트 수준 언어 모델(byte-level LM)은 효율성과 성능 간의 파레토 프론티어 상에서 서브워드 수준 언어 모델을 능가한다고 주장해왔다(Nawrot et al., 2023; Slagle, 2024; Wang et al., 2024; Hwang et al., 2025; Pagnoni et al., 2025; Zheng et al., 2025). 그러나 실제로는 바이트 수준 언어 모델이 아직까지 널리 채택되지 못했으며, 최상위 언어 모델들은 여전히 예외 없이 서브워드 토크나이제이션에 의존하고 있다.

우리는 이러한 이론과 실제 간의 괴리가 발생하는 핵심 원인이, 기존의 바이트 수준 언어 모델 접근법들이 주로 **새로운 바이트 수준 모델을 처음부터 학습(training from scratch)** 하는 데 초점을 맞추고, 동일한 연산량으로 처음부터 학습한 서브워드 수준 언어 모델과 비교해왔기 때문이라고 가정한다. 반면, 최신 서브워드 수준 언어 모델의 학습은 데이터 큐레이션, 모델 아키텍처, 사후 학습(post-training) 전략 전반에서 빠르게 진화하고 있다. 이러한 발전 속도를 대규모 투자 없이 바이트 수준 언어 모델 개발이 따라잡는 것은 현실적으로 어렵다.

이러한 괴리를 해소하기 위해, 우리는 다양한 과제에서 **최신 서브워드 수준 언어 모델과 동등한 성능을 달성하는 최초의 완전 공개 바이트 수준 언어 모델 계열**인 **Bolmo**를 소개한다. 기존의 바이트 수준 언어 모델들이 주로 처음부터 학습하는 데 집중해온 것과 달리, Bolmo는 일반적인 사전학습 예산의 1% 미만(39.3B 토큰)을 사용하여 기존 서브워드 수준 언어 모델을 **바이트화(byteification)** 하는 방식으로 학습된다. 구체적으로, 우리는 Olmo 3 7B(Olmo Team, 2025)를 바이트화하여 Bolmo 7B를, OLMo 2 1B(OLMo et al., 2024)를 바이트화하여 Bolmo 1B를 학습하였다.

Bolmo는 최근의 DTP(Nawrot et al., 2023), BLT(Pagnoni et al., 2025), H-Net(Hwang et al., 2025) 모델과 동일한 전반적 아키텍처를 따르며, 이들을 통칭하여 **잠재 토크나이저 언어 모델(Latent Tokenizer Language Models, LTLMs)** 이라 부른다. 다만, 우리는 Bolmo 아키텍처를 바이트화에 특히 적합하도록 설계하였다(Section 3.1 참고). 특히, Bolmo의 잠재 토크나이제이션이 **미래 문맥(future context)** 을 활용할 수 있도록 함으로써, 서브워드 토크나이제이션의 표현력과 LTLM의 토크나이제이션 간에 존재하던 불일치를 해결한다(Section 3.1.1).

또한, 원본 서브워드 수준 언어 모델에 대한 **정확한 증류(exact distillation)** 로 시작하는 효율적인 2단계 학습 절차(Section 3.2)와 결합함으로써, 우리는 원본 서브워드 모델의 성능을 빠르게 회복할 뿐만 아니라 경우에 따라 이를 능가할 수 있음을 보인다. 우리는 바이트화가, 대규모 투자 없이도 최첨단 바이트 수준 언어 모델을 생성할 수 있게 함으로써, 바이트 수준 언어 모델 연구에서 그동안 결여되어 있던 핵심적인 방향성을 제공한다고 믿는다. 바이트화는 처음부터 학습하는 방식과 상호 보완적이다. 즉, 어떤 서브워드 모델이든 저비용으로 바이트화할 수 있다면, 바이트 수준 언어 모델로 처음부터 학습하기에 유망한 고성능 아키텍처를 빠르게 발굴할 수 있다.

실험 결과, Bolmo 모델은 평균적으로 동일한 규모의 기존 공개 바이트 수준 언어 모델을 모두 능가한다. 예를 들어, Bolmo 7B는 처음부터 학습된 BLT 7B 대비 STEM 과제에서 절대 기준으로 +16.5%의 성능 향상을 달성한다. 또한 Bolmo 7B는 문자 이해 능력과 일부 코딩 과제에서 원본 Olmo 3보다도 훨씬 뛰어난 성능을 보인다. 더 나아가, Bolmo는 패치당 바이트 수(bytes per patch)에 대한 더 높은 압축 비율로 학습함으로써 임의로 추론 속도를 추가적으로 향상시킬 수 있는데, 이는 서브워드 수준 언어 모델에서는 제한적으로만 가능한 특성이다(Section 5.1).

아울러, 우리는 기존의 사후 학습된 체크포인트(post-trained checkpoints)를 활용하여, 추가적인 학습 비용 없이도 바이트화된 모델을 사후 학습할 수 있음을 보인다(Section 5.2). 이는 원본 언어 모델 생태계의 구성 요소를 재사용함으로써 바이트 수준 언어 모델 연구를 더욱 가속화할 수 있게 한다. 마지막으로, 우리는 설계 선택에 대한 광범위한 어블레이션 실험을 통해, 서브워드 수준 언어 모델과의 잔존 격차뿐만 아니라 Bolmo가 이미 해소한 격차를 분석한다(Section 6).

종합하면, 바이트화는 바이트 수준 언어 모델을 다양한 활용 사례 전반에서 서브워드 수준 언어 모델과 경쟁 가능한 **실질적인 선택지**로 만들어준다. 우리는 공개되는 Bolmo 데이터, 모델, 코드가 바이트 수준 언어 모델링 연구를 한층 더 발전시키는 데 기여하기를 기대한다.

¹ **토크나이제이션 편향(tokenization bias)** 이란, 토큰 시퀀스가 해당 토큰화된 텍스트의 미래 내용을 암묵적으로 누설하는 현상을 의미한다. 예를 들어, 영어 단어 어휘를 가정할 때 토큰 시퀀스 {\_Hello, \_Wor}는 \_Wor 뒤에 ld가 오지 않는다는 정보를 누설한다. 만약 ld가 존재했다면 텍스트는 {\_Hello, \_World}로 토큰화되었을 것이기 때문이다. 이는 실제 응용에서 직관적이지 않은 동작을 유발할 수 있다(Minixhofer et al., 2025b; Vieira et al., 2025).

² Graves(2013)는 UTF-8 바이트 단위로 언어를 모델링한 최초의 연구일 가능성이 있으며, 이에 대한 개요는 Mielke et al.(2021)을 참고하라.

## 2 관련 연구 (Related Work)

### 토크나이제이션(Tokenization)

언어 모델(Language Model, LM)은 **토큰(token)** 또는 **패치(patch)** 라 불리는 이산적인 기호들의 시퀀스로 표현된 정보를 처리한다. 입력을 이러한 이산 시퀀스로 분할하는 과정을 **토크나이제이션(tokenization)** 이라 하며, 텍스트(Kudo, 2018), 오디오(Borsos et al., 2023), 이미지(Dosovitskiy, 2020) 등 서로 다른 모달리티에 따라 다양한 토크나이제이션 방식이 사용된다.

언어 모델이 등장한 이래 텍스트 토크나이제이션의 지배적인 접근법은 **서브워드 토크나이제이션(subword tokenization)** 이었다(Sennrich et al., 2016; Kudo, 2018). 이는 텍스트를 유한한 서브워드 토큰 어휘(보통 3만~30만 크기)로부터 선택된 이산적 단위들의 시퀀스로 분할하며, 각 토큰은 일반적으로 정수 ID로 표현된다. 그러나 서브워드 토크나이제이션은 여러 문제를 야기한다.

(i) 각 토큰 내부의 **문자 정보(character information)** 가 손실된다. 언어 모델이 토큰을 구성하는 문자들을 암묵적으로 학습할 수 있음이 알려져 있고(Kaushal and Mahowald, 2022; Edman et al., 2024), 문자 정보를 명시적으로 다시 도입하는 것도 가능하지만(Cosma et al., 2025), 여전히 문자 지식이 요구되는 과제에서는 한계를 보인다(Edman et al., 2024; Uzan and Pinter, 2025).

(ii) 서브워드 토크나이제이션이 텍스트의 **미래 내용에 암묵적으로 의존**한다는 점, 즉 **토크나이제이션 편향(tokenization bias)** 으로 인해, 프롬프트가 단어 중간에서 끝나거나 공백으로 끝나는 경우 추론 시 예기치 않은 동작이 발생한다(Phan et al., 2024; Hayase et al., 2025; Vieira et al., 2025).

(iii) 고정되고 유한한 서브워드 어휘를 필요로 한다는 점에서 **경직성**이 발생한다. 예를 들어, 현재의 사전학습 문서 대부분이 영어이므로 영어를 효율적으로 인코딩하는 것이 중요하지만, 다양한 다운스트림 과제에서는 언어별로 서로 다른 효율성 요구가 존재한다.

(iv) 현대 언어 모델에서 토크나이제이션은 **연산 자원 할당(compute allocation)** 과 강하게 결합되어 있다. 표준 언어 모델에서는 프리필(prefill) 단계에서 모든 토큰을 처리하는 데 동일한 연산량이 사용되며, 각 토큰은 KV 캐시 크기에 동일하게 기여하고, 새로운 토큰 하나를 순차적으로 생성하는 데에도 고정된 연산량이 소요된다. KV 캐시 희소화(Łańcucki et al., 2025)나 멀티 토큰 예측(Gloeckle et al., 2024)과 같은 사후적 완화 기법들이 존재하지만, 입력에 따라 토크나이제이션 자체와 그에 따른 연산 할당을 직접 적응시키는 것이 더 효과적일 수 있다(Nawrot et al., 2023; Pagnoni et al., 2025).

### 바이트 수준 언어 모델(Byte-level LMs)

서브워드 토크나이제이션의 이러한 한계는 다양한 대안에 대한 광범위한 연구를 촉발했으며, 그중에는 텍스트를 픽셀로 렌더링한 뒤 이를 패치로 분할하는 방식까지 포함된다(Lotz et al., 2023; Rust et al., 2023; Wei et al., 2025). 그중 가장 일반적인 대안은 UTF-8 바이트³ 와 같은 더 작고 세분화된 원자 단위로 토크나이제이션하는 것이다.

한 연구 흐름에서는 서브워드 토큰을 UTF-8 바이트로 직접 대체하되, 아키텍처의 다른 요소는 대부분 동일하게 유지한다(Xue et al., 2022; Wang et al., 2024; Minixhofer et al., 2025b; Zheng et al., 2025). 이러한 방식은 서브워드 토크나이제이션의 문제 (i)~(iii)⁴을 잠재적으로 해결할 수 있지만, 평균적으로 최소 네 배 이상 긴 바이트 시퀀스를 처리해야 하므로 연산 자원 할당 문제는 오히려 더 악화된다.

이 문제를 완화하기 위해, 일부 아키텍처는 경량의 로컬 인코더(예: 또 다른 Transformer 네트워크)를 사용해 일정 수의 토큰을 하나의 표현으로 풀링(pooling)하고, 이렇게 축약된 시퀀스에 대해 깊은 글로벌 모델을 적용한 뒤, 로컬 디코더를 통해 다시 원래의 세분화된 단위로 디풀링(depooling)한다. 이러한 접근은 자동회귀 모델을 위해 Hourglass Transformer(Nawrot et al., 2022)에서 처음 제안되었으며, 이후 더 널리 채택되었다(Yu et al., 2023; Ho et al., 2024).

최근 연구들은 정적 풀링을 **동적 토크나이제이션(dynamic tokenization)** 으로 대체함으로써 성능–효율 파레토 프론티어를 개선할 수 있음을 보여주었다(Nawrot et al., 2023; Slagle, 2024). 이 경우 토큰 경계는 엔드투엔드로 학습되거나, 엔트로피 스파이크에 의존하거나, 외부 감독 신호에 의해 결정될 수 있다(Nawrot et al., 2023; Hwang et al., 2025). 우리는 이러한 아키텍처들을 통칭하여 **잠재 토크나이저 언어 모델(Latent Tokenizer Language Models, LTLMs)** 이라 부른다. 이들은 바이트 단위로 동작하지만, 모델 내부에서 바이트 표현을 **잠재 패치(latent patches)** 에 대한 표현으로 집계하는 토크나이제이션 단계를 수행하기 때문이다.

바이트 수준 LTLM은 마침내 서브워드 토크나이제이션의 문제 (i)~(iv)를 모두 해결할 수 있는 잠재력을 갖는다. 가장 최근의 LTLM들은 동일한 총 FLOPs를 사용해 학습할 경우 서브워드 토크나이제이션과 동등한 성능을 달성할 수 있음을 보여주며, 그 가능성을 입증하고 있다(Hwang et al., 2025; Pagnoni et al., 2025).

³ UTF-8 바이트 단위 언어 모델링에 대한 초기 연구로는 Graves(2013)를 참고할 수 있다.  
⁴ 문자 정보 손실, 토크나이제이션 편향, 고정 어휘로 인한 경직성 문제를 의미한다.

### 토크나이저 이전과 레트로피팅 (Tokenizer Transfer and Retrofitting)

추가 학습을 통해 모델의 아키텍처를 변경하는 기법은 일반적으로 **레트로피팅(retrofitting)** 이라 불리며, 이는 종종 **자기 증류(self-distillation)** 에 의존한다(Bick et al., 2024; Łańcucki et al., 2025). 이러한 과정에서 토크나이저 변경이 포함될 경우 가장 큰 어려움은 **새로운 토큰에 대한 임베딩을 어떻게 정의할 것인가**이다. 이 문제는 보통 휴리스틱 기반 방법(Tran, 2020; Minixhofer et al., 2022; Dobler and de Melo, 2023)이나 학습 기반 방법(Minixhofer et al., 2025a)을 통해 해결되어 왔다.

최근에는 **교차 토크나이저 증류(cross-tokenizer distillation)** 에 기반한 효과적인 토크나이저 이전(tokenizer transfer) 기법들이 제안되었다(Dobler et al., 2025; Haltiuk and Smywiński-Pohl, 2025; Minixhofer et al., 2025b). 이 접근에서는 원본 모델을 **교사(teacher)** 로, 토크나이저가 이전된 모델을 **학생(student)** 으로 간주하며, 학생 모델의 동작이 교사 모델의 동작과 일치하도록 학습하는 것을 목표로 한다.

**바이트화(byteification)** 는 이러한 토크나이저 이전의 특수한 경우에 해당한다. 바이트화는 Pagnoni et al.(2025)에 의해 처음 수행되었으며, 가능한 경우 기존 서브워드 모델로부터 LTLM 파라미터를 초기화한 뒤, 처음부터 학습하는 것처럼 학습을 진행하는 방식이었다. 이후 Hwang et al.(2025)는 서브워드 경계(subword boundaries)와 일치하도록 **경계 예측(boundary prediction)** 을 감독 신호로 제공하고, 보조적인 임베딩 정합 손실(auxiliary embedding-matching loss)을 도입하여 바이트화를 수행하였다.

본 연구의 핵심 기여는 **바이트화에 특히 적합한 LTLM 아키텍처를 설계했다는 점**이다. 이를 위해 우리는 새로운 아키텍처를 제안하고(Section 3.1), 원본 서브워드 모델의 동작을 정확하게 복원하는 법을 먼저 학습함으로써 바이트화를 효율적으로 수행하는 **전용 2단계 절차(two-stage procedure)** 를 도입한다(Section 3.2). 이러한 혁신을 통해, 우리는 바이트화된 모델로도 최첨단 서브워드 수준 언어 모델과 매우 근접한 성능을 달성할 수 있음을 처음으로 가능하게 한다.

![](/assets/images/posts/611/img.png)

**그림 1. Bolmo 아키텍처.**  
토크나이제이션 및 임베딩 모듈 **T**는 입력 텍스트를 **바이트(byte) 단위당 하나의 표현**으로 변환한다. 이러한 표현들은 **mLSTM 블록**으로 구성된 **로컬 인코더 E**를 통해 문맥화된다. **경계 예측기 B**는 **1바이트의 미래 문맥**을 사용하여 패치 경계(patch boundary)를 어디에 배치할지를 결정한다. 이후 표현들은 **풀링(Pooling)** 되어, **Transformer 레이어**로 구성된 **글로벌 모델 M**을 통과한 뒤 **디풀링(Depooling)** 된다. 마지막으로, 또 다른 **mLSTM 스택**으로 이루어진 **로컬 디코더 D**가 디풀링된 바이트 표현들을 문맥화하며, **LMHead**는 다음 바이트 예측을 수행함과 동시에 **다음 패치 경계를 어디에 둘지**를 결정한다.

## 3 바이트화된 Olmo (Byteified Olmo)

### 3.1 아키텍처 (Architecture)

Bolmo는 기존 LTLM들과 동일한 전반적 구조를 따르며, 그림 1과 같이 형식화할 수 있다.

#### 토크나이제이션 및 임베딩 (Tokenization & Embedding)

![](/assets/images/posts/611/img_1.png)

이에 BLT의 해시 임베딩(hash embeddings)(Tito Svenstrup et al., 2017; Pagnoni et al., 2025)에서 영감을 받아, 우리는 임베딩 테이블의 크기를 확장한다. 구체적으로, 현재 바이트 위치에서 끝나는 **가장 긴 서브워드 임베딩**(원본 서브워드 수준 LM의 임베딩 테이블에서)을 각 바이트 임베딩에 **잔차적으로(residually)** 더한다.

![](/assets/images/posts/611/img_2.png)

서브워드 임베딩을 유지하는 것은 필수는 아니며, 일반적으로 로컬 인코더의 크기를 키우는 것만으로도 유사한 성능을 달성할 수 있다. 그러나 서브워드 임베딩을 유지함으로써, **저비용으로 희소하게 활성화되는 파라미터의 양을 증가**시켜 더 나은 성능–효율 트레이드오프를 달성할 수 있다.⁶

#### 로컬 인코더 (Local Encoder)

로컬 인코더 **E**는 **mLSTM 레이어**(Beck et al., 2025a)를 통해 바이트 수준 임베딩을 문맥화하여, 문맥화된 표현 e^\hat{e}e^를 생성한다. 우리는 mLSTM이 다른 선형 RNN 변형들에 비해 **추론 속도를 개선**하면서도 경쟁력 있는 성능을 달성함을 확인했다(Section 6.3 참고).

또한, 유지된 서브워드 임베딩 덕분에 로컬 인코더의 표현력이 크게 향상되므로, **단일 mLSTM 레이어만으로도 충분**함을 확인했다.

#### 경계 예측기 (Boundary Predictor)

경계 예측기 **B**는 문맥화된 표현 e^\hat{e}e^를 기반으로 각 바이트에 대해 p∈[0,1]p \in [0,1]p∈[0,1] 범위의 점수를 예측한다. 이 점수가 특정 임계값을 초과하면, 현재 바이트 뒤에 **패치 경계(patch boundary)** 가 배치된다.

기존 LTLM들과 달리, Bolmo의 경계 예측기는 **비인과적(non-causal)** 이다.⁷ 즉, **1바이트의 미래 문맥**에 접근할 수 있으며, 미래 정보를 사용할 수 있으면서도 텍스트 생성 능력을 유지할 수 있는 **프리필(prefill)** 단계에서만 사용된다. 비인과적 경계 예측과 디코딩 중 경계 처리 방식에 대한 자세한 내용은 Section 3.1.1에서 설명한다.

#### 풀링 (Pooling)

우리는 각 패치에서 **마지막 바이트의 표현**을 선택하여 패치 수준 표현 hhh로 사용함으로써, 바이트 수준 표현을 패치 표현으로 풀링한다. 이는 Hwang et al.(2025)에서 사용한 풀링 방식과 동일하며,⁸ 추가적인 파라미터를 도입하지 않는다. 또한 Hwang et al.(2025)와 달리, 로컬 모델과 글로벌 모델이 **동일한 표현 차원**을 사용하므로 업프로젝션(upprojection)이 필요 없다.⁹

#### 글로벌 모델 (Global Model)

대부분의 연산량은 패치 표현 hhh를 문맥화하여 h^\hat{h}h^로 변환하는 **깊은 글로벌 모델 M**에서 사용된다. 우리는 원본 서브워드 수준 언어 모델의 글로벌 모델, 즉 **Olmo 3의 디코더 전용 Transformer 백본**을 그대로 유지한다.

#### 디풀링 (Depooling)

글로벌 모델은 각 패치 경계에서 호출되어, 각 패치에 대한 문맥화된 표현을 제공한다. 이후 이러한 표현을 다시 바이트 수준 표현으로 되돌리는 **디풀링** 단계가 필요하다. 이를 위해 우리는 각 바이트 위치에서 사용 가능한 **가장 최근의 패치 표현 h^\hat{h}h^** 을, 바이트 표현 e^\hat{e}e^에 대한 선형 사영(linear projection)에 더하여 zzz를 생성한다. 이 과정은 Hwang et al.(2025)의 디풀링 방식과 유사하며, 로컬 및 글로벌 차원이 동일하므로 추가적인 사영을 생략한다.

⁵ xxx는 바이트 시퀀스로 취급되며, 즉 x∈{0,…,255}nx \in \{0, \ldots, 255\}^nx∈{0,…,255}n이다.  
⁶ 로컬 인코더의 크기와 희소성을 늘리는 대안으로는 피드포워드 레이어에서 mixture-of-experts를 사용하는 방법이 있으나, 본 연구에서는 이를 다루지 않는다.  
⁷ 기존 연구와의 일관성을 위해, 단방향 문맥만 사용하는 인과적 언어 모델에서의 ‘인과적(causal)’ 개념과 대비되는 의미로 ‘비인과적(non-causal)’이라는 용어를 사용한다. 다만 이는 엄밀히 말하면 오해의 소지가 있을 수 있다.  
⁸ Hwang et al.(2025)는 패치마다 하나의 표현을 생성하는 과정을 ‘routing’이라 부르지만, 우리는 Pagnoni et al.(2025)의 교차 어텐션 풀링까지 포괄하는 의미에서 이를 보다 일반적으로 ‘풀링(pooling)’이라 부른다.  
⁹ 초기 실험에서는 더 작은 로컬 차원을 사용했으나, 업프로젝션 메커니즘이 표현의 랭크를 제한하여 성능 병목을 유발함을 확인했다(부록 E 참고).

---

일단 여기까지 마음에 안두는거 투성이다.

## 1. 네가 마음에 안 드는 지점 (비판 요약)

### ① “byte-level이라고 하면서 subword를 다시 끌어온다”

- 가장 긴 subword embedding을 **residual로 더함**
- 이건 순수 byte 모델이 아니라
  - subword inductive bias를 다시 얹은 형태
- 결과적으로:
  - “byteification”이 아니라
  - **subword 안전장치 위에 byte를 씌운 구조**처럼 보임

? 네 관점:

> “이러면 결국 기존 모델의 세계관을 벗어났다고 말하기 어렵다.”

---
