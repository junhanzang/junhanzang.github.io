---
title: "Diffusion Models Are Real-Time Game Engines (부록 추가 필요)"
date: 2024-09-09 21:21:54
categories:
  - 인공지능
---

<https://arxiv.org/abs/2408.14837>

[Diffusion Models Are Real-Time Game Engines](https://arxiv.org/abs/2408.14837)

요약

우리는 신경망 모델로 완전히 구동되는 최초의 게임 엔진인 GameNGen을 소개합니다. 이 엔진은 복잡한 환경에서 실시간으로 상호작용하며, 긴 경로에서도 고품질의 상호작용을 가능하게 합니다. GameNGen은 단일 TPU에서 초당 20프레임 이상의 속도로 고전 게임인 DOOM을 상호작용적으로 시뮬레이션할 수 있습니다. 다음 프레임 예측은 PSNR 값이 29.4로, 손실이 있는 JPEG 압축과 비슷한 수준을 달성합니다. 인간 평가자는 짧은 게임 클립과 시뮬레이션 클립을 구별하는 데 있어서 무작위 추측보다 약간 더 나은 정도입니다. GameNGen은 두 단계로 학습됩니다: (1) 강화학습(RL) 에이전트가 게임을 학습하고 학습 세션이 기록되며, (2) 과거 프레임과 행동의 시퀀스를 조건으로 다음 프레임을 생성하는 확산 모델이 학습됩니다. 조건 강화 기법은 긴 경로에서 안정적인 자동 회귀 생성이 가능하게 합니다.

![](/assets/images/posts/275/img.png)

**그림 1:** 한 인간 플레이어가 GameNGen에서 20FPS로 DOOM을 플레이하고 있습니다. 데모 비디오는 <https://gamengen.github.io>에서 확인할 수 있습니다.

### 1. 서론

컴퓨터 게임은 다음과 같은 게임 루프를 중심으로 수작업으로 제작된 소프트웨어 시스템입니다: (1) 사용자 입력을 수집하고, (2) 게임 상태를 업데이트하며, (3) 이를 화면의 픽셀로 렌더링합니다. 이 게임 루프는 높은 프레임 속도로 실행되며, 플레이어에게 상호작용하는 가상 세계의 환영을 만들어냅니다. 이러한 게임 루프는 전통적으로 표준 컴퓨터에서 실행되지만, 맞춤형 하드웨어에서 게임을 실행하려는 많은 놀라운 시도가 있었습니다(예: 아이코닉한 게임인 DOOM은 토스터, 전자레인지, 러닝머신, 카메라, 아이팟, 그리고 심지어 Minecraft 게임 안에서 실행된 사례들이 있습니다). 하지만 이 모든 경우에도 하드웨어는 여전히 수작업으로 작성된 게임 소프트웨어를 그대로 에뮬레이팅하고 있습니다. 또한, 다양한 게임 엔진이 존재하지만, 모든 게임 상태 업데이트와 렌더링 로직은 수작업으로 프로그램되거나 구성된 규칙 세트로 구성됩니다.

최근 몇 년 동안 생성 모델은 텍스트나 이미지 같은 다중 모달 입력에 조건화된 이미지와 비디오를 생성하는 데 있어 큰 진전을 이루었습니다. 이 흐름의 선두에는 Dall-E (Ramesh et al., 2022), Stable Diffusion (Rombach et al., 2022), 그리고 Sora (Brooks et al., 2024)와 같은 작품들이 있으며, 확산 모델은 미디어(비언어) 생성의 사실상 표준이 되었습니다. 겉보기에는 비디오 게임의 상호작용적 세계를 시뮬레이션하는 것이 비디오 생성과 유사해 보일 수 있습니다. 하지만 상호작용적 세계 시뮬레이션은 매우 빠른 비디오 생성 이상의 것을 필요로 합니다. 생성 중에만 사용 가능한 입력 행동 스트림에 조건화해야 한다는 요구 사항은 기존 확산 모델 아키텍처의 몇 가지 가정을 깨뜨립니다. 특히, 프레임을 자동 회귀적으로 생성해야 하며, 이는 불안정해지고 샘플링 발산으로 이어질 수 있습니다(3.2.1절 참조).

여러 중요한 연구들(예: Ha & Schmidhuber, 2018; Kim et al., 2020; Bruce et al., 2024)이 신경망 모델로 상호작용적 비디오 게임을 시뮬레이션했습니다. 그러나 이러한 접근법 대부분은 시뮬레이션되는 게임의 복잡성, 시뮬레이션 속도, 긴 시간 동안의 안정성, 또는 시각적 품질에서 제한이 있습니다(그림 2 참조). 따라서 질문을 던져보는 것은 자연스러운 일입니다.

**신경망 모델이 실시간으로 복잡한 게임을 높은 품질로 시뮬레이션할 수 있을까?**

이 연구에서 우리는 그 답이 '예'임을 보여줍니다. 구체적으로, 아이코닉한 비디오 게임인 DOOM을 신경망(오픈 Stable Diffusion v1.4(Rombach et al., 2022)의 보강된 버전)으로 실시간으로 실행할 수 있으며, 원래 게임에 필적하는 시각적 품질을 달성할 수 있음을 입증합니다. 정확한 시뮬레이션은 아니지만, 신경망 모델은 체력과 탄약의 계산, 적 공격, 물체 손상, 문 열기 등의 복잡한 게임 상태 업데이트를 수행하고, 긴 경로에서도 게임 상태를 유지할 수 있습니다.

GameNGen은 게임 엔진에 대한 새로운 패러다임을 향한 중요한 질문 중 하나에 답을 제공합니다. 최근 몇 년 동안 신경망 모델이 이미지와 비디오를 자동으로 생성한 것처럼, 게임이 자동으로 생성되는 패러다임을 향해 나아가고 있는 것입니다. 이 신경망 게임 엔진이 어떻게 학습될지, 게임이 처음부터 효과적으로 어떻게 생성될지, 인간 입력을 어떻게 최적으로 활용할지 등의 중요한 질문들이 남아있습니다. 하지만 우리는 이 새로운 패러다임의 가능성에 대해 매우 기대하고 있습니다.

![](/assets/images/posts/275/img_1.png)

**그림 2:** GameNGen과 이전 최첨단 DOOM 시뮬레이션 비교.

### 2. 상호작용적 세계 시뮬레이션

![](/assets/images/posts/275/img_2.png)

--------------------------------------------------------------

결국 **action 값**(플레이어의 입력)에 따라 **이전 state**(이전 게임 상태나 프레임)를 기반으로 새로운 이미지를 생성해 주는 방식입니다. GameNGen은 강화학습 에이전트가 환경과 상호작용하면서 얻은 행동(action)과 상태(state)를 바탕으로, 다음 프레임을 자동 회귀 방식으로 예측해 내는 것입니다.

이 과정에서, 모델은 입력된 행동과 이전 상태를 바탕으로 새로운 화면(이미지)을 렌더링하며, 이를 통해 실제 게임처럼 상호작용적이고 일관성 있는 시뮬레이션이 가능합니다.

--------------------------------------------------------------

**3. GameNGen**

![](/assets/images/posts/275/img_3.png)

![](/assets/images/posts/275/img_4.png)

**그림 3:** GameNGen 방법 개요. 간결성을 위해 v-예측 세부 사항은 생략되었습니다.

#### 3.1 에이전트 플레이를 통한 데이터 수집

우리의 궁극적인 목표는 인간 플레이어가 우리의 시뮬레이션과 상호작용하는 것입니다. 이를 위해 섹션 2에서 설명한 대로 정책 π는 인간의 게임 플레이를 모방해야 합니다. 그러나 이를 대규모로 직접 샘플링할 수 없기 때문에, 자동 에이전트가 게임을 플레이하도록 가르침으로써 이를 근사하는 것부터 시작합니다. 일반적인 강화 학습(RL) 환경이 게임 점수를 극대화하려는 것과는 달리, 우리의 목표는 인간의 플레이와 유사하거나 적어도 다양한 시나리오에서 충분히 다양한 예시를 포함하는 훈련 데이터를 생성하는 것입니다. 이를 통해 훈련 데이터의 효율성을 극대화할 수 있습니다. 이 목표를 달성하기 위해 간단한 보상 함수를 설계했으며, 이는 우리의 방법에서 환경에 특화된 유일한 부분입니다(부록 A.3 참조).

![](/assets/images/posts/275/img_5.png)

#### 3.2 생성 확산 모델 훈련

![](/assets/images/posts/275/img_6.png)

![](/assets/images/posts/275/img_7.png)

#### 

#### 3.2.1 노이즈 증강을 사용한 자동 회귀 드리프트 완화

교사 강제 학습(teacher-forcing)과 자동 회귀 샘플링 간의 도메인 전환은 오류 축적과 샘플 품질의 빠른 저하로 이어지며, 이는 **그림 4**에서 보여집니다. 모델의 자동 회귀 적용으로 인한 이러한 발산을 피하기 위해, 우리는 훈련 시간에 인코딩된 프레임에 가우시안 노이즈를 다양한 양으로 추가하여 컨텍스트 프레임을 손상시킵니다. 이때 모델에 노이즈 수준을 입력으로 제공하는데, 이는 Ho et al. (2021)을 따릅니다. 이를 위해 우리는 노이즈 수준 α를 최대값까지 균등하게 샘플링하고, 이를 이산화한 후 각 구간에 대해 임베딩을 학습합니다(그림 3 참조). 이 방법은 네트워크가 이전 프레임에서 샘플링된 정보를 수정할 수 있게 하며, 시간이 지남에 따라 프레임 품질을 유지하는 데 매우 중요합니다.

추론 시, 추가된 노이즈 수준은 품질을 극대화하기 위해 조절될 수 있습니다. 하지만 우리는 노이즈가 추가되지 않은 상태에서도 결과가 상당히 개선된다는 것을 발견했습니다. 이 방법의 효과는 섹션 5.2.2에서 분석합니다.

![](/assets/images/posts/275/img_8.png)

**그림 4:** 자동 회귀 드리프트. 상단: 플레이어가 움직이지 않는 50프레임의 단순한 경로에서 10번째 프레임마다 보여줍니다. 20~30 스텝 이후 품질이 빠르게 저하됩니다. 하단: 노이즈 증강이 적용된 동일한 경로에서는 품질 저하가 발생하지 않습니다.

#### 3.2.2 잠재 디코더 미세 조정

Stable Diffusion v1.4의 사전 훈련된 자동 인코더는 8x8 픽셀 패치를 4개의 잠재 채널로 압축하는데, 이는 게임 프레임을 예측할 때 의미 있는 아티팩트를 발생시킵니다. 이 아티팩트는 특히 작은 세부 사항과 하단 HUD(헤드업 디스플레이)에 영향을 미칩니다. 사전 훈련된 지식을 활용하면서 이미지 품질을 향상시키기 위해, 우리는 목표 프레임 픽셀에 대해 계산된 MSE 손실을 사용하여 잠재 자동 인코더의 디코더만을 훈련합니다. LPIPS(Zhang et al., 2018)와 같은 지각적 손실을 사용하면 품질을 더욱 개선할 수 있을 것으로 보이며, 이는 후속 연구로 남겨둡니다. 중요한 점은 이 미세 조정 과정은 U-Net의 미세 조정과 완전히 별개로 이루어지며, 자동 회귀 생성에는 영향을 미치지 않습니다(우리는 픽셀이 아닌 잠재 공간에서 자동 회귀적으로 조건을 설정하기 때문입니다). 부록 A.2에서는 자동 인코더를 미세 조정한 경우와 하지 않은 경우의 생성 예시를 보여줍니다.

--------------------------------------------------------------

Stable Diffusion v1.4는 이미지를 압축하고 복원할 때 약간의 문제가 생기는데, 특히 게임의 작은 디테일이나 화면 하단의 HUD(예: 체력, 탄약 표시 등)에 영향을 줍니다. 이 문제를 해결하기 위해, **이미지를 복원하는 디코더** 부분만 추가로 학습시킵니다. 이렇게 하면 품질이 개선됩니다.

그러나 이 과정은 게임을 생성하는 핵심 모델(U-Net)과는 따로 이루어지며, 우리가 사용하는 자동 회귀 방식(과거 프레임을 바탕으로 다음 프레임을 예측하는 방식)에는 영향을 미치지 않습니다. **자동 회귀 방식**은 이미지를 예측할 때 실제 픽셀이 아닌, 잠재 공간(압축된 데이터)을 사용하기 때문입니다.

--------------------------------------------------------------

#### 3.3 추론

3.3.1 설정

![](/assets/images/posts/275/img_9.png)

또한, 4개의 샘플을 병렬로 생성한 후 결과를 결합하는 실험도 진행했습니다. 이는 극단적인 예측이 받아들여지는 것을 방지하고 오류 축적을 줄이기 위한 시도였습니다. 우리는 샘플들을 평균 내는 방식과 중간값에 가장 가까운 샘플을 선택하는 방식을 실험했습니다. 평균을 내는 방식은 단일 프레임보다 약간 성능이 떨어졌고, 중간값에 가장 가까운 샘플을 선택하는 방식은 성능이 아주 조금 더 나았습니다. 그러나 두 방식 모두 하드웨어 요구 사항을 4개의 TPU로 증가시키기 때문에, 사용하지 않기로 결정했습니다. 다만, 이는 미래 연구에서 흥미로운 주제가 될 수 있다고 생각합니다.

#### 3.3.2 노이즈 제거기 샘플링 단계

추론 시, 우리는 U-Net 노이즈 제거기(여러 단계에서 실행)와 자동 인코더를 실행해야 합니다. 우리의 하드웨어 구성(TPU-v5)에서, 노이즈 제거기 단일 단계와 자동 인코더 평가 모두 각각 10ms가 소요됩니다. 만약 노이즈 제거기 단계를 하나만 실행하면, 설정상 가능한 최소 지연 시간은 프레임당 20ms가 되며, 이는 초당 50프레임에 해당합니다. 일반적으로 Stable Diffusion과 같은 생성 확산 모델은 단일 노이즈 제거 단계로는 고품질 결과를 생성하지 못하고, 고품질 이미지를 생성하기 위해서는 여러 단계의 샘플링이 필요합니다. 그러나 놀랍게도, 우리는 4개의 DDIM 샘플링 단계(Song et al., 2020)만으로도 DOOM을 안정적으로 시뮬레이션할 수 있음을 발견했습니다. 실제로, 4단계 샘플링과 20단계 이상의 샘플링을 사용할 때 시뮬레이션 품질에 차이가 없음을 확인했습니다(부록 A.4 참조).

4개의 노이즈 제거 단계를 사용하는 것은 U-Net 처리에 총 40ms가 소요되며, 자동 인코더를 포함한 전체 추론 비용은 50ms로, 초당 20프레임을 생성할 수 있습니다. 우리는 소수의 샘플링 단계에서 품질에 큰 영향을 주지 않는 이유가 (1) 제한된 이미지 공간, (2) 이전 프레임에 강하게 조건화된 결과 때문이라고 가정합니다.

단일 샘플링 단계만 사용할 경우 품질 저하를 관찰했기 때문에, 우리는 (Yin et al., 2024; Wang et al., 2023)에서와 같이 단일 단계 설정에서 모델 디스틸레이션을 실험했습니다. 디스틸레이션은 성능을 상당히 향상시켰으며, 이를 통해 위에서 언급한 초당 50프레임에 도달할 수 있었습니다. 하지만 시뮬레이션 품질에 약간의 손실이 있었기 때문에, 우리는 디스틸레이션 없이 4단계 버전을 사용하는 방법을 선택했습니다(부록 A.4 참조). 이는 추가 연구가 필요한 흥미로운 주제입니다.

또한, 여러 프레임을 추가 하드웨어에서 병렬로 생성하여 이미지 생성 속도를 크게 높이는 것은 매우 간단하다는 점을 언급합니다. 이는 NVidia의 고전적인 SLI 대체 프레임 렌더링(AFR) 기술과 유사합니다. AFR와 마찬가지로, 실제 시뮬레이션 속도는 증가하지 않고 입력 지연도 줄어들지 않습니다.

#### 4. 실험 설정

4.1 에이전트 훈련

에이전트 모델은 **PPO**(Schulman et al., 2017)를 사용해 훈련되며, 특징 추출 네트워크로는 Mnih et al. (2015)를 따르는 간단한 CNN을 사용합니다. 훈련은 **Stable Baselines 3** 인프라(Raffin et al., 2021)를 이용해 CPU에서 진행됩니다. 에이전트에게는 해상도가 160x120인 프레임 이미지와 게임 내 맵의 축소된 버전이 제공됩니다. 또한, 에이전트는 자신이 수행한 마지막 32개의 행동에 대한 접근 권한도 가집니다. 특징 추출 네트워크는 각 이미지에 대해 크기가 512인 표현을 계산합니다. **PPO**의 액터와 비평가는 이미지 특징 네트워크 출력과 과거 행동 시퀀스의 출력을 결합한 후, 그 위에 2층 MLP 헤드로 구성됩니다.

![](/assets/images/posts/275/img_10.png)

#### 4.2 생성 모델 훈련

우리는 **Stable Diffusion 1.4**의 사전 학습된 체크포인트에서 모든 시뮬레이션 모델을 훈련하며, U-Net의 모든 파라미터를 풀어(freeze 해제) 학습시킵니다. 배치 크기는 128이며, 고정 학습률은 2e-5를 사용하고, **Adafactor** 옵티마이저를 사용하며, 가중치 감소(weight decay)는 사용하지 않고, 그래디언트 클리핑 값은 1.0으로 설정합니다. 확산 손실 매개변수는 **v-예측**으로 변경합니다(Salimans & Ho, 2022a). 추론 시 CFG(Classifier-Free Guidance)를 가능하게 하기 위해, 컨텍스트 프레임 조건은 0.1의 확률로 제거됩니다. 우리는 128개의 TPU-v5e 장치를 사용해 데이터 병렬화를 통해 훈련합니다. 특별한 언급이 없는 한, 이 논문의 모든 결과는 70만 번의 훈련 스텝 이후의 결과입니다.

노이즈 증강(섹션 3.2.1)을 위해, 최대 노이즈 수준을 0.7로 설정하고, 10개의 임베딩 버킷을 사용합니다. 잠재 디코더를 최적화하기 위한 배치 크기는 2,048이며, 다른 훈련 매개변수는 노이즈 제거기와 동일합니다. 훈련 데이터로는 에이전트가 강화 학습 중에 플레이한 모든 경로와 훈련 중 평가 데이터도 사용합니다(별도 언급이 없는 한). 전체적으로 우리는 훈련을 위해 9억 개의 프레임을 생성합니다. 모든 이미지 프레임(훈련, 추론, 조건화 중)은 해상도 320x240에서 320x256으로 패딩됩니다. 우리는 컨텍스트 길이를 64로 사용하며, 이는 모델이 마지막 64개의 예측과 마지막 64개의 행동을 입력으로 받는다는 것을 의미합니다.

#### 5. 결과

5.1 시뮬레이션 품질

전체적으로, 우리의 방법은 긴 경로에서 이미지 품질 면에서 원본 게임에 필적하는 시뮬레이션 품질을 달성합니다. 짧은 경로에서는, 인간 평가자가 시뮬레이션 클립과 실제 게임 클립을 구별하는 능력이 무작위 추측보다 약간 더 나은 정도입니다.

**이미지 품질**: 우리는 LPIPS(Zhang et al., 2018)와 PSNR을 측정하는데, 이는 섹션 2에서 설명한 교사 강제 설정을 사용합니다. 여기서 우리는 초기 상태를 샘플링하고, 실제 과거 관찰 경로를 바탕으로 단일 프레임을 예측합니다. 5개의 서로 다른 레벨에서 수집된 2,048개의 경로에 대해 무작위로 선정한 홀드아웃 데이터를 평가한 결과, 우리 모델은 **PSNR 29.43**과 **LPIPS 0.249**를 달성했습니다. PSNR 값은 JPEG 품질 설정 20-30으로 압축된 손실 압축 이미지와 유사합니다(Petric & Milinkovic, 2018). **그림 5**는 모델 예측과 해당하는 실제 샘플 예시를 보여줍니다.

![](/assets/images/posts/275/img_11.png)

**그림 5:** 모델 예측 대 실제 정답. 과거 관찰 컨텍스트에서 마지막 4프레임만 표시됩니다.

#### 비디오 품질

우리는 섹션 2에서 설명한 자동 회귀 설정을 사용합니다. 여기서 모델은 자체 예측한 과거 프레임에 조건화된 상태에서, 실제 경로에 의해 정의된 행동 시퀀스를 따라 프레임을 반복적으로 샘플링합니다. 자동 회귀 방식으로 샘플링할 때, 예측 경로와 실제 경로는 몇 단계 후에 자주 다르게 나타나는데, 주로 각 경로의 프레임 간 작은 이동 속도 차이의 축적 때문입니다. 그 결과, **프레임당 PSNR** 값은 점차 감소하고, **LPIPS** 값은 점차 증가하는 현상이 발생합니다. 이는 **그림 6**에서 확인할 수 있습니다. 예측 경로는 여전히 콘텐츠와 이미지 품질 측면에서 실제 게임과 유사하지만, 프레임별 측정값은 이를 완전히 포착하지 못합니다(자동 회귀로 생성된 경로 샘플은 부록 A.1 참조).

따라서 우리는 512개의 경로에 대해 무작위로 선정된 홀드아웃 데이터를 사용하여 **FVD**(Unterthiner et al., 2019)를 측정했습니다. 이는 예측 경로와 실제 경로 분포 간의 거리를 측정하며, 16프레임(0.8초)과 32프레임(1.6초) 길이의 시뮬레이션에 대해 측정되었습니다. 16프레임에 대해 우리 모델은 **FVD 114.02**를, 32프레임에 대해 **FVD 186.23**을 얻었습니다.

#### 인간 평가

시뮬레이션 품질에 대한 또 다른 측정으로, 우리는 10명의 평가자에게 130개의 짧은 무작위 클립(길이 1.6초 및 3.2초)을 제공했습니다. 이 클립들은 시뮬레이션과 실제 게임을 나란히 보여주며, 평가자들은 실제 게임을 식별하는 임무를 수행했습니다(부록 A.6의 **그림 14** 참조). 평가자들은 실제 게임을 시뮬레이션보다 각각 1.6초 클립에서 58%, 3.2초 클립에서 60%의 비율로 선택했습니다.

![](/assets/images/posts/275/img_12.png)

![](/assets/images/posts/275/img_13.png)

**그림 6:** 자동 회귀 평가. 64단계의 자동 회귀 과정에서의 **PSNR**과 **LPIPS** 지표.

#### 5.2 제거 실험

우리 방법의 다양한 구성 요소가 얼마나 중요한지를 평가하기 위해, 평가 데이터셋에서 경로를 샘플링하고 실제 프레임과 예측된 프레임 간의 **LPIPS**와 **PSNR** 지표를 계산합니다.

5.2.1 컨텍스트 길이

![](/assets/images/posts/275/img_14.png)

컨텍스트 길이는 중요한 제한 사항이지만, **표 1**은 우리가 더 긴 컨텍스트를 효율적으로 지원하려면 모델의 아키텍처를 변경해야 하며, 조건으로 사용할 과거 프레임을 더 잘 선택해야 할 필요성을 시사합니다. 이는 향후 연구로 남겨둡니다.

**표 1:** 과거 프레임의 개수. 5개 레벨에서 8912개의 테스트 셋 예시를 사용하여 컨텍스트로 사용된 과거 프레임 개수를 분석합니다. 더 많은 프레임이 PSNR과 LPIPS 지표 모두에서 일반적으로 성능을 개선합니다.

![](/assets/images/posts/275/img_15.png)

#### 5.2.2 노이즈 증강

노이즈 증강의 영향을 평가하기 위해, 추가적인 노이즈 없이 모델을 훈련했습니다. 우리는 노이즈 증강을 적용한 표준 모델과 노이즈를 추가하지 않은 모델(20만 스텝 학습 후)을 자동 회귀 방식으로 평가하고, 512개의 경로에서 예측된 프레임과 실제 프레임 간의 **PSNR** 및 **LPIPS** 지표를 계산했습니다. 각 자동 회귀 단계에 대한 평균 지표 값을 64프레임까지 **그림 7**에 보고했습니다.

노이즈 증강 없이, **LPIPS**는 실제 정답과의 거리가 빠르게 증가했으며, 이는 노이즈 증강을 적용한 표준 모델에 비해 더욱 두드러집니다. 또한, **PSNR** 값은 감소하여 시뮬레이션이 실제 정답과 점점 더 많이 벗어나고 있음을 나타냅니다.

![](/assets/images/posts/275/img_16.png)

![](/assets/images/posts/275/img_17.png)

**그림 7:** 노이즈 증강의 영향. 플롯은 각 자동 회귀 단계에 대한 **LPIPS**(값이 낮을수록 좋음) 및 **PSNR**(값이 높을수록 좋음) 값을 보여줍니다. 노이즈 증강을 사용하지 않으면, 품질이 10-20프레임 이후에 빠르게 저하되지만, 노이즈 증강은 이를 방지합니다.

#### 5.2.3 에이전트 플레이

우리는 에이전트가 생성한 데이터로 훈련한 모델과 랜덤 정책으로 생성된 데이터로 훈련한 모델을 비교했습니다. 랜덤 정책의 경우, 관찰에 의존하지 않는 균등 범주 분포를 따르는 행동을 샘플링했습니다. 두 데이터셋을 비교하기 위해 70만 스텝 동안 모델을 각각 훈련하고 디코더도 함께 훈련했습니다. 모델은 5개의 레벨에서 2,048개의 인간 플레이 경로를 평가 대상으로 사용했습니다. 우리는 64개의 실제 프레임으로 조건화된 첫 번째 프레임 생성과, 3초의 자동 회귀 생성 후의 프레임을 비교했습니다.

전반적으로, 랜덤 경로에서 훈련한 모델도 놀랍게 잘 작동하지만, 랜덤 정책의 탐색 능력에 제한이 있었습니다. 단일 프레임 생성의 경우, 에이전트는 약간 더 나은 성능을 보였으며, 에이전트의 **PSNR**은 25.06, 랜덤 정책은 24.42였습니다. 그러나 3초 자동 회귀 생성 후 프레임을 비교했을 때, 차이는 더 커졌습니다(에이전트 19.02, 랜덤 정책 16.84). 모델을 수동으로 플레이하면서, 일부 구역은 둘 다 매우 쉽게 처리하고, 일부는 둘 다 매우 어렵게 처리하며, 일부 구역에서는 에이전트가 훨씬 더 잘 수행하는 것을 관찰했습니다. 이에 따라, 456개의 예시를 쉽게, 중간, 어렵게 3개의 버킷으로 수동으로 나눴습니다. 이는 게임의 시작 위치로부터의 거리로 기준을 잡았습니다. 쉬운 구역과 어려운 구역에서는 에이전트가 랜덤 정책보다 약간 더 잘 수행했지만, 중간 난이도 구역에서는 예상대로 에이전트가 훨씬 더 잘 수행했습니다(표 2 참조). 부록 A.5의 **그림 13**에서 인간 플레이 세션 동안의 점수 예시를 확인할 수 있습니다.

**표 2:** 난이도별 성능. 에이전트가 생성한 데이터와 랜덤 생성 데이터로 훈련한 모델의 성능을 쉬운, 중간, 어려운 데이터셋으로 나누어 비교했습니다. 쉬운과 중간 난이도는 각각 112개의 아이템을 포함하고, 어려운 난이도는 232개의 아이템을 포함합니다. 각 경로에 대해 3초 후의 단일 프레임에서 메트릭이 계산되었습니다.

![](/assets/images/posts/275/img_18.png)

### 6. 관련 연구

#### 상호작용 3D 시뮬레이션

2D 및 3D 환경의 시각적, 물리적 과정을 시뮬레이션하고 이를 상호작용적으로 탐험할 수 있게 하는 분야는 컴퓨터 그래픽스에서 광범위하게 발전된 영역입니다(Akenine-Möller et al., 2018). **Unreal**과 **Unity** 같은 게임 엔진은 장면의 기하학적 표현을 처리하고 사용자 상호작용에 따라 이미지 스트림을 렌더링하는 소프트웨어입니다. 게임 엔진은 세계 상태를 추적하는 역할을 합니다. 예를 들어, 플레이어의 위치와 움직임, 객체, 캐릭터 애니메이션 및 조명 등을 관리하며, 게임 논리(예: 게임 목표 달성으로 얻은 점수)도 추적합니다. 영화와 텔레비전 제작에서는 **레이 트레이싱**(Shirley & Morley, 2008)을 변형한 방법을 사용하지만, 이는 실시간 응용 프로그램에 사용하기에는 너무 느리고 계산 집약적입니다. 반면에 게임 엔진은 매우 높은 프레임 속도(보통 30-60FPS)를 유지해야 하므로, GPU로 가속된 고도로 최적화된 **폴리곤 래스터화**에 의존합니다. 그림자, 입자, 조명과 같은 물리적 효과는 종종 실제 물리적 시뮬레이션이 아닌 효율적인 휴리스틱을 사용해 구현됩니다.

#### 신경망 기반 3D 시뮬레이션

3D 표현을 재구성하는 신경망 기반 방법은 최근 몇 년간 상당한 진전을 이루었습니다. **NeRFs**(Mildenhall et al., 2020)는 다양한 카메라 포즈에서 촬영된 이미지 세트를 기반으로, 특정 장면에 맞게 최적화된 심층 신경망을 사용하여 복사장을 매개 변수화합니다. 한 번 학습되면, 볼륨 렌더링 방법을 사용해 장면의 새로운 관점을 샘플링할 수 있습니다. **Gaussian Splatting**(Kerbl et al., 2023) 방식은 NeRFs에 기반을 두지만, 3D 가우시안과 적응된 래스터화 방법을 사용해 장면을 표현하여 더 빠른 학습 및 렌더링 시간을 제공합니다. 이 방법들은 인상적인 재구성 결과와 실시간 상호작용을 보여주지만, 종종 **정적인 장면**으로 제한됩니다.

#### 비디오 확산 모델

확산 모델은 텍스트에서 이미지 생성 작업에서 최첨단 결과를 달성했습니다(Saharia et al., 2022; Rombach et al., 2022; Ramesh et al., 2022; Podell et al., 2023). 이러한 연구는 텍스트에서 비디오 생성 작업에도 적용되었습니다(Ho et al., 2022; Blattmann et al., 2023a, 2023b; Gupta et al., 2023; Girdhar et al., 2023; Bar-Tal et al., 2024). 비록 현실감, 텍스트 일치, 시간적 일관성에서 인상적인 발전을 이루었지만, **비디오 확산 모델**은 여전히 실시간 응용 프로그램에 사용하기에는 속도가 너무 느립니다. 우리의 연구는 이 연구 방향을 확장하여, 과거 관찰 및 행동의 기록에 자동 회귀적으로 조건화된 실시간 생성을 가능하게 했습니다.

#### 게임 시뮬레이션 및 월드 모델

여러 연구들이 행동 입력을 기반으로 게임 시뮬레이션 모델을 훈련하려고 시도했습니다. \*\*Yang et al. (2023)\*\*는 실제와 시뮬레이션된 다양한 비디오 데이터셋을 구축하고, 이전 비디오 세그먼트와 행동을 설명하는 텍스트를 바탕으로 다음 비디오를 예측하는 확산 모델을 훈련했습니다. \*\*Menapace et al. (2021)\*\*와 \*\*Bruce et al. (2024)\*\*는 비디오로부터 행동을 비지도 학습하는 것에 중점을 둡니다. \*\*Menapace et al. (2024)\*\*는 텍스트 프롬프트를 게임 상태로 변환하고, 이를 NeRF를 사용해 3D 표현으로 변환합니다. 이러한 연구들과 달리, 우리는 **상호작용 가능한 실시간 시뮬레이션**에 집중하고, 긴 시간 경로에서도 강력한 성능을 보여줍니다. 우리는 강화학습(RL) 에이전트를 활용해 게임 환경을 탐색하고, 상호작용과 관찰 데이터를 생성해 우리 모델을 훈련합니다.

또한, 환경의 예측 모델을 학습하고 이를 RL 에이전트 훈련에 사용하는 연구들도 있습니다. \*\*Ha & Schmidhuber (2018)\*\*는 게임 프레임을 잠재 벡터로 인코딩하기 위해 변분 오토인코더(VAE)를 훈련하고, RNN을 사용해 **VizDoom** 게임 환경을 모방합니다. 이들은 무작위 행동을 기반으로 한 경로에서 학습했습니다. 이후 "환상적인" 환경에서 컨트롤러 정책이 학습됩니다. \*\*Hafner et al. (2020)\*\*는 RL 에이전트를 전적으로 학습된 월드 모델의 잠재 공간에서 생성된 에피소드만으로 훈련할 수 있음을 증명했습니다. 우리의 연구와 가까운 또 다른 연구로는 \*\*Kim et al. (2020)\*\*가 있는데, 이 연구는 LSTM 아키텍처를 사용해 월드 상태를 모델링하고, 컨벌루션 디코더로 출력 프레임을 생성하며, 이를 적대적 목표 하에 함께 훈련했습니다. 이 접근법은 PacMan과 같은 단순한 게임에서는 그럴듯한 결과를 생성하지만, **VizDoom**과 같은 복잡한 환경에서는 흐릿한 샘플을 생성하며 어려움을 겪습니다. 반면, **GameNGen**은 원본 게임에 필적하는 샘플을 생성할 수 있습니다(그림 2 참조). 마지막으로, 우리 연구와 동시에 진행된 \*\*Alonso et al. (2024)\*\*는 확산 월드 모델을 훈련해 과거 관찰을 기반으로 다음 관찰을 예측하고, 이 모델과 RL 모델을 Atari 게임에서 반복적으로 훈련했습니다.

#### DOOM

1993년에 출시된 **DOOM**은 게임 산업에 혁신을 가져왔습니다. 획기적인 3D 그래픽 기술을 도입하여, **1인칭 슈팅 게임** 장르의 초석이 되었고, 수많은 게임들에 영향을 미쳤습니다. **DOOM**은 여러 연구에서 연구 대상이 되었으며, 오픈 소스 구현을 제공하고, 모델이 시뮬레이션하기에 충분히 낮은 해상도를 가지면서도 복잡성이 높아 도전적인 테스트 사례로 적합합니다. 마지막으로, 이 게임은 저자들이 많은 시간을 할애한 게임이었으며, 이 연구에서 사용하는 것은 당연한 선택이었습니다.

### 7. 논의

#### 요약

우리는 **GameNGen**을 소개하며, 신경망 모델이 초당 20프레임으로 고품질 실시간 게임 플레이를 가능하게 한다는 것을 증명했습니다. 또한 컴퓨터 게임과 같은 상호작용 소프트웨어를 신경망 모델로 변환하는 방법을 제시했습니다.

#### 한계

**GameNGen**은 제한된 메모리 문제를 가지고 있습니다. 모델은 3초 정도의 짧은 기록만 접근할 수 있는데, 게임 논리가 더 오랜 시간 동안 유지되는 것이 놀라운 일입니다. 게임 상태 일부는 화면 픽셀을 통해 유지됩니다(예: 탄약과 체력, 사용 가능한 무기 등). 모델은 이러한 정보로부터 의미 있는 일반화를 학습하는 강력한 휴리스틱을 배운 것으로 보입니다. 예를 들어, 렌더링된 화면에서 플레이어의 위치를 추론하고, 탄약과 체력 정보를 통해 해당 지역을 이미 탐험했는지, 적을 물리쳤는지 등을 유추할 수 있습니다. 하지만 이 컨텍스트 길이가 충분하지 않은 상황을 만들기는 쉽습니다. 기존 아키텍처에서 컨텍스트 크기를 증가시켜도 제한적인 효과만 있으며(섹션 5.2.1 참조), 이 짧은 컨텍스트 길이는 중요한 제한 사항으로 남아 있습니다.

또 다른 중요한 한계는 에이전트의 행동과 인간 플레이어의 행동 간의 차이입니다. 예를 들어, 에이전트는 훈련이 끝난 후에도 여전히 게임의 모든 장소와 상호작용을 탐험하지 못해, 해당 상황에서 잘못된 행동을 보이기도 합니다.

#### 향후 연구

우리는 클래식 게임 **DOOM**에서 **GameNGen**을 구현했습니다. 이를 다른 게임이나 상호작용 소프트웨어 시스템에서 테스트하는 것도 흥미로울 것입니다. 우리의 방법은 강화학습 에이전트를 위한 보상 함수만 제외하면 DOOM에만 특화된 것이 아니기 때문입니다. **GameNGen**은 게임 상태를 정확하게 유지하지만, 완벽하지 않으며 이를 개선하기 위해서는 더 정교한 아키텍처가 필요할 수 있습니다. 또한, **GameNGen**은 매우 제한적인 메모리만 사용할 수 있습니다. 복잡한 게임이나 소프트웨어에 대응하기 위해 메모리를 더 효과적으로 확장하는 실험이 중요할 것입니다. **GameNGen**은 TPUv5에서 초당 20~50프레임으로 실행됩니다. 더 높은 프레임 속도와 소비자 하드웨어에서의 실행을 위해 추가 최적화 기법을 실험하는 것도 흥미로울 것입니다.

#### 새로운 상호작용 비디오 게임 패러다임을 향하여

현재 비디오 게임은 사람이 프로그래밍합니다. **GameNGen**은 게임이 코드가 아닌 신경망 모델의 가중치로 구성되는 새로운 패러다임의 일부분을 위한 개념 증명입니다. **GameNGen**은 신경망 모델이 복잡한 게임(**DOOM**)을 상호작용적으로 실행할 수 있는 아키텍처와 모델 가중치가 존재함을 보여줍니다. 해결해야 할 중요한 질문이 많지만, 이 새로운 패러다임이 큰 이점을 제공할 수 있을 것이라는 희망을 가지고 있습니다. 예를 들어, 텍스트 설명이나 예시 이미지를 통해 게임을 개발하고 수정하는 방식이 가능해져 개발 비용이 절감되고 접근성이 높아질 수 있습니다. 기존 게임을 수정하거나 새로운 행동을 만드는 것은 단기적으로 가능할 수 있습니다. 예를 들어, 코드 작성 없이도 프레임 세트를 새로운 플레이 가능한 레벨로 변환하거나 예시 이미지만으로 새로운 캐릭터를 만들 수 있을 것입니다. 이 새로운 패러다임의 또 다른 장점은 프레임 속도와 메모리 사용량에 대한 강력한 보장이 가능하다는 것입니다. 이러한 방향으로는 아직 실험하지 않았으며, 많은 작업이 필요하지만, 우리는 시도해보고 싶습니다. 이 작은 진전이 비디오 게임 경험에 의미 있는 개선을 가져올 수 있기를, 더 나아가 상호작용 소프트웨어 시스템과의 일상적인 상호작용에도 기여할 수 있기를 바랍니다.

### 감사의 말

**Eyal Segalis**, **Eyal Molad**, **Matan Kalman**, **Nataniel Ruiz**, **Amir Hertz**, **Matan Cohen**, **Yossi Matias**, **Yael Pritch**, **Danny Lumen**, **Valerie Nygaard**, **Theta Labs** 및 **Google Research 팀**과 가족들에게 귀중한 피드백과 아이디어, 제안, 그리고 지원을 제공해주신 점에 대해 깊은 감사를 드립니다.

### 기여

- **Dani Valevski**: 대부분의 코드베이스 개발, 시스템 전반에 걸친 매개변수 및 세부 조정, 자동 인코더 미세 조정, 에이전트 훈련, 디스틸레이션 추가.
- **Yaniv Leviathan**: 프로젝트, 방법 및 아키텍처 제안, 초기 구현 개발, 구현 및 문서 작성의 주요 기여자.
- **Moab Arar**: 노이즈 증강을 사용한 자동 회귀 안정화 주도, 여러 제거 실험, 인간 플레이 데이터셋 생성.
- **Shlomi Fruchter**: 프로젝트, 방법 및 아키텍처 제안. DOOM을 사용한 초기 구현, 주요 논문 작성, 평가 메트릭 및 랜덤 정책 데이터 파이프라인.

결국 메모리의 사용은 될 수 밖에 없다. 우리도 기억을 위한 최신 공간은 뇌에 마련하니까
