---
title: "mochi-1-preview"
date: 2025-01-10 12:42:21
categories:
  - 인공지능
tags:
  - mochi-1
---

<https://huggingface.co/genmo/mochi-1-preview>

[genmo/mochi-1-preview · Hugging Face](https://huggingface.co/genmo/mochi-1-preview)

<https://github.com/genmoai/mochi>

[GitHub - genmoai/mochi: The best OSS video generation models](https://github.com/genmoai/mochi)

Mochi 1: 오픈소스 비디오 생성 모델의 새로운 최첨단(SOTA)

Mochi 1 프리뷰는 높은 충실도의 모션과 뛰어난 프롬프트 적합성을 갖춘 오픈 형태의 최첨단 비디오 생성 모델입니다. 이 새로운 모델은 폐쇄형과 오픈형 비디오 생성 시스템 간의 격차를 극적으로 줄여줍니다. 또한, Apache 2.0 라이선스로 공개되어 누구나 자유롭게 사용할 수 있습니다. 지금 가중치를 다운로드하거나, 무료로 제공되는 플레이그라운드에서 이 프리뷰 버전을 직접 체험해보세요.

<https://cdn.genmo.dev/results/Sequence%2001.mp4>

**소개**  
우리는 최신 오픈소스 비디오 생성 모델인 Mochi 1의 연구용 프리뷰를 발표하게 되어 매우 기쁩니다. Mochi 1은 모션 품질에서 극적인 발전을 보여줄 뿐만 아니라, 프롬프트에 대한 매우 뛰어난 적합성을 갖추고 있습니다. 이 모델은 Apache 2.0 라이선스 하에 공개되어, 개인 및 상업적 용도로 자유롭게 이용 가능합니다.

이와 함께, 오늘 genmo.ai/play에서 무료로 Mochi 1을 체험해볼 수 있는 호스팅 플레이그라운드도 선보입니다. Mochi 1의 가중치와 아키텍처는 HuggingFace를 통해 공개되어 있습니다. 오늘은 480p 베이스 모델을 우선 공개하며, 올해 말에 Mochi 1 HD도 출시할 예정입니다.

더불어 Genmo는 NEA의 Rick Yang이 주도하고 The House Fund, Gold House Ventures, WndrCo, Eastlink Capital Partners, Essence VC가 참여한 시리즈 A 펀딩 라운드에서 2,840만 달러를 유치했다는 소식도 함께 전해드립니다. 엔젤 투자자로는 Abhay Parasnis(Typespace CEO), Amjad Masad(Replit CEO), Sabrina Hahn, Bonita Stewart, Michele Catasta 등이 참여했습니다.

Genmo의 사명은 인공 일반 지능(AGI)의 ‘우뇌’를 깨우는 것입니다. Mochi 1은 현실적이든 불가능하든, 상상할 수 있는 모든 것을 구현할 수 있는 ‘세계 시뮬레이터’를 구축하기 위한 첫걸음입니다.

Genmo 팀은 DDPM(Denoising Diffusion Probabilistic Models), DreamFusion, Emu Video 등 프로젝트의 핵심 멤버들로 구성되어 있습니다. 또한 Ion Stoica(Databricks·Anyscale 공동 창립자 겸 회장), Pieter Abbeel(Covariant 공동 창립자 및 OpenAI 초기 팀 멤버), Joey Gonzalez(언어 모델 시스템 선구자이자 Turi 공동 창업자) 등 저명한 기술 전문가들이 자문으로 참여하고 있습니다.

**평가**  
오늘날 비디오 생성 모델과 현실 사이에는 여전히 큰 격차가 존재합니다. 모션 품질과 프롬프트 적합성은 비디오 생성 모델에서 여전히 가장 중요한 과제이자, 부족한 부분으로 지목됩니다.

Mochi 1은 오픈소스 비디오 생성 분야에서 새로운 최고 기준을 제시합니다. 또한 폐쇄형 모델과 비교해도 매우 경쟁력 있는 성능을 보입니다. 특히 우리 480p 프리뷰 모델은 다음과 같은 강점을 갖고 있습니다:

- **프롬프트 적합성(Prompt Adherence):** 텍스트 프롬프트와의 정렬도가 매우 높아, 생성된 비디오가 지시사항을 정확하게 반영합니다. 이를 통해 캐릭터, 환경, 행동 등에 대한 사용자의 세부적인 제어가 가능합니다. 우리는 OpenAI DALL-E 3에서 사용된 프로토콜을 참고하여 비전-언어 모델을 심판으로 활용한 자동화된 지표로 프롬프트 적합성을 벤치마크합니다. 비디오 평가는 Gemini-1.5-Pro-002를 통해 진행했습니다.
- **모션 품질(Motion Quality):** Mochi 1은 최대 5.4초 동안, 초당 30프레임으로 부드러운 비디오를 생성하며 높은 시간적 일관성과 사실적인 동작을 보여줍니다. 유체 역학, 털과 머리카락 시뮬레이션 등 물리 시뮬레이션을 적용해, 일관되고 부드러운 인간 움직임을 표현하며 서서히 언캐니 밸리를 넘어서고 있습니다. 평가자들에게는 프레임 단위 미학보다는 동작의 흥미로움, 물리적 타당성, 유려함 등 모션 자체에 집중하도록 지시했습니다. Elo 점수는 LMSYS Chatbot Arena 프로토콜을 따라 계산됩니다.

### 프롬프트 적합성(Prompt Adherence)

주어진 텍스트 지시사항을 얼마나 정확히 따른 비디오를 생성하는지를 측정하며, 사용자 의도에 높은 정확도로 부합하는지 평가합니다.

![](/assets/images/posts/484/img.png)

### Elo 점수(Elo Score)

생성된 비디오의 모션 부드러움과 공간적 사실성을 평가하여, 비디오가 얼마나 유려하고 시각적으로 매력적인지를 측정합니다.

![](/assets/images/posts/484/img_1.png)

**제한 사항**  
연구용 프리뷰 단계에서 Mochi 1은 계속 발전하고 진화하는 체크포인트입니다. 다음과 같은 몇 가지 알려진 제한 사항이 있습니다. 첫 출시 버전은 480p 해상도로 비디오를 생성합니다. 또한 매우 극단적인 움직임이 있는 경우, 경미한 왜곡이나 변형이 발생할 수 있습니다. Mochi 1은 사진과 유사한 스타일에 최적화되어 있기 때문에 애니메이션 콘텐츠에는 성능이 다소 떨어집니다. 향후 커뮤니티가 다양한 미적 취향에 맞추어 모델을 파인튜닝할 것으로 기대합니다. 덧붙여, 우리는 플레이그라운드에서 견고한 안전 검열 프로토콜을 구현하여, 모든 비디오 생성물이 안전하고 윤리적 가이드라인에 부합하도록 하고 있습니다.

**모델 아키텍처**  
Mochi 1은 오픈소스 비디오 생성 분야에서 중요한 진전을 이룬 모델로, 새로운 **비대칭 확산 트랜스포머(Asymmetric Diffusion Transformer, AsymmDiT)** 아키텍처를 기반으로 하는 **100억 파라미터** 규모의 확산 모델입니다. 완전히 처음부터 학습되었으며, 공개된 비디오 생성 모델 중 가장 큰 규모를 자랑합니다. 게다가 구조가 단순해 수정과 해킹이 용이합니다.

효율성은 커뮤니티가 우리의 모델을 활용할 수 있도록 매우 중요합니다. Mochi와 함께 **비디오 VAE**도 공개합니다. 이 VAE는 비디오를 8×8 공간적 압축과 6× 시간적 압축을 통해 12채널 잠재 공간으로 바꾸어, 원본 대비 96배로 압축합니다.

**AsymmDiT**는 텍스트 처리 과정을 간소화하고 시각적 추론에 신경망 자원을 집중시켜, 압축된 비디오 토큰과 사용자 프롬프트를 효율적으로 처리합니다. AsymmDiT는 텍스트와 비주얼 토큰을 함께 멀티모달 자기어텐션으로 다루며, **Stable Diffusion 3**와 유사하게 각 모달리티에 대해 별도의 MLP 계층을 학습합니다. 다만 시각 스트림은 텍스트 스트림보다 네 배 정도 더 큰 히든 차원을 가져, 텍스트보다 시각 정보에 더 많은 파라미터를 할당합니다. 자기어텐션에서 모달리티를 통합하기 위해, 비정방형 QKV와 출력 투영 계층을 사용합니다. 이러한 비대칭 설계 덕에 추론 시 메모리 사용량이 줄어듭니다.

최근 확산 모델은 종종 여러 개의 사전 학습된 언어 모델을 통해 사용자 프롬프트를 표현하지만, Mochi 1은 단 하나의 **T5-XXL** 언어 모델만으로 프롬프트를 인코딩합니다.

Mochi 1은 44,520개의 비디오 토큰을 한 번에 처리하면서, 3차원 어텐션을 통해 종합적으로 추론합니다. 각 토큰을 위치화하기 위해, 학습 가능 로터리 위치 임베딩(RoPE)을 3차원으로 확장했습니다. 네트워크는 공간축과 시간축에 대해 주파수를 혼합하는 방법을 엔드 투 엔드로 학습합니다.

Mochi는 **SwiGLU 피드포워드 레이어**, 안정성을 높이는 **query-key 정규화**, 내부 활성값을 제어하는 **샌드위치 정규화** 등, 최근 언어 모델 스케일링을 통해 제안된 최신 기법들을 활용합니다.

보다 자세한 내용과 함께 비디오 생성 연구 발전을 촉진하고자, 추후 기술 논문을 공개할 예정입니다.

---

![](/assets/images/posts/484/img_2.png)

AsymmDiT의 주요 특징을 설명드리겠습니다:

1. 비대칭 구조

- 기존 확산 모델과 달리 인코더-디코더가 비대칭적 구조를 가집니다
- 인코더는 가벼운 구조로, 디코더는 더 복잡한 구조로 설계되었습니다

1. 효율적인 학습

- 노이즈 제거 과정을 단일 경로로 처리하여 학습 효율성을 높였습니다
- 기존 확산 모델보다 더 적은 계산량으로 비슷한 품질의 결과를 생성할 수 있습니다

1. 트랜스포머 기반 구조

- 셀프 어텐션과 크로스 어텐션을 활용하여 이미지의 전역적 문맥을 파악합니다
- 피드포워드 네트워크를 통해 특징을 변환합니다

1. 주요 장점

- 생성 속도가 빠름
- 메모리 효율성이 좋음
- 높은 품질의 이미지 생성 가능

정리하자면, 비대칭 확산 트랜스포머는 멀티모달(텍스트·비디오) 처리를 위해 트랜스포머 기반 확산 모델을 구조적으로 **비대칭**으로 재설계한 시스템입니다. 텍스트는 최소한의 리소스로 빠르게 인코딩하고, 비디오에는 대규모 파라미터와 3D 어텐션 메커니즘을 집중해 **고품질·고해상도** 비디오를 생성할 수 있도록 돕는 것이 특징입니다.

---
