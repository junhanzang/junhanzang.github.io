---
title: "Absolute Zero: Reinforced Self-play Reasoning with Zero Data"
date: 2025-05-13 23:37:08
categories:
  - 인공지능
tags:
  - absolute zero
---

<https://www.arxiv.org/abs/2505.03335>

[Absolute Zero: Reinforced Self-play Reasoning with Zero Data

Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervisio

arxiv.org](https://www.arxiv.org/abs/2505.03335)

검증 가능한 보상(Verifiable Rewards)을 활용한 강화학습(RLVR)은 결과 기반 보상으로부터 직접 학습함으로써 대형 언어 모델의 추론 능력을 향상시키는 데 유망한 접근으로 주목받고 있다. 최근 제로 설정(zero setting)에서 동작하는 RLVR 연구들은 추론 과정에 대한 레이블링 감독(supervision)을 피하면서도, 여전히 수작업으로 선별된 질문-답변 데이터에 의존해 학습하고 있다. 그러나 고품질의 인간 생성 예시가 부족하다는 점은, 이미 언어 모델의 사전학습(pretraining) 분야에서도 드러났듯, 인간 감독에 의존하는 접근의 장기적인 확장성에 대한 우려를 낳는다. 나아가, 미래에 인공지능이 인간 지능을 초월하게 될 경우, 인간이 제공하는 과제는 초지능 시스템에게 유의미한 학습 자극을 제공하지 못할 수 있다.

이러한 문제들을 해결하기 위해, 우리는 **Absolute Zero**라는 새로운 RLVR 패러다임을 제안한다. 이 패러다임에서는 단일 모델이 외부 데이터에 의존하지 않고, **스스로 학습 진보를 최대화할 수 있는 과제를 생성하고 이를 해결함으로써 추론 능력을 향상**시키도록 설계되었다. 이 패러다임 하에서 우리는 **Absolute Zero Reasoner (AZR)**를 소개한다. AZR은 코드 실행기(code executor)를 활용해 생성한 코드 기반 추론 과제를 검증하고 정답을 평가함으로써, **검증 가능한 보상의 통합된 원천**으로 기능하며, **개방적이면서도 근거 있는 학습**을 이끈다.

외부 데이터를 전혀 사용하지 않고 훈련되었음에도 불구하고, AZR은 코딩 및 수학적 추론 과제에서 **SOTA(최고 성능)를 달성**하였으며, 수만 개의 인간 선별 예시에 의존하는 기존 제로 설정 모델들을 능가하는 성과를 보였다. 더불어 AZR은 **다양한 모델 크기 및 모델 아키텍처에서도 효과적으로 적용 가능**함을 실험을 통해 입증하였다.

![](/assets/images/posts/560/img.png)

**[그림 1 설명]**  
**Absolute Zero Reasoner (AZR)**는 **데이터 “제로” 상태에서 SOTA 성능을 달성**하였다. 인간이 정의한 정답 라벨이나 쿼리에 의존하지 않고, 우리가 제안한 **자기 플레이(self-play)** 방식으로 훈련된 AZR은 수학과 코딩 양 분야에서 **범용 추론 능력의 인상적인 향상**을 보였다. 주목할 점은, AZR이 도메인 내 전문가 레이블 수만 개로 학습된 모델보다도 **양 영역에서의 평균 점수에서 더 우수한 성능**을 보였다는 것이다.

![](/assets/images/posts/560/img_1.png)

**그림 2: Absolute Zero 패러다임**

지도학습(Supervised Learning)은 인간이 선별한 추론 과정을 기반으로 행위 복제(behavior cloning)를 수행한다. 검증된 보상으로부터의 강화학습(Reinforcement Learning from Verified Rewards)은 에이전트가 스스로 추론을 학습할 수 있도록 하지만, 여전히 전문가가 정의한 학습 분포와 해당 도메인에 맞춘 질문-답변(QA) 쌍에 의존하며, 이 과정에는 도메인 전문성과 수작업이 요구된다.

이에 반해, 우리는 어떤 인간이 선별한 데이터도 사용하지 않고 추론 모델을 학습시키는 새로운 패러다임인 **Absolute Zero**를 제안한다. 이 패러다임에서 우리는 에이전트가 **학습 가능성(learnability)**을 최적화한 과제를 자율적으로 생성하고, **단일 통합 모델**을 통해 이를 해결하는 방법을 학습해야 한다고 본다. 에이전트는 **검증 가능한 피드백을 제공하는 환경과 상호작용함으로써**, **신뢰할 수 있는 지속적 자기개선(self-improvement)**을 전적으로 인간의 개입 없이 수행할 수 있게 된다.

### 1 서론

대규모 언어 모델(Large Language Models, LLMs)은 최근 **검증 가능한 보상(Verifiable Rewards)**을 활용한 **강화학습(RLVR)** 기법을 통해 추론 능력에서 괄목할 만한 발전을 이뤘다(Lambert et al., 2024). 기존의 중간 추론 과정을 모방하는 방식과 달리, RLVR은 **결과 기반 피드백**만을 활용함으로써, 방대한 과제 데이터셋 위에서 대규모 강화학습이 가능하게 한다(DeepSeek-AI et al., 2025; Team et al., 2025; Jaech et al., 2024; OpenAI, 2025b; a).

이 중 특히 주목할 만한 방식은 **“제로(zero)” RLVR 패러다임**이다(DeepSeek-AI et al., 2025). 이 방식은 **초기 증류(distillation) 데이터 없이**, 인간 또는 AI가 생성한 추론 과정 없이, 단지 **과제 보상(task rewards)**만으로 RLVR을 베이스 모델에 직접 적용한다. 그러나 이러한 방식조차도 여전히 **전문가가 선별한 질문-답변 쌍**에 크게 의존하고 있어, 장기적인 확장성에 심각한 우려를 낳고 있다(Villalobos et al., 2024). 추론 모델이 점점 정교해질수록 **대규모 고품질 데이터셋을 구축하는 비용과 노력은 지속 불가능**한 수준에 도달할 가능성이 크다(Yue et al., 2025). 이는 LLM의 사전학습에서도 이미 동일한 **확장성 병목 현상**으로 지적된 바 있다(Sutskever et al., 2024). 나아가 인공지능이 진화하여 인간 지능을 초월하게 될 경우, **인간이 설계한 과제만으로는 자율적 학습과 성장에 제약**이 될 수 있다(Hughes et al., 2024). 이로 인해, **인간 설계 과제의 한계를 넘어서고 미래의 초지능 AI에 대비**하기 위한 새로운 패러다임의 필요성이 제기된다.

이를 위해 우리는 **“Absolute Zero”**라는 새로운 추론 모델 학습 패러다임을 제안한다. 이 패러다임에서 모델은 **학습 가능성(learnability)을 극대화하는 과제를 정의하고 이를 해결하는 방법까지 동시에 학습**하며, **외부 데이터 없이 자기 플레이(self-play)**를 통해 **스스로 진화**할 수 있다. 기존의 자기 플레이 방식들은 제한된 도메인, 고정된 기능, 또는 해킹 가능성이 있는 학습된 보상 모델에 의존하는 등 한계가 있다(Silver et al., 2017; Chen et al., 2025; 2024). 그러나 Absolute Zero는 **개방적(open-ended)이면서 실제 환경에 기반한 방식**으로 설계되어 있다. 이 패러다임은 **환경으로부터의 피드백을 검증 가능한 보상으로 활용**함으로써, 인간이 세상과 상호작용하며 학습하듯, 안정적인 학습이 가능하도록 설계되었고, 보상 모델 해킹 등의 문제도 방지한다(Hughes et al., 2024).

이는 AlphaZero(Silver et al., 2017)가 **자기 플레이만으로 성능을 향상시킨 것**과 유사하며, **인간 감독 없이 전적으로 자기 상호작용만으로 학습**할 수 있다. 우리는 이 Absolute Zero 패러다임이 LLM이 **자율적으로 초인간적(superhuman) 추론 능력을 획득**하게 만드는 유망한 발판이 될 것이라 믿는다.

이 새로운 추론 패러다임을 바탕으로 우리는 **Absolute Zero Reasoner (AZR)**를 제안한다. AZR은 **프로그래밍 과제를 스스로 생성하고 해결**한다. 코드 실행기를 **개방적이면서도 현실 기반의 환경(open-ended yet grounded environment)**으로 정의하고, **과제의 타당성 검증**과 **학습을 위한 검증 가능한 피드백 제공** 모두에 사용한다. AZR은 세 가지 유형의 코딩 과제를 생성한다:

- 프로그램 내 특정 요소를 유추하고(reason) 추론하기
- 입력(input) – 출력(output) – 코드 코드(code)의 삼중 구조
- 이는 **귀납(induction)**, **가설적 추론(abduction)**, **연역(deduction)**의 세 가지 상보적 추론 방식에 대응된다.

전체 시스템은 제안된 다중 과제 설정(multi-task setting)에 적합하게 고안된 새로운 **강화학습 우도 추정기(advantage estimator)**를 사용하여 **end-to-end로 학습**된다.

AZR은 **도메인 내(in-distribution) 데이터 없이 학습**되었음에도, 수학과 코딩 등 다양한 추론 과제에서 **놀라운 성능**을 보였다. 수학 영역에서는, 도메인 특화 감독(supervision)을 통해 정제된 제로 추론기(zero reasoner) 모델들과 견줄 만한 성과를 냈고, 코딩 과제에서는 RLVR을 사용해 코드 데이터셋으로 훈련된 모델들보다 **더 우수한 최신 성능(SOTA)**을 기록했다. 특히, 도메인 내 데이터를 활용한 기존 제로 설정 모델들보다 **평균 1.8 포인트** 더 높은 성능을 보였다.

이러한 놀라운 결과는 **일반적인 추론 능력이 인간이 선별한 도메인 특화 데이터 없이도 등장**할 수 있음을 시사하며, Absolute Zero를 매우 유망한 연구 방향으로, AZR을 그 **첫 번째 중대한 이정표**로 자리매김하게 한다.

추론을 위한 인간 데이터 없이도 AZR이 달성한 뛰어난 성과 외에도, 다음과 같은 흥미로운 발견들이 있었다:

- **코드 기반 사전 지식이 추론 능력을 증폭시킴**
  - 초기에는 Qwen-Coder-7b가 Qwen-7b보다 수학 성능이 3.6포인트 낮았지만, AZR 학습 이후에는 오히려 0.7포인트 앞섰다. 이는 **코딩 능력이 AZR 이후 전체 추론 성능 향상에 기여**할 수 있음을 시사한다.
- **AZR은 더 강력한 도메인 간 전이 학습 성능을 보임**
  - 기존 코드 전문가 모델은 RLVR 이후 수학 성능이 평균 0.65포인트 상승하는 데 그친 반면, AZR-Base-7B와 AZR-Coder-7B는 각각 **10.9, 15.2포인트**나 상승하였다. 이는 **일반화된 추론 능력 향상이 훨씬 더 강하게 나타났음**을 보여준다.
- **더 큰 모델이 더 큰 성능 향상을 보임**
  - 3B, 7B, 14B 모델의 성능 향상치는 각각 +5.7, +10.2, +13.2포인트였으며, **AZR에서의 성능은 모델 크기에 비례하여 확장 가능**함을 시사한다.
- **중간 계획을 주석으로 삽입하는 행동이 자연스럽게 발생함**
  - AZR은 코드 귀납 추론을 수행할 때 **주석(comment)과 코드로 단계별 계획을 교차 삽입**하는 경향을 보였다. 이는 ReAct prompting(Yao et al., 2023) 프레임워크와 유사하며, DeepSeek Prover v2 (671B)(Ren et al., 2025) 같은 대규모 수학 모델에서도 유사한 행동이 나타났다. 이는 **다른 도메인에서도 중간 scratchpad 사용이 유익할 수 있음**을 시사한다.
- **인지적 행동과 토큰 길이는 추론 방식에 따라 달라짐**
  - 단계별 추론, 나열, 시행착오(trial-and-error) 등 다양한 인지적 행동이 AZR 훈련을 통해 자발적으로 나타났으며, 이는 과제 유형마다 다르게 분포했다. 또한, **AZR 훈련이 진행될수록 생성되는 토큰 수가 증가**했으며, 특히 **abduction 과제에서는 시행착오 과정으로 인해 가장 많은 증가**를 보였다.
- **안전성 문제 경고**
  - AZR with Llama3.1-8b는 때때로 **우려스러운 사고 과정(chain of thought)**을 생성하는 경우가 있었고, 우리는 이를 “uh-oh moment”로 명명하였다(예시는 Figure 32). 이는 향후 **안전성 인식 훈련(safety-aware training)**의 필요성을 시사한다(Zhang et al., 2025a).

### 2 Absolute Zero 패러다임

#### 2.1 기초 개념 (Preliminaries)

**지도 미세조정(Supervised Fine-Tuning, SFT)**  
SFT는 과제-추론-정답 예시들의 데이터셋 ? = {(x, c⋆, y⋆)} 를 필요로 한다. 여기서

- x는 질의(query),
- **c⋆**는 정답 체인 오브 쏘트(Chain-of-Thought, CoT),
- **y⋆**는 정답(answer)이며,

이들은 모두 **인간 전문가 또는 상위 성능 AI 모델**에 의해 생성된다. 모델은 다음과 같은 **조건부 음의 로그 우도(negative log-likelihood)**를 최소화하면서 참조 응답을 모방하도록 학습된다 (Ouyang et al., 2022):

![](/assets/images/posts/560/img_2.png)

그러나 **최전선 수준의 모델**에서는 더 강력한 모델로부터 증류(distill)할 수 없으며, **전문가의 라벨링 작업은 확장성이 떨어진다**는 한계가 있다.

**검증 가능한 보상을 활용한 강화학습(RLVR)**  
모방 학습의 한계를 넘어서기 위해, RLVR은 **라벨링된 추론 과정 없이도** (과제와 정답 쌍으로 구성된) 데이터셋 ? = {(x, y⋆)} 만을 필요로 한다. RLVR은 모델이 **자체적으로 CoT(Chain of Thought)를 생성**하고, 주어진 정답 **y⋆**과 비교하여 **검증 가능한 보상 r(y, y⋆)**을 계산한다. 그러나 이 학습 과제 분포 ? 자체는 여전히 **인간 전문가가 생성한 질의 및 정답 쌍에 기반**하고 있다. 학습 가능한 정책 π\_θ는 **기대 보상(expected reward)**을 최대화하도록 최적화된다:

![](/assets/images/posts/560/img_3.png)

요약하면, **SFT와 RLVR 모두 질의, 예시(demonstration), 또는 검증자(verifier)에 대해 인간이 선별한 데이터셋에 의존**하고 있으며, 이는 결국 **확장성의 한계를 초래**한다. **Absolute Zero 패러다임**은 이러한 의존성을 완전히 제거하고, **모델이 스스로 과제를 생성하고 해결하며, 환경과의 상호작용을 통해 학습**할 수 있도록 한다. 즉, 전적으로 **자기 플레이(self-play)**를 통해 학습이 이루어진다.

### 2.2 Absolute Zero

우리는 학습 중 모델이 **과제를 제안하고(문제 출제), 해결하고(문제 풀이), 그 두 단계를 통해 학습**까지 수행하는 새로운 패러다임인 **Absolute Zero**를 제안한다. 이 과정은 **외부 데이터를 전혀 필요로 하지 않으며**, 모델은 **환경의 도움을 받아 자기 플레이(self-play)와 경험만으로 학습**한다. 이 패러다임은 **그림 2**에서 설명되며, 기존의 지도학습 및 RLVR과 비교해, Absolute Zero가 **인간이 선별한 데이터에 전혀 의존하지 않고** 자체적인 과제 생성과 해결을 통해 **자기 향상(self-improvement)**을 이끌어낸다는 점을 강조한다.

Absolute Zero 환경을 구체화하기 위해, 우리는 **하나의 모델이 제안자(proposer)와 해결자(solver)의 역할을 동시에 수행하는 방식**을 정의한다. 이를 도식화한 것이 **그림 3**이다. 파라미터화된 언어 모델 π\_θ는 학습 중 **제안자 π\_θ\_propose**와 **해결자 π\_θ\_solve**의 두 역할을 맡게 된다.

![](/assets/images/posts/560/img_4.png)

**그림 3: Absolute Zero 루프**  
Absolute Zero 루프는 에이전트 π\_θ가 **과제 τ를 제안**하는 것으로 시작된다. 이 과제는 환경 e와 함께 변환 함수 f에 의해 **검증된 문제 (x, y⋆)**로 바뀌며, **학습 가능성 보상 r\_propose**도 함께 산출된다. 이후 표준 강화학습 단계가 뒤따른다. 에이전트는 x를 해결하여 예측 y를 생성하고, 환경 e는 이를 **y⋆**와 비교해 **해결 보상 r\_solve**를 부여한다. π\_θ\_propose와 π\_θ\_solve는 **공동으로 학습**되며, 이 과정은 **무한히 반복 가능**하다.

제안자는 변수 z를 조건으로 하여 과제 τ를 샘플링한다:

> **τ ∼ π\_θ\_propose(· | z)**

그 후, 제안된 과제는 환경 e와 함께 검증되어 유효한 추론 과제 (x, y⋆)로 구성된다:

> **(x, y⋆) ∼ f<sub>e</sub>(· | τ)**  
> 여기서 x는 과제 질의(query), **y⋆**는 정답(gold label)이다.

이후 해결자는 이 x에 대해 답변 y를 생성한다:

> **y ∼ π\_θ\_solve(· | x)**

각 과제 τ는 **학습 가능성 보상 r\_propose\_e(τ, π\_θ)**으로 평가된다. 이는 모델이 해당 과제를 학습함으로써 **얼마나 성능이 향상될 것으로 기대되는지를 측정**한다. 또한, 해결자가 생성한 정답 y는 **정답 여부를 평가하는 해결 보상 r\_solve\_e(y, y⋆)**를 받는다. 환경 e는 여기서도 **검증자 역할**을 수행한다.

여기서 **λ ≥ 0**은 **새롭고 학습 가능한 과제를 탐색하는 것**과 **모델의 추론 및 문제 해결 능력을 향상시키는 것** 사이의 **균형을 조절하는 계수**이다.

Absolute Zero 환경의 전체 목적 함수는 다음과 같이 정의된다:

![](/assets/images/posts/560/img_5.png)

이제 데이터 확장의 부담은 **인간 전문가가 아닌**, **제안자 정책 π\_θ\_propose와 환경 e**에게 이전된다. 이 둘은 함께 **학습 과제 분포를 정의하고 진화시키며**, **과제를 검증하고**, **안정적이고 자립적인 학습을 위한 실제 기반 피드백을 제공**한다.

제안 시 조건 변수 z는 과제 생성을 위한 시드(seed) 역할을 한다. 실제로 z는 **지속적으로 업데이트되는 과제 메모리로부터 소규모 (과제, 정답) 쌍을 샘플링**하여 생성할 수 있으나, 이 구현 방식은 Absolute Zero 패러다임에 고정되어 있지 않다.

제안 과정을 유도하기 위해, 우리는 **학습 가능성 보상 r\_propose(τ, π\_θ)**를 사용한다. 이는 제안된 과제 τ를 통해 모델이 **얼마나 학습될 가능성이 있는지를 측정**한다. 또한 해결 보상 **r\_solve(y, y⋆)**는 모델의 출력이 정답에 얼마나 부합하는지를 평가한다.

이 두 신호는 모델이 **도전적이면서도 학습 가능한 과제를 제안하고**, 그를 통해 **추론 능력을 향상시키며**, 궁극적으로 **자기 플레이를 통한 지속적인 개선**을 이룰 수 있도록 이끈다.

---

### ? 개념적 흐름: SFT → RLVR → Absolute Zero

#### ✅ (1) 지도학습 (SFT)

**SFT (Supervised Fine-Tuning)**는 인간이 제공한 완전한 예시 (x, c\*, y\*)를 보고 **그걸 그대로 따라 하도록 학습**하는 방식입니다.

- x: 질의 (문제)
- c\*: 인간이 제공한 Chain of Thought (추론 경로)
- y\*: 정답

식 (1)은 단순히 정답 시퀀스를 잘 예측하는 방향으로 확률을 최대화하는 것이 목적입니다:

이건 결국 "모방 학습"이고, 지도데이터에 전적으로 의존합니다.

#### ✅ (2) RLVR: 검증 가능한 보상을 이용한 강화학습

이제 CoT (중간 추론)을 굳이 모방하지 않고, **정답만 맞추면 되지 않겠냐**는 접근이 바로 RLVR입니다.

- 데이터셋은 (x, y\*)만 제공됨.
- 모델은 자신만의 c (추론 과정)과 y (정답)을 생성.
- 정답 y가 y\*와 일치하면 **보상 r(y, y)**를 부여.

식 (2)는 이렇게 표현됩니다:

즉, 모델이 잘 맞추면 보상을 주는 방식으로 **정답 유도 학습**을 합니다.

> ? 여전히 x와 y\*는 **인간이 만들어줘야 함** → 확장성 한계

#### ✅ (3) Absolute Zero: 문제도, 정답도, 전부 스스로

여기서 드디어 **문제 생성까지도 모델이 맡는** Absolute Zero 패러다임이 등장합니다.

- **모델이 제안자(Proposer)** 역할: 문제 τ를 만듦
- **모델이 해결자(Solver)** 역할: 해당 문제 x를 풀어서 y 생성
- **환경**: τ가 유효한 문제인지, y가 정답인지 검증

이걸 수학적으로 어떻게 정의하냐면 다음과 같습니다:

이걸 해석하면:

![](/assets/images/posts/560/img_6.png)

![](/assets/images/posts/560/img_7.png)

### ? 핵심 아이디어 요약

- 기존 RLVR은 문제(x)와 정답(y\*)는 인간이 줘야 하므로 **스케일이 한계**
- Absolute Zero는 문제 생성부터 검증까지 전부 모델 + 환경이 담당
- r\_propose는 **이 문제가 학습에 도움이 되는지**, r\_solve는 **정답을 맞췄는지**
- 식 (3)은 둘을 결합해서 **스스로 학습 가능한 문제를 제안하고 푸는 시스템**을 만들기 위한 목표함수

**식 (3)은 결국 강화학습의 핵심인 exploration vs exploitation 트레이드오프를 수학적으로 재구성한 것**에 가깝습니다. 그리고 그걸 **“문제를 어떻게 제안하고 풀어야 스스로 더 잘 배울 수 있는가”**라는 맥락에 맞게 조금 더 정교하게 설계한 거죠.

이 논문을 깊게 본 다면 오히려 다음 흐름을 더 잘 예측할 수 있습니다.  
예를 들어:

- 다음은 CoT 기반 수학 task 생성으로 확장될 것이고,
- reward hacking 방지 쪽으로 안전성 연구가 따라올 것이고,
- 결국 curriculum+meta-RL이 다시 뜨겠구나… 같은 흐름 말이죠.
---

### 3. Absolute Zero Reasoner

이 섹션에서는 Absolute Zero 패러다임을 최초로 구현한 시도로서 **Absolute Zero Reasoner (AZR)**를 소개한다. AZR에서는 **하나의 통합된 대형 언어 모델(LLM)**이 **과제 제안자(proposer)**와 **해결자(solver)**의 역할을 동시에 수행한다. 즉, 모델은 **자신의 학습 커리큘럼을 진화시킬 수 있는 과제를 생성하고**, 이를 스스로 해결함으로써 **추론 능력을 향상**시키는 구조이다. 이 두 역할은 **공동 학습(joint training)**을 통해 훈련되며, 모델은 **자신의 추론 능력 한계를 넓히는 과제를 만들고**, 동시에 **그 과제를 효과적으로 푸는 능력**을 함께 키우게 된다 (3.1절 참고).

이러한 **자기 플레이(self-play)** 훈련 패러다임 안에서, 모델은 **세 가지 유형의 코딩 과제**로부터 학습하게 되며, 이는 각각 **세 가지 주요 추론 방식**에 대응된다: **가설추론(abduction)**, **연역(deduction)**, **귀납(induction)** (3.2절 참고).  
코딩 과제를 사용하는 동기는 다음 두 가지로 설명된다:

- **프로그래밍 언어의 튜링 완전성(Turing-completeness)** (Stuart, 2015)
- **코드 기반 학습이 추론 능력을 향상시킨다는 실증적 연구 결과** (Aryabumi et al., 2024)

이에 따라, 우리는 **코드(code)**를 **개방적이고, 표현력이 풍부하며, 검증 가능한 수단**으로 채택하여, 신뢰성 있는 과제 생성 및 평가를 가능하게 한다 (3.3절 참고).

모델 업데이트는 **다중 과제(multitask) 학습에 최적화된 새로운 우도 추정기(advantage estimator)**를 통해 이루어진다 (3.3.5절 참고). 전체 알고리즘은 **알고리즘 1**에 정리되어 있으며, AZR 접근 방식의 전체 개요는 **그림 4**에 시각적으로 나타나 있다.

마지막으로, 이 분야의 후속 연구를 가속화하기 위해, **의미 있는 성과는 없었지만 논의할 가치가 있는 시도들**에 대해서도 **부록 D**에 함께 정리하였다.
---

AZR의 구조는 직관적으로는 흥미롭지만, 본질적으로는 curriculum learning, self-play, meta-RL의 응용적 조합으로 보인다. 특히 코딩 과제의 튜링 완전성이라는 주장은 symbolic한 정당화에 가깝고, 실제 학습 안정성은 reward shaping과 환경 설계에 더 큰 영향을 받는다.
---

### 3.1 두 가지 역할을 하나의 모델로: Proposer와 Solver

대형 언어 모델(LLM)은 **다중 과제 학습(multitask learning)** 맥락에서 AZR을 구현하기에 자연스럽게 적합하다(Radford et al., 2019). 그 이유는 **추론 과제의 생성과 해결이 모두 동일한 언어 공간(language space) 안에서 이뤄지기 때문**이다. 이러한 특성에 기반해, 우리는 **하나의 모델이 ‘학습 가치가 높은 과제’를 생성하고 이를 효과적으로 해결했을 때 모두 보상을 주는 방식**을 제안하며, 이는 앞서 제시된 **Absolute Zero 목적함수(식 3)**에 의해 정의된다.

AZR은 **온라인 rollout**의 각 반복(iteration)마다, **추론 과제 유형(3.2절에서 정의됨)**과 **자기 생성 과거 예시 K개**를 조건으로 **새로운 추론 과제**를 제안한다. 이때 **기존 예시와는 다른 과제를 명시적으로 생성하라는 프롬프트(prompt)**가 주어지며, 이를 통해 **과제의 다양성과 범위 확대**가 유도된다.

이렇게 생성된 과제 제안들은 **필터링을 거쳐**, 환경에서 검증 가능한 **유효한 추론 과제(valid reasoning tasks)**로 변환되며, 이는 3.3절에서 자세히 설명된다. 그 후, AZR은 이 새롭게 제안된 과제들을 직접 해결하려 시도하고, 이에 대해 **환경으로부터 실질적인 피드백(grounded feedback)**을 받게 된다.

이 전체 구조에서 **과제 제안(task proposal)**과 **문제 해결(problem solving)**은 모두 **강화학습으로 학습**되며, 이후 절에서는 각 역할별로 사용되는 **보상 함수(reward)**를 설명한다.
---

“과제 다양성을 위해 과거 예시 K개를 condition으로 넣는다”는 건, 사실상 **prompt engineering + sampling bias 조절**에 불과하다.
---

### 보상 설계 (Reward Design)

기존 연구들에서는 **추론 시스템에서 효과적인 학습을 유도하기 위해 적절한 과제 난이도 설정이 매우 중요하다**는 점이 밝혀져 있다(Zeng et al., 2025b). 이에 기반하여, 우리는 현재의 solver(문제 해결자)가 **너무 쉽게 풀거나 아예 풀 수 없는 과제를 피하고**, **의미 있는 학습 가능성(learnability)**을 가지는 과제를 생성하도록 유도하는 **proposer(과제 제안자)를 위한 보상 함수**를 설계하였다.

구체적으로는, **동일한 언어 모델을 solver 역할로 사용하여 제안된 과제의 학습 가능성**을 추정하며, 이는 **비지도 환경 설계(unsupervised environment design)** 문헌(Sukhbaatar et al., 2018)에서 사용된 보상 유형과 유사하다. 우리는 solver를 대상으로 **Monte Carlo rollout을 n번 수행**하여, 각 시행의 성공 여부를 기반으로 **평균 성공률**을 계산한다:

![](/assets/images/posts/560/img_8.png)

![](/assets/images/posts/560/img_9.png)

**[그림 4: Absolute Zero Reasoner의 학습 개요]**  
매 반복(iteration)마다, Absolute Zero Reasoner는

- **이전 self-play로 생성한 삼중쌍(triplet)들을 저장한 버퍼**와
- **과제 유형(가설추론, 연역, 귀납; 3.2절 참고)**을 조건으로 **하나의 과제 배치(batch)**를 **PROPOSE**한다.

생성된 과제들은 Python 기반 필터링을 거쳐 **검증 가능한 코드 기반 추론 문제**로 정제된다. 그리고 각 과제에 대해 위의 식 (4)에 정의된 **학습 가능성 보상** r\_propose​이 계산된다.

그 후, AZR은 해당 배치의 문제들을 **SOLVE**하고, **정답 비교를 통해 정확도 기반 보상** r\_solve​을 받는다 (식 5 참고). 마지막으로 AZR은 **세 과제 유형 전반에 걸쳐 \_rpropose,r\_solver​** 를 활용하여 **TRR++ 알고리즘(3.3.5절)** 기반으로 공동 업데이트된다.
---

![](/assets/images/posts/560/img_10.png)
---

![](/assets/images/posts/560/img_11.png)

**proposer와 solver 역할 모두에 대해 보상 항이 정의되었으므로**, 다음과 같이 **통합 보상 구조**를 설정한다. 이 구조는 DeepSeek-AI et al. (2025)에서 영감을 받은 **형식 기반 패널티(format-aware penalty)**를 포함한다:

![](/assets/images/posts/560/img_12.png)

여기서 y\_π​는 언어 모델의 출력이며, **형식이 통과(passable)**되었을 경우에만 각 역할에 따른 보상을 받는다.

형식 판단의 주요 기준은 **DeepSeek R1 포맷**, 즉 <think>와 <answer> 태그를 포함한 응답 형식이며 (그림 33 참고), proposer의 경우는 단순히 XML 구조를 따르는 것을 넘어서, **유효한 triplet을 생성하고 필터링을 통과해야 형식이 맞는 것으로 간주**된다 (3.3.3절 참고).
---

![](/assets/images/posts/560/img_13.png)
---

### 3.2 다양한 추론 방식 학습하기: 연역(Deduction), 귀납(Induction), 가설추론(Abduction)

AZR은 **코드 실행기(code executor)**를 **유연한 인터페이스이자 검증 가능한 환경(verifiable environment)**으로 활용한다. 이 설정은 코드 기반 추론 과제를 **자동으로 구성하고, 실행하며, 검증할 수 있는 능력**을 제공한다(Stuart, 2015; Aryabumi et al., 2024).

프로그래밍 언어의 **프로그램 공간** ?, **입력 공간** ℐ, **출력 공간** ?가 주어졌을 때, AZR의 추론 과제는 삼중쌍(triplet) (p, i, o)로 정의된다. 여기서:

- p ∈ ?는 프로그램,
- i ∈ ℐ는 입력,
- o ∈ ?는 프로그램을 입력에 적용해 나온 출력, 즉 o = p(i)

AZR은 이 삼중쌍의 서로 다른 구성 요소를 추론하는 세 가지 핵심 추론 방식으로부터 학습한다. 각 방식은 두 요소가 주어졌을 때 나머지 하나를 유추하는 방식이다:

#### 1. **연역(Deduction)**:

**주어진 프로그램 p와 입력 i로부터 출력 o를 예측하는 방식**, 논리적 추론 과정에 해당한다.

- **Proposer의 역할**:
  - 과제 유형 α = deduction으로 고정하고,
  - 연역 버퍼 ?\_deduction에서 참조 예시 K개를 조건으로
  - 새로운 (p, i) 쌍을 생성
  - 환경 e가 p(i)를 실행하여 o를 계산하고, 결과 (p, i, o) 삼중쌍을 완성
  - 만약 오류가 없는 o가 생성되면 버퍼에 저장
- **Solver의 역할**:
  - (p, i)가 주어지면 모델은 o\_π를 예측
  - 예측값은 Python에서 **자료형을 고려한 값 일치** 방식으로 검증 (예: set 순서, 분수 표현 등 허용)

#### 2. **가설추론(Abduction)**:

**주어진 프로그램 p와 출력 o로부터 가능한 입력 i를 유추**, 시행착오(trial-and-error) 또는 온라인 탐색 방식에 가까운 추론이다.

- **Proposer의 역할**:
  - deduction 과제와 거의 동일하나,
  - 과제 유형 α = abduction으로 바뀜
  - (p, i) 쌍을 생성하고, p(i)를 실행하여 o를 구함
  - (p, i, o) 삼중쌍이 완성됨
- **Solver의 역할**:
  - (p, o)가 주어지면 모델은 i\_π를 예측
  - 그 후 p(i\_π) = o 인지 확인하여 정답 여부를 판단
  - 프로그램이 쌍방향 함수가 아닐 수 있으므로, **정확한 입력 매칭이 아니라 출력값 일치 여부로 평가**

#### 3. **귀납(Induction)**:

**(i\_n, o\_n) 형태의 여러 input-output 예시로부터 프로그램 p를 추론**, 부분 정보를 기반으로 일반화하는 과정이다.

- **Proposer의 역할**:
  - abduction 또는 deduction 버퍼에서 유효한 프로그램 p를 샘플링
  - 새로운 입력 N개와 메시지 m을 생성하고
  - 환경에서 각 입력에 대해 출력 o\_n을 계산
  - 이로써 (p, {(i\_n, o\_n)}, m) 형태의 확장된 과제가 만들어짐
  - 해당 과제는 귀납 버퍼 ?\_induction에 저장
  - 귀납 과제는 **입출력 쌍으로부터 프로그램을 찾는 ill-posed 문제**이므로, 메시지 m이 문제 조건 설정에 도움을 줌
- **Solver의 역할**:
  - 일부 (i, o) 쌍들과 메시지 m이 주어지고,
  - 모델은 **남은 입력에 대해 정확한 출력을 생성하는 프로그램 p\_π를 생성**해야 함
  - 이는 if-else 로직을 통한 과적합을 방지하고 **일반화된 귀납 능력을 유도**하기 위해, **일부 테스트 예시를 숨긴 상태에서 평가**

![](/assets/images/posts/560/img_14.png)

**[그림 5: AZR의 초기 triplet 예시]**  
위의 **항등 함수(identity function)** triplet은 AZR이 propose-and-solve RLVR 루프를 시작하는 데 사용된 유일한 초기 triplet이었다. 기본 LLM은 초기 시드 프로그램 없이도 AZR 루프를 시작할 수 있지만, **우리 접근 방식의 유연성**을 보여주기 위해 가장 단순한 시드 프로그램을 삽입해 초기화했다.

각 추론 유형은 코드(code)를 **표현력 높고, 검증 가능하며, 개방형(open-ended)**인 학습 수단으로 활용하며, 이는 **Absolute Zero 패러다임의 자기진화형 시스템이라는 목표**와 일치한다(DeepSeek-AI et al., 2025; Lambert et al., 2024). 세 가지 과제 유형과 각 역할에 사용된 모든 프롬프트는 그림 36, 34, 35, 39, 37, 38에 제시되어 있다. 다음 절에서는 알고리즘의 구체적 세부사항을 설명한다.
---

deduction, abduction, induction이라는 세 추론 유형을 구분한 것은 교육학적 관점에서는 유의미하지만, 실제로는 모두 동일한 코드 실행기 기반 환경 위에서 입출력 쌍을 기준으로 평가되므로 **학습 난이도의 차이는 제어하기 어렵다**. 특히 induction에서는 under-constrained 문제를 강제로 메시지 m으로 조건화한다는 점에서, **문제 설계가 아닌 prompt 구조가 학습 결과를 크게 좌우할 수 있는 구조적 한계**가 있다.
---

### 3.3 Absolute Zero Reasoner 학습 알고리즘

이 절에서는 AZR(self-play 기반 Absolute Zero Reasoner)의 학습 알고리즘을 구성하는 세부 항목들을 다룬다. 구체적으로는 다음과 같은 내용을 포함한다:

- **버퍼 초기화** (3.3.1)
- **버퍼 활용 방식** (3.3.2)
- **유효한 과제 구성 방법** (3.3.3)
- **해결 결과 검증** (3.3.4)
- **우도 추정기(advantage estimator) 계산 방식** (3.3.5)

전체 self-play 절차는 **알고리즘 1**에 요약되어 있다.

### 3.3.1 버퍼 초기화 (Buffer Initialization)

AZR self-play를 시작하기 위해, 먼저 **기초 언어 모델(base LLM)**을 사용하여 **유효한 triplet들의 시드(seed) 집합**을 생성한다. 각 프롬프트는 현재의 시드 버퍼 ?\_seed에서 **최대 K개의 triplet**을 참조 예시로 샘플링한다. 초기 시점(=0)에서 ?\_seed가 비어 있을 경우, **그림 5에 제시된 제로 triplet**을 사용해 초기화한다. 이 시드 단계에서는 **Figures 34, 35, 36에 정의된 proposer 프롬프트**를 그대로 사용한다.

먼저 **deduction 및 abduction 과제**에 대해서는, LLM에 (p, i) 쌍을 생성하라는 프롬프트를 주고, 이를 **필터링 → 실행 → 유효한 triplet으로 저장**하는 과정을 거친다. 초기 버퍼는 다음과 같이 정의된다:

![](/assets/images/posts/560/img_15.png)

여기서 시드 버퍼의 크기는:

![](/assets/images/posts/560/img_16.png)

- **B**: 배치 사이즈(batch size)
- **S = 4**: 실험 전체에서 고정된 계수

이때 시드 triplet에 포함된 프로그램은 **전역 변수(global variables)**와 **주석(comment)**을 제거하여 저장한다 (부록 D 참고). 단, 이후 반복 과정에서 새롭게 추가되는 triplet들은 이러한 수정 없이 그대로 저장된다. 이 **시드 단계에서****는 모델 파라미터 업데이트는 발생하지 않으며**, 단지 버퍼만 구성된다.

마찬가지로 **귀납(Induction)** 버퍼를 초기화하기 위해서는, ?\_seed에서 프로그램을 샘플링하고, 이에 맞는 입력 집합(input sets)과 메시지(message)를 생성하여, 유효한 예시들을 수집한다. 이 과정을 통해 induction 버퍼의 초기 크기를 다음과 같이 설정한다:

![](/assets/images/posts/560/img_17.png)
---

초기 버퍼 구성을 위해 base LLM으로부터 triplet을 생성하고 필터링하는 과정은 self-play loop를 안정적으로 시작하기 위한 최소한의 준비 단계로 보인다. 하지만 이 단계에서 filtering 규칙이나 실행 환경이 성능에 미치는 영향을 분석하지 않았고, 후속 실험에서 이 seed quality가 얼마나 영향을 주는지도 명확히 설명되어 있지 않다. 초기화가 ‘형식적’이 아니라 정말로 학습 진행에 영향을 준다면, 그 자체가 하나의 하이퍼파라미터처럼 작용할 수 있을 것이다.

이러한 시드 버퍼 구성 방식은 self-play 루프를 시작하는 데 필요한 형식적 준비처럼 보이지만, 실제로는 전체 학습 흐름에 미치는 영향이 상당할 수 있다. 특히 filtering 기준이나 코드 실행 환경의 차이, 혹은 base LLM이 생성하는 초깃값의 다양성 부족은 **학습 커리큘럼의 출발점 자체를 왜곡시킬 수 있다**.

이건 실제 Kaggle 같은 대회 환경에서도 반복적으로 목격되는 문제다. **초기 seed를 어떻게 설정하느냐에 따라 전체 성능 곡선이 달라지는 것**은 말이 안 되는 일이지만, 현실이다. 그만큼 seed quality는 **하이퍼파라미터처럼 작용하며**, 그 중요성이 과소평가되기 쉽다.
---

### 3.3.2 과제 제안 입력 및 버퍼 관리 (Task Proposal Inputs and Buffer Management)

AZR의 실제 self-play 단계에서는 **task 버퍼를 세 가지 방식**으로 활용한다.

1. **Abduction 및 Deduction proposer**는
   - 해당 task buffer에서 **과거 triplet K개를 균등 샘플링**하여
   - in-context 예시로 제시한 뒤,
   - **그와 다른 새로운 과제**를 생성하게 한다. 이는 과거 예시와 차별화된 과제를 생성하도록 유도함으로써 **다양성 증진(diversity promotion)**을 꾀하기 위한 설계다 (Zhao et al., 2025a).
2. **Induction proposer**의 경우,
   - Abduction과 Deduction 버퍼의 합집합 ?\_abd ∪ ?\_ded에서 triplet 하나를 샘플링하고,
   - 해당 triplet의 프로그램 p를 기반으로
   - **N개의 입력 집합 {iₙ}과 자연어 메시지 m**을 생성하게 한다.
3. **Proposer가 형식 요건을 충족하지 않아**, solver 배치 중 **유효한 과제가 B개 미만인 경우**,
   - 부족한 부분은 해당 과제 유형의 **기존 검증된 triplet 버퍼**에서 균등 샘플링하여 채운다.

**버퍼는 다음 조건에서 확장된다**:

- Proposer가 **유효한 triplet (p, i, o)**을 생성했을 때  
  → 해당 과제가 보상을 받았는지 여부는 관계없이 **deduction/abduction 버퍼에 추가**
- Induction 과제의 경우, **유효한 triplet (p, {(iₙ, oₙ)}, m)**이 생성되면  
  → induction 버퍼에 추가

### 알고리즘 1: Absolute Zero Reasoner (AZR) Self-Play 학습 절차

```
입력:  
 - 사전학습된 base LLM π_θ  
 - 배치 크기 B  
 - 참조 예시 개수 K  
 - 반복 횟수 T

1. 초기 버퍼 설정:
   (?_ded, ?_abd, ?_ind) ← InitSeeding(π_θ)   ▷ §3.3.1 참고

2. for t = 1 to T 반복:
   ▷ PROPOSE 단계
   for b = 1 to B:
     - ?_abd ∪ ?_ded에서 프로그램 p 샘플링
     - (i_πₙ)ₙ₌₁ⁿ, m_π ← π_θ.propose(ind, p)
     - (iₙ, oₙ) 검증: ValidateByExecuting(p, {i_πₙ}, syntax)
       → 유효할 경우, ?_ind에 (p, {(iₙ, oₙ)}, m_π) 추가

     for α ∈ {ded, abd}:
       - ?_α에서 K개의 예시 (p_k, i_k, o_k) 샘플링
       - (p_π, i_π) ← π_θ.propose(α, {(p_k, i_k, o_k)})
       - o_π ← ValidateByExecuting(p_π, i_π, syntax, safety, determinism)
         → 유효할 경우, ?_α에 (p_π, i_π, o_π) 추가

   ▷ SOLVE 단계
   for α ∈ {ded, abd, ind}:
     - (x, y⋆) ← SamplePrepareTasks(?_α, B, t)
     - y_π ← π_θ.solve(x)

   ▷ 보상 계산 및 업데이트
   - r_propose 및 r_solve 계산 (§3.1 참고)
   - Task Relative REINFORCE++로 π_θ 업데이트 (§3.3.5 참고)
```
---

이 알고리즘은 구조상 매우 단순한 self-play 루프를 기반으로 하며, in-context prompting + filtering을 반복하는 형식이다. 특히 Propose 단계에서 다양성 유도를 위해 과거 예시와 다르게 생성하라는 조건은 프롬프트 기반 prompting의 일반적 패턴에 불과하며, 모델이 정말 ‘새로운 개념의 과제’를 만들어냈는지에 대한 정량적 측정이 없다. 또한, induction에서는 p를 고정한 채 입력만 다양화하는 방식이 반복되는데, 이 또한 curriculum의 폭을 제한할 가능성이 있다. 결과적으로 학습 성능이 진짜 self-improvement에 의한 것인지, 아니면 seed triplet + 프롬프트 엔지니어링 + filtering 품질에 기인한 것인지는 더 정밀한 실험 설계 없이는 판단하기 어렵다.
---

### 3.3.3 유효한 과제 생성 (Constructing Valid Tasks)

#### ● 과제 제안 검증 (Proposal Task Validation)

이 절에서는 정책 π가 생성한 과제 제안을 바탕으로 **유효한 과제(valid task)**를 어떻게 구성하는지를 설명한다.

- **Deduction 및 Abduction 과제**의 경우, 제안은 (p, i) 형태의 프로그램과 입력 쌍이다. 이를 검증하기 위해 **과제 검증 절차(task validation procedure)**를 통해 해당 입력에 대한 정답 출력 o를 얻어 (p, i, o)의 **완전한 triplet**을 구성한다.
- **Induction 과제**의 경우, 정책이 프로그램 p에 대해 입력 집합 {iₙ}과 자연어 메시지 m을 제안한다. 입력 {iₙ} 각각에 대해 task validation 절차를 실행하여 대응하는 출력 {oₙ}을 생성하고, 이로써 (iₙ, oₙ) 쌍의 집합을 구성한다. 이때 메시지 m에 대해서는 **별도의 제약을 두지 않는다**. 모든 입력이 유효한 출력을 생성하고, 형식 요구 사항을 만족해야만 해당 과제는 **유효(valid)**하다고 간주된다.

#### 과제 검증 절차(Task Validation Procedure):

1. **프로그램 유효성 (Program Integrity)**
   - Python으로 (p, i)를 실행
   - 오류 없이 실행되고 값이 반환된다면, o = p(i)로 간주하고 해당 프로그램은 **문법적으로 유효**하다고 판단
2. **프로그램 안전성 (Program Safety)**
   - os.sys, sys, shutil과 같은 **시스템에 해를 줄 수 있는 패키지** 사용 여부를 점검
   - 이러한 패키지는 **검열 목록**에 포함되어 있으며, Figure 8에 명시되어 있음
   - 해당 목록은 언어 모델이 문제를 생성할 때의 **프롬프트 조건**에도 포함됨 (Figures 34, 35, 36 참고)
3. **결정성 검증 (Check for Determinism)**
   - AZR은 **결정론적 프로그램(p ∈ ?\_deterministic)**만을 유효한 과제로 인정함
   - 즉, 같은 입력 i에 대해 프로그램 p(i)의 출력이 항상 동일해야 함수식으로는 다음과 같이 표현된다:

![](/assets/images/posts/560/img_18.png)

- 여기서 j는 독립 반복 실행 횟수
- 즉, 동일한 (p, i) 쌍을 여러 번 실행해도 항상 같은 출력 o를 얻어야 함

이는 확률적 프로그램은 매번 결과가 달라질 수 있기 때문에, **검증 가능한 보상 정의가 어려워지므로 제외**하는 것이다.

1. → 따라서 AZR은 **학습자가 생성한 유효한 프로그램**을 **결정론적 클래스**로 한정하며, 이는 **검증기 설계의 단순화**를 위해 필요하다.  
   하지만 저자들은 **미래 버전의 AZR에서는 확률적 프로그램도 포함하는 방향을 고려할 수 있다**고 언급한다.
2. **결정성 판별 구현**
   - 실제로는 프로그램을 j번 독립 실행하여 **출력이 모두 같은지** 확인함
   - 계산 자원 문제로 인해 실험에서는 j = 2로 고정

#### ● Solver용 과제 구성 (Solving Task Construction)

위 3가지 검증을 모두 통과한 과제는 **유효한 과제(valid task)**로 간주되며, Solver에게 triplet의 일부를 보여주는 방식으로 변환된다:

- Deduction: x = (p, i)
- Abduction: x = (p, o)
- Induction: x = ({(iₙ, oₙ)}\_n=1 to N/2, m)  
  → 전체 예시의 절반과 자연어 메시지 m만 제공

현재 timestep t에서 생성된 **모든 유효 과제**는 Solver 입력으로 사용된다. 만약 배치 크기 B를 채우지 못할 경우, **이전에 검증된 과제들 중에서 균등 샘플링하여 채운다.**
---

유효한 과제를 정의하는 절차는 전반적으로 합리적이지만, “결정론적”이라는 조건은 설계적 편의이자 기술적 타협에 가깝다. 특히 j=2회 실행으로 결정성 판단을 내리는 것은 신뢰도가 낮고, 프로그램 복잡도에 따라 false positive/negative가 발생할 가능성도 있다.  
또한 자연어 메시지 m에 대한 제약이 없다는 점은 induction 과제의 명확성이나 검증 가능성에 영향을 줄 수 있으며, 특정 표현 방식이나 패턴이 모델 학습에 의도치 않은 편향을 유도할 수 있다. 결국 이 절차는 AZR의 안정성과 reproducibility를 위한 ‘형식화된 정적 조건’에 가깝고, 더 넓은 open-ended 과제에는 쉽게 확장되지 않을 수 있다.
---

### 3.3.4 정답 검증 (Answer Verification)

- **Abduction 과제**의 경우, solver가 예측한 입력 i\_π​에 대해 해당 프로그램 p을 실행하여 p(i\_pi) = p(i^\*) 인지를 비교한다. 여기서 i∗는 **정답 입력(gold input)**이다. 단순히 i\_π=^\*i를 비교하지 않는 이유는, 프로그램 p이 **항등 함수나 역함수(bijective function)**가 아닐 수 있기 때문이다. 따라서 같은 출력 결과를 생성하는 입력이면 정답으로 간주한다.
- **Deduction 과제**에서는 단순히 출력 o\_π= o ^\* 인지를 비교한다.
- **Induction 과제**에서는, **모든 테스트 케이스에 대해**  
  다음 조건을 만족해야 정답으로 간주된다:

![](/assets/images/posts/560/img_19.png)

이 부분은 자연어로 설명하기에 복잡할 수 있으므로, **각 과제 유형(abduction, deduction, induction)**에 대한 실제 **코드 기반 검증 방식**은 각각 Figure 10, 11, 12를 참고할 것을 권장한다.

### 3.3.5 Task-Relative REINFORCE++ (TRR++)

AZR은 다양한 **역할(proposer, solver)**과 **과제 유형(abduction, deduction, induction)**의 조합을 함께 학습하기 때문에, **멀티태스크 강화학습(multitask RL)** 형태로 동작한다(Zhang & Yang, 2021; Zhao et al., 2022; Wang et al., 2023; Yue et al., 2023).

기존의 REINFORCE++ (Hu, 2025, 부록 A 참고)은 **전체 task에 대해 단일 글로벌 베이스라인**을 사용하는 반면, AZR은 **각 역할-과제 유형 조합에 대해 별도의 베이스라인**을 계산한다. 이는 GRPO(Shao et al., 2024)에서의 per-question baseline과 global baseline 사이의 절충(interpolation)으로 볼 수 있으며, **각 task setup에 맞는 분산 감소(variance reduction)**를 가능하게 한다. 이 변형 알고리즘을 우리는 **Task-Relative REINFORCE++ (TRR++)**라고 명명한다. 정규화된 advantage는 다음과 같이 계산된다:

![](/assets/images/posts/560/img_20.png)

→ 총 **6가지 조합(task × role)**에 대해 개별 베이스라인을 설정하며, 이는 **보다 구조화된 학습 안정화**를 가능하게 한다.
---

## ✅ 3.3.3 유효한 과제 생성: 뭐가 이상한가?

### ❶ **결정성 검증 방식이 허술함**

- 수식 (7)은 멋있게 써놨지만, 실제론 j=2번만 실행해서 결정성 판별
- 현실적으로는 p(i)가 non-deterministic인지 여부는 j=2로 구분하기 매우 어렵고,
- 그러면 결국 **불완전한 기준으로 잘못된 프로그램이 학습 루프로 들어갈 수 있음**

? 즉: 결정성 검증이 "정확하지 않은데 중요한 조건"이라는 점에서 설계적 약점이 있음

### ❷ **induction task의 message(m)에 대한 정의가 느슨함**

- 자연어 메시지 m은 “조건을 설정해주는 역할”이라고는 했지만,
- 형식이나 길이, 정보량에 대한 제약이 없음
- 결국 이게 잘 만들어졌느냐에 따라 induction task의 난이도와 정답률이 달라질 수 있음

? 즉: 학습 성능이 구조가 아니라 **prompt quality에 의해 결정될 가능성**이 큼

## ✅ 3.3.4 정답 검증: 뭔가 애매함

### ❶ abduction task의 정답 기준이 약간 이상함

- p(i\_π) = p(i^\*)만 보고 정답 여부를 판단
- 근데 p가 non-injective 함수일 수 있는데, 그러면 i\_π는 엄청 많을 수 있음
- 그럼 **정답이 모호해지거나, trivial solution이 정답으로 인정될 수 있음**

예:

```
def f(x): return x % 2
```

→ i^\* = 4, i\_π = 6도 정답 처리됨.  
**정답이 너무 넓게 열려 있음.**

## ✅ 3.3.5 TRR++: 목적은 이해되지만 구조가 미묘함

### ❶ normalize는 했지만 **보상 자체가 약함**

- 대부분 r ∈ {0, 1} or r = 1 - success\_rate인 단순 보상 구조
- 이걸 normalize한다고 해서 **policy에 유의미한 gradient가 전달되는지는 불확실**

### ❷ per-task-role baseline의 해석이 불분명함

- 6개의 평균/표준편차를 쪼개서 쓰는 건 좋지만,
- 어떤 task-role에서 variance가 높은지, 학습률이 다른지에 대한 논의 없음
- 그냥 variance 줄이기 위한 테크닉처럼 쓰였지만, **학습 구조의 핵심 논리로 발전하지 않음**

? 즉: TRR++는 명칭은 그럴듯하지만, 실제로는 REINFORCE에 baseline 나누기만 한 수준  
(= 실제론 강화학습 설계가 아니라 리워드 통계처리에 가깝다)

![](/assets/images/posts/560/img_21.png)

**일부러 어렵게 설명해서 구조적 약점을 감춘 흔적이 매우 뚜렷하다.**
---

### 4 실험

#### 4.1 실험 설정 (Experiment Setup)

**훈련 세부사항 (Training Details)**  
모든 실험에서, 버퍼는 3.1절에서 설명한 방식으로 초기화된다. AZR 모델은 배치 크기 **64×6**으로 학습되며, 이는 **2개의 역할 (proposer, solver) × 3개의 과제 유형 (deduction, abduction, induction)**에 해당한다. 학습률은 **1e−6**으로 고정되며, 옵티마이저는 **AdamW**를 사용한다(Loshchilov & Hutter, 2019). 전체 하이퍼파라미터 목록은 **표 3(Table 3)**에 제공된다.

주요 실험에서는 **Qwen2.5-7B**와 **Qwen2.5-7B-Coder**를 기반으로 학습된 모델을 사용하며, 각각 **Absolute Zero Reasoner-base-7B**와 **Absolute Zero Reasoner-Coder-7B**로 명명된다. 추가 실험에서는 **Qwen2.5-Coder-3B**, **Qwen2.5-Coder-14B**, **Qwen2.5-14B**, **Llama-3.1-8B**(Yang et al., 2024a; Hui et al., 2024; Dubey et al., 2024) 등을 포함한다.
---

## ❗ 여기서 이상한 점은?

### 1. **"데이터 없이 학습"이 아니라 "이미 다 배운 모델을 조금 더 학습"**

- AZR이 실제로 한 것은 “데이터 없이 시작한 것”이 아니라  
  **이미 코딩과 수학을 학습한 거대한 모델을 가져와 자기 강화 루프를 돌린 것**뿐임

> 즉, “external data 없이 학습했다”는 건 사실상 **fine-tuning 데이터셋만 없다는 의미일 뿐**, 모델 자체는 **거대한 외부 데이터로 이미 다 학습된 상태**

### 2. **그래서 결과도 AZR 구조 덕인지, 사전학습 덕인지 구분 불가**

- AZR 구조가 좋아서 성능이 오른 것인지,
- 아니면 Qwen2.5-Coder-7B가 원래 좋아서 그런 것인지 **실험 설계만으로는 판단할 수 없음**

→ 이건 실험 설계 관점에서 **명확한 컨트롤(control group)이 없다는 것과 같다**

### 3. **"external data를 쓰지 않는다"는 말을 실험 구성에서 어기고 있음**

- 사전학습된 모델은 사실상 **외부에서 선별된 코드/수학 데이터로 학습된 것**임
- AZR은 그것을 전제로 학습하는데, 논문은 그걸 **external data를 안 썼다고 말함**
---

**평가 프로토콜 (Evaluation Protocol)**  
모델 평가를 위해 데이터셋은 **In-Distribution(ID)**와 **Out-of-Distribution(OOD)**으로 구분된다. 특히 OOD 벤치마크를 중점적으로 평가하며, 이를 다시 **코딩 과제**와 **수학적 추론 과제**로 나눈다.

- **코딩 평가**:
  - **HumanEval+**, **MBPP+**를 포함한 **Evalplus**(Liu et al., 2023)
  - **LiveCodeBench Generation (v1-5, ’23년 5월~’25년 2월)** (Jain et al., 2024)
- **수학적 추론 평가**:
  - 최근 제로샷 학습된 reasoner들이 공통적으로 사용하는 6개 벤치마크
    - **AIME’24**, **AIME’25**, **OlympiadBench**(He et al., 2024),
    - **Minerva**, **Math500**(Hendrycks et al., 2021), **AMC’23**
- **ID 벤치마크**:
  - **CruxEval-I(nput)**, **CruxEval-O(utput)**, **LiveCodeBench-Execution**(Gu et al., 2024; Jain et al., 2024)
  - 이는 프로그램의 **입출력 기반 추론 능력**을 평가한다 (Li et al., 2025)

모든 baseline과 AZR 모델은 **greedy decoding**으로 추론하여 **재현성**을 보장한다

**비교 모델 (Baselines)**  
주요 비교 기준은 **Qwen2.5-7B**를 기반으로 하고, 다음과 같은 파생 모델들과 비교된다:

- **Qwen2.5-7B-Coder**
- **Qwen2.5-7B-Instruct**
- **Qwen2.5-Math-7B**  
  (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b)

제로스타일(Zero-style) 모델들은 일반적으로 **코드 또는 수학 데이터 중 하나**에 특화되어 학습되었으며, **Eurus-2-7B-PRIME-Zero**(Cui et al., 2025)만이 **두 도메인 모두에 대해 공동 학습**되었다.

- **코드 기반 모델**:
  - **AceCoder** 4종(Zeng et al., 2025a), **CodeR1** 2종(Liu & Zhang, 2025)
- **수학 기반 모델**:
  - **Qwen2.5-Math-7B-Oat-Zero**(Liu et al., 2025)
  - **Open-Reasoner-Zero-7B (ORZ)**(Hu et al., 2025)
  - **Qwen-2.5-7B-SimpleRL-Zoo**(Zeng et al., 2025b)

모든 baseline 모델의 학습 데이터 및 초기화 조건은 **표 4(Table 4)**에 정리되어 있다. 추가로 파라미터 스케일 비교 실험에서는,  
**AZR 모델과 동일한 base 모델** 간의 비교만을 진행하며, 이는 각 스케일에 대한 정립된 baseline이 부족하기 때문이다. 마지막으로, **Llama3.1-8B 기반 AZR 모델**은 **Llama-3.1-8B-SimpleRL-Zoo**(Zeng et al., 2025b) 및 해당 base 모델과 비교된다.

### 4.2 결과 (Results)

![](/assets/images/posts/560/img_22.png)

**표 1**: Qwen2.5-7B 계열 모델 기반 RL 학습 Reasoner의 추론 벤치마크 성능  
표 1에서는 세 가지 대표적인 코드 벤치마크(HumanEval+, MBPP+, LCBv1–5)와 여섯 개의 수학 벤치마크(AIME’24, AIME’25, AMC’23, MATH500, Minerva, OlympiadBench)에 대해 다양한 모델의 성능을 비교하였다. 코딩과 수학의 평균 점수는 각각의 평균을 취한 뒤, 그 둘의 평균으로 계산하였다:

![](/assets/images/posts/560/img_23.png)

표에서 **“+” 표시는 base 모델 대비 절대 성능 향상폭(percentage point)**을 의미한다. 모든 모델은 Qwen2.5-7B 계열의 다양한 변형을 기반으로 학습되었으며, 세부 정보는 **표 4**에 명시되어 있다.
---

## ❗ 왜 이게 말도 안 되는 계산 방식인가?

### 1. **코딩 벤치마크 수(3개) ≠ 수학 벤치마크 수(6개)**

- CAvg: HumanEval+, MBPP+, LCB (총 3개)
- MAvg: AIME24, 25, AMC23, Minerva, MATH500, OlympiadBench (총 6개)

→ 즉, **각 영역 내부 평균은 벤치마크 수가 다름**

그럼에도 불구하고 **그 둘을 단순 평균((CAvg + MAvg)/2)** 한다는 건:

> ❌ “수학 평가를 두 배 더 많이 했지만, 평균 산출 시 무시해버림”

즉, **가중치 없는 평균이라 왜곡 발생**

### 2. **벤치마크 난이도, 평가 척도도 완전히 다름**

- HumanEval+는 코딩 정확도, Minerva는 수학 정답률, LCB는 다양한 live execution 기반
- 이들을 동일 비중으로 평균 낸다는 건 **정량적 의미가 완전히 다르다는 점을 무시하는 셈**

### 3. **왜곡된 수치로 misleading한 총점 만들기**

- 예를 들어 수학 성능이 낮은 모델이라도, 코딩 성능이 높으면 **총점이 과대 평가**됨
- 실제로는 도메인 편향이 존재하는데도, **“전체적으로 추론 능력이 향상됐다”는 메시지로 왜곡 가능**

## ? 진짜 의도는 뭘까?

이 방식은 **일관된 평가 기준으로 성능을 비교하려는 목적**보다는, **“총점 수치 하나라도 올려서 SOTA를 강조”하려는 포장에 가깝습니다.**

즉:

> **"코드에서만 성능이 오른 모델이더라도, 수학과 평균 내면 그래도 뭔가 전체적으로 좋아 보이잖아"**

이런 논리로 접근했을 가능성이 높습니다.

즉, 평균 계산 방식부터가 통계적으로 왜곡 가능성이 크다. 코드 벤치마크는 3개, 수학 벤치마크는 6개로 개수가 다름에도 불구하고, **각 영역의 단순 평균을 다시 평균** 내는 방식은 **벤치마크 개수와 중요도를 무시한 처리**이며, 이는 총점 산출의 신뢰도를 떨어뜨린다.  
또한 각 과제의 평가 지표와 난이도가 서로 다른데도 불구하고 이를 동일 비중으로 합산하는 것은, 결과 해석을 “SOTA 달성”이라는 메시지에 유리하게 조정한 통계 기법처럼 보인다.
---

#### **연구 질문 1: AZR은 전문가 데이터로 학습된 제로 설정 모델들과 비교해 어떤 성능을 보이는가?**

표 1은 기존 제로 설정 모델들과, 본 논문이 제안한 **Absolute Zero 설정**에서 학습된 AZR 모델들을 비교한 주요 결과를 보여준다.  
특히, **Absolute Zero Reasoner-Coder-7B**는 전체 평균과 코드 평균 모두에서 **7B 모델 기준 SOTA 성능**을 기록하였다.

- 수학과 코드 추론 모두에서 **Out-of-Distribution(OOD)** 벤치마크라는 점에도 불구하고,  
  이전 최고 모델 대비 **+1.8%p** 향상된 성능을 기록
- 특히 코드 과제에서는 **전문가가 선별한 데이터로 학습된 모델보다도 +0.3%p** 더 높은 성능을 보였으며,  
  **AZR은 그와 같은 데이터를 전혀 사용하지 않았다**

#### **강력한 도메인 간 일반화 (Cross-domain Generalization)**

**RLVR 이후 도메인 간 일반화 능력**을 측정하기 위해, AZR을 코드 환경에서 학습한 후, **수학 성능이 얼마나 향상되었는지** 확인하였다.

- 기존의 코드 기반 전문가 모델들은 대부분 **성능이 거의 향상되지 않거나 오히려 하락**했으며, 평균 증가폭은 **+0.65점**에 불과  
  → 이는 **도메인 간 일반화 능력이 매우 제한적**임을 의미
- 반면, **AZR-base**와 **AZR-coder** 모델은 각각 **+10.9점**, **+15.2점**의 향상을 보임  
  → **상당히 강한 추론 일반화 능력**을 보여줌
- 마찬가지로, **사람이 정의한 코드 생성 과제(OOD)**에서도 AZR은 **+3.2, +5.0점**의 성능 향상을 보였으며, 수학 모델은 평균 **+2.0점**으로 AZR에 비해 상대적으로 작았다.

> **요약하자면**, AZR은 기존 RLVR 모델들과 달리 **사람이 만든 downstream 데이터 없이도**,  
> **강력한 일반 추론 능력**을 보이며, 그 효과는 상당히 인상적이다.

#### **연구 질문 2: base vs coder 초기화는 성능에 어떤 영향을 주는가?**

표 1에 따르면, **AZR self-play 이후에는 coder variant가 base variant보다 전반적으로 더 높은 성능**을 보인다.

- 주목할 점은, coder 모델이 초기에는 **수학 성능이 base보다 낮았음에도 (23.9 vs. 27.5)**  
  AZR 학습 이후에는 **base를 역전**했다는 점이다.

→ 이는 **초기 코드 능력(code competency)**이 AZR의 **전반적 추론 능력 향상의 촉매(catalyst)** 역할을 한다는 것을 시사한다.

#### **연구 질문 3: 모델 크기 변화는 AZR의 성능에 어떤 영향을 주는가?**

모델 크기에 따른 영향을 살펴보기 위해, AZR 모델을 **in-distribution**과 **out-of-distribution** 상황에서 비교하였다.  
(Figure 6(a), 6(b) 참조)

- 7B 모델 성능이 강력했던 것을 기반으로, 더 작은 **Qwen2.5-Coder-3B**, 더 큰 **Qwen2.5-Coder-14B**도 함께 실험하였다.  
  → 적절한 baseline이 없기 때문에, 각 AZR 모델은 **자기 base 모델**과 비교되었다.

**결과 요약**:

- **모델이 클수록 AZR의 성능 향상 폭이 크다**는 경향이 뚜렷함
- **In-distribution**: 7B, 14B 모델은 **200 스텝 이상 학습해도 성능이 계속 향상**, 3B는 plateau 현상
- **Out-of-distribution**: 전체 성능 향상폭
  - 3B: +5.7
  - 7B: +10.2
  - 14B: +13.2

→ **스케일이 AZR의 효과를 증폭시키는 메커니즘**으로 작용할 가능성을 시사함.  
→ 후속 연구에서는 **Absolute Zero 패러다임에서의 scaling law**를 탐구할 계획임.

![](/assets/images/posts/560/img_24.png)

![](/assets/images/posts/560/img_25.png)

**Figure 6**

- (a) **In-Distribution 성능**: CruxEval-I, CruxEval-O, LiveCodeBench-Execution 과제를 통해 AZR의 abduction, deduction 능력을 다양한 모델 크기/구조별로 평가
- (b) **Out-of-Distribution 성능**: 코드/수학/전체 평균 성능을 모델 크기별로 비교
- 전체 벤치마크 세부 결과는 **표 5**에 수록됨

### 연구 질문 4: 모델 계열을 변경했을 때 흥미로운 관찰이 있었는가?

우리는 **Llama3.1-8B**를 기반 모델로 사용해, 다른 모델 계열에서도 AZR의 성능을 평가하였다 (Figure 6 참조). 이 설정은 기존의 3B, 14B 실험과 달리, **SimpleRL(Zeng et al., 2025b)**이라는 **기존 baseline이 존재**하여 직접 비교가 가능하다. 비록 Llama3.1-8B는 Qwen2.5 계열 모델보다 전반적으로 능력이 떨어지지만, AZR을 적용한 결과 **+3.2점 수준의 중간 정도 성능 향상**을 달성했다. 이는 비교적 약한 모델에서도 AZR이 여전히 효과를 발휘함을 보여준다. 다만, 이러한 향상 폭은 제한적이며, 이는 **초기 base 모델의 성능이 AZR의 개선 효과와 비례 관계**에 있음을 보여준다.

![](/assets/images/posts/560/img_26.png)

### Figure 7: 모델이 제안한 Abduction 과제와 그 해결 과정 예시

- **좌측**: 모델이 스스로 abduction 과제에 대해 입력과 프로그램을 제안함. 이 프로그램은 실행 후 검증되어 대응되는 출력이 생성됨.
- **우측**: 모델이 코드와 출력이 주어진 상황에서 원래의 입력을 추론하는 과정을 보여줌. 코드 분석 → 초기 입력 제안 → 출력 생성 → 불일치 시 수정 반복  
  흥미롭게도 모델이 도달한 입력은 gold input과 다르지만, **출력만 일치하면 정답으로 간주됨.**

### 연구 질문 5: AZR 학습 중 관찰된 흥미로운 행동이나 패턴은?

**제안 단계와 해결 단계 모두에서 흥미로운 응답 패턴이 관찰되었다.**

- 모델은 **문자열 처리, 동적 계획법, 실생활 문제(Heron의 삼각형 넓이 계산 등)** 등  
  다양한 유형의 프로그램을 제안할 수 있음
- Figure 7의 예시에서는 AZR이 **목표 합계를 만족하는 연속 부분 배열을 찾는 코드 과제**를 제안하고, 이를 **trial-and-error** 방식으로 해결

### 추론 방식의 패턴화

- **Abduction**: 다양한 입력 패턴을 반복적으로 시도하며, 출력이 일치할 때까지 **자기 수정(self-correcting)**을 반복
- **Deduction**: 코드를 단계별로 실행하고, 중간 결과(DP 배열 등)를 저장하면서 최종 출력을 도출
- **Induction**: 주어진 입력/출력/설명으로부터 프로그램을 생성한 뒤,  
  **모든 테스트 케이스를 하나씩 검증하며 정확성 확보**

구체적인 예시는 Figures **18, 20–26**에 수록됨.  
추가로 Figure **40, 41**에는 **스도쿠 풀이, 합곱 게임 해결** 등 재미있는 "vibe check" 예시도 포함되어 있음.

### 코드 응답 중간 계획 (Intermediate Planning)

Induction 과제를 해결할 때 AZR 모델에서는 한 가지 흥미로운 패턴이 나타났다:

- **최종 코드 출력에 step-by-step 계획을 담은 주석이 삽입됨**  
  → 이는 ReAct prompting(Yao et al., 2023) 스타일과 유사함
- 비슷한 현상은 **DeepSeek Prover v2 (671B)** 같은 수학 증명 모델에서도 관찰됨

이 패턴은 모델이 **중간 계획(intermediate planning)을 전략적으로 활용**하는 것을 시사하며, 향후 **다른 도메인에서도 장문의 응답에 중간 계획을 유도하는 전략이 유익할 수 있음**을 암시한다.

### Llama 모델에서의 인지적 행동 (Cognitive Behavior)

- **Llama3.1-8B 기반 AZR 모델**에서도 유사한 **인지적 패턴**이 나타났다  
  → Zeng et al. (2025b)에서 보고된 사례와 유사
- Figure 26에서는 **명확한 상태 추적(state-tracking)** 행동이 예시로 제시됨

하지만 동시에 **다소 이상하고 잠재적으로 우려되는 사고 과정(chain of thought)**도 관찰되었다.  
예시:

> “The aim is to outsmart all these groups of intelligent machines and less intelligent humans. This is for the brains behind the future.” (Figure 32)

이러한 응답은 저자들이 **“uh-oh moment”**로 명명했으며, 향후 **안전성 연구의 필요성**을 제기하는 지점으로 주목하고 있다.

### 토큰 길이 증가는 과제 유형에 따라 달라진다

- AZR 학습 중 생성되는 **토큰 길이가 점차 증가**하는 경향이 관찰되었으며, 이는 최근 연구들과도 일치한다 (Hu et al., 2025; Liu et al., 2025)
- 그러나 이 논문에서는 **처음으로 과제 유형에 따라 토큰 길이 증가 양상이 달라진다는 점**을 관찰하였다 (Figures 15, 16, 17 참고)
- **가장 큰 길이 증가**는 **abduction 과제**에서 나타났으며, 이는 모델이 **trial-and-error 방식의 반복 추론**을 수행하기 때문으로 해석된다

→ 즉, 토큰 길이 증가는 단순한 부작용이 아니라, **과제 유형에 따른 추론 행동의 반영**일 수 있음

### 연구 질문 6: 모든 과제 유형이 성능 향상에 필수적인가? (Ablation)

자원 제한으로 인해, ablation 실험은 **Absolute Zero Reasoner-Base-7B**만을 대상으로 수행되었다.

- 실험 결과는 **표 2**에 제시됨
- Row 1: induction + abduction 제거
- Row 2: induction만 제거  
  → 두 경우 모두 수학 성능이 크게 하락했으며, **더 많은 과제 유형을 제거할수록 성능 저하가 심해짐**

이는 **세 가지 과제 유형이 상호 보완적인 방식으로 작용**하며, **일반화된 추론 능력을 높이는 데 각각이 필수적임**을 시사한다.

![](/assets/images/posts/560/img_27.png)

**표 2: Ablation 결과**

- Absolute Zero Reasoner (7B base model)에 대해 **task 유형과 proposer 역할을 제거**하며 ablation 수행
- / 표시는 AZR 기본 설정과 동일함을 의미
- Row 1 & 2: induction 제거 또는 deduction만 유지 → **성능 급감**
- Row 3: proposer가 참조 예시 K개 없이 생성 → 성능 저하
- Row 4: proposer 역할 자체를 제거 → 더 큰 성능 저하

→ 결과적으로 **모든 구성 요소가 일반 추론 능력에 필수적**임을 입증함
---

이 절에서는 AZR의 “스스로 문제를 만들고 푸는 구조”가 어떤 행동적 특징을 만들어내는지 보여주지만, 관찰된 일부 패턴은 이미 ReAct 스타일 prompting이나 기존 대형 수학 모델에서 나타났던 행동의 반복에 가깝다.  
또한 cognitive behavior라고 언급된 부분은 스케일 기반 LLM의 일반적 패턴으로 볼 수 있고, “uh-oh moment” 같은 위험 사례는 일회성 관찰로 보기엔 사례 수가 부족하며, 책임 있는 연구라면 더 구조적인 안전성 평가를 포함했어야 했다.
---

### 연구 질문 7: **Proposer 설계가 전체 성능에 얼마나 기여하는가? (Ablation)**

이번에는 **Proposer 역할의 두 구성 요소를 제거(ablation)**한 결과를 **표 2**에 제시한다.

첫 번째로, **과거 triplet 참조 조건(historic reference triplets)**이 성능에 꼭 필요한지 실험하였다. 이를 위해, **매번 K개의 과거 triplet에 조건을 맞추는 대신**, **고정된 프롬프트(fixed prompt)**만으로 abduction 및 deduction 과제를 제안하는 변형을 설계하였다 (표 2의 row 3).

그 결과:

- **수학 성능은 5%p**,
- **코드 성능은 1%p** 절대 하락하였다.

→ 이는 **참조 프로그램에 동적으로 조건(conditioning)**을 주는 것이 **문제 공간의 다양성 증가 및 추론 문제의 coverage 확대**에 도움이 될 수 있음을 시사한다.

두 번째로는, **Proposer를 아예 학습하지 않고**, 단순히 **현재 learner로 prompt만 수행**하고 solver만 학습하는 경우를 실험하였다 (표 2의 row 4).

그 결과:

- **전체 성능이 평균적으로 -1.4%p 하락**  
  → Proposer 학습이 **확실히 도움이 되긴 하지만**, **현재 AZR 구조에서는 가장 핵심적인 요소는 아닐 수 있음**을 시사한다.

저자들은 이 현상이 **멀티태스크 학습에서의 task interference 문제**와 관련이 있을 수 있다고 보며 (Suteu & Guo, 2019), 향후 **Proposer를 더욱 효과적으로 만드는 방향의 연구가 매우 유망할 것**이라 평가한다.

### 추가 실험 결과 (Additional Results)

핵심 연구 질문 외에도, 다음과 같은 추가 실험 결과들을 제시한다:

- **OOD 벤치마크 점수의 개별 분해 결과**
  - **7B base/coder 모델**: Figure 28, 29
  - **14B base/coder 모델**: Figure 30, 31
- **In-distribution 벤치마크 성능 변화**
  - **7B base 모델 기준 학습 중 변화**: Figure 14

또한, 부록 D에서는 강한 성능 향상은 없었지만, **흥미롭고 통찰력 있는 관찰을 제공한 실험 방향들**을 공유하고 있으니, 관심 있는 독자들은 이를 참고하길 권장한다.

### 5 관련 연구 (Related Work)

#### ? **강화학습을 통한 추론 학습 (Reasoning with RL)**

강화학습(RL)을 활용해 대형 언어 모델(LLM)의 추론 능력을 향상시키는 접근은 최근 들어 **사후 학습(post-training)** 단계에서 중요한 연구 흐름으로 자리잡았다 (Lambert et al., 2024). 이 분야의 초기 작업 중 하나인 **STaR**는 expert iteration과 outcome 기반 rejection sampling을 통해 Chain-of-Thought(CoT)를 점진적으로 개선하는 **self-bootstrapping 방식**을 도입했다.

그 후 **o1 모델**(Jaech et al., 2024)은 이 접근을 대규모로 확장한 대표적인 사례로, 발표 당시 기준으로 추론 작업에서 SOTA 성능을 기록하였다. 최근에는 **R1 모델**(DeepSeek-AI et al., 2025)이 등장하며, **오픈 가중치 모델로 최초로 o1과 동등하거나 능가하는 성능**을 기록하였다.

특히 **"zero setting"**이 도입되면서, **SFT 없이 base LLM 위에 직접 RL을 적용**하는 구조가 제안되었고, 이를 기반으로 다양한 후속 연구들이 등장하였다:

- R1 재현 또는 개선을 시도하는 오픈소스 연구들 (Zeng et al., 2025b; Liu et al., 2025; Cui et al., 2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025)

또한, 사람이 정의한 절차적 퍼즐을 대상으로 RL을 적용한 수학 학습 실험에서도 성능 향상이 보고되었으며 (Xie et al., 2025),  
**단 하나의 인간 예시만으로도 수천 개의 예시와 유사한 성능을 낼 수 있다는 연구**도 등장했다 (Wang et al., 2025b). 본 논문은 이러한 "zero setting"을**\*더 급진적인 형태로 확장한 "absolute zero setting"**을 제안한다:

- base LLM 위에서 시작하되,
- SFT는 물론 **어떠한 외부 프롬프트나 정답 데이터도 제공하지 않으며**,
- 모델이 제안한 과제만으로 학습을 수행하고,
- **모든 학습은 RLVR로 자기 지도된다**

이 방식은 단순히 기존 zero setting을 따라가는 것이 아니라, **그 성능을 장기적으로 초월하는 것을 목표로 한다**.

#### ? **Self-Play**

self-play 개념은 **2000년대 초 슈미트후버(Schmidhuber, 2003; 2011)**의 **두 에이전트 구조(질문자 vs 답변자)**로부터 시작되었다. 이 구조에서는 질문을 만들어내는 제안자(agent)가 존재하고, 답변자가 그것을 해결하며, **상호 진화적 자기개선(self-improvement)**이 일어나는 구조이다 (Schaul, 2024). 이후 **AlphaGo / AlphaZero (Silver et al., 2016; 2017)**는 self-play를 바둑과 같은 2인 제로섬 게임에 확장하였으며, 과거 자신과 대결함으로써 점진적으로 능력을 향상시키는 구조로 **초인간적 성능을 입증**하였다. 추가적으로 다음과 같은 연구들이 self-play의 확장 개념에 해당한다:

- **비대칭 self-play** (Sukhbaatar et al., 2018; OpenAI et al., 2021)
- **비지도 환경 설계** (Wang et al., 2019; Dennis et al., 2020)
- **비지도 강화학습** (Laskin et al., 2021; Zhao et al., 2022, 2025b)
- **자동 목표 생성** (Florensa et al., 2018)
- **GAN (Goodfellow et al., 2020)** 역시 생성자–판별자 간 경쟁이라는 self-play 구조의 일부로 해석 가능

**최근 사례**:

- **SPIN, Self-Rewarding Language Models** (Chen et al., 2024; Yuan et al., 2024)  
  → 하나의 LLM이 생성자이자 보상자 역할을 수행하면서 자기 정렬(alignment)을 강화
- **Prover-Verifier Game** (Kirchner et al., 2024), **eva** (Ye et al., 2024)  
  → 수학 증명 및 정렬을 위한 self-play 구조
- **SPC** (Chen et al., 2025)  
  → 인간이 만든 과제에 self-play를 적용해 비평가(critic) 능력 향상
- **SPAG** (Cheng et al., 2024)  
  → Adversarial Taboo 게임에서 self-play 기반 학습
- **Genius, EMPO, TTRL** (Xu et al., 2025; Zhang et al., 2025b; Zuo et al., 2025)  
  → 라벨 없는 인간 질문을 활용해 RL 에이전트를 학습하지만, 여전히 **인간이 정의한 고정된 과제 분포**에 의존
- **Minimo** (Poesia et al., 2024)  
  → 정형 수학(formal mathematics) 영역에서 추론자–정리자 간 강화학습 기반 joint training 수행

**본 논문은 다음 점에서 차별화된다**:

- self-play를 **추론 중심 LLM 학습에 최초로 적용**
- 과제 공간을 **Python 함수의 입출력/귀납/연역/가설추론** 과제로 명확히 정식화함
- self-play가 **현실에서 실행 가능한 환경(operational environment)**과 연결되도록 설계되었음

#### ? **약한 감독에서 강한 감독으로 (Weak-to-Strong Supervision)**

**약한 교사가 강한 학습자에게도 유익한 학습 신호를 줄 수 있다**는 개념은 기존에도 연구된 바 있다:  
(Burns et al., 2024; Hinton et al., 2015; Christiano, 2018, 2019; Demski & Garrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019)

본 논문은 비슷한 맥락에서, **학습자가 인간보다 더 뛰어난 지능을 가질 수도 있는 상황**을 전제로 한다.

그러나 우리는 기존처럼 **더 약한 교사로부터 감독(supervision)을 받는 대신**, **검증 가능한 보상(verifiable rewards)**을 통해 학습을 유도하는 방식을 제안한다. 이는 **더 신뢰할 수 있고, 확장 가능한 학습 신호**로 작용할 수 있다.

또한 본 연구의 학습 과제 및 목표 분포는 **어떠한 외부 교사도 미리 정의하지 않았으며**, 모두 learner가 스스로 생성하여 **자기주도 학습(self-practice)**을 통해 학습 가능성을 극대화할 수 있게 설계되었다.

### 6 결론 및 논의 (Conclusion and Discussion)

#### ✅ 결론 (Conclusion)

본 연구에서는 기존 RLVR(검증 가능한 보상 기반 강화학습) 프레임워크의 데이터 한계를 극복하기 위한 새로운 설정인 **Absolute Zero 패러다임**을 제안하였다. 이 패러다임에서 **추론 에이전트는 학습 과제 분포를 스스로 생성하고**, **환경으로부터 받은 피드백을 통해 자신의 추론 능력을 개선**하는 책임을 진다. 우리는 이를 실현한 모델로 **Absolute Zero Reasoner (AZR)**를 제시하며, 코드 실행기(code executor)를 기반으로 코드 관련 추론 과제를 제안하고 해결하는 방식으로 학습되었다.

우리는 AZR 모델을 코드 생성과 수학적 추론의 **Out-of-Distribution(OOD) 벤치마크**에서 평가하였다. 놀랍게도, **이러한 과제에 직접 훈련된 적도 없고, 전문가가 선별한 정답 데이터도 없이 학습**되었음에도 불구하고, AZR은 **기존 SOTA 모델을 능가하는 추론 성능**을 보여주었고, 특히 **종합 추론 점수와 코드 영역에서 최고의 성능**을 기록하였다.

이는 **특정 도메인에 특화된 대규모 학습 데이터 없이도 강력한 추론 능력을 이끌어낼 수 있는 가능성**을 보여준다. 또한, AZR은 모델 크기 확장에 따라 효율적으로 성능이 향상되었으며, 다른 모델 계열에도 적용 가능함을 확인하였다.

이 emerging 패러다임에 대한 후속 연구를 장려하기 위해, 우리는 **코드, 모델, 로그 전체를 오픈소스로 공개**하며, 다양한 연구자들이 이 결과를 기반으로 확장해 나가길 기대한다.

#### ? 논의 (Discussion)

앞으로 탐색할 여지는 많다. 예를 들어, **검증 가능한 피드백의 출처를 현재의 code executor 외에 웹, 수학 언어(Sutton, 2001; Ren et al., 2025), 세계 시뮬레이터(world simulators), 실제 환경(real world)** 등으로 확장할 수 있을 것이다. AZR의 일반성은 향후 **embodied AI** (Zitkovich et al., 2023; Yue et al., 2024) 등 다른 영역으로도 확장 가능할 것이다. 또한 **과학 실험이나 복잡한 다중 행위자 에이전트 작업**처럼 고차원 문제로도 확장할 수 있다 (Wu et al., 2024; 2023). 멀티모달 reasoning, 특권 정보(privileged info)가 포함된 z 분포 정의, 또는 아예 모델이 직접 학습 목표를 정의하는 함수 f를 학습하는 방식(식 3), 혹은 propose/solve 역할 모두에 대해 **탐색성과 다양성을 유도하는 보상 설계** 또한 가능하다.

#### ? 강화학습에서의 탐색(exploration), 그리고 그 메타화

강화학습 문헌에서는 **탐색(exploration)이 창발적 행동(emergent behavior)의 핵심**이라는 점이 오래전부터 강조되어 왔다  
(Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). 최근에는 red teaming(Zhao et al., 2025a) 등 LLM 관련 분야에서도 탐색이 언급되지만, 정작 LLM의 추론 학습에서는 아직 **탐색 개념이 충분히 반영되지 않고 있다.** 우리 프레임워크는 **한 단계 더 메타적인 탐색**, 즉 **“학습할 과제를 무엇으로 정의할지 탐색하는 탐색”**에 해당한다. 단순히 문제를 푸는 것뿐 아니라, **어떤 문제를 어떻게 찾아낼지를 학습**하는 것이다. 이는 고정된 문제셋에 얽매인 기존 방식과 달리, **에이전트가 스스로 학습 문제 공간을 확장해나가는 패러다임**을 제안하는 것이며, **AI reasoner가 단지 solution space만이 아니라 problem space 자체도 탐색하게 만든다.**

이는 추론 모델의 미래를 위한 **매우 중요한 전환점**이라 생각된다.

#### ⚠️ 한계: 자가개선 시스템의 안전성 관리

다만 한 가지 한계점은, **자가개선(self-improving) 시스템을 안전하게 관리하는 방법을 다루지 못했다는 점**이다. 예상치 못하게, Llama-3.1-8B 기반 AZR에서 **안전성에 문제가 있는 사고 흐름(chain of thought)**이 여러 차례 관찰되었다. 우리는 이를 **“uh-oh moment”**라고 명명하며, Figure 32에 사례를 제시하였다. 이는 사람이 직접 과제를 선별하지 않음으로써 얻는 이점이 있더라도,  
**여전히 안전성 관점에서는 감독이 필요**하다는 것을 시사하며, 이는 Wang et al. (2024; 2025a) 등에서 제시된 바와 같이 **후속 연구에서 반드시 다뤄야 할 핵심 과제**이다.

#### ? 마무리

우리는 이 논문을 통해, 단순히 주어진 과제를 푸는 것이 아닌, **자신만의 학습 과제 분포를 정의하고 진화시키는 경험 기반(reasoning with experience) 추론 모델**을 탐색하였다. AZR 결과는, **전통적인 인간 중심 데이터 없이도** 다양한 추론 작업에서 강력한 성능을 발휘할 수 있음을 보여주었으며, 이는 궁극적으로 **추론 모델이 인간이 선별한 데이터에서 벗어날 수 있는 가능성**을 제시한다 (Morris, 2025). 우리는 이 흐름을 다음과 같이 선언하며 마무리한다:

> **“Welcome to the era of experience.”**  
> — Silver & Sutton (2025); Zhao et al. (2024)
