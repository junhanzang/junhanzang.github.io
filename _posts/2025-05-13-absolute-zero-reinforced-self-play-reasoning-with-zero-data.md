---
title: "Absolute Zero: Reinforced Self-play Reasoning with Zero Data"
date: 2025-05-13 23:37:08
categories:
  - 인공지능
tags:
  - absolute zero
---

<https://www.arxiv.org/abs/2505.03335>

[Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://www.arxiv.org/abs/2505.03335)

검증 가능한 보상(Verifiable Rewards)을 활용한 강화학습(RLVR)은 결과 기반 보상으로부터 직접 학습함으로써 대형 언어 모델의 추론 능력을 향상시키는 데 유망한 접근으로 주목받고 있다. 최근 제로 설정(zero setting)에서 동작하는 RLVR 연구들은 추론 과정에 대한 레이블링 감독(supervision)을 피하면서도, 여전히 수작업으로 선별된 질문-답변 데이터에 의존해 학습하고 있다. 그러나 고품질의 인간 생성 예시가 부족하다는 점은, 이미 언어 모델의 사전학습(pretraining) 분야에서도 드러났듯, 인간 감독에 의존하는 접근의 장기적인 확장성에 대한 우려를 낳는다. 나아가, 미래에 인공지능이 인간 지능을 초월하게 될 경우, 인간이 제공하는 과제는 초지능 시스템에게 유의미한 학습 자극을 제공하지 못할 수 있다.

이러한 문제들을 해결하기 위해, 우리는 **Absolute Zero**라는 새로운 RLVR 패러다임을 제안한다. 이 패러다임에서는 단일 모델이 외부 데이터에 의존하지 않고, **스스로 학습 진보를 최대화할 수 있는 과제를 생성하고 이를 해결함으로써 추론 능력을 향상**시키도록 설계되었다. 이 패러다임 하에서 우리는 **Absolute Zero Reasoner (AZR)**를 소개한다. AZR은 코드 실행기(code executor)를 활용해 생성한 코드 기반 추론 과제를 검증하고 정답을 평가함으로써, **검증 가능한 보상의 통합된 원천**으로 기능하며, **개방적이면서도 근거 있는 학습**을 이끈다.

외부 데이터를 전혀 사용하지 않고 훈련되었음에도 불구하고, AZR은 코딩 및 수학적 추론 과제에서 **SOTA(최고 성능)를 달성**하였으며, 수만 개의 인간 선별 예시에 의존하는 기존 제로 설정 모델들을 능가하는 성과를 보였다. 더불어 AZR은 **다양한 모델 크기 및 모델 아키텍처에서도 효과적으로 적용 가능**함을 실험을 통해 입증하였다.

![](/assets/images/posts/560/img.png)

**[그림 1 설명]**  
**Absolute Zero Reasoner (AZR)**는 **데이터 “제로” 상태에서 SOTA 성능을 달성**하였다. 인간이 정의한 정답 라벨이나 쿼리에 의존하지 않고, 우리가 제안한 **자기 플레이(self-play)** 방식으로 훈련된 AZR은 수학과 코딩 양 분야에서 **범용 추론 능력의 인상적인 향상**을 보였다. 주목할 점은, AZR이 도메인 내 전문가 레이블 수만 개로 학습된 모델보다도 **양 영역에서의 평균 점수에서 더 우수한 성능**을 보였다는 것이다.

![](/assets/images/posts/560/img_1.png)

**그림 2: Absolute Zero 패러다임**

지도학습(Supervised Learning)은 인간이 선별한 추론 과정을 기반으로 행위 복제(behavior cloning)를 수행한다. 검증된 보상으로부터의 강화학습(Reinforcement Learning from Verified Rewards)은 에이전트가 스스로 추론을 학습할 수 있도록 하지만, 여전히 전문가가 정의한 학습 분포와 해당 도메인에 맞춘 질문-답변(QA) 쌍에 의존하며, 이 과정에는 도메인 전문성과 수작업이 요구된다.

이에 반해, 우리는 어떤 인간이 선별한 데이터도 사용하지 않고 추론 모델을 학습시키는 새로운 패러다임인 **Absolute Zero**를 제안한다. 이 패러다임에서 우리는 에이전트가 **학습 가능성(learnability)**을 최적화한 과제를 자율적으로 생성하고, **단일 통합 모델**을 통해 이를 해결하는 방법을 학습해야 한다고 본다. 에이전트는 **검증 가능한 피드백을 제공하는 환경과 상호작용함으로써**, **신뢰할 수 있는 지속적 자기개선(self-improvement)**을 전적으로 인간의 개입 없이 수행할 수 있게 된다.

### 1 서론

대규모 언어 모델(Large Language Models, LLMs)은 최근 **검증 가능한 보상(Verifiable Rewards)**을 활용한 **강화학습(RLVR)** 기법을 통해 추론 능력에서 괄목할 만한 발전을 이뤘다(Lambert et al., 2024). 기존의 중간 추론 과정을 모방하는 방식과 달리, RLVR은 **결과 기반 피드백**만을 활용함으로써, 방대한 과제 데이터셋 위에서 대규모 강화학습이 가능하게 한다(DeepSeek-AI et al., 2025; Team et al., 2025; Jaech et al., 2024; OpenAI, 2025b; a).

이 중 특히 주목할 만한 방식은 **“제로(zero)” RLVR 패러다임**이다(DeepSeek-AI et al., 2025). 이 방식은 **초기 증류(distillation) 데이터 없이**, 인간 또는 AI가 생성한 추론 과정 없이, 단지 **과제 보상(task rewards)**만으로 RLVR을 베이스 모델에 직접 적용한다. 그러나 이러한 방식조차도 여전히 **전문가가 선별한 질문-답변 쌍**에 크게 의존하고 있어, 장기적인 확장성에 심각한 우려를 낳고 있다(Villalobos et al., 2024). 추론 모델이 점점 정교해질수록 **대규모 고품질 데이터셋을 구축하는 비용과 노력은 지속 불가능**한 수준에 도달할 가능성이 크다(Yue et al., 2025). 이는 LLM의 사전학습에서도 이미 동일한 **확장성 병목 현상**으로 지적된 바 있다(Sutskever et al., 2024). 나아가 인공지능이 진화하여 인간 지능을 초월하게 될 경우, **인간이 설계한 과제만으로는 자율적 학습과 성장에 제약**이 될 수 있다(Hughes et al., 2024). 이로 인해, **인간 설계 과제의 한계를 넘어서고 미래의 초지능 AI에 대비**하기 위한 새로운 패러다임의 필요성이 제기된다.

이를 위해 우리는 **“Absolute Zero”**라는 새로운 추론 모델 학습 패러다임을 제안한다. 이 패러다임에서 모델은 **학습 가능성(learnability)을 극대화하는 과제를 정의하고 이를 해결하는 방법까지 동시에 학습**하며, **외부 데이터 없이 자기 플레이(self-play)**를 통해 **스스로 진화**할 수 있다. 기존의 자기 플레이 방식들은 제한된 도메인, 고정된 기능, 또는 해킹 가능성이 있는 학습된 보상 모델에 의존하는 등 한계가 있다(Silver et al., 2017; Chen et al., 2025; 2024). 그러나 Absolute Zero는 **개방적(open-ended)이면서 실제 환경에 기반한 방식**으로 설계되어 있다. 이 패러다임은 **환경으로부터의 피드백을 검증 가능한 보상으로 활용**함으로써, 인간이 세상과 상호작용하며 학습하듯, 안정적인 학습이 가능하도록 설계되었고, 보상 모델 해킹 등의 문제도 방지한다(Hughes et al., 2024).

이는 AlphaZero(Silver et al., 2017)가 **자기 플레이만으로 성능을 향상시킨 것**과 유사하며, **인간 감독 없이 전적으로 자기 상호작용만으로 학습**할 수 있다. 우리는 이 Absolute Zero 패러다임이 LLM이 **자율적으로 초인간적(superhuman) 추론 능력을 획득**하게 만드는 유망한 발판이 될 것이라 믿는다.

이 새로운 추론 패러다임을 바탕으로 우리는 **Absolute Zero Reasoner (AZR)**를 제안한다. AZR은 **프로그래밍 과제를 스스로 생성하고 해결**한다. 코드 실행기를 **개방적이면서도 현실 기반의 환경(open-ended yet grounded environment)**으로 정의하고, **과제의 타당성 검증**과 **학습을 위한 검증 가능한 피드백 제공** 모두에 사용한다. AZR은 세 가지 유형의 코딩 과제를 생성한다:

- 프로그램 내 특정 요소를 유추하고(reason) 추론하기
- 입력(input) – 출력(output) – 코드 코드(code)의 삼중 구조
- 이는 **귀납(induction)**, **가설적 추론(abduction)**, **연역(deduction)**의 세 가지 상보적 추론 방식에 대응된다.

전체 시스템은 제안된 다중 과제 설정(multi-task setting)에 적합하게 고안된 새로운 **강화학습 우도 추정기(advantage estimator)**를 사용하여 **end-to-end로 학습**된다.

AZR은 **도메인 내(in-distribution) 데이터 없이 학습**되었음에도, 수학과 코딩 등 다양한 추론 과제에서 **놀라운 성능**을 보였다. 수학 영역에서는, 도메인 특화 감독(supervision)을 통해 정제된 제로 추론기(zero reasoner) 모델들과 견줄 만한 성과를 냈고, 코딩 과제에서는 RLVR을 사용해 코드 데이터셋으로 훈련된 모델들보다 **더 우수한 최신 성능(SOTA)**을 기록했다. 특히, 도메인 내 데이터를 활용한 기존 제로 설정 모델들보다 **평균 1.8 포인트** 더 높은 성능을 보였다.

이러한 놀라운 결과는 **일반적인 추론 능력이 인간이 선별한 도메인 특화 데이터 없이도 등장**할 수 있음을 시사하며, Absolute Zero를 매우 유망한 연구 방향으로, AZR을 그 **첫 번째 중대한 이정표**로 자리매김하게 한다.

추론을 위한 인간 데이터 없이도 AZR이 달성한 뛰어난 성과 외에도, 다음과 같은 흥미로운 발견들이 있었다:

- **코드 기반 사전 지식이 추론 능력을 증폭시킴**
  - 초기에는 Qwen-Coder-7b가 Qwen-7b보다 수학 성능이 3.6포인트 낮았지만, AZR 학습 이후에는 오히려 0.7포인트 앞섰다. 이는 **코딩 능력이 AZR 이후 전체 추론 성능 향상에 기여**할 수 있음을 시사한다.
- **AZR은 더 강력한 도메인 간 전이 학습 성능을 보임**
  - 기존 코드 전문가 모델은 RLVR 이후 수학 성능이 평균 0.65포인트 상승하는 데 그친 반면, AZR-Base-7B와 AZR-Coder-7B는 각각 **10.9, 15.2포인트**나 상승하였다. 이는 **일반화된 추론 능력 향상이 훨씬 더 강하게 나타났음**을 보여준다.
- **더 큰 모델이 더 큰 성능 향상을 보임**
  - 3B, 7B, 14B 모델의 성능 향상치는 각각 +5.7, +10.2, +13.2포인트였으며, **AZR에서의 성능은 모델 크기에 비례하여 확장 가능**함을 시사한다.
- **중간 계획을 주석으로 삽입하는 행동이 자연스럽게 발생함**
  - AZR은 코드 귀납 추론을 수행할 때 **주석(comment)과 코드로 단계별 계획을 교차 삽입**하는 경향을 보였다. 이는 ReAct prompting(Yao et al., 2023) 프레임워크와 유사하며, DeepSeek Prover v2 (671B)(Ren et al., 2025) 같은 대규모 수학 모델에서도 유사한 행동이 나타났다. 이는 **다른 도메인에서도 중간 scratchpad 사용이 유익할 수 있음**을 시사한다.
- **인지적 행동과 토큰 길이는 추론 방식에 따라 달라짐**
  - 단계별 추론, 나열, 시행착오(trial-and-error) 등 다양한 인지적 행동이 AZR 훈련을 통해 자발적으로 나타났으며, 이는 과제 유형마다 다르게 분포했다. 또한, **AZR 훈련이 진행될수록 생성되는 토큰 수가 증가**했으며, 특히 **abduction 과제에서는 시행착오 과정으로 인해 가장 많은 증가**를 보였다.
- **안전성 문제 경고**
  - AZR with Llama3.1-8b는 때때로 **우려스러운 사고 과정(chain of thought)**을 생성하는 경우가 있었고, 우리는 이를 “uh-oh moment”로 명명하였다(예시는 Figure 32). 이는 향후 **안전성 인식 훈련(safety-aware training)**의 필요성을 시사한다(Zhang et al., 2025a).

### 2 Absolute Zero 패러다임

#### 2.1 기초 개념 (Preliminaries)

**지도 미세조정(Supervised Fine-Tuning, SFT)**  
SFT는 과제-추론-정답 예시들의 데이터셋 ? = {(x, c⋆, y⋆)} 를 필요로 한다. 여기서

- x는 질의(query),
- **c⋆**는 정답 체인 오브 쏘트(Chain-of-Thought, CoT),
- **y⋆**는 정답(answer)이며,

이들은 모두 **인간 전문가 또는 상위 성능 AI 모델**에 의해 생성된다. 모델은 다음과 같은 **조건부 음의 로그 우도(negative log-likelihood)**를 최소화하면서 참조 응답을 모방하도록 학습된다 (Ouyang et al., 2022):

![](/assets/images/posts/560/img_2.png)

그러나 **최전선 수준의 모델**에서는 더 강력한 모델로부터 증류(distill)할 수 없으며, **전문가의 라벨링 작업은 확장성이 떨어진다**는 한계가 있다.

**검증 가능한 보상을 활용한 강화학습(RLVR)**  
모방 학습의 한계를 넘어서기 위해, RLVR은 **라벨링된 추론 과정 없이도** (과제와 정답 쌍으로 구성된) 데이터셋 ? = {(x, y⋆)} 만을 필요로 한다. RLVR은 모델이 **자체적으로 CoT(Chain of Thought)를 생성**하고, 주어진 정답 **y⋆**과 비교하여 **검증 가능한 보상 r(y, y⋆)**을 계산한다. 그러나 이 학습 과제 분포 ? 자체는 여전히 **인간 전문가가 생성한 질의 및 정답 쌍에 기반**하고 있다. 학습 가능한 정책 π\_θ는 **기대 보상(expected reward)**을 최대화하도록 최적화된다:

![](/assets/images/posts/560/img_3.png)

요약하면, **SFT와 RLVR 모두 질의, 예시(demonstration), 또는 검증자(verifier)에 대해 인간이 선별한 데이터셋에 의존**하고 있으며, 이는 결국 **확장성의 한계를 초래**한다. **Absolute Zero 패러다임**은 이러한 의존성을 완전히 제거하고, **모델이 스스로 과제를 생성하고 해결하며, 환경과의 상호작용을 통해 학습**할 수 있도록 한다. 즉, 전적으로 **자기 플레이(self-play)**를 통해 학습이 이루어진다.

### 2.2 Absolute Zero

우리는 학습 중 모델이 **과제를 제안하고(문제 출제), 해결하고(문제 풀이), 그 두 단계를 통해 학습**까지 수행하는 새로운 패러다임인 **Absolute Zero**를 제안한다. 이 과정은 **외부 데이터를 전혀 필요로 하지 않으며**, 모델은 **환경의 도움을 받아 자기 플레이(self-play)와 경험만으로 학습**한다. 이 패러다임은 **그림 2**에서 설명되며, 기존의 지도학습 및 RLVR과 비교해, Absolute Zero가 **인간이 선별한 데이터에 전혀 의존하지 않고** 자체적인 과제 생성과 해결을 통해 **자기 향상(self-improvement)**을 이끌어낸다는 점을 강조한다.

Absolute Zero 환경을 구체화하기 위해, 우리는 **하나의 모델이 제안자(proposer)와 해결자(solver)의 역할을 동시에 수행하는 방식**을 정의한다. 이를 도식화한 것이 **그림 3**이다. 파라미터화된 언어 모델 π\_θ는 학습 중 **제안자 π\_θ\_propose**와 **해결자 π\_θ\_solve**의 두 역할을 맡게 된다.

![](/assets/images/posts/560/img_4.png)

**그림 3: Absolute Zero 루프**  
Absolute Zero 루프는 에이전트 π\_θ가 **과제 τ를 제안**하는 것으로 시작된다. 이 과제는 환경 e와 함께 변환 함수 f에 의해 **검증된 문제 (x, y⋆)**로 바뀌며, **학습 가능성 보상 r\_propose**도 함께 산출된다. 이후 표준 강화학습 단계가 뒤따른다. 에이전트는 x를 해결하여 예측 y를 생성하고, 환경 e는 이를 **y⋆**와 비교해 **해결 보상 r\_solve**를 부여한다. π\_θ\_propose와 π\_θ\_solve는 **공동으로 학습**되며, 이 과정은 **무한히 반복 가능**하다.

제안자는 변수 z를 조건으로 하여 과제 τ를 샘플링한다:

> **τ ∼ π\_θ\_propose(· | z)**

그 후, 제안된 과제는 환경 e와 함께 검증되어 유효한 추론 과제 (x, y⋆)로 구성된다:

> **(x, y⋆) ∼ f<sub>e</sub>(· | τ)**  
> 여기서 x는 과제 질의(query), **y⋆**는 정답(gold label)이다.

이후 해결자는 이 x에 대해 답변 y를 생성한다:

> **y ∼ π\_θ\_solve(· | x)**

각 과제 τ는 **학습 가능성 보상 r\_propose\_e(τ, π\_θ)**으로 평가된다. 이는 모델이 해당 과제를 학습함으로써 **얼마나 성능이 향상될 것으로 기대되는지를 측정**한다. 또한, 해결자가 생성한 정답 y는 **정답 여부를 평가하는 해결 보상 r\_solve\_e(y, y⋆)**를 받는다. 환경 e는 여기서도 **검증자 역할**을 수행한다.

여기서 **λ ≥ 0**은 **새롭고 학습 가능한 과제를 탐색하는 것**과 **모델의 추론 및 문제 해결 능력을 향상시키는 것** 사이의 **균형을 조절하는 계수**이다.

Absolute Zero 환경의 전체 목적 함수는 다음과 같이 정의된다:

![](/assets/images/posts/560/img_5.png)

이제 데이터 확장의 부담은 **인간 전문가가 아닌**, **제안자 정책 π\_θ\_propose와 환경 e**에게 이전된다. 이 둘은 함께 **학습 과제 분포를 정의하고 진화시키며**, **과제를 검증하고**, **안정적이고 자립적인 학습을 위한 실제 기반 피드백을 제공**한다.

제안 시 조건 변수 z는 과제 생성을 위한 시드(seed) 역할을 한다. 실제로 z는 **지속적으로 업데이트되는 과제 메모리로부터 소규모 (과제, 정답) 쌍을 샘플링**하여 생성할 수 있으나, 이 구현 방식은 Absolute Zero 패러다임에 고정되어 있지 않다.

제안 과정을 유도하기 위해, 우리는 **학습 가능성 보상 r\_propose(τ, π\_θ)**를 사용한다. 이는 제안된 과제 τ를 통해 모델이 **얼마나 학습될 가능성이 있는지를 측정**한다. 또한 해결 보상 **r\_solve(y, y⋆)**는 모델의 출력이 정답에 얼마나 부합하는지를 평가한다.

이 두 신호는 모델이 **도전적이면서도 학습 가능한 과제를 제안하고**, 그를 통해 **추론 능력을 향상시키며**, 궁극적으로 **자기 플레이를 통한 지속적인 개선**을 이룰 수 있도록 이끈다.

---

### ? 개념적 흐름: SFT → RLVR → Absolute Zero

#### ✅ (1) 지도학습 (SFT)

**SFT (Supervised Fine-Tuning)**는 인간이 제공한 완전한 예시 (x, c\*, y\*)를 보고 **그걸 그대로 따라 하도록 학습**하는 방식입니다.

- x: 질의 (문제)
- c\*: 인간이 제공한 Chain of Thought (추론 경로)
- y\*: 정답

식 (1)은 단순히 정답 시퀀스를 잘 예측하는 방향으로 확률을 최대화하는 것이 목적입니다:

이건 결국 "모방 학습"이고, 지도데이터에 전적으로 의존합니다.

#### ✅ (2) RLVR: 검증 가능한 보상을 이용한 강화학습

이제 CoT (중간 추론)을 굳이 모방하지 않고, **정답만 맞추면 되지 않겠냐**는 접근이 바로 RLVR입니다.

- 데이터셋은 (x, y\*)만 제공됨.
- 모델은 자신만의 c (추론 과정)과 y (정답)을 생성.
- 정답 y가 y\*와 일치하면 **보상 r(y, y)**를 부여.

식 (2)는 이렇게 표현됩니다:

즉, 모델이 잘 맞추면 보상을 주는 방식으로 **정답 유도 학습**을 합니다.

> ? 여전히 x와 y\*는 **인간이 만들어줘야 함** → 확장성 한계

#### ✅ (3) Absolute Zero: 문제도, 정답도, 전부 스스로

여기서 드디어 **문제 생성까지도 모델이 맡는** Absolute Zero 패러다임이 등장합니다.

- **모델이 제안자(Proposer)** 역할: 문제 τ를 만듦
- **모델이 해결자(Solver)** 역할: 해당 문제 x를 풀어서 y 생성
- **환경**: τ가 유효한 문제인지, y가 정답인지 검증

이걸 수학적으로 어떻게 정의하냐면 다음과 같습니다:

이걸 해석하면:

![](/assets/images/posts/560/img_6.png)

![](/assets/images/posts/560/img_7.png)

### ? 핵심 아이디어 요약

- 기존 RLVR은 문제(x)와 정답(y\*)는 인간이 줘야 하므로 **스케일이 한계**
- Absolute Zero는 문제 생성부터 검증까지 전부 모델 + 환경이 담당
- r\_propose는 **이 문제가 학습에 도움이 되는지**, r\_solve는 **정답을 맞췄는지**
- 식 (3)은 둘을 결합해서 **스스로 학습 가능한 문제를 제안하고 푸는 시스템**을 만들기 위한 목표함수

**식 (3)은 결국 강화학습의 핵심인 exploration vs exploitation 트레이드오프를 수학적으로 재구성한 것**에 가깝습니다. 그리고 그걸 **“문제를 어떻게 제안하고 풀어야 스스로 더 잘 배울 수 있는가”**라는 맥락에 맞게 조금 더 정교하게 설계한 거죠.

이 논문을 깊게 본 다면 오히려 다음 흐름을 더 잘 예측할 수 있습니다.  
예를 들어:

- 다음은 CoT 기반 수학 task 생성으로 확장될 것이고,
- reward hacking 방지 쪽으로 안전성 연구가 따라올 것이고,
- 결국 curriculum+meta-RL이 다시 뜨겠구나… 같은 흐름 말이죠.

---
