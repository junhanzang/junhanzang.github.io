---
title: "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training"
date: 2025-04-05 01:45:51
categories:
  - 인공지능
tags:
  - when precision meets position
  - bfloat16 breaks down rope
---

<https://arxiv.org/abs/2411.13476>

[When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476)

초록

확장된 컨텍스트 윈도우(context window)를 이용하면 대규모 언어 모델(LLMs)이 더 긴 시퀀스를 처리하고 복잡한 작업을 수행할 수 있다. 이와 관련해 Rotary Positional Embedding (RoPE)은 상대적 위치 인코딩(relative positional encoding)의 장점을 활용해 긴 컨텍스트를 효과적으로 처리할 수 있어 사실상 표준 기법으로 자리 잡았다. 그러나 본 연구는 RoPE를 BFloat16 포맷으로 사용할 때 수치적 문제(numerical issues)가 발생하여, 특히 긴 컨텍스트 시나리오에서 의도한 상대적 위치 인코딩에서 벗어난 결과를 초래한다는 점을 발견하였다. 이러한 문제는 BFloat16의 제한된 정밀도로 인해 발생하며, 컨텍스트의 길이가 길어질수록 누적되는데, 특히 첫 번째 토큰이 이 문제에 크게 기여한다.

이를 해결하기 위해 본 연구에서는 **AnchorAttention**을 제안한다. AnchorAttention은 플러그 앤 플레이(plug-and-play) 방식의 어텐션 방법으로, BFloat16으로 인해 발생하는 수치적 문제를 완화하고, 긴 컨텍스트 처리 성능을 향상시키며, 훈련 속도를 가속화한다. AnchorAttention은 첫 번째 토큰을 모든 문서에 공통적으로 보이는 '앵커(anchor)'로 취급하고 고정된 위치 ID를 부여함으로써 불필요한 어텐션 계산을 줄이고 의미적 일관성(semantic coherence)을 유지하는 동시에 계산 효율성을 높인다.

세 가지 유형의 대규모 언어 모델(LLM)을 대상으로 진행한 실험 결과, AnchorAttention은 기존의 전체 어텐션(full attention) 방식과 비교했을 때 긴 컨텍스트에서의 성능을 크게 향상시키고, 훈련 소요 시간을 50% 이상 단축하면서도 일반적인 작업에서 원래 모델의 성능을 유지하는 것으로 나타났다.

\* AnchorAttention의 구현체인 AnchorContext는 FlashAttention2와 FlexAttention을 활용하여 여러 인기 있는 모델을 지원하며, 코드는 <https://github.com/haonan3/AnchorContext> 에서 확인할 수 있다.

# 1 서론

최근 자연어 처리(Natural Language Processing, NLP) 분야에서는 더 긴 시퀀스를 처리할 수 있는 모델의 연구가 활발히 진행되고 있다(Yang et al., 2024; Dubey et al., 2024; Jiang et al., 2023; Team et al., 2024). 128K 토큰(token) 규모의 긴 컨텍스트 윈도우(context window)는 다중 문서 질의응답(multi-document question answering) (Wang et al., 2024a), 저장소 수준의 코드 이해(repository-level code comprehension) (Jimenez et al., 2024), 긴 범위 의존성(long-range dependencies)을 포착하는 다중 사례 학습(many-shot learning) (Agarwal et al., 2024) 등 복잡한 작업들을 처리할 수 있게 하여 더욱 일관되고 상황에 적합한 출력을 가능하게 한다(Mazumder & Liu, 2022).

대규모 언어 모델(Large Language Models, LLMs)이 긴 컨텍스트를 처리할 수 있도록 하기 위해, 위치 인코딩(positional encoding)의 주요한 기법으로 Rotary Positional Embedding (RoPE) (Su et al., 2021)이 널리 사용되고 있다(Liu et al., 2023b). RoPE의 성공은 주로 삼각함수(trigonometric) 기반의 회전(rotational) 특성과 상대적 위치 인코딩(relative positional encoding) 방식 덕분인데, 이를 통해 모델이 분포 밖(out-of-distribution, OOD)의 회전 각도(rotation angles)를 효과적으로 피할 수 있게 한다(Wang et al., 2024b; Chen et al., 2023a; Peng et al., 2023; LocalLLaMA, 2023; Men et al., 2024). 이론적으로는, 회전 주파수(rotary frequencies)를 조정함으로써 확장된 컨텍스트의 위치 ID(position IDs, OOD)를 훈련이 잘 이루어진 범위(in-distribution)로 매핑할 수 있으며, 상대적 위치 특성 덕분에 모델이 확장된 컨텍스트 내 입력 토큰의 상대적 위치를 인식할 수 있도록 원래 익숙한 범위로 보간(interpolation)이 가능하다. 그 결과, 비교적 적은 양의 추가적인 긴 컨텍스트 훈련을 통해 LLM들이 확장된 컨텍스트 길이에 효과적으로 적응할 수 있다(Zhao et al., 2024a; Fu et al., 2024; Zhang, 2023). 그러나 최소한의 긴 컨텍스트 훈련(minimal long-context training)은 컨텍스트 길이가 늘어남에 따라 GPU 메모리 소비가 이차적으로(quadratic) 증가하기 때문에 여전히 도전적이다(Xiong et al., 2023). 이를 해결하기 위해 사전 훈련(pre-training) 단계에서 널리 사용되는 브레인 부동소수점(Brain Floating Point, BFloat16) 형식이 긴 컨텍스트 훈련 단계에서도 채택되고 있다. BFloat16은 모델 정확도에 큰 영향을 미치지 않으면서 메모리 대역폭 요구사항을 감소시키기 때문에(Kalamkar et al., 2019) 긴 컨텍스트 모델의 계산 부담을 관리하는 데 이상적이다.

![](/assets/images/posts/532/img.png)

**그림 1**: 서로 다른 설정에서 위치 이동(positional shift)이 어텐션(attention) 계산에 미치는 영향.  
**왼쪽**: 어텐션 차이(D, Eq.4)는 위치 이동 Δ₁을 변화시키며 측정(Δ₂=16 고정). 사전 훈련된 모델이 BFloat16(파란색)일 때는 Float32(노란색), 무작위 초기화(초록색)와 비교하여 상당한 불일치를 보이며, BFloat16에서는 RoPE의 상대적 위치 인코딩 특성이 깨지고, 사전 훈련으로 인해 이 현상이 더욱 심화됨을 나타낸다.  
**가운데**: 토큰별 어텐션 차이(Δ₁=0 vs Δ₂=16). 첫 번째 토큰이 전체 어텐션 차이 대부분을 차지함을 보여줌.  
**오른쪽**: 시퀀스 길이가 증가함에 따라 첫 번째 토큰의 어텐션 로짓(logit) 차이(Eq.5). 길이가 길어질수록 차이가 증가함.

그러나 BFloat16의 계산적 이점에도 불구하고, 본 연구는 BFloat16과 RoPE를 함께 사용했을 때 RoPE의 상대적 위치 인코딩 특성이 무너지는 심각한 문제를 발견하였다. 그림 1에서 보여지듯, 이 문제는 BFloat16의 제한된 정밀도(limited precision)로 인해 발생한다. 훈련 컨텍스트 길이가 길어질수록 수치적 오류(numerical errors)가 누적되어 이 현상을 악화시키고, 결과적으로 더 큰 불일치를 유발한다. 반면 Float32 형식을 사용하면 이와 같은 열화 현상은 발생하지 않으며, RoPE의 상대적 위치 인코딩 특성이 그대로 유지된다. 본 연구의 실험적 관찰은 이러한 붕괴가 긴 컨텍스트 훈련에서 RoPE가 제공하는 이점을 감소시킨다는 사실을 입증하였다.

긴 컨텍스트 훈련을 개선하기 위해 본 연구에서는 RoPE의 상대적 위치 인코딩 붕괴 원인을 조사했고, 어텐션 윈도우(attention window)의 첫 번째 토큰이 이 현상에 가장 크게 기여함을 관찰했다. 하지만 기존 방식들은 이러한 문제를 특별히 고려하지 않고 있다. 전체 어텐션(full attention) 방식에서는 각 토큰이 이전 모든 토큰을 참조하기 때문에 윈도우 크기가 증가할수록 편차가 누적된다. 표준 문서 내 어텐션(intra-document attention, 그림 2 왼쪽)은 교차 문서 마스킹(cross-document masking)을 통해 큰 윈도우를 여러 작은 윈도우로 분할하는데, 이로 인해 각 작은 윈도우마다 여러 개의 첫 번째 토큰이 등장하며, 이들에게 각기 다른 위치 ID를 할당하면 모델의 위치 이해에 불일치가 발생한다.

![](/assets/images/posts/532/img_1.png)

**그림 2**: 다양한 어텐션 기법의 도식화.  
**왼쪽**: 표준 문서 내 어텐션 방식.  
**가운데**: 본 연구에서 개선한 방식(문서별로 위치 ID를 리셋하는 방식).  
**오른쪽**: 본 연구가 제안한 AnchorAttention 방식(공유 앵커 토큰 ? 사용).

본 연구는 문서별로 위치 ID를 리셋하여 윈도우 간의 일관성을 유지하는 것(본 연구가 개선한 방식, 그림 2 가운데)이 긴 컨텍스트 성능을 향상시킨다는 것을 경험적으로 확인하였다. 이는 위치 ID의 불일치가 핵심 문제임을 입증한다. 그러나 위치 ID를 리셋하면 최대 컨텍스트 길이에 도달하거나 초과하는 데이터 시퀀스를 처리할 때만 전체 회전 각도(rotational angles)를 학습할 수 있다는 새로운 문제가 발생한다. 이러한 관찰을 바탕으로 본 연구는 긴 컨텍스트 처리 능력을 강화하고 훈련 속도를 높이는 유연한 플러그 앤 플레이 방식의 어텐션 기법인 **AnchorAttention**을 제안한다(그림 2 오른쪽).

AnchorAttention의 핵심 아이디어는 첫 번째 토큰을 모든 문서에서 공유하는 '앵커(anchor)'로 취급하여 항상 동일한 위치 ID를 부여하고 문서 간에는 서로 보이지 않게 하는 방식이다. 이렇게 함으로써 AnchorAttention은 모델이 짧은 시퀀스로부터도 모든 회전 각도를 학습할 수 있게 하며, 위치 ID의 불일치를 제거할 뿐 아니라 수치 오류의 누적을 방지하여 긴 컨텍스트 벤치마크(RULER, LongBench)에서 기존 방식 대비 일관된 성능 향상을 달성하였다.

본 연구의 주요 기여점은 다음과 같다:

- BFloat16에서 RoPE의 상대적 위치 인코딩 특성이 깨짐을 발견하였다.
- 시퀀스의 첫 번째 토큰이 이 현상의 주요 원인임을 밝혔으며, 긴 컨텍스트일수록 편차가 더 심해짐을 확인하였다.
- 이를 기반으로 긴 컨텍스트 훈련을 위한 AnchorAttention을 제안했으며, 표준 어텐션 대비 훈련 시간을 절반 이하로 단축하고 기존 훈련 파이프라인에 쉽게 적용 가능함을 입증하였다.

# 2. RoPE의 상대적 위치 인코딩에서 발생하는 불일치

## 2.1 Rotary Position Embedding (RoPE)의 배경

최신 대규모 언어 모델(LLM)의 구조는 주로 트랜스포머(Transformer; Vaswani et al., 2017)에 기반하고 있다. 트랜스포머의 핵심 구성요소 중 하나는 어텐션(attention) 메커니즘으로, 수식은 다음과 같이 표현할 수 있다:

![](/assets/images/posts/532/img_2.png)

![](/assets/images/posts/532/img_3.png)

## 2.2 BFloat16이 RoPE의 상대적 위치 특성을 손상시키는 문제 (특히 긴 컨텍스트에서)

![](/assets/images/posts/532/img_4.png)

![](/assets/images/posts/532/img_5.png)

![](/assets/images/posts/532/img_6.png)

### 요약

BFloat16 정밀도를 사용할 경우, RoPE의 상대적 위치 인코딩 특성이 깨진다. 특히 첫 번째 토큰이 이 불일치의 주요 원인이며, 긴 컨텍스트로 갈수록 이 문제는 더욱 심각해진다.

# 3. AnchorAttention (앵커 어텐션)

앞선 섹션에서, 우리는 BFloat16 정밀도로 인해 RoPE의 상대적 위치 인코딩(relative positional encoding)에 문제가 발생하며, 시퀀스 길이가 증가할수록 이 문제가 더욱 심화된다는 점을 확인하였다. 그럼에도 불구하고, BFloat16은 계산 효율성 때문에 긴 컨텍스트를 처리하는 현실적 상황에서는 여전히 매력적인 선택이다. 긴 컨텍스트 성능 향상을 위한 가장 효과적인 전략 중 하나는 긴 컨텍스트 데이터를 기반으로 추가적인 사전 훈련(pre-training)을 수행하는 것이다(Fu et al., 2024; Gao et al., 2024). 그러나 이 방법은 어텐션 메커니즘의 계산량이 컨텍스트 길이에 따라 이차적(quadratic)으로 증가하여 실질적으로 어려움이 크다. 따라서 우리는 긴 시퀀스로 인해 증가하는 오차를 줄이고 BFloat16의 장점을 유지하기 위해 **AnchorAttention**을 제안한다.

## 3.1 작은 윈도우 크기로 긴 컨텍스트 모델 훈련하기

긴 컨텍스트 모델을 훈련하는 전통적 방식은 긴 시퀀스를 직접 처리하는 것이다. 그러나 이는 BFloat16의 정밀도 문제를 악화시키며, 처리되는 토큰 수가 증가할수록 오차가 커진다는 점이 그림 1(오른쪽)에서도 나타난다. 시퀀스 길이를 줄이면 정밀도 문제를 완화할 수 있지만, 긴 컨텍스트 모델을 짧은 시퀀스로 훈련하는 것은 모순적으로 보일 수 있다.

이 모순을 해결하기 위해 기존 문헌의 방법을 검토한 결과, 문서 간 어텐션(cross-document attention)을 마스킹하여 문서 내 어텐션(intra-document attention)만 사용하는 방법(Zhao et al., 2024b; 그림 2 왼쪽)을 활용하면 전체 어텐션보다 훨씬 짧은 길이로 긴 컨텍스트 모델을 효과적으로 훈련할 수 있음을 알게 되었다. 실제로 문서 내 어텐션 기법은 오픈 소스 LLM인 LLaMA-3 시리즈에서 성공적으로 사용되었으며, Gao et al. (2024)는 문서 경계를 넘는 어텐션을 마스킹하면 단기 및 장기 컨텍스트 성능이 모두 개선됨을 검증하였다.

## 3.2 BFloat16에서 발생한 RoPE의 편차가 실제 긴 컨텍스트 성능에 미치는 영향은?

앞서 우리는 BFloat16 사용 시 RoPE의 상대적 위치 인코딩에 편차가 발생함을 어텐션 점수 차이로 확인했다. 이 편차가 실제 긴 컨텍스트 성능에도 유의미한 영향을 미칠까? 특히 문서 내 어텐션이 어텐션 점수의 편차를 완화시키므로, 문서 내 어텐션 방식으로 훈련된 모델에서는 BFloat16의 영향이 미미할 가능성도 있다. 이를 확인하기 위해 두 가지 위치 인덱스 방식을 비교하였다:

- 첫 번째 방식(기존 방식): 시퀀스 처음부터 끝까지 위치 ID를 연속적으로 부여 (그림 2 왼쪽).
- 두 번째 방식(개선된 방식): 각 문서별로 위치 ID를 1로 리셋하여 재부여 (그림 2 가운데).

이론적으로는 두 방식이 같은 성능을 보여야 하므로, 이 실험을 통해 RoPE의 편차가 성능에 미치는 영향을 평가하였다.

### 실험 구성 및 결과

LLaMA-2-7B 아키텍처를 기반으로 Slimpajama 데이터셋에서 문서 내 어텐션으로 128K 모델을 훈련하고, 널리 사용되는 긴 컨텍스트 벤치마크인 RULER에서 평가하였다. 위치 ID를 리셋한 경우와 그렇지 않은 경우의 성능을 비교하였다(구체적인 훈련 및 평가 프로토콜은 섹션 5에서 논의됨).

![](/assets/images/posts/532/img_7.png)

**그림 3**: 위치 ID를 리셋하면 성능이 향상되며, 이는 RoPE의 이론적 예측과 상반된다.

결과는 그림 3에 제시되어 있다. 위치 ID를 문서마다 리셋하면 일관되게 긴 컨텍스트 성능이 향상되었다. 반면, 기존의 방식(리셋하지 않은 방식)은 상대적으로 성능이 떨어졌다. 이러한 성능 차이는 각 문서의 첫 번째 토큰에 서로 다른 위치 ID를 부여할 때 모델이 혼란을 느껴 일관성이 깨지기 때문일 수 있다. 위치 ID를 리셋하면 모든 문서가 일관된 위치 관계(첫 번째 토큰은 항상 위치 ID 1)를 유지할 수 있다. 이 분석은 BFloat16 환경에서 RoPE가 절대 위치 인코딩의 성격을 가지게 된다는 가정 하에 이루어진 것이므로, 추가적인 연구를 통해 이 가정에 대한 엄밀한 검증이 필요하다.

---

이 가정이 중요한 이유는 다음과 같습니다:

### 1. **RoPE의 이론적 전제와의 충돌**

RoPE(Rotary Positional Embedding)는 원래 \*\*상대적 위치 인코딩(relative positional encoding)\*\*으로 설계되었습니다. 즉, 두 토큰 사이의 상대적 거리만 중요하고, 각 토큰의 절대적 위치는 의미가 없어야 합니다. 하지만 BFloat16 환경에서는 이 특성이 깨지고, RoPE가 마치 \*\*절대 위치 인코딩(absolute positional encoding)\*\*처럼 동작하게 된다는 것이 본 논문의 관찰입니다.  
이러한 상황은 RoPE의 이론적 전제와 정면으로 충돌합니다. 따라서 이 가정이 사실이라면, RoPE의 설계 목적과 실제 동작 사이의 괴리를 명확히 밝혀야 합니다.

### 2. **모델 성능 및 일반화 능력에 미치는 영향**

만약 RoPE가 BFloat16 환경에서 절대 위치 인코딩의 성격을 갖는다면, 모델이 특정 위치에만 고정된 패턴을 학습하게 될 위험이 큽니다. 이는 긴 컨텍스트 환경에서 모델의 일반화 능력(generalization)을 제한할 가능성이 큽니다. 위치에 대한 절대적 의존성이 모델의 예측력을 떨어뜨리고, 긴 컨텍스트 환경에서의 성능을 오히려 악화시킬 수 있기 때문입니다.

### 3. **후속 연구 방향에 대한 명확성 제공**

이 가정이 맞다면 후속 연구자들이 RoPE와 BFloat16의 조합에서 발생하는 문제를 해결하기 위해 **절대 위치 특성을 고려한 새로운 포지션 인코딩 방법**을 개발하거나, 혹은 RoPE의 연산을 BFloat16 환경에서 보다 안정적으로 유지하는 개선 방식을 연구할 필요가 있습니다. 이 가정이 사실인지 아닌지에 따라 후속 연구 방향이 완전히 달라질 수 있습니다.

### 결론적으로,

이 가정의 검증은 RoPE의 이론적 정합성, 모델의 실제 성능 및 효율성, 나아가 후속 연구 및 적용 방향을 결정하는 매우 중요한 요소이므로 추가적인 엄밀한 연구가 반드시 필요합니다.

---
