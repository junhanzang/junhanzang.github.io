---
title: "Extending video masked autoencoders to 128 frames"
date: 2024-12-19 21:24:35
categories:
  - Article
---

<https://research.google/blog/extending-video-masked-autoencoders-to-128-frames/>

[Extending video masked autoencoders to 128 frames](https://research.google/blog/extending-video-masked-autoencoders-to-128-frames/)

### 비디오 마스킹 오토인코더를 128 프레임으로 확장

2024년 12월 4일

니테시 바라드와즈 군다바라푸, 시니어 소프트웨어 엔지니어  
루크 프리드먼, 스태프 소프트웨어 엔지니어, Google Research

이 연구에서는 강력한 토크나이저를 사용해 가장 중요한 토큰만을 적응적으로 재구성함으로써, 마스킹 비디오 모델링을 긴 비디오로 확장하는 방안을 제안합니다.

---

### 연구 배경

비디오로부터 고수준의 의미와 세밀한 동작을 동시에 포착할 수 있는 견고한 표현을 학습하는 것은 비디오 검색부터 로봇공학 및 가상 에이전트에 이르기까지 다양한 도메인에 적용될 수 있는 중요한 연구 과제입니다. 이 분야의 최근 발전은 특히 **마스킹 오토인코더(MAEs)**와 같은 강력한 자가 지도 학습 기법의 개발에 의해 이루어졌습니다. 예를 들어, **VideoMAE**는 마스킹된 비디오 프레임을 복원함으로써 액션 인식 벤치마크에서 인상적인 결과를 달성했으며, **VideoPrism** 및 **InternVideo**와 같은 비디오 기반 모델들은 MAE를 핵심 구성 요소로 활용합니다.

그러나 기존 MAE는 긴 비디오를 처리하는 데 있어 계산 병목 현상으로 인해 한 번에 짧은 클립(스니펫)만을 학습하며, 이는 긴 비디오를 완전히 이해하는 데 한계를 초래합니다.

### 연구 내용

새로운 논문인 **"Extending Video Masked Autoencoders to 128 Frames"**에서, 우리는 더 긴 맥락을 가진 비디오 모델을 학습할 때의 계산 부담을 줄이기 위한 새로운 접근 방식을 개발했습니다. 이를 통해 학습된 표현의 품질이 크게 향상됨을 보여줍니다. 우리는 긴 맥락 비디오 MAE 학습의 유용성을 입증하고 이를 실현 가능한 기술적 도구를 제공함으로써, 긴 형식의 비디오 이해에서 중요한 발전의 길을 열고자 합니다.

### 긴 비디오 처리의 도전 과제

전통적인 비디오 이해용 MAE는 일반적으로 16~32 프레임 길이의 짧은 비디오 클립에서 작동합니다. 이러한 제약은 디코더에서 사용되는 자가 주의(self-attention) 메커니즘의 계산 요구가 비디오 길이에 따라 급격히 증가하기 때문입니다. 결과적으로 기존 방법은 긴 비디오에서 복잡한 동작이나 사건을 이해하는 데 필수적인 장기적인 시간적 의존성을 포착하는 데 어려움을 겪습니다. 예를 들어, 체조 루틴이나 복잡한 요리 과정 같은 긴 비디오 작업에서는 이러한 한계가 두드러집니다.

### 우리의 해결책: 적응형 디코더 마스킹

긴 비디오로 인한 문제를 해결하기 위해, 우리는 **"적응형 디코더 마스킹(adaptive decoder masking)"**이라는 새로운 접근 방식을 도입했습니다. 이 기술은 디코딩 과정에서 비디오의 가장 중요한 토큰만을 전략적으로 복원함으로써 계산 비용을 줄이고, 단일 기계에서 최대 128 프레임의 비디오를 처리할 수 있도록 합니다.

![](/assets/images/posts/437/img.png)

**적응형 디코더 마스킹의 사전 학습 중 작동 방식**

우리의 적응형 마스킹 전략은 **MAGVIT** 아키텍처를 기반으로 한 강력한 토크나이저를 활용합니다. 이 토크나이저는 토큰과 그 중요도를 함께 학습하기 위해 **토큰 스코어링 모듈**을 사용하며, 모델이 복원에 있어 가장 정보량이 많은 토큰에 우선순위를 두도록 합니다. 이 토크나이저는 마스킹 모델링 프레임워크와는 독립적으로 학습됩니다.

![](/assets/images/posts/437/img_1.png)

**MAGVIT 인코더를 기반으로 한 적응형 토크나이저의 작동 원리**

### 주요 결과

우리는 **EPIC-Kitchens-100** 및 **Diving-48**과 같은 벤치마크 데이터셋에서 우리의 접근법의 효과를 평가하기 위해 광범위한 실험을 진행했습니다. 주요 결과는 다음과 같습니다:

- **성능 향상**: 적응형 디코더 마스킹 전략은 기존의 균일 마스킹(uniform masking) 및 동작 기반 마스킹(motion-based masking) 전략에 비해 정확도 측면에서 더 나은 성능을 보여주었습니다.

![](/assets/images/posts/437/img_2.png)

**긴 비디오 사전 학습**: 적응형 마스킹을 통해 얻은 메모리 효율성 덕분에 128 프레임의 긴 비디오에 대한 사전 학습이 가능해졌으며, 이는 짧은 비디오를 사용한 사전 학습에 비해 상당한 성능 향상을 가져왔습니다.

![](/assets/images/posts/437/img_3.png)

**최신 기술 수준 결과**: 우리의 접근법은 **Diving-48** 데이터셋에서 이전 방법을 2.7 포인트 능가하며 최신 성능(state-of-the-art)을 달성했습니다. 또한, **EPIC-Kitchens-100**에서도 경쟁력 있는 결과를 보여주었으며, 특히 행동 이해에 중요한 **동사 분류(verb classification)**에서 뛰어난 성능을 보였습니다.

### 향후 방향

128 프레임에서 MAE가 학습할 수 있게 된 것은 기존 최신 기술 대비 큰 도약이지만, 더 높은 프레임 속도에서는 여전히 몇 초의 비디오만을 처리할 수 있습니다. 궁극적으로, 우리는 수 분 또는 수 시간 길이의 비디오에서 학습하면서도 액션 인식, 비디오 요약 및 세계 모델링과 같은 응용 분야에 필요한 세밀한 이해를 유지할 수 있기를 바랍니다.

앞으로 우리는 긴 비디오 인코더를 더 큰 시스템에 통합하여 비디오를 청크 단위로 스트리밍하며 평가하거나, 외부 메모리를 활용하여 이를 확장하는 연구를 계획하고 있습니다. 이 연구가 진정으로 긴 비디오에서 학습하고, AI 시스템이 비디오를 통해 세상을 더 잘 이해하고 상호작용할 수 있도록 하는 비전을 실현하는 중요한 초기 단계라고 믿습니다.

### 감사의 말

이 연구는 Google Research 팀의 연구원들과 브리티시 컬럼비아 대학의 Raghav Goyal 및 Leonid Sigal(각각 학생 연구자 및 방문 교수로 참여) 간의 협업으로 진행되었습니다. 연구에 참여한 주요 연구원은 Nitesh B. Gundavarapu, Luke Friedman, Chaitra Hegde, Eirikur Agustsson, Sagar Waghmare, Mikhail Sirotenko, Ming-Hsuan Yang, Tobias Weyand, Boqing Gong입니다. 이외에도 Chris Duvarney, Huisheng Wang, Nisarg Kothari, Philip Mansfield 및 Hartwig Adam의 지속적인 지원에 감사드립니다.

특히, Nitesh는 프로젝트 기간 동안 아기를 돌보며 큰 힘이 되어준 아내 Sowmya Bhuvanapalli에게 깊은 감사를 표합니다.
