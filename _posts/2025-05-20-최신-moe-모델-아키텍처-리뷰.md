---
title: "최신 MoE 모델 아키텍처 리뷰"
date: 2025-05-20 17:12:20
categories:
  - 개인용
---

<https://www.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/>

[From the LocalLLaMA community on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/)

### 논의

DeepSeek V3가 공개된 이후, 새로운 MoE(Mixture of Experts) 모델들이 급격히 등장하고 있습니다. 본 리뷰에서는 각 모델의 논문과 config.json, modeling\_\*.py 파일들을 검토한 후, 주요 정보를 아래 표에 정리하였습니다. 이에 기반한 주요 관찰 사항은 다음과 같습니다:

- **DeepSeek은 V2에서 MLA(Memory Layout-Aware Attention)를 도입한 이후 KV 캐시 효율성이 매우 높아졌습니다.**
- **Qwen의 MoE 아키텍처는 Mixtral과 거의 동일하지만, 더 많은 expert 수와 더 깊은 레이어 구조를 가지고 있습니다.**
- **Llama-4와 DeepSeek은 모두 expert를 공유하는 구조의 MoE입니다. 반면 Scout는 모든 레이어가 MoE로 구성되어 있지만, 다른 모델들은 일부 dense 레이어를 포함하고 있습니다. 특히 Maverick은 dense와 MoE 레이어가 교차(interleaved)되어 있는 구조입니다.**
- **성능 측면에서 보면, lmarena와 livebench 기준으로는 Qwen3-235B-A22B > DeepSeek-V3 >> Llama-4-Maverick 순으로 평가됩니다. Qwen3는 코드 생성(Coding)을 제외한 거의 모든 영역에서 DSV3보다 뛰어난 것으로 보입니다.**

### 모델별 구조 비교

![](/assets/images/posts/565/img.png)
