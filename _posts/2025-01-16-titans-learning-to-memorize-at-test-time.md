---
title: "Titans: Learning to Memorize at Test Time"
date: 2025-01-16 18:41:37
categories:
  - 인공지능
tags:
  - Titans
  - earning to memorize at test time
---

<https://arxiv.org/abs/2501.00663>

[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)

댓글에 논문 추천도를 확인하는 것을 추천

**개요**  
10년이 넘는 기간 동안 순환 모델(recurrent model)과 어텐션(attention)을 효과적으로 활용하는 방법에 대해 폭넓은 연구가 이루어져 왔다. 순환 모델은 데이터를 고정된 크기의 메모리(은닉 상태)로 압축하는 것을 목표로 하는 반면, 어텐션은 전체 문맥 윈도우에 집중하여 모든 토큰의 직접적인 의존 관계를 포착할 수 있도록 한다. 그러나 이러한 더 정확한 의존 관계 모델링은 이차적(quadratic)인 계산 비용을 발생시켜, 모델이 고정된 길이의 문맥만을 처리하도록 제한한다.

본 연구에서는 과거 문맥을 학습해 기억하면서, 어텐션이 장기 과거 정보를 활용함과 동시에 현재 문맥에 집중하도록 돕는 새로운 신경망 기반 장기 메모리 모듈(neural long-term memory module)을 제안한다. 우리는 이 신경 메모리가 빠른 병렬 학습(parallelizable training)이 가능하고 동시에 빠른 추론 속도를 유지한다는 이점을 지닌다는 점을 보인다. 메모리 관점에서, 문맥 길이가 제한적이지만 정확한 의존 관계를 모델링하는 어텐션은 ‘단기 메모리’ 역할을, 데이터를 기억하는 능력으로 인해 신경 메모리는 ‘장기적이고 지속적인 메모리’ 역할을 한다고 주장한다.

이 두 모듈을 기반으로 우리는 **Titans**라 불리는 새로운 아키텍처 계열을 제안하고, 이 아키텍처에 메모리를 효과적으로 통합하기 위한 세 가지 변형 모델(variant)을 소개한다. 언어 모델링, 상식 추론(common-sense reasoning), 유전체학(genomics), 시계열(time series) 과제에서의 실험 결과를 통해 Titans가 트랜스포머(Transformer) 및 최근의 선형 순환 모델(linear recurrent model)보다 더 높은 성능을 보여 줌을 확인했다. 또한, ‘건초 더미에서 바늘 찾기(needle-in-haystack)’ 같은 극단적인 상황에서도 더 높은 정확도를 유지하면서 200만(2M) 이상의 문맥 윈도우 크기까지 효과적으로 확장 가능함을 보였다.

**1 서론**  
“진정한 기억의 기술은 주의(attention)의 기술이다!”  
— 사무엘 존슨(Samuel Johnson), 1787

트랜스포머(Transformers, Vaswani et al. 2017)는 순수 어텐션(attention) 기반 아키텍처로, 문맥 내 학습(in-context learning) 능력과 대규모 확장 학습(Kaplan et al. 2020) 덕분에 시퀀스 모델링 분야에서 사실상의 최신(state-of-the-art) 모델로 자리매김했다. 트랜스포머의 핵심 구성 요소인 **어텐션 모듈**은 연관 기억(associative memory) 블록처럼 작동하여(Bietti et al. 2024), 쿼리(검색 신호)와 키(문맥)의 쌍wise 유사도를 계산함으로써 키-값(key-value) 연관 관계를 저장하고 검색한다. 이렇게 설계된 트랜스포머의 출력은 현재 문맥 윈도우 내 토큰들의 직접적인 의존 관계에 전적으로 좌우된다. 그러나 이러한 높은 의존 관계 모델링 정확도는 문맥 길이에 대해 시간과 메모리 사용량 모두 이차적(quadratic)으로 증가하는 복잡도를 야기한다. 복잡한 실제 과제—예컨대 언어 모델링(N. F. Liu et al. 2024), 비디오 이해(C.-Y. Wu et al. 2019), 장기 시계열 예측(H. Zhou et al. 2021) 등—에서는 문맥 윈도우가 극도로 커질 수 있으므로, 이러한 다운스트림 과제에서 트랜스포머를 활용하기가 어려워질 수 있다.

트랜스포머의 확장성 문제를 해결하기 위해, 최근 연구들은 다양한 **선형 트랜스포머(linear Transformers)** 변형 모델을 설계하고 있다(Kacham, Mirrokni, and P. Zhong 2024; Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et al. 2024). 이들은 어텐션에서 소프트맥스(softmax)를 커널 함수로 대체(자세한 내용은 §2.1 참고)하여 메모리 사용량을 크게 줄인다. 그러나 긴 문맥을 처리하는 효율성과 확장 가능성을 확보했음에도, 선형 트랜스포머는 여전히 기존 트랜스포머와 비교했을 때 성능이 떨어지는 경우가 많다. 이는 커널 트릭이 선형 순환 신경망(linear recurrent network)을 형성하게 만들어, 데이터 전체를 행렬 형태(matrix-valued states)로 압축하여 처리하기 때문이다(Katharopoulos et al. 2020). 이로 인해 선형 순환(또는 선형 트랜스포머) 모델에는 모순되는 사실이 존재한다. 한편으로는 (선형 vs. 이차적 복잡도) 확장성과 효율성을 높이기 위해 이러한 선형 모델을 적용하지만, 다른 한편으로는 **아주 긴 문맥**을 소수의 벡터나 행렬 상태에 제대로 압축하기 어려운 문제가 발생한다(S. Wang 2024).

더 나아가, 효율성뿐 아니라, 호프필드 네트워크(Hopfield Networks, Hopfield 1982)부터 LSTM(Jürgen Schmidhuber and Hochreiter 1997), 트랜스포머(Vaswani et al. 2017)에 이르기까지 대부분의 기존 아키텍처들은 **일반화(generalization), 길이 외삽(length extrapolation), 추론(reasoning)** 과제에서 어려움을 겪는다(Anil et al. 2022; Qin, Y. Zhong, and Deng 2024). 이는 복잡한 실제 과제에서 불가분한 핵심 요소들이다. 비록 이러한 아키텍처들이 인간 뇌에서 영감을 받았다고는 하지만, 각각 다음과 같은 부분이 결여되어 있다: (1) 단기 메모리, 장기 메모리, 메타 메모리, 현재 문맥 집중(attending to current context) 등 **학습 과정에 필수적인** 구성 요소(Cowan 2008); (2) 이들이 독립적으로 작동하면서도 상호 연결되는 시스템; 그리고/또는 (3) 데이터로부터 능동적으로 학습하고 과거 기록의 추상화를 기억하는 능력. 우리는 **인간 뇌와 유사한** 효과적인 학습 패러다임에서는 학습에 중요한 역할을 담당하는 **서로 다른 모듈**이 존재하되, 이 모듈들이 긴밀하게 상호 연결되어야 한다고 주장한다.

**메모리 관점(Memory Perspective)**  
메모리는 인간 학습에서 매우 근본적인 정신 과정으로, 떼어놓을 수 없는 구성 요소다 (Terry 2017). 메모리 시스템이 제대로 작동하지 않는다면, 인간과 동물은 기본적인 반사 작용이나 정형화된 행동만 할 수 있을 것이다. 이러한 이유로, 메모리는 기계 학습 분야의 많은 기념비적 연구에 영감을 주었다. 예컨대 호프필드 네트워크(Hopfield Networks, Hopfield 1982), LSTM(Jürgen Schmidhuber and Hochreiter 1997), 트랜스포머(Vaswani et al. 2017) 등이 대표적인 사례다.

신경심리학(neuropsychology) 문헌에서 제시하는 일반적인 기억(memory)과 학습(learning)의 정의(Okano, Hirano, and Balaban 2000)에서 영감을 받아, 대부분의 기존 아키텍처들은 메모리를 ‘입력에 의해 야기되는 신경 업데이트(neural update)’로 간주하며, 주어진 목적(objective)에 부합하는 ‘효과적이고 유용한 메모리 획득 과정’을 학습이라고 정의한다. 이런 관점에서, 순환 신경망(Recurrent Neural Networks, RNNs) (Williams and Zipser 1989)은 벡터 형태의 메모리 모듈 ?(은닉 상태라고도 함)을 갖는 모델로 볼 수 있으며, 이 모델은 크게 두 단계를 수행한다. 시간 ?에서 새 입력 ?ₜ가 주어졌을 때, (1) ?(?ₜ₋₁, ?ₜ) 함수를 통해 메모리를 업데이트(압축 포함)하고, (2) ?(?ₜ, ?ₜ) 함수를 통해 해당 입력에 대응하는 메모리를 검색한다(자세한 내용은 §2.1 참조).

유사하게, 트랜스포머(Transformers)도 확장 가능한 메모리를 갖춘 아키텍처이며, 이 역시 두 단계로 나눌 수 있다. 즉, 키(key)와 값(value)로 구성된 행렬 쌍이 모델의 메모리 역할을 하며, 모델은 (1) 키와 값을 메모리에 계속 덧붙여가면서(압축 없이) 메모리를 업데이트하고, (2) 쿼리(query)와 키 사이의 유사도를 구함으로써 쿼리 벡터와 대응되는 메모리를 검색한 뒤, 이 유사도를 이용해 값 벡터를 가중하여 출력을 생성한다.

이 같은 관점은 기존 패러다임의 핵심 차이를 더욱 잘 이해하고, 보다 효율적인 아키텍처를 설계하는 데 도움을 준다. 예컨대, 트랜스포머(Vaswani et al. 2017)와 선형 트랜스포머(linear Transformers, Katharopoulos et al. 2020)의 가장 큰 차이는 바로 메모리 구조와 메모리 업데이트 단계다. 선형 트랜스포머는 과거 데이터를 고정된 크기의 행렬형 메모리(matrix-valued memory)로 압축하는 데 비해, 트랜스포머는 (문맥 길이가 허용하는 한) 과거 데이터를 전혀 압축 없이 모두 보관한다. 선형 트랜스포머와 선형 RNN(상태공간 모델 포함)은 모두 메모리 업데이트 단계에서 정보를 압축하지만, 두 모델 간 결정적 차이는 ‘메모리 구조’에 있다. 선형 RNN은 (선형 트랜스포머와 달리) 벡터 형태의 메모리를 사용한다(선형 트랜스포머는 행렬 형태). 이러한 사실을 통해 다음과 같은 질문을 제기할 수 있다.

- (Q1) “좋은 메모리 구조란 무엇인가?”
- (Q2) “적절한 메모리 업데이트 메커니즘은 무엇인가?”
- (Q3) “효과적인 메모리 검색 과정은 어떠해야 하는가?”

인간의 기억을 다시 살펴보면, 기억은 단일한 과정이 아니며 단일 기능만 수행하는 것도 아니다(Cowan 2008). 실제로 단기 기억, 작업 기억(working memory), 장기 기억 등 다양한 기억 시스템이 존재하며, 각기 다른 신경 구조를 갖고 서로 독립적으로 작동할 수 있다(Willingham 1997). 이는 또 다른 질문을 이끈다.

- (Q4) “서로 긴밀하게 연결된 여러 메모리 모듈을 효율적으로 포함하는 아키텍처는 어떻게 설계할 수 있을까?”

마지막으로, 기억을 저장하는 것은 과거 정보의 추상화를 부호화하고 저장해야 하는 신경 과정이다. 하나의 벡터나 행렬만으로—그리고 이 안의 매개변수가 선형적 방식으로 데이터를 담고 있다고 가정함으로써—장기 이력을 모두 담기에 충분하다고 보는 것은 과도한 단순화일 수 있다.

- (Q5) “오랜 과거를 효과적으로 저장·기억하기 위해서는 ‘깊은(deep) 메모리 모듈’이 필요한가?”

**공헌 및 로드맵**  
본 논문에서는 효율적으로 장기 기억을 학습하고 테스트 시점에 이를 기억할 수 있는 신경망 기반 장기 메모리 모듈을 설계함으로써, 앞서 제기한 다섯 가지 질문에 답해 보고자 한다. 이를 바탕으로, 이 메모리 모듈을 전체 아키텍처에 어떻게 통합할 수 있는지도 논의한다.

1. **신경 메모리(§3)**  
   우리는 (딥(deep)) 신경 기반 장기 메모리(neural long-term memory)를 제안한다. 이 모듈은 ‘메타 인-컨텍스트 모델(meta in-context model)’로서, 테스트 시점에 데이터를 자신의 파라미터에 저장·기억하는 법을 학습한다.  
   인간 장기 기억 시스템(Mandler 2014)에서 영감을 받아, 놀라움(surprise)을 유발하는(즉, 예상을 벗어나는) 이벤트가 더욱 잘 기억되도록 이 메모리 모듈을 설계했다. 이를 위해, 우리는 연관 기억 손실(associative memory loss)에 대해 입력에 대한 신경망의 기울기(gradient)를 측정함으로써 입력의 ‘놀라움’을 계산한다(자세한 내용은 §3.1 참조).  
   또한 제한된 메모리 용량을 더욱 효율적으로 활용하기 위해, 메모리 크기와 데이터의 놀라움 정도를 함께 고려하는 **감쇠(decaying) 메커니즘**을 제안함으로써 더 나은 메모리 관리가 가능하도록 했다. 우리는 이 감쇠 메커니즘이 최신 순환 모델에서 사용되는 망각(forgetting) 메커니즘(예: Dao and Gu 2024; Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024)의 일반화 버전임을 보인다. 더 흥미로운 점은, 이 메커니즘이 곧 메타 신경망(meta neural network)을 미니배치 경사 하강(mini-batch gradient descent), 모멘텀, 가중 감쇠(weight decay)로 최적화하는 것과 동일하다는 사실이다.  
   마지막으로, 미니배치 경사 하강을 텐서 연산으로 확장(tensorizing)하여 보다 많은 행렬 곱(matmul) 연산을 활용하는 기존 기법(Yu Sun et al. 2024)에 기반해, 우리는 병렬 처리가 가능하면서도 빠른 딥 신경 장기 메모리 학습 알고리즘을 제시한다.
2. **Titans 아키텍처(§4)**  
   장기 신경 메모리를 설계한 뒤에는, 이를 어떻게 깊은 학습(deep learning) 아키텍처에 효과적이면서도 효율적으로 통합할지 결정하는 문제가 남는다. 우리는 이를 위해 **Titans**라는 새로운 딥 모델 계열을 제안한다. Titans는 세 가지 하이퍼-헤드(hyper-head)로 구성된다.
   1. **Core**: 이 모듈은 ‘단기 메모리(short-term memory)’를 포함하며, 입력 데이터를 주 흐름(main flow)에서 처리하는 핵심 역할을 담당한다(우리는 제한된 윈도우 크기의 어텐션을 사용한다).
   2. **장기 메모리(Long-term Memory)**: 이 브랜치는 우리가 제안하는 신경 기반 장기 메모리 모듈로서, 오래전 정보를 저장·기억하는 기능을 담당한다.
   3. **영속 메모리(Persistent Memory)**: 이는 태스크와 관련된 사전 학습(또는 지식)을 담고 있는, 데이터와는 독립적인 매개변수들의 집합이다.  
      마지막으로, 개념 증명을 위해 이 메모리 모듈을 활용하는 세 가지 Titans 변형 모델을 제안한다: (i) 메모리를 맥락(context)으로 사용하는 방식, (ii) 메모리를 층(layer)에 통합하는 방식, (iii) 게이트(gated) 브랜치로 사용하는 방식 등이다.
3. **실험 결과(§5)**  
   우리는 언어 모델링, 상식 추론, ‘기억 회상’이 중요한 과제(needle in haystack), 시계열 예측, DNA 모델링 등의 실험을 수행하였다. 그 결과, Titan 아키텍처는 다양한 벤치마크 전반에서 최신 순환 모델들과 이들을 슬라이딩 윈도우 어텐션과 결합한 하이브리드 모델들을 모두 능가하였다. 또한, Titans는 같은 문맥 윈도우를 사용하는 트랜스포머보다 높은 성능을 보였으며, 전체 문맥을 활용하는 트랜스포머와도 대등한 성능을 달성했다. 더 나아가 트랜스포머와 달리, Titans는 **문맥 윈도우 크기가 200만(2M)을 넘어서는 극단적인 상황**에도 확장 가능하다는 점이 특징이다.

**2 사전 지식(Preliminaries)**

![](/assets/images/posts/492/img.png)

### 2.1 배경

#### **어텐션(Attention)**

![](/assets/images/posts/492/img_1.png)

#### **효율적 어텐션(Efficient Attentions)**

긴 시퀀스에서 소프트맥스 어텐션의 메모리 사용량과 처리량 문제를 완화하기 위해, 다양한 연구들이 I/O 처리에 대한 고려(Dao 2024; Dao, D. Fu, et al. 2022), 어텐션 행렬 희소화(sparsifying) (B. Chen et al. 2021; Choromanski et al. 2021; Dai et al. 2019), 소프트맥스 근사(Arora et al. 2024), 커널 기반(선형) 어텐션(Aksenov et al. 2024; Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and Jürgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et al. 2024) 등 다양한 접근을 시도해 왔다.

![](/assets/images/posts/492/img_2.png)

**최신 선형 모델과 그 메모리 관점**  
앞서 살펴본 바와 같이, 학습을 ‘효과적이고 유용한 메모리를 획득하기 위한 과정’으로 정의할 수 있다. 이 관점에서 순환 신경망(RNN)의 은닉 상태(hidden state)는 모델이 정보 전체를 압축해 담으려는 메모리 단위로 볼 수 있다. 이에 따라, 일반적인 순환 신경망을 다음과 같은 두 단계(읽기·쓰기 연산)로 나누어 볼 수 있다.

![](/assets/images/posts/492/img_3.png)

이를 해결하기 위해, 최근 연구들은 다음 두 가지 방향에서 접근하고 있다.

1. **망각(forgetting) 메커니즘 추가**:  
   선형 모델에서 필요할 때 메모리를 지우도록 해주는 적응형(데이터 종속형) 망각 게이트를 설계하는 연구들이 진행되어 왔다. 예컨대 GLA (S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et al. 2023), Griffin (De et al. 2024), xLSTM (Beck et al. 2024), Mamba2 (Dao and Gu 2024) 등이며, 이 중 Mamba2는 기존 상태공간 모델(state space models)의 이산화(discretized) 버전과 연결된다(Gu and Dao 2024).
2. **쓰기 연산 개선**:  
   전통적 순환 모델의 ‘덧셈’ 방식 쓰기 연산의 한계를 극복하기 위해, Widrow and Hoff (1988)는 쓰기 직전에 과거 메모리(즉, 키-값 쌍)를 제거하는 ‘델타 규칙(Delta Rule)’을 제안했다. S. Yang, B. Wang, Yu Zhang, et al. (2024)은 병렬 학습(parallelizable training)과 확장성(scaling)을 높이기 위한 빠른 병렬 알고리즘을 제안했으며, 최근에는 S. Yang, Kautz, and Hatamizadeh (2024)가 델타넷(DeltaNets)에 망각 게이트를 추가해 성능을 개선했다.

### 메모리 모듈(Memory Modules)

메모리는 신경망 설계에서 언제나 핵심 요소였다(Graves, Wayne, and Danihelka 2014; JH Schmidhuber 1992; Jürgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024). 특히 선형 계층(linear layers)을 키-값(연관) 메모리로 바라보는 아이디어는 ‘빠른 가중치 프로그램(fast weight programs)’ 개념까지 거슬러 올라가는데, 이는 순환 신경망 안에 동적으로 기록할 수 있는 빠른 프로그램을 도입해 메모리 역할을 수행하게 하는 방식이다(JH Schmidhuber 1992). 여기에는 Hebbian(Hebb 2005)과 Delta(Prados and Kak 1989)라는 두 가지 주요 학습 규칙이 적용되며, 다양한 연구에서 폭넓게 활용되어 왔다(Irie, Schlag, et al. 2021; Munkhdalai, Sordoni, et al. 2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and Jürgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, and Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. 2024). 그러나 이들 모델은 **순간적인 놀라움(momentary surprise)** 에만 기반하고 있어(3.1절 참조), 시퀀스 내 토큰의 흐름을 반영하지 못한다는 한계가 있다. 또한 망각 게이트가 없어 메모리 관리가 미흡한 경우가 많다.

추가로, 본 논문의 아키텍처와 최신 모델들의 관련성은 부록 C에서 자세히 논의하고, 그 외 관련 연구는 부록 A에서 소개한다.

**3 테스트 시점에서의 메모리 학습(Learning to Memorize at Test Time)**  
장기 기억(long-term memory)의 부재를 해결하고, 모델이 정보를 학습·망각·검색(retrieve)할 수 있도록 하기 위해, 본 섹션에서는 테스트 시점에 메타 학습(meta learning)을 통해 데이터를 기억하는 **신경 기반 장기 메모리 모듈**을 제안한다. 먼저 3.1절에서 신경 메모리 모듈의 설계 동기와 구체적인 메커니즘을 논의하고, 이어서 3.2절에서는 이러한 아키텍처 설계가 빠르고 병렬적인 학습을 통해 어떻게 이점을 얻을 수 있는지 설명한다. 마지막으로 3.3절에서는 태스크에 대한 메타 정보를 학습하기 위한 데이터 비종속적(data-independent) 파라미터로 구성된 **영속 메모리(Persistent Memory) 모듈**을 결합하는 방법을 소개한다.

### 3.1 장기 메모리(Long-term Memory)

장기 메모리를 설계하려면, 과거 이력을 추상화하여 파라미터로 부호화(encode)할 수 있는 모델이 필요하다. 예를 들어, 대규모 언어 모델(LLMs)은 훈련 데이터 자체를 기억하는 양상을 보이기도 한다(Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024). 가장 간단한 생각은 모델을 훈련해 **훈련 데이터**를 그대로 기억하게 하는 것이다. 하지만 이런 식의 ‘암기(memorization)’는 대부분 바람직하지 않은 현상으로 여겨진다. 그 이유는 (1) 모델의 일반화 성능을 저해한다(Bayat et al. 2024), (2) 사생활 정보 보호 문제(privacy concerns)가 발생할 수 있다(Staab et al. 2024), (3) 테스트 시점에서 분포가 다른 데이터가 들어왔을 때 별 도움이 되지 않을 수 있기 때문이다.

따라서 본 연구에서는 **온라인 메타 모델(online meta-model)** 이 테스트 시점에서 데이터를 어떻게 기억/망각해야 할지를 학습하도록 하는 방식을 택했다. 이 경우, 모델은 데이터를 효과적으로 기억할 수 있는 함수를 학습하지만, 훈련 데이터 자체에 오버피팅(overfitting)되지 않으므로 테스트 시점에서의 일반화 성능이 향상된다.

#### 학습 과정과 놀라움 지표(Surprise Metric)

![](/assets/images/posts/492/img_4.png)

단, 위 놀라움 지표만 사용하면, **강한 놀라움**이 발생한 이후에 들어오는 중요한 정보를 놓칠 위험이 있다. 예를 들어, 연속된 놀라운 이벤트 몇 개 이후 기울기가 매우 작아져서(평탄 지점에 빠져서) 시퀀스 일부 정보를 잃어버릴 수 있다. 인간 기억 관점에서도, 처음 놀라웠던 사건이 오랜 기간 꾸준히 놀라움을 유발하지는 않지만, 한 번 충분히 놀라움을 줬다면 그 후 시점까지의 전체 맥락을 계속 기억하는 경우가 많다.

이 문제를 개선하기 위해, 우리는 놀라움을 두 가지로 나누어 측정한다.

1. **과거 놀라움(past surprise)**: 바로 직전까지의 놀라움을 측정한다.
2. **순간 놀라움(momentary surprise)**: 새로 들어오는 데이터의 놀라움을 측정한다.

이를 통해, 메모리 업데이트 공식을 다음과 같이 정의한다.

![](/assets/images/posts/492/img_5.png)

**목표(Objective)**

![](/assets/images/posts/492/img_6.png)

#### 망각(forgetting) 메커니즘

매우 긴 시퀀스(예: 수백만 개의 토큰)를 다룰 때, **어떤 과거 정보는 더 이상 필요치 않을 수 있으므로** 이를 적절히 망각(제거)하는 것이 중요하다. 심지어 딥(deep)하거나 매우 큰 행렬형 메모리를 쓰더라도, **유한**한 메모리 용량은 결국 제한이 있기 때문이다.

이를 위해, 우리는 적응형(adaptive) 망각 기법을 도입한다. 이는 필요한 경우 메모리를 지우고, 그렇지 않다면 유지함으로써 메모리를 더 효과적으로 관리할 수 있도록 해준다. 구체적으로, 다음과 같이 업데이트 규칙을 수정한다.

![](/assets/images/posts/492/img_7.png)

이후 본 절에서 보이겠지만, 이런 **가중 감쇠(weight decay)** 형태의 망각 메커니즘은 최신 순환 신경망(RNN)의 게이트 구조(Dao and Gu 2024; Orvieto et al. 2023)와 밀접한 관련이 있다.

**메모리 아키텍처(Memory Architecture)**

![](/assets/images/posts/492/img_8.png)

이 식은 **온라인 선형 회귀(online linear regression)** 문제에 해당하므로, 결국 과거 데이터의 종속 관계가 선형적이라고 가정하는 해가 최적이라는 뜻이다.

![](/assets/images/posts/492/img_9.png)

---

메모리 아키텍처 비교   선형 메모리 (Matrix)  입력 데이터   W ∈ ℝᵈⁱⁿˣᵈⁱⁿ   출력   딥 메모리 (MLP)  입력 데이터    Hidden Layer 1   Hidden Layer 2   출력

1. **왼쪽: 선형 메모리 (Matrix)**
   - 단순한 행렬(W)을 사용하여 메모리를 구현합니다
   - 입력 데이터가 선형 변환만 거치게 됩니다
   - 데이터의 선형적 관계만 포착할 수 있습니다
   - 표현력이 제한적입니다
2. **오른쪽: 딥 메모리 (MLP)**
   - 2개 이상의 층을 가진 신경망을 사용합니다
   - 비선형 활성화 함수를 통해 복잡한 패턴을 학습할 수 있습니다
   - 더 풍부한 표현력을 가집니다
   - 복잡한 데이터 관계도 포착할 수 있습니다

논문에서 주장하는 핵심은, 단순한 선형 변환(행렬)보다는 여러 층을 가진 신경망(MLP)을 사용하면 더 효과적으로 정보를 기억하고 처리할 수 있다는 것입니다. 실제 실험 결과(5.5절)에서도 이러한 딥 메모리 구조가 더 좋은 성능을 보였다고 합니다.

---
