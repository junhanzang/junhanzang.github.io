---
title: "DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators"
date: 2025-02-06 15:29:44
categories:
  - 인공지능
---

<https://arxiv.org/abs/1910.03193>

[DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of op](https://arxiv.org/abs/1910.03193)

초록  
신경망이 연속 함수의 보편적 근사자임은 널리 알려져 있으나, 덜 알려진 그리고 어쩌면 더욱 강력한 결과는 단일 은닉층을 가진 신경망이 임의의 비선형 연속 연산자[5]를 정확하게 근사할 수 있다는 점이다. 이러한 보편적 근사 정리는 데이터로부터 비선형 연산자를 학습하는 데 신경망을 적용할 수 있는 잠재력을 시사한다. 그러나 이 정리는 충분히 큰 네트워크에 대해 작은 근사 오차만을 보장할 뿐이며, 중요한 최적화 및 일반화 오차는 고려하지 않는다. 이 정리를 실제로 구현하기 위해, 우리는 비교적 작은 데이터셋으로부터 연산자를 정확하고 효율적으로 학습할 수 있는 딥 오퍼레이터 네트워크(DeepONets)를 제안한다. 하나의 DeepONet은 두 개의 서브 네트워크로 구성되는데, 하나는 고정된 수의 센서(x\_i, i = 1, …, m)에서 입력 함수를 인코딩하는 브랜치 네트워크(branch net)이고, 다른 하나는 출력 함수의 위치를 인코딩하는 트렁크 네트워크(trunk net)이다. 우리는 동적 시스템과 편미분 방정식, 즉 두 가지 유형의 연산자를 식별하기 위한 체계적인 시뮬레이션을 수행하고, DeepONet이 완전 연결 네트워크에 비해 일반화 오차를 크게 줄임을 입증한다. 또한, 입력 함수가 정의되는 센서 수와 입력 함수 유형에 따른 근사 오차의 의존성을 이론적으로 도출하고, 이를 계산 결과를 통해 검증한다. 더욱이, 우리는 계산 테스트에서 고차 오차 수렴, 즉 다항식 수렴률(반차에서 4차까지)과 심지어 훈련 데이터셋 크기에 따른 지수 수렴까지 관찰하였다.

1 서론  
보편 근사 정리에 따르면, 은닉층의 너비와 깊이에 어떠한 제약도 두지 않는다면 신경망은 임의의 연속 함수를 원하는 정확도로 근사할 수 있다 [7, 11]. 그러나 지금까지 충분히 인식되지 않은 또 다른 근사 결과는, 단일 은닉층을 가진 신경망이 임의의 비선형 연속 함수(함수 공간에서 실수로의 매핑) [3, 18, 25] 또는 (비선형) 연산자(함수 공간에서 다른 함수 공간으로의 매핑) [5, 4]를 정확하게 근사할 수 있음을 보여준다.

연산자에 대한 근사 정리를 살펴보기 전에, 본 논문 전반에서 사용될 몇 가지 기호를 소개한다. 입력 함수 u를 받는 연산자 G가 있다고 하자. 그러면 G(u)는 그에 대응하는 출력 함수이다. G(u)의 정의역 상의 임의의 점 y에 대해, 출력 G(u)(y)는 실수이다. 따라서 네트워크는 두 부분으로 구성된 입력, 즉 u와 y를 받아 G(u)(y)를 출력한다 (그림 1A). 우리의 목표는 함수를 입력으로 받는 연산자를 학습하는 것이지만, 네트워크 근사를 적용하기 위해 입력 함수를 이산적으로 표현해야 한다. 실제로 충분하지만 유한한 개의 위치 {x\_1, x\_2, … , x\_m\}에서 함수 값을 사용하는 간단한 방법을 채택하며, 이러한 위치를 “센서”라고 부른다 (그림 1A). 다음으로, Chen & Chen [5]에 의해 제시된 정리를 기술한다(자세한 내용은 부록을 참조하라).

![](/assets/images/posts/502/img.png)

![](/assets/images/posts/502/img_1.png)

![](/assets/images/posts/502/img_2.png)

이 근사 정리는 신경망이 데이터를 통해 비선형 연산자를 학습할 수 있는 잠재적 응용 가능성을 보여준다. 즉, 딥러닝 커뮤니티에서 현재 수행 중인, 즉 데이터를 통해 함수를 학습하는 것과 유사한 방식으로 비선형 연산자를 학습할 수 있음을 시사한다. 그러나 이 정리는 연산자를 효과적으로 학습하는 방법에 대해서는 아무런 정보를 제공하지 않는다.

고전적인 이미지 분류 작업을 예로 들면, 함수에 대한 신경망의 보편 근사 정리 [7, 11]는 완전 연결 신경망(FNN)이 실제 분류 함수를 정확하게 근사할 수 있음을 보여주지만, 실제로 FNN의 성능은 널리 사용되는 합성곱 신경망(CNN) [14]이나 최근의 캡슐 신경망(CapsNet) [27]과 같은 특정 아키텍처를 갖는 네트워크에 비해 크게 뒤떨어진다. 이러한 성능 차이는 신경망의 정확도를 전체 오차를 근사 오차, 최적화 오차, 일반화 오차의 세 가지 주요 요소로 분해하여 평가할 수 있다는 사실에 기인한다 [1, 17, 13, 16]. 보편 근사 정리는 충분히 큰 네트워크에 대해 작은 근사 오차만을 보장할 뿐, 최적화 오차와 일반화 오차는 전혀 고려하지 않는다. 실제로 이 두 오차는 전체 오차에 대해 똑같이 중요하며 종종 주된 기여 요인이 된다. 따라서 유용한 네트워크는 학습이 용이하여 최적화 오차가 작고, 미지의 데이터에 대해서도 잘 일반화되어 일반화 오차가 작아야 한다.

![](/assets/images/posts/502/img_3.png)

연산자를 정확하고 효율적으로 학습하기 위해, 우리는 전체 오차를 줄이기 위한 특정 네트워크 아키텍처인 딥 오퍼레이터 네트워크(DeepONet)를 제안한다. 이 네트워크는 입력 함수에 대한 정보를 처리하는 브랜치 네트워크와, 출력 함수의 평가 위치 정보를 처리하는 트렁크 네트워크라는 두 개의 서브 네트워크 설계를 기반으로 하여, DeepONet이 일반화 성능을 크게 향상시킴을 입증할 것이다.

우리는 두 가지 유형의 연산자를 고려한다. 하나는 동적 시스템으로, 예를 들어 상미분방정식(ODE)의 형태를 가지며, 다른 하나는 편미분방정식(PDE)이다. 동적 시스템은 보통 차분방정식이나 미분방정식으로 기술되며, 비선형 동적 플랜트의 식별은 제어 이론에서 중요한 문제이다. 일부 연구 [22, 33]에서는 신경망을 이용해 동적 시스템을 식별하였으나, 이들은 차분방정식으로 기술된 시스템만을 고려하였다. 반면, 다른 연구들 [20, 24, 23, 9]은 새로운 미지 입력 신호에 대해 시스템의 동작을 식별하기보다는 특정 동적 시스템의 진화를 예측하는 데 초점을 맞추었다. 이들이 사용한 네트워크 아키텍처에는 FNN [24], 순환 신경망(RNN) [20], 리저버 컴퓨팅 [20], 잔차 네트워크 [23], 오토인코더 [9], 신경 상미분방정식 [6], 신경 점프 확률 미분방정식 [12] 등이 있다.

편미분방정식을 식별하기 위한 접근법 중 일부는 입력 함수와 출력 함수를 이미지로 취급한 후, CNN을 이용해 이미지-이미지 매핑을 학습하는 방법을 사용한다 [30, 34]. 그러나 이 방법은 입력 함수 u의 센서가 균등 간격 격자에 분포되어 있고, 학습 데이터가 출력 G(u)(y)의 모든 값(역시 균등 간격 격자 상)이 포함되어야 하는 특정 문제에만 적용될 수 있다. 이러한 제한 없이 진행되는 다른 접근법에서는 PDE를 미지의 계수들로 매개화한 후, 계수 값들만 데이터를 통해 식별한다 [2, 26, 31, 21, 15]. 또는 일반화 이동 최소 제곱법에 기반한 일반화된 CNN [28]을 비구조적 데이터에 적용할 수도 있으나, 이는 국소 연산자만을 근사할 수 있어 적분 연산자와 같은 다른 연산자 학습에는 한계가 있다.

논문의 구성은 다음과 같다. 제2장에서는 스택형 DeepONet과 비스택형 DeepONet라는 두 가지 DeepONet 네트워크 아키텍처를 제시하고, 데이터 생성 절차를 소개한다. 제3장에서는 ODE 연산자 근사를 위해 입력 함수를 정확하게 표현하는 데 필요한 센서 수에 대한 이론적 분석을 제시한다. 제4장에서는 다양한 예제를 통해 FNN, 스택형 DeepONet, 비스택형 DeepONet의 성능을 평가하고, 특히 비스택형 DeepONet의 정확도와 수렴 속도를 입증한다. 마지막으로, 제5장에서 논문을 결론짓는다.

2 방법론  
2.1 딥 오퍼레이터 네트워크(DeepONets)

![](/assets/images/posts/502/img_4.png)

![](/assets/images/posts/502/img_5.png)

![](/assets/images/posts/502/img_6.png)

![](/assets/images/posts/502/img_7.png)

2.2 데이터 생성  
프로세스의 입력 신호 u(x)는 시스템 식별에서 중요한 역할을 한다. 분명히, 입력 신호는 응답에 대한 정보를 수집하기 위해 프로세스에 영향을 줄 수 있는 유일한 수단이며, 식별 신호의 품질은 어떤 모델이 최선의 경우 달성할 수 있는 정확도의 상한을 결정한다. 본 연구에서는 주로 두 가지 함수 공간, 즉 가우시안 랜덤 필드(GRF)와 직교(체비셰프) 다항식을 고려한다.

![](/assets/images/posts/502/img_8.png)

3 비선형 동적 시스템 식별을 위한 센서의 수  
본 절에서는 비선형 동적 시스템을 식별하기 위해 정확도 ϵ를 달성하는 데 필요한 센서 점의 개수를 조사한다. 동적 시스템이 다음의 상미분방정식(ODE) 시스템에 의해 지배된다고 가정하자:

![](/assets/images/posts/502/img_9.png)

![](/assets/images/posts/502/img_10.png)

![](/assets/images/posts/502/img_11.png)

4 시뮬레이션 결과  
본 절에서는 가장 간단한 선형 문제에서도 DeepONet이 FNN보다 일반화 오차가 작아 우수한 성능을 보임을 먼저 보여주고, 이어서 세 가지 비선형 ODE 및 PDE 문제에 대해 DeepONet의 능력을 입증한다. 모든 문제에서는 학습률 0.001인 Adam 옵티마이저를 사용하며, 학습이 수렴하도록 반복 횟수를 적절히 선택하였다. 기타 매개변수와 네트워크 크기는 별도로 명시되지 않는 한 표 1과 표 2에 나와 있다. 모든 예제 코드들은 GitHub ([https://github.com/lululxvi/deepxde)에](https://github.com/lululxvi/deepxde)%EC%97%90) 공개되어 있다.

![](/assets/images/posts/502/img_12.png)

![](/assets/images/posts/502/img_13.png)

![](/assets/images/posts/502/img_14.png)

![](/assets/images/posts/502/img_15.png)

![](/assets/images/posts/502/img_16.png)

그림 2는 역미분 연산자(선형 경우)를 학습한 FNN의 오차를 보여준다.

- (A, C, E) 실선과 점선은 각각 학습 과정 중의 학습 오차와 테스트 오차를 나타낸다.
- (A, C, E)에서 파란색, 빨간색, 녹색 선은 각각 (깊이 2, 폭 10), (깊이 3, 폭 160), (깊이 4, 폭 2560)의 FNN을 나타낸다.
- (B, D, F)에서 파란색, 빨간색, 녹색 선은 각각 깊이 2, 3, 4의 FNN을 나타낸다.
- (D)의 음영 영역은 서로 다른 학습/테스트 데이터 및 네트워크 초기화를 사용한 10회 실행에서의 1 표준 편차(SD)를 나타낸다. (명확성을 위해 학습률 0.001의 SD만 표시함.)
- u의 센서 수는 m=100이다.

FNN에 비해, DeepONet은 일반화 오차가 훨씬 작아 테스트 오차도 더 작다. 여기서는 최적의 하이퍼파라미터를 찾는 것이 아니라, 표 2에 나열된 두 가지 DeepONet의 성능만을 평가한다. 바이어스가 포함된 비스택형 DeepONet의 학습 궤적은 그림 3A에 나타나 있으며, 일반화 오차는 무시할 정도이다. 스택형 및 비스택형 DeepONet 모두에서 브랜치 네트워크와 식 (2)에 바이어스를 추가하면 학습 오차와 테스트 오차 모두가 감소함을 관찰하였다 (그림 3B). 바이어스가 포함된 DeepONet은 불확실성이 작아, 무작위 초기화에서 학습할 때도 더 안정적이다. 스택형 DeepONet에 비해, 비스택형 DeepONet은 학습 오차는 다소 크지만 일반화 오차가 작아 테스트 오차가 더 작다. 따라서 바이어스가 포함된 비스택형 DeepONet이 최상의 성능을 달성한다. 추가로, 비스택형 DeepONet은 스택형 DeepONet보다 파라미터 수가 적어 메모리 사용량이 훨씬 적으며 학습 속도도 더 빠르다.

![](/assets/images/posts/502/img_17.png)

그림 3은 역미분 연산자(선형 경우)를 학습한 DeepONet의 오차를 보여준다.

- (A) 바이어스가 포함된 비스택형 DeepONet의 학습 궤적.
- (B) 스택형 및 비스택형 DeepONet의 바이어스 유무에 따른 학습/테스트 오차를 FNN의 최적 오차와 비교한 결과.
- 오차 막대는 서로 다른 학습/테스트 데이터 및 네트워크 초기화를 사용한 10회 실행에서의 1 표준 편차(SD)를 나타낸다.

![](/assets/images/posts/502/img_18.png)

![](/assets/images/posts/502/img_19.png)

그림 4는 비선형 ODE에서 비스택형 DeepONet이 스택형 DeepONet보다 일반화 오차와 테스트 MSE가 작음을 보여준다.

- (A) 학습 과정 중 한 스택형/비스택형 DeepONet의 학습 MSE와 테스트 MSE 간의 상관관계.
- (B) 무작위 학습 데이터셋과 네트워크 초기화를 사용한 10회 실행에서 스택형/비스택형 DeepONet의 최종 학습 MSE와 테스트 MSE 간의 상관관계. 비스택형 DeepONet의 학습 및 테스트 MSE는 선형 상관관계(검은 점선)를 따른다.
- (C) (B)에서의 데이터 포인트의 평균과 1 표준 편차.

![](/assets/images/posts/502/img_20.png)

그림 5는 비선형 ODE 문제에서 학습된 비스택형 DeepONet이 세 가지 분포 외 입력 신호에 대해 예측한 결과를 보여준다. 파란색 선은 기준 해를, 빨간색 선은 DeepONet의 예측을 나타낸다.

![](/assets/images/posts/502/img_21.png)

![](/assets/images/posts/502/img_22.png)

![](/assets/images/posts/502/img_23.png)

![](/assets/images/posts/502/img_24.png)

![](/assets/images/posts/502/img_25.png)

![](/assets/images/posts/502/img_26.png)

![](/assets/images/posts/502/img_27.png)

![](/assets/images/posts/502/img_28.png)

![](/assets/images/posts/502/img_29.png)

![](/assets/images/posts/502/img_30.png)

그림 8: 중력 진자 – 서로 다른 입력 함수 공간에 따른 오차.  
(A) 서로 다른 길이 스케일 lll을 가진 GRF 함수 공간에서의 학습 오차(실선)와 테스트 오차(점선). 색상은 삽입된 그림에서 나타난 센서 수에 대응된다.  
(B) 서로 다른 기저함수 개수를 가진 체비셰프 다항식에 대한 결과.

![](/assets/images/posts/502/img_31.png)

![](/assets/images/posts/502/img_32.png)

![](/assets/images/posts/502/img_33.png)

![](/assets/images/posts/502/img_34.png)

![](/assets/images/posts/502/img_35.png)

![](/assets/images/posts/502/img_36.png)

5 결론  
본 논문에서는 보다 일반적인 환경에서 연산자 학습 문제를 정의하고, 비선형 연산자를 학습하기 위해 DeepONet을 제안하였다. DeepONet에서는 먼저 입력 함수와 위치 변수를 각각 인코딩하기 위해 두 개의 서브 네트워크를 구성한 후, 이를 결합하여 출력을 계산한다. 우리는 DeepONet을 네 가지 상미분/편미분방정식 문제에 대해 테스트하였으며, 이 유도 편향(inductive bias)을 활용함으로써 DeepONet이 일반화 오차를 작게 유지할 수 있음을 보였다. 시뮬레이션을 통해 센서 수, 최대 예측 시간, 입력 함수 공간의 복잡성, 학습 데이터셋 크기 및 네트워크 크기 등 다양한 요인이 테스트 오차에 미치는 영향을 체계적으로 분석하였다. 학습 데이터셋 크기에 대해 서로 다른 차수의 다항식 및 지수 수렴 현상이 관찰되었는데, 이는 우리가 아는 한 딥러닝 분야에서 처음으로 지수 수렴이 관찰된 사례이다. 또한, 다양한 요인에 따른 근사 오차의 의존성을 이론적으로 도출하였으며, 이는 계산 결과와 일치함을 확인하였다.

6 감사의 글  
본 연구에서 정리 2에 대한 도움을 주신 복주대학교의 Yanhui Su님께 감사를 드린다. 부록 C의 증명에 도움을 주신 Worcester Polytechnic Institute의 Zhongqiang Zhang님께도 감사를 드린다. 이 연구는 DOE PhILMs 프로젝트(No. de-sc0019453), AFOSR 지원금 FA9550-17-1-0013, DARPA-AIRA 지원금 HR00111990025의 지원을 받았다. Pengzhan Jin의 연구는 중국 과학기술부의 신세대 인공지능 주요 프로젝트(Grant No. 2018AAA010100)의 일부 지원을 받았다.
