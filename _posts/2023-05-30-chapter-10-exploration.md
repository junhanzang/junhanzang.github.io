---
title: "Chapter 10. Exploration"
date: 2023-05-30 14:10:51
categories:
  - 강화학습
tags:
  - Exploration
---

Exploration vs. Exploitation을 다시 되돌아보자. 아래의 이미지를 통해 항상 가던 길을 가는게 Exploitation, 새로운 곳으로 가는게 Exploration이라는 것을 이전 chapter에서 배웠다.

![](/assets/images/posts/85/img.png)

**Exploration vs. Exploitation Dilemma**

그지만 우리는 새로운 상태나 행동을 탐색하면서 최적의 정책을 찾아야 하지만, 이미 알고 있는 지식을 이용하여 최대한의 보상을 얻어야 합니다. 이런 걸 Exploration vs. Exploitation Dilemma라고 합니다.

![](/assets/images/posts/85/img_1.png)

탐색과 이용의 딜레마(Exploration vs. Exploitation Dilemma)은 온라인 의사 결정에서 기본적인 선택을 의미합니다. 이는 다음과 같은 선택 사항을 포함합니다:

- 이용(Exploitation): 현재 정보를 바탕으로 최선의 결정을 내리는 것입니다. 이미 알고 있는 정보를 최대한 활용하여 최적의 결과를 얻으려고 하는 것입니다.
- 탐색(Exploration): 추가적인 정보를 수집하는 것입니다. 더 많은 정보를 얻기 위해 다양한 선택을 시도하고 새로운 경험을 모으려는 것입니다.
- 최고의 장기적인 전략은 단기적인 희생을 포함할 수 있습니다. 전체적인 결정을 내리기 위해 충분한 정보를 수집하는 것이 중요합니다.

탐색과 이용의 딜레마는 많은 의사 결정 문제에서 발생하는 고려사항입니다. 한쪽을 과도하게 강조하면 최적의 솔루션을 찾는 데 어려움을 겪을 수 있습니다. 따라서 합리적인 균형을 유지하며 정보를 수집하고 최선의 결정을 내리는 것이 중요합니다.

다음은 딜레마의 예시입니다.

레스토랑 선택:

- 이용: 좋아하는 레스토랑으로 가는 것
- 탐색: 새로운 레스토랑을 시도하는 것

온라인 배너 광고:

- 이용: 가장 성공적인 광고를 보여주는 것
- 탐색: 다른 광고를 보여주는 것

석유 탐사:

- 이용: 가장 잘 알려진 장소에서 탐사하는 것
- 탐색: 새로운 장소에서 탐사하는 것

게임 플레이:

- 이용: 최선으로 생각되는 수를 둬서 게임을 진행하는 것
- 탐색: 실험적인 수를 둬서 게임을 진행하는 것

이 예시들은 탐색과 이용의 딜레마를 보여주는 일반적인 상황들입니다. 각 상황에서는 최적의 결정을 내리기 위해 탐색과 이용을 적절히 조합해야 합니다. 탐색을 통해 새로운 정보나 경험을 얻고, 이용을 통해 이미 알고 있는 지식을 활용하여 최적의 선택을 할 수 있습니다.

탐색 방법:

- 단순한 탐색: 탐색적인 정책에 노이즈를 추가함 (예: ?-탐욕 정책)
- 낙관적 초기화: 뒷받침되지 않는 한 가장 좋은 상태로 가정함
- 불확실성에서의 낙관: 불확실한 가치를 가진 행동을 선호함
- 내재적 보상으로의 탐구: 내재적 동기 (예: 호기심)를 활용함
- 포기하지 않는 탐구: 평생의 참신함 및 에피소드별 참신함

이러한 탐색 방법은 다양한 상황에서 사용될 수 있습니다. 단순한 탐색은 탐험적인 정책을 사용하여 노이즈를 추가함으로써 새로운 정보를 얻을 수 있습니다. 낙관적 초기화는 아직 증명되지 않은 상태를 가장 좋은 상태로 가정하여 탐색을 진행합니다. 불확실성에서의 낙관은 불확실한 가치를 가진 행동을 우선적으로 선택하여 더 많은 정보를 얻을 수 있도록 합니다. 내재적 보상으로의 탐구는 호기심이나 다른 내재적 동기를 활용하여 탐색을 유도합니다. 포기하지 않는 탐구는 평생의 참신함과 에피소드별 참신함을 중요하게 여겨 탐색을 지속합니다.

**The Multi Armed Bandit**

다중 암기된 밴딧(Multi-Armed Bandit)은 다음과 같은 튜플로 구성됩니다.

- ?는 알려진 m개의 행동(또는 "팔")의 집합입니다.
- ??r은 알려지지 않은 보상에 대한 확률 분포입니다.
- 각 단계 ?에서 에이전트는 행동 ??를 선택합니다 (?? ∈ ?).
- 환경은 보상 ??~???을 생성합니다.
- 목표는 누적 보상을 최대화하는 것입니다 (σ ?=1 ? ??).

다중 암기된 밴딧 문제에서는 에이전트가 초기에 ??r을 모르기 때문에 최적의 행동을 선택하는 것이 도전입니다. 에이전트는 탐험과 이용의 균형을 고려하여 최대한 많은 보상을 얻을 수 있는 행동을 찾아야 합니다.

![](/assets/images/posts/85/img_2.png)

**Regret**

행동 가치(Action value)는 행동 ?에 대한 평균 보상으로 정의됩니다 (??=??|?). 최적 가치(Optimal value) ?∗는 ?∗=??∗=max?∈??(?)로 표현됩니다. 손실(regret)은 한 단계에 대한 기회 손실로 정의되며 ??=??∗−?(??)로 계산됩니다. 전체 손실(total regret)은 총 기회 손실을 나타내며 ??=?∑???∗−?(??)로 계산됩니다. **누적 보상을 최대화하는 것은 총 손실을 최소화하는 것과 동일합니다.**   
  
이러한 개념에서 "손실"은 최적의 행동과 실제 선택한 행동 사이의 기회 손실을 나타냅니다. 최적 행동을 항상 선택한다면 손실은 0이 될 것이지만, 실제 행동 선택에서는 항상 최적이 아니기 때문에 손실이 발생합니다. 누적 보상을 최대화하는 것은 이러한 손실을 최소화하고 가능한 한 많은 보상을 얻는 것을 의미합니다. 따라서 "손실"을 최소화하는 것과 "누적 보상"을 최대화하는 것은 동일한 목표를 가지고 있습니다.

**Linear or Sublinear Regret**

만약 알고리즘이 계속해서 탐색을 수행한다면, 총 손실은 선형적으로 증가할 것입니다. 또한, 알고리즘이 전혀 탐색을 수행하지 않는다면, 총 손실 또한 선형적으로 증가할 것입니다. 그렇다면 서브리니어(sublinear) 총 손실을 달성하는 것이 가능할까요?   
  
네, 서브리니어 총 손실을 달성하는 것은 가능합니다. 좋은 탐색 전략과 알고리즘 디자인을 통해 서브리니어 총 손실을 달성할 수 있습니다. 서브리니어 총 손실을 달성하려면 최적 행동에 대한 정보를 빠르게 습득하고, 잘못된 행동을 피하기 위해 효율적으로 탐색하는 알고리즘이 필요합니다. 이를 위해 다양한 탐색 전략과 알고리즘을 적용하여 탐색과 활용 사이의 균형을 유지하며, 최적 행동에 대한 추정을 개선하고 보상을 최대화하는 방향으로 진화시킬 수 있습니다.

![](/assets/images/posts/85/img_3.png)

**Optimistic Initialization**

낙관적 초기화(Optimistic Initialization)는 간단하고 실용적인 아이디어입니다. ?(?)를 높은 값으로 초기화하는 것입니다. 이후에는 점진적인 몬테카를로 방법을 사용하여 행동 가치를 업데이트합니다. 이 방법은 초기에 체계적인 탐색을 유도합니다. 그러나 여전히 부적절한 행동에 고정될 수 있습니다.

![](/assets/images/posts/85/img_4.png)

낙관적 초기화는 모든 행동에 대해 높은 가치를 부여함으로써 초기에 모든 행동을 적극적으로 탐색하는 것입니다. 이렇게 하면 초기에는 보다 많은 행동을 시도하고 탐험할 수 있습니다. 그러나 이 방법은 최적 행동에 대한 추정치를 과도하게 높게 설정할 수 있으며, 결과적으로 하나의 행동에만 고착되는 문제가 발생할 수 있습니다. 따라서 다양한 탐색 전략과 조절 가능한 초기화 값을 사용하여 효과적인 탐색과 활용 사이의 균형을 유지하는 것이 중요합니다.

Optimism in the Face of Uncertainty

어떤 행동을 선택해야 할까요?   
우리가 특정 행동 가치에 대해 더 불확실할수록, 그 행동을 탐색하는 것이 더 중요해집니다. 그 행동이 최선의 행동일 수도 있기 때문입니다.

![](/assets/images/posts/85/img_5.png)

파란색 행동을 선택한 후, 우리는 해당 가치에 대해 더 확실해집니다.   
그리고 다른 행동을 선택할 가능성이 높아집니다 (예: 빨간색 행동).   
최선의 행동을 찾을 때까지 이 과정을 반복합니다.   
  
불확실성 속에서의 낙관성은 우리가 더 불확실한 행동에 대해 더 많은 탐색을 할 것을 장려합니다. 이렇게 함으로써 최선의 행동을 찾을 수 있습니다.

**Upper Confidence Bounds**

상한 신뢰 구간은 탐색과 활용을 균형 있게 수행하기 위한 기법 중 하나입니다. 이 방법은 불확실한 행동에 대한 정보를 얻기 위해 상한 신뢰 구간을 사용하여 행동 가치를 추정합니다.   
  
각 행동의 가치에 대해 상한 신뢰 구간을 계산합니다. 이 신뢰 구간은 행동 가치의 추정값에 일정한 간격을 더한 값으로서, 이 값을 통해 해당 행동이 실제 가치보다 얼마나 높은지를 추정할 수 있습니다. 상한 신뢰 구간은 행동을 선택할 때 가장 높은 값으로 설정되는 행동을 선택합니다.   
  
상한 신뢰 구간의 크기는 선택된 횟수에 따라 조절됩니다. 선택된 횟수가 적을수록 큰 상한 신뢰 구간이 설정되며, 이는 더 많은 탐색을 유도합니다. 선택된 횟수가 많을수록 작은 상한 신뢰 구간이 설정되며, 이는 더 많은 활용을 유도합니다.

![](/assets/images/posts/85/img_6.png)

Upper Confidence Bounds

**Exploration in Reinforcement Learning**

탐색(Exploration)은 강화학습에서 매우 중요한 개념입니다. 탐색은 불확실한 환경에서 최적의 정책을 찾기 위해 더 많은 정보를 수집하는 과정을 의미합니다.   
  
강화학습에서 탐색은 정책을 개선하는데 필수적입니다. 초기에는 환경에 대해 알려지지 않은 정보가 많기 때문에 다양한 행동을 시도하고 그 결과를 관찰하여 얻은 지식을 바탕으로 정책을 개선해야 합니다. 탐색을 통해 새로운 행동을 시도하고 불확실성을 해소하기 위해 더 많은 경험을 쌓을 수 있습니다.

![](/assets/images/posts/85/img_7.png)

**Trouble with Counts**

카운트 기반 접근 방법에는 몇 가지 문제가 있을 수 있습니다.   
  
첫째, 상태의 수가 많을 경우 카운트를 관리하는 것이 어려울 수 있습니다. 많은 상태가 있을 때 모든 상태를 독립적으로 카운트하기는 어렵습니다. 이러한 경우에는 일부 상태를 그룹화하거나 근사치를 사용하여 카운트를 관리할 수 있습니다.   
  
둘째, 어떻게 카운트를 수행할지에 대한 문제가 있을 수 있습니다. 상태의 방문을 정확하게 카운트하기 위해서는 모든 상태에 대한 정보를 추적해야 합니다. 이는 메모리 요구량과 계산 비용을 증가시킬 수 있습니다. 따라서 효율적인 카운트 방법을 개발하거나 근사치를 사용하여 카운트를 대체하는 방법을 고려해야 합니다.   
  
셋째, 일부 상태는 매우 유사하지만 동일하지 않을 수 있습니다. 예를 들어, 게임에서 유사한 게임 상태가 여러 번 발생할 수 있습니다. 이러한 경우에는 유사한 상태를 구분하여 별도로 카운트해야 합니다. 이를 위해 "pseudo count"라는 개념을 사용할 수 있습니다. "pseudo count"는 상태의 유사성을 기반으로 카운트를 추정하는 방법으로, 유사한 상태를 하나의 그룹으로 처리하여 카운트를 적용합니다.   
  
카운트 기반 접근 방법을 사용할 때 이러한 문제에 대해 고려해야 합니다. 효과적인 카운트 관리 및 유사한 상태의 처리 방법을 고려하여 탐색 전략을 개발하는 것이 중요합니다. 이를 통해 카운트 기반 접근 방법을 더 효과적으로 활용할 수 있습니다.

**Intrinsic Motivation ( Curiosity )**

내재적 동기 (호기심)   
  
환경으로부터의 외부적 보상 vs. 탐색을 위한 내재적 보상   
  
외부적 보상은 주어진 환경에서 얻는 외부적인 보상이며, 주어진 과제나 목표를 달성했을 때 주어지는 보상입니다. 이러한 외부적 보상은 행동을 유도하고 강화하는 데 중요한 역할을 합니다.   
  
반면에 내재적 보상은 탐색을 촉진하기 위해 동기부여하는 내부적인 보상입니다. 이는 지식의 증가, 새로운 경험의 획득, 예측 오류의 해결 등의 내재적인 동기부여로 이루어집니다. 내재적 보상은 호기심을 자극하고 새로운 지식과 정보를 얻기 위한 탐색적인 행동을 유도합니다.   
  
내재적 동기는 학습 시스템에 추가적인 특성을 부여하여 탐색의 영역을 확장시킵니다. 이는 환경의 불확실성을 줄이고 새로운 정보를 얻기 위해 탐구적인 행동을 촉진합니다. 내재적 동기는 목표 달성에 초점을 맞춘 외부적 보상만으로는 얻을 수 없는 추가적인 동기부여를 제공합니다.

![](/assets/images/posts/85/img_8.png)

![](/assets/images/posts/85/img_9.png)

![](/assets/images/posts/85/img_10.png)

위의 의견을 바탕으로 아래의 논문이 나왔다.

**Curiosity driven Exploration by Self supervised Prediction [2017 ICML]**

자기 지도학습 예측을 통한 호기심 기반 탐색   
  
"Curiosity driven Exploration by Self supervised Prediction"은 2017년 ICML에서 발표된 논문입니다. 이 논문은 강화학습에서의 탐색을 촉진하기 위해 자기 지도학습 예측을 사용하는 방법을 제안합니다.   
  
기존의 강화학습에서는 외부 보상 신호에 의존하여 학습을 진행합니다. 하지만 외부 보상 신호만으로는 학습 과정에서 탐색을 충분히 수행하기 어렵습니다. 이에 "Curiosity driven Exploration by Self supervised Prediction"은 내재적인 호기심을 기반으로 탐색을 촉진하는 방법을 소개합니다.   
  
이 방법은 학습 대상이 되는 환경에서 다양한 상태를 예측하는 자기 지도학습 네트워크를 구축합니다. 이 네트워크는 현재 상태를 입력으로 받아 다음 상태를 예측하는 역할을 수행합니다. 예측 오류의 크기를 측정하여 호기심 보상을 계산하고, 이를 원래의 보상과 조합하여 강화학습 알고리즘을 향상시킵니다.   
  
자기 지도학습 예측을 통해 얻은 호기심 보상은 새로운 경험과 지식의 획득을 장려하며, 더 넓은 탐색 공간을 탐구하도록 유도합니다. 이를 통해 강화학습 에이전트는 보다 효과적이고 지속적인 탐색을 수행하며, 더 많은 경험을 얻어 성능을 향상시킬 수 있습니다.

![](/assets/images/posts/85/img_11.png)

**Curiosity driven Exploration by Self supervised Prediction [2017 CVPRW]**

기존의 강화학습에서는 외부 보상 신호에 의존하여 학습을 진행하는 경우가 많았습니다. 하지만 외부 보상 신호만으로는 탐색을 충분히 수행하기 어려웠습니다. 이에 "Curiosity driven Exploration by Self supervised Prediction"은 내재적 호기심을 활용하여 탐색을 촉진하는 방법을 소개합니다.   
  
이 방법은 주어진 환경에서 예측을 수행하는 자기 지도학습 모델을 사용합니다. 모델은 현재 상태를 입력으로 받아 다음 상태를 예측하는 역할을 수행합니다. 예측된 결과와 실제 다음 상태를 비교하여 예측 오차를 측정하고, 이를 호기심 보상으로 활용합니다.   
  
호기심 보상은 예측 오차의 크기를 기반으로 계산되며, 큰 오차를 가진 상태에 대해 더 큰 보상을 부여합니다. 이를 통해 에이전트는 더 많은 탐색을 수행하고 새로운 경험을 얻을 수 있게 됩니다. 이러한 호기심 기반의 탐색은 강화학습 에이전트의 학습과 성능 향상에 도움을 줄 수 있습니다.

![](/assets/images/posts/85/img_12.png)

**Noisy TV Problem**

Noisy TV Problem는 무작위 잡음(TV)이 강화학습 에이전트의 주의를 계속해서 끌어들이는 문제입니다.   
  
이 문제에서는 강화학습 에이전트가 어떤 환경에서 무작위로 발생하는 잡음(TV)에 끊임없이 주의를 기울이게 됩니다. 이러한 잡음은 예기치 않게 나타나며, 에이전트의 주의를 분산시켜 원래 목표에 집중하는 것을 어렵게 만듭니다.   
  
무작위 잡음(TV)은 강화학습 에이전트의 탐색과 환경 탐구에 큰 영향을 미칩니다. 에이전트는 이 잡음을 계속해서 감지하고 관찰하려고 하며, 이로 인해 원래의 목표나 정책에서 벗어나게 됩니다. 따라서 에이전트는 효율적인 학습과 성능 향상을 위해서는 이러한 잡음에 대한 처리 방법을 고민해야 합니다.   
  
Noisy TV Problem은 강화학습에서 주의의 분산과 탐색의 어려움을 보여주는 예시입니다.

<https://www.youtube.com/shorts/Zw4ZBT0fL4I>

**Random Network Distillation [ICLR 2019]**

Random Network Distillation은 ICLR 2019에 제안된 방법으로, 예측 오류에 기반한 탐색 보너스를 활용하는 탐색 전략입니다.

이 방법에서는 예측 오류의 원인으로 다음과 같은 요소들을 고려합니다:

1. 예측기(predictor)가 적은 샘플을 본 경우
2. 확률적인 전이(stochastic transitions)
3. 필요한 정보가 누락된 경우

Random Network Distillation은 주어진 환경에서 다음 상태를 예측하는 예측기를 학습합니다. 이때, 예측기에는 무작위로 초기화된 신경망(Random Network)을 사용합니다. 예측기는 현재 상태와 선택된 동작을 입력으로 받아서 다음 상태를 예측하고, 이를 통해 예측 오류를 계산합니다.   
  
탐색 보너스는 이 예측 오류를 기반으로 계산됩니다. 예측 오류가 크다는 것은 예측기가 적절한 정보를 얻지 못했거나 예측이 어려운 상태를 나타냄을 의미합니다. 이러한 경우, 탐색 보너스를 적용하여 에이전트가 해당 상태를 더 많이 탐색하도록 유도합니다. 이는 환경을 더 잘 이해하고 더 효과적인 정책을 학습하기 위해 필요한 탐색을 촉진하는 역할을 합니다.

![](/assets/images/posts/85/img_13.png)

![](/assets/images/posts/85/img_14.png)

APEX DQN, R2D2가 약점이었던 montezuma\_revenge에서 다음과 같은 성능을 보여주었다. Random Network Distillation [ICLR 2019]의 결과는 다음과 같다.

![](/assets/images/posts/85/img_15.png)

사람을 능가하는 성능을 보여주었다!

**Never Give Up [ICLR 2020]**

"Never Give Up"이라는 논문은 딥 강화 학습에서의 탐색 문제를 해결하기 위한 새로운 방법을 제안합니다. 이 논문에서 제안하는 강화 학습 에이전트는 가장 어려운 탐색 게임을 해결하기 위해 다양한 탐험 정책을 학습합니다.   
  
그들의 주요 아이디어는 같은 네트워크에서 파생된 별도의 탐험 정책과 이용 정책을 함께 학습하는 것입니다. 이를 통해 이용 정책은 외부 보상을 최대화하면서 (즉, 주어진 작업을 해결하면서) 탐험 정책은 무작위 정책으로 축소되지 않고 탐험을 유지할 수 있습니다. 이들은 Universal Value Function Approximators (UVFA) 프레임워크를 사용하여 탐험 행동의 다양한 정도를 가진 정책 가족을 함께 학습하려고 합니다​​.   
  
이 논문에서 제안하는 내재적 보상은 에피소드별 신묘성과 평생 신묘성을 결합하여 에이전트가 환경의 모든 제어 가능한 상태를 반복적으로 방문하도록 명확히 장려합니다. 에피소드별 신묘성은 에이전트가 여러 에피소드 동안 익숙하지만 아직 완전히 탐색되지 않은 상태를 주기적으로 다시 방문하도록 장려합니다.

![](/assets/images/posts/85/img_16.png)

Never Give Up: Feature Embedding

"Never Give Up: Feature Embedding"은 "Never Give Up" 논문 시리즈의 일부로, 특징 임베딩(Feature Embedding)에 초점을 맞춘 연구입니다. 이 연구는 현재 상태와 다음 상태 간의 액션을 예측하는 훈련을 통해 상태가 조종 가능한지를 확인하는 기능을 개발하는 것을 목표로 합니다.   
  
논문에서는 상태와 액션 간의 연결을 탐색하기 위해 특징 임베딩을 사용합니다. 이는 상태와 액션을 벡터 공간으로 임베딩하여 상태 공간에서의 구조와 액션의 특징을 파악하는 것을 의미합니다. 특징 임베딩은 상태의 특성을 추출하고 유용한 정보를 보존하여 액션 선택과 탐사에 도움을 줄 수 있는 중요한 기술입니다.   
  
논문에서는 특징 임베딩을 사용하여 상태-액션 예측 문제를 해결하고, 상태의 조종 가능성을 평가하는 방법을 제안합니다.

![](/assets/images/posts/85/img_17.png)

1. 에피소드 간에 여러 번 방문된 상태에 대한 방문을 점차적으로 억제합니다. 이는 에이전트가 이미 많이 방문한 상태를 계속 방문하는 것을 막기 위한 전략입니다. 에이전트는 상태를 다양하게 탐색하고 더 많은 정보를 얻기 위해 이전에 자주 방문한 상태를 피하려고 합니다.
2. 같은 에피소드 내에서 같은 상태를 다시 방문하는 것을 빠르게 억제합니다. 이는 에이전트가 같은 상태를 반복해서 방문하여 시간을 낭비하는 것을 막기 위한 전략입니다. 에이전트는 가능한 한 빨리 다른 상태로 이동하여 더 다양한 경험을 얻을 수 있도록 합니다.

![](/assets/images/posts/85/img_18.png)

다음은 결과입니다.

![](/assets/images/posts/85/img_19.png)

![](/assets/images/posts/85/img_20.png)

![](/assets/images/posts/85/img_21.png)

**Self-Supervised Exploration via Disagreement [2019 ICML]**

"Self-Supervised Exploration via Disagreement" (2019)은 다음과 같은 내용을 다룹니다:

1. 외부적인 보상이 부족하거나 없을 때, 내부적인 보상을 활용하여 탐사를 진행합니다. 내부적인 보상은 카운트 기반 보상이나 호기심 (예측 오류) 등으로 구성되며, 방문한 상태의 다양성을 촉진합니다. 이를 통해 상태 공간에서 다양한 경험을 얻을 수 있도록 유도합니다.
2. 확률적인 특성으로 인해 예측 모델의 학습에 어려움이 있습니다. 확률적인 전이, 액션 또는 무작위 노이즈로 인해 예측 모델의 학습에 도전이 있습니다. 이에 대응하기 위해, 스스로 학습하는 예측 모델 자체가 확률적인 특성을 가지도록 합니다.

Main Idea : Disagreement

![](/assets/images/posts/85/img_22.png)

주요 아이디어는 '의견의 불일치'입니다.   
  
이 방법은 여러 개의 예측 모델을 사용하여 불확실성을 측정하고, 이들 사이의 불일치를 활용하여 탐사를 진행합니다. 불일치는 예측 모델들이 다른 예측을 내놓을 때 발생하며, 이는 상태의 불확실성을 나타냅니다. 예를 들어, 여러 예측 모델이 다른 행동 또는 다른 상태를 예측하는 경우, 해당 상태는 불확실성이 높은 상태로 간주됩니다.   
  
이 방법은 불일치를 활용하여 보다 불확실한 상태로 집중하여 탐사하고, 이를 통해 새로운 경험을 얻을 수 있습니다.

Results in Non-Stochastic Environments

![](/assets/images/posts/85/img_23.png)

Intrinsic Reward : Less vs. High Stochastic

![](/assets/images/posts/85/img_24.png)

Results: 3D Navigation with / without noisy TV

![](/assets/images/posts/85/img_25.png)

Results: Stochastic Atari Games

![](/assets/images/posts/85/img_26.png)

Results: Real-World Robotic Manipulation

![](/assets/images/posts/85/img_27.png)

https://pathak22.github.io/exploration
by disagreement/

**Adversarially Guided Actor-Critic [ICLR 2021]**

Adversarially Guided Actor Critic" (AGAC)는 ICLR 2021에서 제안된 방법입니다.   
  
이 방법은 보통 훈련과 테스트에 동일한 환경을 사용하는 탐사 문제에서 적용됩니다. 특히, 저보상 환경에서 저보상 문제를 가진 절차적으로 생성된 환경에 도전하고 있습니다. 이러한 환경에서는 매 에피소드마다 맵이 다르게 구성되기 때문에 이전에 방문한 상태들을 기억하는 것만으로는 충분하지 않습니다.   
  
AGAC는 "적대적인 요소"라는 세 번째 구성요소를 도입합니다. 이 적대적인 요소는 배우 역할을 수행합니다. 배우는 적대적인 요소의 예측과 구별되도록 행동하면서 기대되는 반환값의 합을 최대화합니다. 이를 통해 보상이 극도로 희박한 상황에서도 새로운 행동을 만들어낼 수 있습니다.   
  
AGAC는 기존의 Actor-Critic 방법에 적대적인 학습 요소를 추가하여 탐사와 학습을 보다 효과적으로 이루어낼 수 있는 방법입니다.

![](/assets/images/posts/85/img_28.png)

AGAC

![](/assets/images/posts/85/img_29.png)

AGAC: Overview

AGAC: Loss Functions

![](/assets/images/posts/85/img_30.png)

Performance on 3D Navigation Task (Vizdoom)

![](/assets/images/posts/85/img_31.png)

MinigGid Environment

MiniGrid 환경은 각 에피소드마다 환경이 다르게 구성됩니다. 이 환경은 부분적으로 관찰 가능한 뷰로 표현되는데, 크기는 7 x 7이며 각 셀은 (OBJECT\_IDX, COLOR\_IDX, STATE)로 표현됩니다.   
  
MiniGrid 환경은 일반적으로 작은 격자 세계로 구성되어 있으며, 에이전트는 해당 세계에서 목표를 달성하기 위해 동작합니다. 각 셀은 객체 인덱스(OBJECT\_IDX), 색상 인덱스(COLOR\_IDX), 상태(STATE) 정보를 포함하고 있습니다. 이러한 정보를 기반으로 에이전트는 현재 상태를 파악하고 목표를 달성하기 위한 행동을 결정합니다.   
  
MiniGrid 환경에서는 에피소드마다 다른 구성을 가지기 때문에, 에이전트는 각 에피소드에서 다양한 상황과 도전에 직면하게 됩니다. 

![](/assets/images/posts/85/img_32.png)

https://github.com/maximecb/gym
minigrid

Performance evaluation of AGAC

![](/assets/images/posts/85/img_33.png)

Exploration in Reward Free Environment

![](/assets/images/posts/85/img_34.png)

![](/assets/images/posts/85/img_35.png)

![](/assets/images/posts/85/img_36.png)

Partially Observed vs. Full Observed

탐사를 위해서는 완전한 관찰(full observation)이 아닌 부분적인 관찰(partial observation)이 더 유리할 수 있습니다. 완전한 관찰은 에이전트에게 환경의 모든 상태 정보를 제공하여 최적의 행동을 쉽게 결정할 수 있게 해줍니다. 그러나 완전한 관찰을 갖는 경우, 에이전트는 이미 알고 있는 정보에 의존하여 새로운 상황을 탐색하기 어려울 수 있습니다.   
  
부분적인 관찰은 에이전트에게 일부 상태 정보만 제공하는데, 이는 더 현실적인 탐사 환경을 모델링하고 학습하는 데 도움을 줄 수 있습니다. 부분적인 관찰에서는 에이전트가 불완전한 정보를 기반으로 상태를 추론하고 행동을 선택해야 합니다. 이는 더 많은 탐사와 탐구를 유도하며, 새로운 상황에 대한 학습과 적응을 장려합니다.   
  
따라서 탐사를 강조하는 경우에는 부분적인 관찰이 더 선호될 수 있습니다. 완전한 관찰은 탐사보다는 최적화에 더 적합한 상황에서 활용될 수 있습니다.

![](/assets/images/posts/85/img_37.png)

![](/assets/images/posts/85/img_38.png)

![](/assets/images/posts/85/img_39.png)
