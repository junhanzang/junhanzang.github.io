---
title: "Rho-1: Not All Tokens Are What You Need"
date: 2025-01-10 15:02:40
categories:
  - 인공지능
tags:
  - rho-1
---

<https://arxiv.org/abs/2404.07965>

[Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965)

**초록**  
기존의 언어 모델 사전 학습 기법은 모든 학습 토큰에 동일하게 다음 토큰 예측 손실을 적용해 왔습니다. 이러한 관행에 의문을 제기하며, 우리는 “코퍼스 내의 모든 토큰이 언어 모델 학습에 똑같이 중요하지는 않다”고 주장합니다. 이를 위해 토큰 단위에서의 언어 모델 학습 동태를 살펴본 결과, 서로 다른 토큰은 서로 다른 손실 패턴을 보인다는 사실을 확인했습니다. 이러한 통찰을 바탕으로, 우리는 새로운 언어 모델인 **Rho-1**을 소개합니다. 기존의 언어 모델이 코퍼스 내 모든 차기 토큰을 예측하도록 학습하는 것과 달리, **Rho-1**은 **Selective Language Modeling(SLM)**을 도입하여 원하는 분포에 부합하는 유용한 토큰만 선별적으로 학습합니다. 이 방법은 우선 참고 모델을 통해 토큰에 점수를 매긴 뒤, 점수가 높은 토큰에 집중적으로 손실을 부여하여 언어 모델을 학습하는 과정으로 이루어집니다.

15억 개 규모의 **OpenWebMath** 코퍼스로 지속적 사전 학습을 수행한 결과, **Rho-1**은 9가지 수학 과제에서 최대 30%에 달하는 few-shot 정확도 절대 향상을 달성했습니다. 이어진 파인튜닝 과정에서, **Rho-1-1B**와 **Rho-1-7B** 모델은 **MATH** 데이터셋에서 각각 40.6%와 51.8%의 최고 성능을 기록했으며, 이는 사전 학습 토큰의 3%만 사용하고도 **DeepSeekMath** 수준에 도달한 결과입니다. 또한 800억 개에 달하는 일반 토큰으로 지속적 사전 학습을 추가로 수행했을 때, **Rho-1**은 15개의 다양한 과제에서 평균 6.8%의 성능 향상을 보이며 데이터 효율성과 언어 모델 사전 학습 성능을 모두 끌어올렸습니다.

![](/assets/images/posts/485/img.png)

**Figure 1**: 우리는 파라미터 규모가 10억(1B)과 70억(7B)인 모델을 대상으로 150억 개의 OpenWebMath 토큰을 활용해 지속적 사전 학습을 진행했습니다. **Rho-1**은 제안된 **Selective Language Modeling(SLM)** 기법으로 학습했으며, 비교 모델들은 전통적인 인과적 언어 모델링(CLM)을 사용했습니다. 그 결과, **SLM**은 GSM8k와 MATH 과제에서 평균 few-shot 정확도를 16% 이상 끌어올렸으며, 동일한 정확도에 도달하기까지 필요한 학습 단계도 기존 대비 5~10배 절감했습니다.

---

전통적인 **인과적 언어 모델링(Causal Language Modeling, CLM)**은 언어 모델이 텍스트를 순차적으로 생성하도록 훈련하는 표준적인 방법을 가리킵니다. 쉽게 말해, 모델이 “지금까지 본 단어(토큰)를 바탕으로 다음 단어를 예측”하도록 학습하는 것입니다.

예를 들어, 문장을 왼쪽에서 오른쪽으로 읽어나가면서, 모델은 ‘이미 주어진 단어들만’ 보고 그다음 단어를 맞추는 방식으로 훈련됩니다. 이를 통해 모델이 텍스트 생성을 점진적으로 익히도록 유도하죠. GPT 시리즈처럼 널리 알려진 대규모 언어 모델들이 모두 이 CLM 방식을 사용합니다

---
