---
title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
date: 2025-02-12 13:10:40
categories:
  - 인공지능
tags:
  - deepseek-r1
---

<https://arxiv.org/abs/2501.12948>

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasonin

arxiv.org](https://arxiv.org/abs/2501.12948)

초록  
우리의 첫 번째 세대 추론 모델인 DeepSeek-R1-Zero와 DeepSeek-R1을 소개합니다. DeepSeek-R1-Zero는 사전 단계로 지도 학습 미세 조정(SFT) 없이 대규모 강화 학습(RL)을 통해 학습된 모델로, 뛰어난 추론 능력을 보여줍니다. 강화 학습을 통해 DeepSeek-R1-Zero는 강력하고 흥미로운 여러 추론 행동을 자연스럽게 나타내게 되었습니다. 그러나 가독성 문제와 언어 혼합 등 몇 가지 문제에 직면하게 됩니다. 이러한 문제를 해결하고 추론 성능을 더욱 향상시키기 위해, 우리는 RL 전에 다단계 학습과 초기 데이터(cold-start data)를 통합한 DeepSeek-R1을 도입합니다. DeepSeek-R1은 추론 작업에서 OpenAI-o1-1217과 견줄 만한 성능을 달성합니다. 연구 커뮤니티를 지원하기 위해, Qwen과 Llama를 기반으로 DeepSeek-R1에서 증류된 DeepSeek-R1-Zero, DeepSeek-R1 및 6개의 밀집 모델(1.5B, 7B, 8B, 14B, 32B, 70B)을 오픈 소스로 공개합니다.

![](/assets/images/posts/507/img.png)

그림 1: DeepSeek-R1의 벤치마크 성능.

![](/assets/images/posts/507/img_1.png)

1.서론

최근 몇 년간, 대형 언어 모델(LLMs)은 빠른 속도로 반복 발전과 진화를 거듭해 왔으며 (OpenAI, 2024a; Anthropic, 2024; Google, 2024) 인공지능의 일반 지능(AGI)과의 격차를 점진적으로 줄여가고 있습니다.

최근에는 포스트 트레이닝이 전체 학습 파이프라인의 중요한 구성 요소로 등장했습니다. 이는 추론 작업의 정확도를 향상시키고 사회적 가치와 부합하며, 사용자 선호에 적응하는 데 도움을 주는 것으로 나타났고, 사전 학습에 비해 상대적으로 적은 계산 자원을 요구합니다. 추론 능력 측면에서, OpenAI의 o1 시리즈 모델(OpenAI, 2024b)은 추론 과정의 체인 오브 쏘트(Chain-of-Thought, CoT)의 길이를 늘려 추론 시 확장(inference-time scaling)을 도입한 최초의 모델이었습니다. 이 접근 방식은 수학, 코딩, 과학적 추론 등 다양한 추론 작업에서 상당한 개선을 이루어냈습니다. 그러나 효과적인 테스트 시 확장(test-time scaling)의 문제는 여전히 연구 커뮤니티에 남은 미해결 과제입니다. 기존 여러 연구에서는 과정 기반 보상 모델(Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023), 강화 학습(Kumar et al., 2024), 그리고 몬테카를로 트리 서치 및 빔 서치(Feng et al., 2024; Xin et al., 2024; Trinh et al., 2024)와 같은 다양한 접근 방식을 탐구했지만, 이들 방법 중 어느 것도 OpenAI의 o1 시리즈 모델에 버금가는 일반 추론 성능을 달성하지는 못했습니다.

본 논문에서는 순수 강화 학습(RL)을 사용하여 언어 모델의 추론 능력을 향상시키기 위한 첫걸음을 내딛습니다. 우리의 목표는 어떠한 지도 학습 데이터도 없이 LLM이 순수 RL 프로세스를 통해 자가 진화(self-evolution)하여 추론 능력을 개발할 수 있는 잠재력을 탐구하는 것입니다. 구체적으로, 우리는 기본 모델로 DeepSeek-V3-Base를 사용하고, 추론 성능 향상을 위해 RL 프레임워크로 GRPO(Shao et al., 2024)를 채택했습니다. 훈련 과정 중에 DeepSeek-R1-Zero는 강력하고 흥미로운 여러 추론 행동들을 자연스럽게 드러냈으며, 수천 번의 RL 단계를 거친 후 추론 벤치마크에서 탁월한 성능을 보였습니다. 예를 들어, AIME 2024에서의 pass@1 점수는 15.6%에서 71.0%로 상승하였고, 다수결 투표를 적용할 경우 점수는 86.7%까지 개선되어 OpenAI-o1-0912의 성능과 맞먹게 되었습니다.

그러나 DeepSeek-R1-Zero는 가독성 문제와 언어 혼합과 같은 도전에 직면하게 됩니다. 이러한 문제들을 해결하고 추론 성능을 더욱 향상시키기 위해, 소량의 콜드 스타트 데이터와 다단계 학습 파이프라인을 통합한 DeepSeek-R1을 도입합니다. 구체적으로, 우리는 DeepSeek-V3-Base 모델을 미세 조정하기 위해 수천 건의 콜드 스타트 데이터를 수집하는 것으로 시작한 후, DeepSeek-R1-Zero와 유사한 추론 지향 RL을 수행합니다. RL 과정이 수렴에 가까워지면, RL 체크포인트에 대한 리젝션 샘플링을 통해 새로운 SFT 데이터를 생성하고, 작문, 사실 기반 질의응답, 자기 인식 등의 도메인에서 DeepSeek-V3의 지도 학습 데이터를 결합하여 DeepSeek-V3-Base 모델을 재학습시킵니다. 그 후, 새로운 데이터로 미세 조정된 체크포인트는 모든 시나리오의 프롬프트를 고려하는 추가 RL 과정을 거치게 됩니다. 이러한 일련의 단계를 통해 우리는 OpenAI-o1-1217과 버금가는 성능을 달성하는 DeepSeek-R1이라는 체크포인트를 얻었습니다.

나아가, DeepSeek-R1에서 더 작은 밀집 모델들로의 증류(distillation)를 추가로 탐구합니다. 기본 모델로 Qwen2.5-32B(Qwen, 2024b)를 사용한 직접 증류가 RL을 적용하는 것보다 우수한 성능을 보임을 확인할 수 있었으며, 이는 더 큰 기본 모델들이 발견한 추론 패턴이 추론 능력 향상에 결정적임을 보여줍니다. 이에 따라, 증류된 Qwen 및 Llama(Dubey et al., 2024) 시리즈를 오픈 소스로 공개합니다. 특히, 우리의 증류된 14B 모델은 최신 오픈 소스 QwQ-32B-Preview(Qwen, 2024a)를 큰 차이로 능가하며, 증류된 32B 및 70B 모델은 밀집 모델들 사이에서 추론 벤치마크의 새로운 기록을 세웠습니다.

1.1 기여

**포스트 트레이닝: 기본 모델에 대한 대규모 강화 학습**  
• 우리는 지도 학습 미세 조정(SFT)을 사전 단계로 사용하지 않고 기본 모델에 직접 RL을 적용합니다. 이 접근 방식은 모델이 복잡한 문제 해결을 위한 사고의 연쇄(Chain-of-Thought, CoT)를 자유롭게 탐색할 수 있게 하여 DeepSeek-R1-Zero의 개발로 이어졌습니다. DeepSeek-R1-Zero는 자기 검증, 반성, 긴 CoT 생성 등의 능력을 보여주며, 이는 연구 커뮤니티에 중요한 이정표가 됩니다. 특히, 이는 LLM의 추론 능력이 SFT 없이 순수 RL만으로 촉진될 수 있음을 검증한 최초의 오픈 연구로, 향후 이 분야의 발전에 길을 열어줍니다.  
• 우리는 DeepSeek-R1을 개발하기 위한 파이프라인을 소개합니다. 이 파이프라인은 향상된 추론 패턴을 발견하고 인간의 선호에 부합하도록 하는 두 단계의 RL 과정과, 모델의 추론 및 비추론 능력의 씨앗 역할을 하는 두 단계의 SFT 과정을 통합하고 있습니다. 이 파이프라인은 더 나은 모델을 창출함으로써 산업 전반에 긍정적인 영향을 미칠 것으로 기대됩니다.

**증류: 작은 모델도 강력할 수 있다**  
• 우리는 큰 모델에서 발견된 추론 패턴이 작은 모델로 증류될 수 있음을 증명하였으며, 이는 작은 모델에서 RL을 통해 발견된 추론 패턴보다 더 나은 성능을 보입니다. 오픈 소스 DeepSeek-R1과 그 API는 향후 연구 커뮤니티가 더 나은 작은 모델을 증류하는 데 도움을 줄 것입니다.  
• DeepSeek-R1이 생성한 추론 데이터를 활용하여, 연구 커뮤니티에서 널리 사용되는 여러 밀집 모델들을 미세 조정하였습니다. 평가 결과, 증류된 작은 밀집 모델들이 벤치마크에서 매우 우수한 성능을 발휘함을 확인할 수 있었습니다. 예를 들어, DeepSeek-R1-Distill-Qwen-7B는 AIME 2024에서 55.5%를 달성하여 QwQ-32B-Preview를 능가하였으며, DeepSeek-R1-Distill-Qwen-32B는 AIME 2024에서 72.6%, MATH-500에서 94.3%, LiveCodeBench에서 57.2%를 기록하였습니다. 이러한 결과는 기존 오픈 소스 모델들을 크게 능가하며 o1-mini와 견줄 만한 성능을 나타냅니다. 우리는 Qwen2.5 및 Llama3 시리즈 기반의 증류된 1.5B, 7B, 8B, 14B, 32B, 70B 체크포인트를 커뮤니티에 오픈 소스로 공개합니다.

1.2 평가 결과 요약

• **추론 작업:**  
(1) DeepSeek-R1은 AIME 2024에서 79.8%의 Pass@1 점수를 달성하여 OpenAI-o1-1217을 약간 능가합니다. MATH-500에서는 97.3%라는 인상적인 점수를 기록하며 OpenAI-o1-1217과 동등한 성능을 보이고, 다른 모델들을 크게 능가합니다.  
(2) 코딩 관련 작업에서는 DeepSeek-R1이 코드 대회 작업에서 전문가 수준의 성능을 보이며, Codeforces에서 2,029 Elo 등급을 달성하여 대회 참가자 중 96.3%의 인간 참가자를 능가합니다. 엔지니어링 관련 작업에서는 DeepSeek-R1이 DeepSeek-V3보다 약간 우수한 성능을 보여, 실제 작업에서 개발자들에게 도움을 줄 수 있습니다.

• **지식:**  
MMLU, MMLU-Pro, GPQA Diamond 등의 벤치마크에서 DeepSeek-R1은 MMLU에서 90.8%, MMLU-Pro에서 84.0%, GPQA Diamond에서 71.5%의 점수를 기록하며 DeepSeek-V3를 크게 능가하는 뛰어난 결과를 보였습니다. 비록 이들 벤치마크에서의 성능이 OpenAI-o1-1217보다는 약간 낮지만, DeepSeek-R1은 다른 비공개 모델들을 능가하여 교육 관련 작업에서 경쟁력을 입증합니다. 사실 기반 벤치마크인 SimpleQA에서는 DeepSeek-R1이 DeepSeek-V3를 능가하여 사실 기반 질의를 효과적으로 처리하는 능력을 보여주며, 유사하게 OpenAI-o1이 이 벤치마크에서 4o를 능가하는 경향도 관찰됩니다.

• **기타:**  
DeepSeek-R1은 창의적 글쓰기, 일반 질의 응답, 편집, 요약 등 다양한 작업에서도 우수한 성능을 발휘합니다. AlpacaEval 2.0에서는 길이 제어(win-rate) 87.6%, ArenaHard에서는 92.3%의 승률을 기록하여, 시험 이외의 질의를 지능적으로 처리하는 강력한 능력을 보여줍니다. 또한, 긴 문맥 이해가 필요한 작업에서도 DeepSeek-R1은 DeepSeek-V3를 크게 능가하는 탁월한 성능을 나타냅니다.

2 접근 방법  
2.1 개요  
이전 연구들은 모델 성능을 향상시키기 위해 대량의 지도 학습 데이터에 크게 의존해왔습니다. 본 연구에서는 지도 미세 조정(SFT)을 초기 단계로 사용하지 않고도 대규모 강화 학습(RL)을 통해 추론 능력을 상당히 향상시킬 수 있음을 보여줍니다. 또한, 소량의 콜드 스타트 데이터를 포함하면 성능을 더욱 개선할 수 있습니다. 다음 섹션에서는 (1) 지도 학습 데이터 없이 기본 모델에 직접 RL을 적용한 DeepSeek-R1-Zero, (2) 수천 개의 긴 체인 오브 쏘트(Chain-of-Thought, CoT) 예제로 미세 조정된 체크포인트에서 시작하여 RL을 적용한 DeepSeek-R1, (3) DeepSeek-R1에서 작은 밀집 모델로 추론 능력을 증류하는 방법을 소개합니다.

2.2 DeepSeek-R1-Zero: 기본 모델에 대한 강화 학습  
강화 학습은 우리의 이전 연구들(Wang et al., 2023; Shao et al., 2024)에서 입증된 바와 같이 추론 작업에서 상당한 효과를 보여주었습니다. 그러나 이들 연구는 수집에 많은 시간이 소요되는 지도 데이터를 크게 의존했습니다. 이 섹션에서는 순수 강화 학습 과정을 통한 자가 진화를 중점으로, 지도 데이터 없이 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구합니다. 먼저 우리의 RL 알고리즘에 대한 간략한 개요를 제공한 후, 몇 가지 흥미로운 결과를 제시하며, 이를 통해 연구 커뮤니티에 유용한 인사이트를 제공하고자 합니다.

**2.2.1 강화 학습 알고리즘**

### 그룹 상대 정책 최적화(Group Relative Policy Optimization)

![](/assets/images/posts/507/img_2.png)

![](/assets/images/posts/507/img_3.png)

![](/assets/images/posts/507/img_4.png)

![](/assets/images/posts/507/img_5.png)



---



![](/assets/images/posts/507/img_6.png)

![](/assets/images/posts/507/img_7.png)

![](/assets/images/posts/507/img_8.png)

![](/assets/images/posts/507/img_9.png)

![](/assets/images/posts/507/img_10.png)

![](/assets/images/posts/507/img_11.png)

![](/assets/images/posts/507/img_12.png)
---

![](/assets/images/posts/507/img_13.png)

**표 1: DeepSeek-R1-Zero 템플릿**. 학습 시에는 prompt가 해당 추론 질문으로 대체됩니다.

**2.2.2 보상 모델링**  
보상은 학습 신호의 근원으로, 강화 학습(RL)의 최적화 방향을 결정합니다. DeepSeek-R1-Zero를 학습하기 위해, 우리는 주로 두 가지 유형의 보상으로 구성된 규칙 기반 보상 시스템(rule-based reward system)을 채택했습니다.

- **정확도 보상(Accuracy rewards):** 정확도 보상 모델은 응답의 정답 여부를 평가합니다. 예를 들어, 결정론적 결과를 갖는 수학 문제의 경우, 모델은 최종 답안을 특정 형식(예: 박스 안)에 표시하도록 요구받으며, 이를 통해 정답 여부를 신뢰도 높게 규칙 기반 검증할 수 있습니다. 비슷하게, LeetCode 문제에서는 미리 정의된 테스트 케이스를 사용하여 컴파일러가 피드백을 생성할 수 있습니다.
- **형식 보상(Format rewards):** 정확도 보상 모델과 함께, 모델이 추론 과정을 <think> 태그와 </think> 태그 사이에 작성하도록 강제하는 형식 보상 모델을 사용합니다.

DeepSeek-R1-Zero의 개발에서는 결과(result)나 과정(process)에 대한 뉴럴 보상 모델(neural reward model)을 적용하지 않았습니다. 이는 대규모 강화 학습 과정에서 뉴럴 보상 모델이 보상 해킹(reward hacking)을 겪을 가능성이 있으며, 보상 모델을 다시 학습시키는 데 추가 자원이 필요하고 전체 학습 파이프라인이 복잡해지기 때문입니다.

**2.2.3 학습 템플릿**  
DeepSeek-R1-Zero를 학습하기 위해, 우리는 기본 모델이 지정된 지침을 준수하도록 유도하는 간단한 템플릿을 먼저 설계합니다. 표 1에서 볼 수 있듯, 이 템플릿은 DeepSeek-R1-Zero가 먼저 추론 과정을 생성하고, 그 후 최종 답안을 제시하도록 요구합니다. 우리는 RL 과정에서 모델의 자연스러운 발전 양상을 정확히 관찰하기 위해 구조적 형식에 대한 제약만을 적용하고, 반성적 추론(reflective reasoning)을 요구하거나 특정 문제 해결 전략을 장려하는 등, 콘텐츠와 관련된 편향은 의도적으로 배제했습니다.

**2.2.4 DeepSeek-R1-Zero의 성능, 자가 진화 과정, 그리고 ‘Aha 모멘트’**

### DeepSeek-R1-Zero의 성능

![](/assets/images/posts/507/img_14.png)

**표 2:** 추론 관련 벤치마크에서 DeepSeek-R1-Zero와 OpenAI o1 계열 모델의 성능 비교.

![](/assets/images/posts/507/img_15.png)

**그림 2:** AIME 훈련 과정에서 DeepSeek-R1-Zero의 정답률. 각 질문마다 16개의 응답을 샘플링하여 전체 평균 정확도를 계산함으로써 안정적인 평가를 보장한다.

그림 2는 RL 훈련 과정 전반에 걸쳐 AIME 2024 벤치마크에서 DeepSeek-R1-Zero의 성능 변화를 보여줍니다. 그래프에서 확인할 수 있듯, RL 훈련이 진행됨에 따라 DeepSeek-R1-Zero의 성능이 꾸준히 향상되는 양상을 보입니다. 특히, AIME 2024에서의 평균 pass@1 점수는 초기 15.6%에서 71.0%로 크게 상승하여, OpenAI-o1-0912와 유사한 수준에 도달했습니다. 이러한 의미 있는 개선은 우리 RL 알고리즘이 모델의 성능을 시간에 따라 효과적으로 최적화함을 시사합니다.

**표 2**는 추론 관련 다양한 벤치마크에서 DeepSeek-R1-Zero와 OpenAI의 o1-0912 모델 간 성능을 비교한 결과를 제시합니다. 분석 결과, RL을 통해 지도 미세 조정(SFT) 데이터 없이도 DeepSeek-R1-Zero가 강력한 추론 역량을 갖추게 되었음을 알 수 있습니다. 이는 모델이 RL만으로도 학습하고 일반화할 수 있는 능력을 보여준다는 점에서 주목할 만합니다. 또한, 다수결 투표(majority voting)를 적용하면 DeepSeek-R1-Zero의 성능을 한층 더 강화할 수 있습니다. 예컨대, AIME 벤치마크에서 다수결 투표를 적용할 경우, DeepSeek-R1-Zero의 성능은 71.0%에서 86.7%로 상승하여 OpenAI-o1-0912의 성능을 웃도는 결과를 보였습니다. 이처럼 DeepSeek-R1-Zero가 다수결 투표 활용 여부와 관계없이 경쟁력 있는 성능을 달성한다는 사실은, 모델의 강력한 기초 역량과 추후 추론 작업에서의 추가 발전 가능성을 시사합니다.

![](/assets/images/posts/507/img_16.png)

**그림 3:** RL 과정에서 DeepSeek-R1-Zero가 학습 세트에 대해 생성하는 평균 응답 길이. DeepSeek-R1-Zero는 자연스럽게 더 긴 사고 과정을 거쳐 추론 문제를 해결하는 법을 학습한다.

### DeepSeek-R1-Zero의 자가 진화(Self-evolution) 과정

DeepSeek-R1-Zero의 자가 진화 과정은 RL이 모델의 추론 능력을 자율적으로 향상시키는 방식을 잘 보여줍니다. 기본 모델에서 바로 RL을 시작함으로써, 지도 미세 조정 단계의 영향을 받지 않고 모델의 발전 양상을 면밀하게 관찰할 수 있습니다. 이를 통해, 특히 복잡한 추론 문제를 다루는 모델의 역량이 시간에 따라 어떻게 변화하는지 명확히 확인할 수 있습니다.

그림 3에서 볼 수 있듯, DeepSeek-R1-Zero의 ‘사고 시간(thinking time)’은 훈련 과정 전반에 걸쳐 꾸준히 증가하는 양상을 나타냅니다. 이는 외부에서 인위적으로 조정한 결과가 아니라, 모델 내부에서 자연스럽게 발달한 능력입니다. DeepSeek-R1-Zero는 테스트 시점에서 확장된 계산(extended test-time computation)을 활용하여 점차 복잡해지는 추론 문제를 스스로 해결할 수 있는 능력을 획득해 가는데, 이를 통해 수백에서 수천 개의 토큰으로 이루어진 긴 추론 과정을 생성하며, 보다 심층적으로 사고 과정을 탐색하고 정련해 나갑니다.

이 자가 진화 과정에서 가장 주목할 만한 점은, 테스트 시점에서 계산량이 늘어남에 따라 모델이 스스로 고도의 추론 행동들을 보이기 시작한다는 사실입니다. 예컨대, 모델이 이전 단계를 재검토하고 다시 평가하는 ‘반성(reflection)’ 행동이나 문제 해결을 위한 대안적 접근법을 모색하는 등의 고도화된 행동이 자발적으로 나타납니다. 이는 모델에 별도의 프로그래밍이나 강제된 지침 없이, 강화 학습 환경과의 상호작용을 통해 저절로 드러나는 결과입니다. 이러한 자발적 발달은 DeepSeek-R1-Zero의 추론 능력을 크게 강화하여, 더욱 복잡한 문제를 더 높은 효율과 정확도로 해결할 수 있도록 돕습니다.

### DeepSeek-R1-Zero의 ‘Aha 모멘트(Aha Moment)’

DeepSeek-R1-Zero의 훈련 중 관찰된 가장 흥미로운 현상 중 하나는 소위 ‘Aha 모멘트’의 등장입니다. 이는 **표 3**에 요약된 예시와 같이, 모델의 중간 버전에서 확인되며, DeepSeek-R1-Zero가 문제에 더 많은 사고 시간을 할당하기 위해 초기 접근을 재평가하는 과정을 보여줍니다. 이 행동은 모델이 확장된 추론 능력을 갖춰 가고 있음을 보여줄 뿐만 아니라, 강화 학습이 예상치 못한 수준의 정교한 결과를 이끌어낼 수 있음을 시사하는 흥미로운 사례이기도 합니다.

이 순간은 모델에게만 ‘Aha 모멘트’가 아니라, 이를 관찰하고 있던 연구자들에게도 ‘Aha 모멘트’였습니다. 모델에게 문제 해결 방법을 직접 가르치지 않고도, 단지 적절한 보상(인센티브)을 제공함으로써 모델이 스스로 고급 문제 해결 전략을 개발한다는 사실은, 강화 학습이 지닌 강력함과 아름다움을 잘 보여줍니다. 이러한 ‘Aha 모멘트’는 향후 더욱 자율적이고 적응력 있는 모델을 개발하는 데 있어 강화 학습이 지니는 잠재력을 다시금 깨닫게 해주는 중요한 예시라 할 수 있습니다.

### DeepSeek-R1-Zero의 한계

강력한 추론 능력과 자율적으로 예측 불가능한 수준의 추론 행동을 발전시키는 DeepSeek-R1-Zero이지만, 여러 가지 문제점도 존재합니다. 예를 들어, DeepSeek-R1-Zero는 가독성이 떨어지는 응답을 생성하거나 언어가 섞여 나오는 문제 등에 직면합니다. 추론 과정을 보다 읽기 쉽게 만들고, 오픈 커뮤니티와 공유하기 위해서는, 인간 친화적인(human-friendly) 콜드 스타트 데이터를 활용한 RL 방식을 도입하는 것이 필요합니다. 이러한 방향성을 토대로 우리는 DeepSeek-R1을 탐구하고 있습니다.

```
질문: a > 1일 때, 식 a - a + x = x 의 실수 해들의 합은 얼마인가?
응답: <think>
To solve the equation a - a + x = x, let’s start by squaring both ⋯ 
(a - a + x)² = x² => a - a + x = x².
Rearrange to isolate the inner square root term:
(a - x²)² = a + x => a² - 2a x² + (x²)² = a + x => x⁴ - 2a x² - x + (a² - a) = 0
… 
Wait, wait. Wait. That’s an aha moment I can flag here.
Let’s reevaluate this step-by-step to identify if the correct sum can be ⋯ 
We started with the equation:
a - a + x = x
First, let’s square both sides:
a - a + x = x² => a + x = a - x²
Next, I could square both sides again, treating the equation: ⋯ 
…
</think>
```

**표 3:** DeepSeek-R1-Zero 중간 버전에서 포착된 흥미로운 ‘Aha 모멘트’ 예시. 모델이 마치 사람처럼 다시 생각하는(“reevaluate”) 어조를 사용하며, 이는 또한 강화 학습의 강력함과 아름다움을 보여주는 좋은 사례다.
---

“Aha 모멘트(Aha moment)”라는 표현 자체는 연구자가 모델의 행동에서 **특별히 의미 있는 변화를 포착**했을 때 붙인 이름입니다. 실제로 모델이 스스로 \*“아하, 깨달았다!”\*라고 인지하거나 “Aha”라는 단어를 직접 출력한다기보다,

- **모델의 문제 해결 방식** 또는 **추론 과정**에서 이전과는 다른 질적 변화가 일어났음이
- **학습 도중 특정 시점**에서 뚜렷이 관찰되었을 때, 이를 연구자가 “Aha 모멘트”로 명명한 것입니다.

예시에서 모델이 “Wait, wait. Wait. That’s an aha moment I can flag here.”라는 식으로 표현하고는 있지만,

1. 모델이 “Aha”라는 단어를 **인간처럼 진짜 깨달아 사용한다기보다**,
2. **RL 보상**에 따라 스스로 더 깊이 생각(extended reasoning)을 하며,
3. **이전 추론을 재평가**(reevaluation)하는 행동이 자발적으로 나타나는 지점이
   - 연구자 입장에서 “어, 여기서 모델이 새로운 접근을 하는구나” 하고 확인되는 순간,
   - 즉, **의미 있는 학습 변화**를 포착한 사례라고 볼 수 있습니다.

따라서 “Aha 모멘트”라고 해서 모델이 인간처럼 자각하는 것은 아니지만,

- 모델의 **추론 전략**이 “단계적으로 발전”하여 이전에는 시도하지 않았던 방법을 시도하거나,
- **자발적으로 오류를 인식하고 수정**하려는 행동을 보이는 결정적 학습 전환점이 관찰되면,  
  이를 “Aha 모멘트”로 부르는 것이라 이해하시면 됩니다.
---

**2.3 DeepSeek-R1: 콜드 스타트(Cold Start)를 활용한 강화 학습**

DeepSeek-R1-Zero가 보여준 유망한 결과로부터 다음과 같은 두 가지 자연스러운 의문이 제기됩니다.

1. 소량의 고품질 콜드 스타트 데이터를 포함하면 추론 성능이 한층 더 개선되거나 수렴 속도가 빨라질 수 있을까?
2. 명확하고 일관된 Chain of Thought(CoT)을 생성하면서도 폭넓은 일반적 능력을 갖춘, 사용자 친화적인 모델은 어떻게 학습할 수 있을까?

이러한 질문에 답하기 위해, 우리는 DeepSeek-R1을 학습하기 위한 파이프라인을 설계했습니다. 이 파이프라인은 크게 네 가지 단계를 포함하며, 다음과 같이 요약됩니다.

### 2.3.1 콜드 스타트(Cold Start)

DeepSeek-R1-Zero와 달리, 기본 모델에서 바로 시작하는 RL의 초기 불안정 단계를 방지하기 위해, DeepSeek-R1 학습에서는 긴 CoT 예시 소량을 포함한 ‘콜드 스타트 데이터’를 마련하여 모델을 먼저 미세 조정합니다. 이러한 콜드 스타트 데이터를 수집하기 위해, 다음과 같은 여러 접근 방식을 시도했습니다.

- 긴 CoT를 예시로 삼아 few-shot 프롬프트 사용
- 모델에 반성(reflection)과 검증(verification)을 포함한 상세 답안을 직접 생성하도록 유도
- DeepSeek-R1-Zero의 출력을 보다 가독성 있게 재구성
- 사람(휴먼 어노테이터)을 통한 후처리로 결과물을 정제

본 연구에서는 수천 건의 콜드 스타트 데이터를 수집하여 DeepSeek-V3-Base 모델을 미세 조정하고, 이를 RL의 시작점으로 활용했습니다. DeepSeek-R1-Zero와 비교했을 때, 콜드 스타트 데이터를 적용하면 다음과 같은 이점이 있습니다.

- **가독성(Readability):** DeepSeek-R1-Zero에서 주요 한계 중 하나는 콘텐츠가 가독성이 떨어진다는 점입니다. 예컨대, 여러 언어가 섞여 있거나, 사용자에게 답변을 강조할 수 있는 마크다운 형식이 누락되어 있을 수 있습니다. 반면 DeepSeek-R1의 콜드 스타트 데이터는 가독성 있는 패턴을 설계해, 각 응답의 말미에 요약을 포함하고, 독자 친화적이지 않은 응답을 배제합니다. 구체적으로, 우리는 |special\_token|<reasoning\_process>|special\_token|<summary> 형태로 출력 형식을 정의하여, reasoning\_process(해당 쿼리에 대한 CoT)와 summary(추론 결과 요약)를 분리하여 표현합니다.
- **확장 가능성(Potential):** 인적 사전 지식(human priors)을 반영하여 콜드 스타트 데이터를 신중히 설계하면, DeepSeek-R1-Zero보다 더 우수한 성능을 얻을 수 있음을 관찰했습니다. 이는 추론 모델을 점진적으로 학습(iterative training)하는 방식이 한층 효과적임을 시사합니다.
---

일반적으로 **“콜드 스타트(Cold Start)”**라는 말은, 무언가를 시작할 때 **기존의 데이터나 사전 학습**이 거의 없이 처음부터(혹은 ‘아주 작은 정보’만을 가지고) 시작하는 상황을 의미합니다. 예컨대, 추천 시스템에서 사용자 로그가 쌓여 있지 않아 “신규 사용자에게 어떤 콘텐츠를 보여줘야 할지 모르는” 상태를 콜드 스타트라고 부르기도 합니다.

## LLM(대형 언어 모델) 맥락에서의 콜드 스타트

LLM 학습 파이프라인에서도 **콜드 스타트**라는 용어를 비슷한 맥락으로 사용합니다.

- **본연의 의미:**
  - “아직 해당 모델이 특정 영역(예: 수학 풀이, 코딩, 특정 언어)에 대해 잘 학습되지 않은 상태에서, 갑자기 RL(강화 학습)이나 추가 미세 조정을 적용하려 할 때”
  - 혹은 “학습에 활용할 수 있는 적절한 지도(supervised) 데이터가 매우 부족한 상태”
  - 등을 “콜드 스타트”라고 부릅니다.
- **문제점:**  
  콜드 스타트 상태에서는 모델이 최소한의 방향성도 갖지 못한 채, RL을 수행하거나 특정 태스크에 대응하려고 하다 보니 **성능이 들쭉날쭉**, **학습 안정성 불안정**, **가독성 떨어지는 출력** 등이 발생할 가능성이 높습니다.

따라서,

1. **충분한 예시 데이터**가 없으면, 모델이 문맥이나 형식을 이해하지 못해 어색한 답변을 하거나,
2. **어떻게 문제를 풀어야 하는지** 기초적인 힌트를 전혀 얻지 못한 채 강화 학습을 수행하다가 학습 난조를 겪을 수 있습니다.

## “콜드 스타트”를 풀어내는 전략

위 문제를 완화하기 위해, “소량이지만 핵심이 되는 고품질 데이터”로 모델을 먼저 미세 조정(초기 세팅)하는 방법을 들 수 있습니다. 이를 흔히 **콜드 스타트 데이터(Cold-start data)**라고 부릅니다. 예시:

1. **문제 해결 예시(Chain of Thought) 몇 개라도 미리 제시**
   - 모델이 어떤 형식으로 답변해야 하고(예: 수학 풀이 과정을 적절히 설명),
   - 어떻게 출력을 구조화해야 하는지(예: <think>추론 과정</think><answer>최종 답</answer> 형태)  
     를 미리 “예시”로 보여주면, 모델이 RL이나 추가 학습을 진행할 때 훨씬 안정적으로 출력을 생성할 수 있습니다.
2. **가독성 좋고, 깨끗한 형식의 예시**
   - 예를 들어, 영어로만 작성된 문장 예시 1,000개를 넣어주고 “최종적으로는 이런 식으로 요약을 달아라” 등의 지침을 미세 조정으로 학습시키면,
   - 모델은 아무런 예시 없이 RL을 시도했을 때와 비교해, 훨씬 **가독성 좋고 일관된** 답변을 생성하게 됩니다.

## 간단한 예시 들어보기

### 상황 가정

- 우리가 **“수학 문제를 풀어주는 AI”**를 만들고자 한다고 하겠습니다.
- 이미 거대한 언어 모델(예: GPT 계열, Llama 계열 등)이 **기본 문장 이해나 생성**은 할 수 있지만, **수학적 해법 과정을 단계별로 기술하는** 능력은 부족하다고 칩시다.
- 여기서 “강화 학습(RL)”으로 모델을 수학 문제 해결에 특화하고 싶을 때, 콜드 스타트 문제를 겪을 수 있습니다.

### (A) 콜드 스타트 없이 진행한다고 가정

1. **학습 초기**에 바로 RL을 적용하면, 모델은 “어떤 형식으로 풀이 과정을 작성해야 하는지”조차 불분명합니다.
2. 결과적으로 **엉뚱한 수학식**, **언어 혼합**, **수학 문법 오류**, **가독성 최악** 등의 문제를 지닌 출력이 다수 생성됩니다.
3. RL 보상(정답률 등)을 주긴 하지만, 초기 모델의 상태가 너무 불안정해 학습이 효율적으로 진행되지 않을 수 있습니다.

### (B) 소량의 콜드 스타트 데이터로 미리 미세 조정

1. **콜드 스타트 데이터 예시**:
   - 고등학교 수학 문제 50개 정도를 골라, 풀이 과정을 단계적으로 적은 답안(Chain of Thought)을 아주 명료하게 작성합니다.
   - 예: “1단계에서는 식을 전개한다. 2단계에서는 인수분해한다. 마지막에 정리하면 답이 x=3이다.” 같은 식으로, 사람이 보기에도 깔끔한 형식.
2. **이 데이터로 모델을 미세 조정**(SFT)하면, 모델은 “아, 이런 식으로 풀이라는 걸 쓰고, 마지막에는 정답을 명시해야 하는구나”라는 패턴을 익히게 됩니다.
3. 이후 **RL을 적용**하면, 이미 어느 정도 수학 풀이 형식을 익힌 상태에서 보상(정답률 등)에 따라 한층 더 효율적이고 안정적으로 학습합니다.

## 정리

- “콜드 스타트”란, **아무런 사전 정보나 지침 없이** 모델(또는 시스템)이 바로 학습이나 서비스를 시작해 **초기에 어려움을 겪는 상황**을 뜻합니다.
- 이를 보완하기 위해 “콜드 스타트 데이터”라는 **소량의 고품질 예시**를 준비하여, 모델을 미리 **기본 형식이나 패턴에 익숙**하게 만들어놓으면,
  - 이후에 강화 학습(RL)이나 기타 학습 기법을 적용할 때 **훨씬 빠르게 수렴**하고,
  - **가독성**이나 **출력 품질**도 높아지는 효과가 있습니다.

이처럼 콜드 스타트 데이터를 통해 모델이 기본 방향성을 잡고, 불필요한 시행착오를 줄여 더 빠르게 좋은 성능에 도달할 수 있도록 돕는 것이 **“콜드 스타트를 해결한다”**는 핵심 개념입니다.
---

### 2.3.2 추론 중심 강화 학습(Reasoning-oriented Reinforcement Learning)

콜드 스타트 데이터를 이용해 DeepSeek-V3-Base를 미세 조정한 이후, DeepSeek-R1-Zero에서와 동일한 대규모 강화 학습(RL) 과정을 적용합니다. 이 단계는 코딩, 수학, 과학, 논리 추론처럼 정답이 명확히 정의되어 있는 문제 영역에서 모델의 추론 능력을 향상하는 데 초점을 맞춥니다.

훈련 과정 중, CoT가 여러 언어를 혼용하는 문제가 특히 다언어 프롬프트로 RL을 진행할 때 자주 관찰되었습니다. 이를 완화하기 위해, 우리는 RL 훈련 시 ‘언어 일관성 보상(language consistency reward)’을 추가 도입했습니다. 이는 CoT에서 목표 언어 단어가 차지하는 비율을 기준으로 계산됩니다. 소규모 단절 실험(ablation study)에 따르면 이 정렬 작업(alignment)으로 인해 모델 성능이 소폭 저하되기도 하지만, 사람이 읽기 더 편한 결과물을 제공한다는 점에서 가치가 있습니다. 결국, 우리는 추론 과제의 정확도와 언어 일관성 보상을 단순 합산하여 최종 보상을 구성하고, 모델이 추론 작업에서 수렴할 때까지 미세 조정된 모델에 대해 RL을 적용했습니다.

### 2.3.3 리젝션 샘플링(Rejection Sampling)과 지도 학습(Supervised Fine-Tuning)

추론 중심 RL이 수렴하면, 해당 체크포인트를 사용해 다음 라운드의 지도 학습(SFT) 데이터를 수집합니다. 초기 콜드 스타트 데이터는 주로 추론에 집중되어 있었지만, 이번 단계에서는 작문, 롤플레이(role-playing) 같은 일반 목적의 작업에 관한 데이터도 포함하여 모델의 전반적 역량을 강화합니다. 구체적으로, 데이터 생성 및 모델 미세 조정 과정은 아래와 같습니다.

1. **추론 데이터**  
   우리는 추론 프롬프트를 큐레이팅(curate)하고, 위 단계에서 학습된 RL 체크포인트로부터 리젝션 샘플링을 수행해 추론 과정을 생성합니다. 이전 단계에서는 규칙 기반 보상(rule-based rewards)으로 평가할 수 있는 데이터만 포함했으나, 이번 단계에서는 추가적인 데이터도 확장하여 포함합니다. 일부 데이터에서는 DeepSeek-V3에 정답(ground-truth)과 모델 예측 결과를 입력해 판단하는 생성형 보상 모델(generative reward model)을 사용합니다. 또한 모델 출력이 뒤섞이거나 가독성이 낮은 경우를 걸러내기 위해, 언어가 혼합된 긴 단락이나 코드 블록을 포함한 CoT는 필터링합니다. 각 프롬프트마다 여러 응답을 샘플링한 뒤, 정답만 남겨둡니다. 최종적으로 약 60만(600k) 개의 추론 관련 학습 샘플을 모았습니다.
2. **비추론(Non-Reasoning) 데이터**  
   작문, 사실 기반 QA, 자기 인식(self-cognition), 번역 등 비추론형 데이터의 경우, DeepSeek-V3 파이프라인을 재사용하며 DeepSeek-V3의 지도 학습 데이터 일부를 활용합니다. 특정 비추론 작업의 경우, DeepSeek-V3를 호출해 답변 전에 잠재적 CoT를 생성하도록 프롬프트를 작성하지만, “hello” 같은 간단 질의에는 CoT를 제공하지 않습니다. 결과적으로, 약 20만(200k) 개의 추론과 무관한 학습 샘플을 확보했습니다.

이후, 위에서 큐레이팅한 총 80만(800k) 개의 샘플로 DeepSeek-V3-Base를 두 번(epoch) 학습(미세 조정)합니다.

### 2.3.4 모든 시나리오를 아우르는 강화 학습(Reinforcement Learning for all Scenarios)

마지막으로, 모델을 인간 선호(human preference)에 더욱 부합하도록 만들고, 동시에 추론 능력을 정교화하기 위해 두 번째 강화 학습 단계를 도입합니다. 구체적으로, 보상 신호와 다양한 프롬프트 분포를 혼합해 모델을 훈련합니다.

- **추론 데이터:** DeepSeek-R1-Zero에서 사용한 것과 동일한 규칙 기반 보상 방식을 유지해, 수학, 코딩, 논리 추론 등 분야에서 학습을 진행합니다.
- **일반 데이터:** 보다 복잡하고 미묘한 시나리오에서도 인간의 선호를 반영할 수 있도록 보상 모델(reward model)을 사용합니다. DeepSeek-V3 파이프라인을 기반으로, 유사한 분포의 선호 쌍(preference pairs)과 훈련용 프롬프트를 적용합니다.

도움(Helpfulness)을 높이기 위해서는 주로 최종 요약(summary)만을 평가 대상으로 삼아 사용자의 요구에 얼마나 유용하고 적합한 답변인지를 강조하며, 추론 과정에는 최소한의 간섭만 가합니다. 반면, 무해성(Harmlessness)을 확보하기 위해서는 모델이 생성하는 전체 응답(추론 과정 + 요약)을 평가해 잠재적인 위험, 편향, 유해한 콘텐츠 발생 가능성을 판단하고 완화합니다. 결국, 이러한 보상 신호와 다양한 데이터 분포를 통합함으로써, 우리는 추론 역량이 뛰어나면서도 사용자가 선호하는 방향으로 유용하고 안전한 모델을 학습할 수 있게 됩니다.

**2.4 증류(Distillation): 작은 모델에 추론 능력을 부여하기**

DeepSeek-R1과 유사한 추론 능력을 보다 효율적인 소형 모델에 적용하기 위해, 우리는 §2.3.3에서 소개한 약 80만(800k) 개의 샘플로 구성된 DeepSeek-R1의 데이터셋을 활용하여, Qwen(Qwen, 2024b)과 Llama(AI@Meta, 2024) 같은 오픈 소스 모델들을 직접 미세 조정했습니다. 실험 결과, 이러한 단순한 증류 방식만으로도 작은 모델의 추론 능력이 크게 향상됨을 확인했습니다. 여기서 사용한 기본(base) 모델은 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, 그리고 Llama-3.3-70B-Instruct입니다. Llama-3.3을 선택한 이유는 Llama-3.1보다 약간 더 나은 추론 능력을 보이기 때문입니다.

증류된 모델의 경우, 지도 학습(SFT)만 적용하며 RL(강화 학습) 단계는 포함하지 않았습니다. RL을 도입한다면 모델 성능이 크게 향상될 수 있지만, 본 연구의 주된 목적은 증류 기법의 효과를 입증하는 것에 있으며, RL 단계에 대한 탐구는 연구 커뮤니티에 맡기기로 했습니다.

**3 실험(Experiment)**

### 벤치마크(Benchmarks)

모델 평가는 MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider[1](#user-content-fn-1), LiveCodeBench (Jain et al., 2024) (2024-08~2025-01), Codeforces[2](#user-content-fn-2), Chinese National High School Mathematics Olympiad (CNMO 2024)[3](#user-content-fn-3), 그리고 American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024)에서 수행했습니다.

표준 벤치마크 외에도, LLM을 평가자로 활용하는 오픈 엔드(open-ended) 생성 과제에서도 모델을 평가했습니다. 구체적으로, AlpacaEval 2.0 (Dubois et al., 2024)과 Arena-Hard (Li et al., 2024)의 원래 설정을 따르며, GPT-4-Turbo-1106을 페어와이즈(pairwise) 비교의 심판으로 활용했습니다. 여기서는 출력 길이에 따른 편향(length bias)을 방지하기 위해 **최종 요약 부분**만 평가에 사용했습니다. 증류된 모델(distilled models)의 경우, AIME 2024, MATH-500, GPQA Diamond, Codeforces, LiveCodeBench에 대한 대표 결과를 보고합니다.

### 평가 프롬프트(Evaluation Prompts)

DeepSeek-V3의 설정을 따르면서, MMLU, DROP, GPQA Diamond, SimpleQA와 같은 표준 벤치마크는 simple-evals 프레임워크의 프롬프트를 사용해 평가했습니다. MMLU-Redux는 Zero-Eval 프롬프트 포맷(Lin, 2024)을 활용하여 제로샷(Zero-shot) 방식으로 진행했습니다. MMLU-Pro, C-Eval, CLUE-WSC는 원본 프롬프트가 몇 샷(few-shot)이므로, 이를 제로샷 형태로 약간 수정했습니다. 이는 few-shot 속의 CoT가 DeepSeek-R1의 성능을 저해할 수 있기 때문입니다. 이 밖의 다른 데이터셋은 제작자들이 제공한 기본 평가 프로토콜과 기본 프롬프트를 그대로 따랐습니다.

코드 및 수학 벤치마크의 경우, HumanEval-Mul 데이터셋은 Python, Java, C++, C#, JavaScript, TypeScript, PHP, Bash 등 8개 주요 프로그래밍 언어를 다룹니다. LiveCodeBench에서는 2024년 8월부터 2025년 1월 사이의 데이터를 사용했으며, CoT 형식으로 모델을 평가했습니다. Codeforces 데이터셋은 10개의 Div.2 대회 문제와 전문가가 작성한 테스트 케이스로 평가하며, 이를 통해 예상 레이팅(rating)과 참가자 대비 백분위를 산출합니다. SWE-Bench의 검증 결과(verified results)는 agentless 프레임워크 (Xia et al., 2024)를 통해 얻었고, AIDER 관련 벤치마크는 “diff” 포맷으로 측정했습니다. DeepSeek-R1의 출력은 각 벤치마크에서 최대 32,768토큰으로 제한합니다.

### 베이스라인(Baselines)

우리는 DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, OpenAI-o1-1217 등 강력한 베이스라인과 포괄적으로 비교 평가했습니다. 중국 본토(Mainland China)에서 OpenAI-o1-1217 API에 접근하기가 어려워, 이 모델의 성능은 공식 문서를 기반으로 보고합니다. 증류 모델(distilled models)에 대해서는 오픈 소스 모델인 QwQ-32B-Preview(Qwen, 2024a)와도 비교했습니다.

### 평가 설정(Evaluation Setup)

모든 모델의 최대 생성 길이를 32,768토큰으로 설정했습니다. 긴 출력의 추론 모델을 그리디(greedy) 디코딩으로 평가하면 반복률이 높아지고 체크포인트별로 편차가 커짐을 확인했습니다. 따라서, 우리는 디코딩 시 **온도(temperature)를 0 이상으로 설정**하고, pass@k 평가(Chen et al., 2021)를 기본 방식으로 적용하여 pass@1을 보고합니다. 구체적으로, **샘플링 온도 0.6**과 **top-p=0.95**를 사용해 질문마다 k개의 응답(테스트 세트 크기에 따라 4~64 사이)을 생성하고, 이를 기반으로 pass@1 점수를 계산합니다. 이때,

![](/assets/images/posts/507/img_17.png)

여기서 p\_i​는 iii번째 응답의 정답 여부를 나타냅니다. 이 방식이 더 신뢰도 높은 성능 추정치를 제공합니다. AIME 2024의 경우, 64개의 샘플을 이용한 다수결(majority voting) 결과(cons@64)도 함께 보고합니다 (Wang et al., 2022).

## Footnotes

1. <https://aider.chat> [↩](#user-content-fnref-1)
2. <https://codeforces.com> [↩](#user-content-fnref-2)
3. <https://www.cms.org.cn/Home/comp/comp/cid/12.html> [↩](#user-content-fnref-3)
---

간단히 말해, “**동일 문제에 대해 여러 번(예: 64번) 샘플링한 뒤, 그중 정답을 맞힌 비율을 측정”**하는 방식입니다. 이를 **Pass@k**(또는 Cons@64처럼 다수결로 점수를 내는 경우)로 표현합니다. 구체적으로:

1. **동일 문제**에 대해 모델이 정답을 생성하도록, 랜덤 샘플링(온도=0.6, top-p=0.95 등)을 여러 번(예: 4~64회) 수행합니다.
2. 이렇게 얻은 여러 개의 답안을 각각 맞았는지(정답 여부)를 평가합니다.
3. pass@1은 “생성된 여러 응답 중에서 **특정 하나(여기서는 첫 번째 샘플)**가 정답일 확률”을 평균해 구합니다.
   - 예를 들어, 동일 문제를 64번 샘플링했을 때, 그중 32번이 정답이었다면 pass@1은 50%가 됩니다.
4. 반면, “다수결(majority voting)”(cons@64) 방식은 64개의 응답 중 가장 많이 나온 답을 최종 정답으로 간주했을 때 정답 여부를 확인합니다.

즉, **64번 중 몇 번 맞추었는가**가 그대로 모델의 추론 성능으로 이어지는 셈이 됩니다. 모델이 무작위 샘플링으로 길고 복잡한 답안을 낼 때, **“운이 좋으면”** 한두 번 정답이 나올 수도 있지만, **“안정적으로”** 여러 번 샘플링했을 때 평균적으로 얼마나 잘 맞추는지를 보는 것이 핵심입니다.
---

**3.1 DeepSeek-R1 평가**

![](/assets/images/posts/507/img_18.png)

**표 4: DeepSeek-R1과 대표 모델 간 비교**

교육 지향적 지식 벤치마크(MMLU, MMLU-Pro, GPQA Diamond 등)에서 DeepSeek-R1은 DeepSeek-V3 대비 우수한 성능을 보였습니다. 특히 STEM 관련 문제에서 대규모 강화 학습을 통해 정확도가 크게 향상되었다는 점이 주요 원인입니다. 또한, 긴 문맥에 의존하는 QA 과제인 FRAMES에서도 DeepSeek-R1은 강력한 문서 분석 능력을 선보였는데, 이는 향후 AI 기반 검색과 데이터 분석 작업에서 추론 모델이 지닌 가능성을 시사합니다. 사실 기반 벤치마크(SimpleQA)에서도 DeepSeek-R1은 DeepSeek-V3를 능가하며, OpenAI-o1이 이 벤치마크에서 GPT-4o를 능가한 것과 비슷한 추세를 보입니다. 반면, 중국어 SimpleQA에서는 안전성(security) 관련 RL 이후 특정 질의에 답변을 거부하는 경향이 나타나 DeepSeek-R1의 점수가 DeepSeek-V3보다 낮아졌습니다. 만약 안전성 RL을 적용하지 않을 경우, DeepSeek-R1은 70% 이상의 정확도를 달성할 수 있었습니다.

DeepSeek-R1은 IF-Eval에서도 인상적인 결과를 보이는데, 이는 최종 지도 학습(SFT) 및 RL 훈련 단계에서의 지시문 준수(instruction-following) 데이터 추가 덕분입니다. 한편, AlpacaEval2.0과 ArenaHard 평가에서도 두드러진 성과가 확인되어, DeepSeek-R1이 작문 및 오픈도메인 질의응답 과제에서 강점을 지님을 알 수 있습니다. DeepSeek-V3 대비 큰 폭의 성능 향상은 대규모 RL이 추론 능력뿐 아니라 다양한 도메인 전반에서의 성능도 높이는 데 기여함을 보여줍니다. 또한 DeepSeek-R1이 생성하는 요약은 상대적으로 짧으며(ArenaHard 평균 689 토큰, AlpacaEval 2.0 평균 2,218자), 이는 GPT 기반 평가에서 발생하는 길이 편향(length bias)을 피할 수 있음을 시사합니다.

수학 과제에서 DeepSeek-R1은 OpenAI-o1-1217과 견줄 만한 성능을 보이면서, 다른 모델보다 훨씬 앞선 결과를 거둡니다. 코딩 알고리즘 과제(예: LiveCodeBench, Codeforces)에서도 추론에 특화된 모델이 뛰어난 성능을 보여주는 추세가 동일하게 확인됩니다. 한편, 엔지니어링 지향 코딩 과제(Aider)에서는 OpenAI-o1-1217이 DeepSeek-R1보다 우수한 성능을 보이나, SWE Verified에서는 대체로 유사한 성능을 보였습니다. 관련 강화 학습 데이터가 아직 충분하지 않으므로, 다음 버전에서 DeepSeek-R1의 엔지니어링 성능이 한층 향상될 것으로 기대합니다.
---

**STEM은** Science(과학), Technology(기술), Engineering(공학), Mathematics(수학)의 앞 글자를 딴 약어입니다.

- **STEM 관련 문제**란, 일반적으로 수학, 물리, 화학, 생물, 컴퓨터 알고리즘, 공학 설계 등과 같이 과학·기술·공학·수학 분야에 속하는 문제를 뜻합니다. 예컨대, 수학적 계산 문제나 프로그래밍 알고리즘 문제, 물리 공식 응용 문제 등이 이에 해당합니다.
- 연구 논문에서 “STEM 관련 문제에서 모델의 성능이 개선되었다”라고 하면, 모델이 이러한 **과학·기술·공학·수학 분야의 문제 해결**에서 더 높은 정확도나 추론 능력을 보여주었다는 의미입니다.
---

**3.2 증류된 모델 평가(Distilled Model Evaluation)**

![](/assets/images/posts/507/img_19.png)

**표 5:** DeepSeek-R1에서 증류된 모델들과 다른 비교 모델들을 대상으로 한 추론 관련 벤치마크 결과 비교.

표 5에서 볼 수 있듯, DeepSeek-R1의 출력을 단순히 증류하는 것만으로도, 효율적 구조의 DeepSeek-R1-7B(즉, DeepSeek-R1-Distill-Qwen-7B)는 전반적으로 GPT-4o-0513 같은 비추론형 모델을 능가합니다. 또한 DeepSeek-R1-14B는 모든 평가 지표에서 QwQ-32B-Preview를 상회하며, DeepSeek-R1-32B와 DeepSeek-R1-70B는 대부분의 벤치마크에서 o1-mini보다 훨씬 높은 점수를 기록합니다. 이 결과들은 증류(distillation)의 높은 가능성을 보여줍니다. 아울러, 증류된 모델들에 RL을 추가로 적용하면 성능이 더욱 크게 향상된다는 사실도 발견했습니다. 이러한 점에 대해서는 연구 커뮤니티가 추가로 탐구할 여지를 남기기 위해, 본 논문에서는 간단히 SFT만 적용한 증류 모델들의 결과만을 보고합니다.

**4 토의(Discussion)**

**4.1 증류(Distillation) vs. 강화 학습(Reinforcement Learning)**

![](/assets/images/posts/507/img_20.png)

**표 6:** 추론 관련 벤치마크에서 증류 모델과 RL 모델의 성능 비교.

섹션 3.2에서 확인했듯, DeepSeek-R1을 증류함으로써 소형 모델도 인상적인 성능을 달성할 수 있습니다. 그러나 “본 논문에서 제시한 대규모 RL 방식을 직접 적용하여 증류 없이도 유사한 성능을 낼 수 있을까?”라는 의문이 남아 있습니다.

이를 알아보기 위해, 우리는 Qwen-32B-Base 모델에 수학, 코딩, STEM 데이터를 활용해 1만 스텝(10K steps) 이상 대규모 RL 훈련을 수행했으며, 그 결과물이 DeepSeek-R1-Zero-Qwen-32B입니다. **표 6**에 제시된 실험 결과를 보면, 대규모 RL 훈련을 거친 32B 기반 모델(DeepSeek-R1-Zero-Qwen-32B)은 QwQ-32B-Preview와 유사한 수준의 성능을 달성합니다. 그러나 DeepSeek-R1에서 증류한 DeepSeek-R1-Distill-Qwen-32B는 모든 벤치마크에서 DeepSeek-R1-Zero-Qwen-32B보다 훨씬 뛰어난 성능을 보였습니다.

결국 다음과 같은 두 가지 결론을 도출할 수 있습니다.

1. **더 강력한 모델을 작은 모델로 증류**하는 방식은 매우 우수한 결과를 낸다. 반면, 이 논문에서 다룬 대규모 RL만으로 작은 모델을 직접 학습하는 것은 막대한 계산 비용이 들고, 성능 또한 증류 방식에 미치지 못할 수 있다.
2. **증류 전략은 경제적이며 효과적**이지만, 지능의 한계를 뛰어넘기 위해서는 여전히 **더 강력한 기본(base) 모델**과 **더 대규모의 강화 학습**이 필요할 가능성이 높다.

**4.2 미완의 시도(Unsuccessful Attempts)**

### 프로세스 보상 모델(Process Reward Model, PRM)

PRM은 모델이 추론 작업에서 더 나은 접근 방식을 학습하도록 유도하는 타당한 방법으로 알려져 있습니다 (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023). 그러나 실제 적용 과정에서 PRM은 다음과 같은 세 가지 주요 한계로 인해 만족스러운 성과를 거두기 어려웠습니다.

1. **미세 단계 정의의 어려움**  
   일반 추론에서 “각 중간 단계를 명시적으로 정의”하기가 쉽지 않습니다. 즉, 추론 과정을 세밀한 단계로 나누고 각각의 올바름을 확인하기가 매우 복잡합니다.
2. **중간 단계 정·오 판별의 어려움**  
   현재 중간 단계가 맞는지 판별하는 작업은 도전적입니다. 모델을 활용해 자동으로 주석(annotate)하는 시도도 만족스러운 결과를 내지 못했고, 사람을 활용한 수작업은 규모 확장에 적합하지 않습니다.
3. **보상 해킹(Reward Hacking) 및 복잡성 증가**  
   모델 기반 PRM을 도입하면 보상 해킹(Gao et al., 2022)을 피하기 어렵고, 보상 모델을 재학습하는 데 추가 자원이 필요해 전체 학습 파이프라인을 복잡하게 만듭니다.

결론적으로, PRM은 모델이 생성한 상위 N개의 응답을 재랭킹(rerank)하거나, 제한된 상황에서 탐색을 유도(guided search)하는 데는 도움이 될 수 있습니다(Snell et al., 2024). 그러나 대규모 강화 학습 과정에서 발생하는 **추가 계산 비용**을 감안하면, 실험에서는 그 이점이 한정적이었습니다.

### 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)

AlphaGo(Silver et al., 2017b)와 AlphaZero(Silver et al., 2017a)에서 영감을 받아, 우리는 추론 시 확장 가능한 계산을 활용하기 위해 몬테카를로 트리 탐색(MCTS)을 시도했습니다. 이는 답안을 여러 작은 부분으로 나누어, 모델이 솔루션 공간을 체계적으로 탐색할 수 있게끔 하는 방법입니다. 이를 위해, 모델을 프롬프트로 유도하여 특정 추론 단계별로 여러 태그를 생성하게 하고, 그 태그들이 탐색에 필요한 단계들을 나타내도록 설정했습니다.

- **훈련 과정:**
  1. 먼저 수집한 프롬프트를 활용해, 사전 학습된 값(value) 모델의 지도를 받아 MCTS를 사용하여 답안을 찾습니다.
  2. 이후 이로부터 얻은 질의-응답 쌍(question-answer pairs)을 다시 배우(actor) 모델과 값(value) 모델 모두에 학습시켜, 과정을 반복적으로 정련(refine)합니다.

그러나 이 접근 방식은 학습 규모를 키우는 과정에서 여러 문제에 부딪혔습니다.

1. **거대한 탐색 공간**  
   체스와 달리, **토큰 생성**은 훨씬 더 큰 탐색 공간을 갖습니다. 이를 완화하기 위해 각 노드의 최대 확장을 제한했지만, 그 결과 모델이 지역 최적점(local optima)에 머무를 가능성이 커집니다.
2. **값(value) 모델의 한계**  
   값 모델은 탐색 과정의 각 단계를 안내하므로, 생성 품질에 직접적인 영향을 미칩니다. 그러나 매우 미세한 단계까지 판단해야 하는 값 모델을 학습하기는 inherently 어렵습니다. 이는 모델이 반복적으로 스스로를 개선해 나가는 과정을 더욱 까다롭게 만듭니다. AlphaGo가 성능을 단계적으로 높일 수 있었던 핵심은 **값 모델을 지속적으로 학습**하는 데 있었으나, 토큰 생성의 복잡성으로 인해 동일한 접근 방식을 이 구조에 적용하기 어렵습니다.

결론적으로, 사전 학습된 값 모델과 결합하여 추론 시점(inference-time)의 성능을 높이는 데는 MCTS가 유효할 수 있으나, **자기 탐색(self-search)을 통해 모델 성능을 반복적으로 끌어올리는** 것은 여전히 큰 도전으로 남아 있습니다.

**5 결론, 한계, 그리고 향후 과제**

본 연구에서는 강화 학습을 통해 모델의 추론 능력을 향상시키는 과정을 공유했습니다. **DeepSeek-R1-Zero**는 콜드 스타트 데이터를 전혀 사용하지 않는 순수 RL 접근 방식으로, 다양한 작업에서 우수한 성능을 달성했습니다. 반면, **DeepSeek-R1**은 콜드 스타트 데이터를 활용하여 반복적(iterative) RL 미세 조정을 수행함으로써 더 강력한 성능을 확보했으며, 결과적으로 다양한 작업에서 OpenAI-o1-1217와 견줄 만한 결과를 보였습니다.

또한, 우리는 추론 능력을 더 작은 밀집 모델에도 적용하기 위해, DeepSeek-R1을 교사 모델(teacher model)로 활용하여 80만(800K) 개의 학습 샘플을 생성한 뒤 여러 소형 밀집 모델들을 미세 조정했습니다. 그 결과, 예를 들어 **DeepSeek-R1-Distill-Qwen-1.5B**는 AIME에서 28.9%, MATH에서 83.9%를 기록하며 GPT-4o와 Claude-3.5-Sonnet 모델을 능가하는 등 유의미한 성능 향상을 보였습니다. 같은 체크포인트를 활용한 다른 인스트럭션 기반 모델에 비해서도 현저히 우수한 결과가 확인되었습니다.

앞으로 우리는 DeepSeek-R1을 다음과 같은 방향으로 발전시키는 데 주력할 예정입니다.

- **일반적 능력(General Capability):**  
  현재 DeepSeek-R1의 일부 기능(예: 함수 호출, 다중 턴 대화, 복잡한 롤플레이, JSON 출력 등)은 DeepSeek-V3에 비해 부족합니다. 이에 따라 긴 CoT를 활용하여 이러한 작업 영역에서 모델을 어떻게 강화할 수 있을지 모색할 계획입니다.
- **언어 혼합(Language Mixing):**  
  DeepSeek-R1은 중국어와 영어를 중심으로 최적화되어 있기 때문에, 다른 언어로 된 질의 처리 시 언어가 섞여 나오는 문제가 발생할 수 있습니다. 예컨대, 영어가 아닌 언어로 된 질의라도 모델이 추론과 응답을 영어로 하는 경우가 있습니다. 추후 업데이트에서는 이러한 제약을 해결하고자 합니다.
- **프롬프트 엔지니어링(Prompting Engineering):**  
  DeepSeek-R1을 평가하는 과정에서, 모델이 프롬프트(prompt)에 민감하다는 점을 확인했습니다. 특히 few-shot 프롬프트를 사용하면 성능이 지속적으로 저하되는 양상을 보였습니다. 따라서 사용자들이 가장 좋은 결과를 얻기 위해서는 **제로샷(zero-shot) 방식으로 문제를 기술하고 출력 형식을 명확히 지정하는** 것을 권장합니다.
- **소프트웨어 엔지니어링 작업(Software Engineering Tasks):**  
  긴 평가 시간 때문에 RL 프로세스의 효율이 저하되는 문제로 인해, 현재 대규모 RL은 소프트웨어 엔지니어링 분야에 충분히 적용되지 못했습니다. 그 결과 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3 대비 큰 개선폭을 보이지 못했습니다. 향후 버전에서는 소프트웨어 엔지니어링 데이터를 대상으로 리젝션 샘플링을 적용하거나, 비동기(asynchronous) 평가 방식을 RL 과정에 도입해 효율성을 높임으로써 이 문제를 해결할 예정입니다.
