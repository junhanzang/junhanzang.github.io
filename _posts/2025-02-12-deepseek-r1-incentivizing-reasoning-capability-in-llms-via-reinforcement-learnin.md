---
title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
date: 2025-02-12 13:10:40
categories:
  - 인공지능
tags:
  - deepseek-r1
---

<https://arxiv.org/abs/2501.12948>

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

초록  
우리의 첫 번째 세대 추론 모델인 DeepSeek-R1-Zero와 DeepSeek-R1을 소개합니다. DeepSeek-R1-Zero는 사전 단계로 지도 학습 미세 조정(SFT) 없이 대규모 강화 학습(RL)을 통해 학습된 모델로, 뛰어난 추론 능력을 보여줍니다. 강화 학습을 통해 DeepSeek-R1-Zero는 강력하고 흥미로운 여러 추론 행동을 자연스럽게 나타내게 되었습니다. 그러나 가독성 문제와 언어 혼합 등 몇 가지 문제에 직면하게 됩니다. 이러한 문제를 해결하고 추론 성능을 더욱 향상시키기 위해, 우리는 RL 전에 다단계 학습과 초기 데이터(cold-start data)를 통합한 DeepSeek-R1을 도입합니다. DeepSeek-R1은 추론 작업에서 OpenAI-o1-1217과 견줄 만한 성능을 달성합니다. 연구 커뮤니티를 지원하기 위해, Qwen과 Llama를 기반으로 DeepSeek-R1에서 증류된 DeepSeek-R1-Zero, DeepSeek-R1 및 6개의 밀집 모델(1.5B, 7B, 8B, 14B, 32B, 70B)을 오픈 소스로 공개합니다.

![](/assets/images/posts/507/img.png)

그림 1: DeepSeek-R1의 벤치마크 성능.

![](/assets/images/posts/507/img_1.png)

1.서론

최근 몇 년간, 대형 언어 모델(LLMs)은 빠른 속도로 반복 발전과 진화를 거듭해 왔으며 (OpenAI, 2024a; Anthropic, 2024; Google, 2024) 인공지능의 일반 지능(AGI)과의 격차를 점진적으로 줄여가고 있습니다.

최근에는 포스트 트레이닝이 전체 학습 파이프라인의 중요한 구성 요소로 등장했습니다. 이는 추론 작업의 정확도를 향상시키고 사회적 가치와 부합하며, 사용자 선호에 적응하는 데 도움을 주는 것으로 나타났고, 사전 학습에 비해 상대적으로 적은 계산 자원을 요구합니다. 추론 능력 측면에서, OpenAI의 o1 시리즈 모델(OpenAI, 2024b)은 추론 과정의 체인 오브 쏘트(Chain-of-Thought, CoT)의 길이를 늘려 추론 시 확장(inference-time scaling)을 도입한 최초의 모델이었습니다. 이 접근 방식은 수학, 코딩, 과학적 추론 등 다양한 추론 작업에서 상당한 개선을 이루어냈습니다. 그러나 효과적인 테스트 시 확장(test-time scaling)의 문제는 여전히 연구 커뮤니티에 남은 미해결 과제입니다. 기존 여러 연구에서는 과정 기반 보상 모델(Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023), 강화 학습(Kumar et al., 2024), 그리고 몬테카를로 트리 서치 및 빔 서치(Feng et al., 2024; Xin et al., 2024; Trinh et al., 2024)와 같은 다양한 접근 방식을 탐구했지만, 이들 방법 중 어느 것도 OpenAI의 o1 시리즈 모델에 버금가는 일반 추론 성능을 달성하지는 못했습니다.

본 논문에서는 순수 강화 학습(RL)을 사용하여 언어 모델의 추론 능력을 향상시키기 위한 첫걸음을 내딛습니다. 우리의 목표는 어떠한 지도 학습 데이터도 없이 LLM이 순수 RL 프로세스를 통해 자가 진화(self-evolution)하여 추론 능력을 개발할 수 있는 잠재력을 탐구하는 것입니다. 구체적으로, 우리는 기본 모델로 DeepSeek-V3-Base를 사용하고, 추론 성능 향상을 위해 RL 프레임워크로 GRPO(Shao et al., 2024)를 채택했습니다. 훈련 과정 중에 DeepSeek-R1-Zero는 강력하고 흥미로운 여러 추론 행동들을 자연스럽게 드러냈으며, 수천 번의 RL 단계를 거친 후 추론 벤치마크에서 탁월한 성능을 보였습니다. 예를 들어, AIME 2024에서의 pass@1 점수는 15.6%에서 71.0%로 상승하였고, 다수결 투표를 적용할 경우 점수는 86.7%까지 개선되어 OpenAI-o1-0912의 성능과 맞먹게 되었습니다.

그러나 DeepSeek-R1-Zero는 가독성 문제와 언어 혼합과 같은 도전에 직면하게 됩니다. 이러한 문제들을 해결하고 추론 성능을 더욱 향상시키기 위해, 소량의 콜드 스타트 데이터와 다단계 학습 파이프라인을 통합한 DeepSeek-R1을 도입합니다. 구체적으로, 우리는 DeepSeek-V3-Base 모델을 미세 조정하기 위해 수천 건의 콜드 스타트 데이터를 수집하는 것으로 시작한 후, DeepSeek-R1-Zero와 유사한 추론 지향 RL을 수행합니다. RL 과정이 수렴에 가까워지면, RL 체크포인트에 대한 리젝션 샘플링을 통해 새로운 SFT 데이터를 생성하고, 작문, 사실 기반 질의응답, 자기 인식 등의 도메인에서 DeepSeek-V3의 지도 학습 데이터를 결합하여 DeepSeek-V3-Base 모델을 재학습시킵니다. 그 후, 새로운 데이터로 미세 조정된 체크포인트는 모든 시나리오의 프롬프트를 고려하는 추가 RL 과정을 거치게 됩니다. 이러한 일련의 단계를 통해 우리는 OpenAI-o1-1217과 버금가는 성능을 달성하는 DeepSeek-R1이라는 체크포인트를 얻었습니다.

나아가, DeepSeek-R1에서 더 작은 밀집 모델들로의 증류(distillation)를 추가로 탐구합니다. 기본 모델로 Qwen2.5-32B(Qwen, 2024b)를 사용한 직접 증류가 RL을 적용하는 것보다 우수한 성능을 보임을 확인할 수 있었으며, 이는 더 큰 기본 모델들이 발견한 추론 패턴이 추론 능력 향상에 결정적임을 보여줍니다. 이에 따라, 증류된 Qwen 및 Llama(Dubey et al., 2024) 시리즈를 오픈 소스로 공개합니다. 특히, 우리의 증류된 14B 모델은 최신 오픈 소스 QwQ-32B-Preview(Qwen, 2024a)를 큰 차이로 능가하며, 증류된 32B 및 70B 모델은 밀집 모델들 사이에서 추론 벤치마크의 새로운 기록을 세웠습니다.

1.1 기여

**포스트 트레이닝: 기본 모델에 대한 대규모 강화 학습**  
• 우리는 지도 학습 미세 조정(SFT)을 사전 단계로 사용하지 않고 기본 모델에 직접 RL을 적용합니다. 이 접근 방식은 모델이 복잡한 문제 해결을 위한 사고의 연쇄(Chain-of-Thought, CoT)를 자유롭게 탐색할 수 있게 하여 DeepSeek-R1-Zero의 개발로 이어졌습니다. DeepSeek-R1-Zero는 자기 검증, 반성, 긴 CoT 생성 등의 능력을 보여주며, 이는 연구 커뮤니티에 중요한 이정표가 됩니다. 특히, 이는 LLM의 추론 능력이 SFT 없이 순수 RL만으로 촉진될 수 있음을 검증한 최초의 오픈 연구로, 향후 이 분야의 발전에 길을 열어줍니다.  
• 우리는 DeepSeek-R1을 개발하기 위한 파이프라인을 소개합니다. 이 파이프라인은 향상된 추론 패턴을 발견하고 인간의 선호에 부합하도록 하는 두 단계의 RL 과정과, 모델의 추론 및 비추론 능력의 씨앗 역할을 하는 두 단계의 SFT 과정을 통합하고 있습니다. 이 파이프라인은 더 나은 모델을 창출함으로써 산업 전반에 긍정적인 영향을 미칠 것으로 기대됩니다.

**증류: 작은 모델도 강력할 수 있다**  
• 우리는 큰 모델에서 발견된 추론 패턴이 작은 모델로 증류될 수 있음을 증명하였으며, 이는 작은 모델에서 RL을 통해 발견된 추론 패턴보다 더 나은 성능을 보입니다. 오픈 소스 DeepSeek-R1과 그 API는 향후 연구 커뮤니티가 더 나은 작은 모델을 증류하는 데 도움을 줄 것입니다.  
• DeepSeek-R1이 생성한 추론 데이터를 활용하여, 연구 커뮤니티에서 널리 사용되는 여러 밀집 모델들을 미세 조정하였습니다. 평가 결과, 증류된 작은 밀집 모델들이 벤치마크에서 매우 우수한 성능을 발휘함을 확인할 수 있었습니다. 예를 들어, DeepSeek-R1-Distill-Qwen-7B는 AIME 2024에서 55.5%를 달성하여 QwQ-32B-Preview를 능가하였으며, DeepSeek-R1-Distill-Qwen-32B는 AIME 2024에서 72.6%, MATH-500에서 94.3%, LiveCodeBench에서 57.2%를 기록하였습니다. 이러한 결과는 기존 오픈 소스 모델들을 크게 능가하며 o1-mini와 견줄 만한 성능을 나타냅니다. 우리는 Qwen2.5 및 Llama3 시리즈 기반의 증류된 1.5B, 7B, 8B, 14B, 32B, 70B 체크포인트를 커뮤니티에 오픈 소스로 공개합니다.

1.2 평가 결과 요약

• **추론 작업:**  
(1) DeepSeek-R1은 AIME 2024에서 79.8%의 Pass@1 점수를 달성하여 OpenAI-o1-1217을 약간 능가합니다. MATH-500에서는 97.3%라는 인상적인 점수를 기록하며 OpenAI-o1-1217과 동등한 성능을 보이고, 다른 모델들을 크게 능가합니다.  
(2) 코딩 관련 작업에서는 DeepSeek-R1이 코드 대회 작업에서 전문가 수준의 성능을 보이며, Codeforces에서 2,029 Elo 등급을 달성하여 대회 참가자 중 96.3%의 인간 참가자를 능가합니다. 엔지니어링 관련 작업에서는 DeepSeek-R1이 DeepSeek-V3보다 약간 우수한 성능을 보여, 실제 작업에서 개발자들에게 도움을 줄 수 있습니다.

• **지식:**  
MMLU, MMLU-Pro, GPQA Diamond 등의 벤치마크에서 DeepSeek-R1은 MMLU에서 90.8%, MMLU-Pro에서 84.0%, GPQA Diamond에서 71.5%의 점수를 기록하며 DeepSeek-V3를 크게 능가하는 뛰어난 결과를 보였습니다. 비록 이들 벤치마크에서의 성능이 OpenAI-o1-1217보다는 약간 낮지만, DeepSeek-R1은 다른 비공개 모델들을 능가하여 교육 관련 작업에서 경쟁력을 입증합니다. 사실 기반 벤치마크인 SimpleQA에서는 DeepSeek-R1이 DeepSeek-V3를 능가하여 사실 기반 질의를 효과적으로 처리하는 능력을 보여주며, 유사하게 OpenAI-o1이 이 벤치마크에서 4o를 능가하는 경향도 관찰됩니다.

• **기타:**  
DeepSeek-R1은 창의적 글쓰기, 일반 질의 응답, 편집, 요약 등 다양한 작업에서도 우수한 성능을 발휘합니다. AlpacaEval 2.0에서는 길이 제어(win-rate) 87.6%, ArenaHard에서는 92.3%의 승률을 기록하여, 시험 이외의 질의를 지능적으로 처리하는 강력한 능력을 보여줍니다. 또한, 긴 문맥 이해가 필요한 작업에서도 DeepSeek-R1은 DeepSeek-V3를 크게 능가하는 탁월한 성능을 나타냅니다.

2 접근 방법  
2.1 개요  
이전 연구들은 모델 성능을 향상시키기 위해 대량의 지도 학습 데이터에 크게 의존해왔습니다. 본 연구에서는 지도 미세 조정(SFT)을 초기 단계로 사용하지 않고도 대규모 강화 학습(RL)을 통해 추론 능력을 상당히 향상시킬 수 있음을 보여줍니다. 또한, 소량의 콜드 스타트 데이터를 포함하면 성능을 더욱 개선할 수 있습니다. 다음 섹션에서는 (1) 지도 학습 데이터 없이 기본 모델에 직접 RL을 적용한 DeepSeek-R1-Zero, (2) 수천 개의 긴 체인 오브 쏘트(Chain-of-Thought, CoT) 예제로 미세 조정된 체크포인트에서 시작하여 RL을 적용한 DeepSeek-R1, (3) DeepSeek-R1에서 작은 밀집 모델로 추론 능력을 증류하는 방법을 소개합니다.

2.2 DeepSeek-R1-Zero: 기본 모델에 대한 강화 학습  
강화 학습은 우리의 이전 연구들(Wang et al., 2023; Shao et al., 2024)에서 입증된 바와 같이 추론 작업에서 상당한 효과를 보여주었습니다. 그러나 이들 연구는 수집에 많은 시간이 소요되는 지도 데이터를 크게 의존했습니다. 이 섹션에서는 순수 강화 학습 과정을 통한 자가 진화를 중점으로, 지도 데이터 없이 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구합니다. 먼저 우리의 RL 알고리즘에 대한 간략한 개요를 제공한 후, 몇 가지 흥미로운 결과를 제시하며, 이를 통해 연구 커뮤니티에 유용한 인사이트를 제공하고자 합니다.

**2.2.1 강화 학습 알고리즘**

### 그룹 상대 정책 최적화(Group Relative Policy Optimization)

![](/assets/images/posts/507/img_2.png)

![](/assets/images/posts/507/img_3.png)

![](/assets/images/posts/507/img_4.png)

![](/assets/images/posts/507/img_5.png)

---

![](/assets/images/posts/507/img_6.png)

![](/assets/images/posts/507/img_7.png)

![](/assets/images/posts/507/img_8.png)

![](/assets/images/posts/507/img_9.png)

![](/assets/images/posts/507/img_10.png)

![](/assets/images/posts/507/img_11.png)

![](/assets/images/posts/507/img_12.png)

---
