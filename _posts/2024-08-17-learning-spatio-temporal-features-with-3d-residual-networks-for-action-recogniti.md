---
title: "Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition (3D-ResNets)"
date: 2024-08-17 17:37:43
categories:
  - 인공지능
---

<https://github.com/kenshohara/3D-ResNets-PyTorch?tab=readme-ov-file>

[GitHub - kenshohara/3D-ResNets-PyTorch: 3D ResNets for Action Recognition (CVPR 2018)](https://github.com/kenshohara/3D-ResNets-PyTorch?tab=readme-ov-file)

<https://arxiv.org/abs/1708.07632>

[Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition](https://arxiv.org/abs/1708.07632)

**초록**  
공간-시간적 3D 커널을 사용하는 합성곱 신경망(3D CNNs)은 비디오에서 동작 인식을 위한 공간-시간적 특징을 직접 추출할 수 있는 능력을 가지고 있습니다. 그러나 3D 커널은 매개변수의 수가 많아 과적합(overfitting) 되는 경향이 있으며, 최근의 대규모 비디오 데이터베이스를 사용함으로써 3D CNNs의 성능이 크게 향상되었습니다. 하지만 3D CNNs의 구조는 2D 기반 CNNs, 특히 잔차 네트워크(ResNets)와 같은 매우 깊은 신경망의 성공에 비해 상대적으로 얕습니다. 본 논문에서는 더 나은 동작 표현을 위한 ResNets 기반의 3D CNN을 제안합니다. 우리는 3D ResNets의 훈련 절차를 자세히 설명하며, ActivityNet과 Kinetics 데이터셋에서 3D ResNets를 실험적으로 평가합니다. Kinetics에서 훈련된 3D ResNets는 모델의 많은 매개변수에도 불구하고 과적합 문제를 겪지 않았으며, C3D와 같은 상대적으로 얕은 네트워크보다 더 나은 성능을 달성하였습니다. 우리의 코드와 사전 훈련된 모델(예: Kinetics 및 ActivityNet)은 공개적으로 제공되며, <https://github.com/kenshohara/3D-ResNets에서> 확인할 수 있습니다.

**1. 서론**

실제 세계에서 중요한 정보의 유형 중 하나는 인간의 행동입니다. 비디오에서 인간의 행동을 자동으로 인식하고 감지하는 것은 감시 시스템, 비디오 색인화, 인간-컴퓨터 상호작용 등 다양한 응용 분야에서 널리 사용됩니다.

합성곱 신경망(CNNs)은 행동 인식에서 높은 성능을 발휘합니다. 대부분의 CNN은 이미지 인식에 사용되는 CNN과 유사한 2D 합성곱 커널을 사용합니다. RGB와 광학 흐름 스트림으로 구성된 두 스트림 아키텍처는 비디오에서 공간-시간 정보를 나타내는 데 자주 사용됩니다. 두 스트림을 결합하면 행동 인식 성능이 향상됩니다.

또 다른 접근 방식은 2D 커널 대신 공간-시간적 3D 합성곱 커널을 사용하는 것입니다. 3D CNN은 매개변수의 수가 많기 때문에, UCF101과 HMDB51과 같은 상대적으로 작은 비디오 데이터셋에서 훈련하면 성능이 낮아집니다. 이는 대규모 이미지 데이터셋인 ImageNet에서 사전 훈련된 2D CNN과 비교됩니다. 최근의 대규모 비디오 데이터셋인 Kinetics는 3D CNN의 인식 성능을 크게 향상시키는 데 기여합니다. 3D CNN은 2D CNN의 아키텍처에 비해 상대적으로 얕더라도 경쟁력을 갖추고 있습니다.

행동 인식을 위한 매우 깊은 3D CNN은 매개변수가 많아 훈련이 어려워 충분히 탐색되지 않았습니다. 이미지 인식에 대한 선행 연구는 매우 깊은 CNN 아키텍처가 인식 정확도를 향상시킨다고 보여줍니다. 3D CNN의 다양한 깊은 모델을 탐색하고 수렴 시 낮은 손실을 달성하는 것은 행동 인식 성능을 개선하는 데 중요합니다. 잔차 네트워크(ResNets)는 가장 강력한 아키텍처 중 하나입니다. ResNets의 아키텍처를 3D CNN에 적용하면 행동 인식 성능이 더욱 향상될 것으로 기대됩니다.

본 논문에서는 행동 인식을 위한 좋은 모델을 얻기 위해 3D ResNets를 실험적으로 평가합니다. 즉, 목표는 공간-시간적 인식에 대한 표준 사전 훈련 모델을 생성하는 것입니다. 2D 기반 ResNets에서 3D로 간단히 확장하여 네트워크를 훈련시키고, ActivityNet과 Kinetics 데이터셋을 사용하여 인식 성능을 평가합니다.

우리의 주요 기여는 3D 합성곱 커널을 사용한 ResNets의 효과성을 탐색하는 것입니다. 이 연구가 3D CNN을 사용한 행동 인식의 추가 발전에 기여할 것으로 기대합니다.

**2. 관련 연구**

여기서는 행동 인식 데이터베이스와 접근 방식을 소개합니다.

**2.1. 행동 인식 데이터베이스**

HMDB51과 UCF101은 행동 인식에서 가장 성공적인 데이터베이스입니다. 그러나 최근의 합의에 따르면, 이 두 데이터베이스는 대규모 데이터베이스라고 보기는 어렵습니다. 이러한 데이터베이스를 사용하여 과적합 없이 좋은 모델을 훈련시키는 것은 어렵습니다. 최근에는 Sports-1M과 YouTube-8M과 같은 대규모 데이터베이스가 제안되었습니다. 이러한 데이터베이스는 충분히 크지만, 주석이 노이즈가 많고 비디오 수준의 레이블만 포함되어 있으며, 목표 활동과 관련 없는 프레임이 포함되어 있습니다. 이러한 노이즈와 관련 없는 프레임은 모델의 훈련에 방해가 될 수 있습니다.

2D CNN이 ImageNet에서 훈련된 것처럼 성공적인 사전 훈련 모델을 만들기 위해, Google DeepMind는 Kinetics라는 인간 행동 비디오 데이터셋을 출시했습니다. Kinetics 데이터셋은 30만 개 이상의 잘린 비디오와 400개의 카테고리를 포함하고 있습니다. Kinetics의 크기는 Sports-1M과 YouTube-8M보다 작지만, 주석의 품질이 매우 높습니다.

우리는 3D ResNets 최적화를 위해 Kinetics를 사용합니다.

![](/assets/images/posts/256/img.png)

**그림 1: 잔차 블록. 스킵 연결은 블록의 상단에서 꼬리로 신호를 우회시킵니다. 신호는 꼬리에서 합쳐집니다.**

**2.2. 행동 인식 접근 방식**

CNN 기반 행동 인식의 인기 있는 접근 방식 중 하나는 2D 합성곱 커널을 사용하는 두 스트림 CNN입니다. Simonyan 외 연구자들은 RGB와 쌓인 광학 흐름 프레임을 각각 외형 및 운동 정보로 사용하는 방법을 제안했습니다. 그들은 두 스트림을 결합하면 행동 인식 정확도가 향상된다고 보여주었습니다. 두 스트림 CNN을 기반으로 한 많은 방법들이 행동 인식 성능을 향상시키기 위해 제안되었습니다. Feichtenhofer 외 연구자들은 두 스트림 CNN을 ResNets와 결합하는 방법을 제안했습니다. 그들은 ResNets의 아키텍처가 2D CNN을 사용한 행동 인식에 효과적이라는 것을 보여주었습니다. 위에서 언급된 접근 방식과 달리, 우리는 최근 대규모 비디오 데이터셋을 사용하여 2D CNN을 초월한 3D CNN에 중점을 두었습니다

.

또 다른 접근 방식은 3D 합성곱 커널을 사용하는 CNN을 채택합니다. Ji 외 연구자들은 비디오에서 공간-시간적 특징을 추출하기 위해 3D 합성곱을 적용하는 방법을 제안했습니다. Tran 외 연구자들은 Sports-1M 데이터셋을 사용하여 C3D라는 이름의 3D CNN을 훈련시켰습니다. 그들은 3 × 3 × 3 합성곱 커널이 최고의 성능을 달성한다고 실험적으로 발견했습니다. Varol 외 연구자들은 3D CNN의 입력 시퀀스의 시간 길이를 확장하면 인식 성능이 향상된다고 보여주었습니다. 또한, 광학 흐름을 3D CNN의 입력으로 사용하는 것이 RGB 입력보다 성능이 우수하며, RGB와 광학 흐름을 결합하면 최고의 성능을 달성한다고 밝혔습니다. Kay 외 연구자들은 Kinetics 데이터셋에서의 3D CNN 성과가 ImageNet에서 사전 훈련된 2D CNN의 성과와 경쟁력이 있지만, UCF101과 HMDB51에서의 3D CNN 성과는 2D CNN의 성과에 미치지 못한다고 보여주었습니다. Carreira 외 연구자들은 매우 깊은 네트워크(22층)인 inception 아키텍처를 3D CNN에 도입하여 최신 성능을 달성했습니다. 본 논문에서는 이미지 인식에서 inception 아키텍처를 초월하는 ResNet 아키텍처를 3D CNN에 도입합니다.

**3. 3D 잔차 네트워크**

**3.1. 네트워크 아키텍처**

우리의 네트워크는 ResNets를 기반으로 합니다. ResNets는 신호를 한 층에서 다음 층으로 우회시키는 스킵 연결을 도입합니다. 이러한 연결은 네트워크의 후속 층에서 초기 층으로의 기울기 흐름을 통과시키며, 매우 깊은 네트워크의 훈련을 용이하게 합니다. 그림 1은 ResNets의 요소인 잔차 블록을 보여줍니다. 이 연결은 블록의 상단에서 꼬리로 신호를 우회시킵니다. ResNets는 여러 개의 잔차 블록으로 구성됩니다.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

skip connection 사용

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

표 1은 우리의 네트워크 아키텍처를 보여줍니다. 우리의 네트워크와 원래 ResNets의 차이점은 합성곱 커널과 풀링의 차원 수입니다. 우리의 3D ResNets는 3D 합성곱과 3D 풀링을 수행합니다. 합성곱 커널의 크기는 3 × 3 × 3이며, conv1의 시간적 스트라이드는 1로 C3D와 유사합니다. 네트워크는 16 프레임 RGB 클립을 입력으로 사용합니다. 입력 클립의 크기는 3 × 16 × 112 × 112입니다. 입력의 다운샘플링은 conv3 1, conv4 1, conv5 1에서 스트라이드 2로 수행됩니다. 특성 맵의 수가 증가할 때, 우리는 파라미터 수의 증가를 피하기 위해 제로 패딩이 포함된 항등 스킵(형식 A)을 채택합니다.

![](/assets/images/posts/256/img_1.png)

**표 1: 네트워크 아키텍처. 잔차 블록은 괄호 안에 표시됩니다. 각 합성곱 층 뒤에는 배치 정규화와 ReLU가 따릅니다. 다운샘플링은 conv3 1, conv4 1, conv5 1에서 스트라이드 2로 수행됩니다. 마지막 완전 연결 층의 차원은 Kinetics 데이터셋(400 카테고리)에 맞추어 설정됩니다.**

**3.2. 구현**

**3.2.1 훈련**

우리 네트워크를 훈련하기 위해 모멘텀을 사용하는 확률적 경사 하강법(SGD)을 사용합니다. 데이터 증강을 수행하기 위해 훈련 데이터의 비디오에서 훈련 샘플을 무작위로 생성합니다. 먼저, 각 샘플의 시간적 위치를 균일 샘플링을 통해 선택합니다. 선택된 시간적 위치를 기준으로 16 프레임 클립을 생성합니다. 비디오가 16 프레임보다 짧으면 필요한 만큼 비디오를 반복합니다. 그런 다음, 4개의 모서리 또는 1개의 중심에서 무작위로 공간적 위치를 선택합니다. 또한, 공간적 스케일을 선택하여 다중 스케일 크롭을 수행합니다. 스케일은

![](/assets/images/posts/256/img_2.png)

에서 선택됩니다. 스케일 1은 최대 스케일을 의미합니다(즉, 프레임의 짧은 쪽의 길이와 동일). 자른 프레임의 종횡비는 1입니다. 생성된 샘플은 50% 확률로 수평으로 뒤집습니다. 또한 각 샘플에 대해 평균 빼기를 수행합니다. 모든 생성된 샘플은 원본 비디오와 동일한 클래스 레이블을 가집니다.

Kinetics 데이터셋에서 3D ResNets를 훈련하기 위해, 위에서 설명한 훈련 샘플을 사용하여 4개의 GPU(NVIDIA TITAN X)에서 미니 배치 크기 256으로 SGD를 사용합니다. 가중치 감쇠(weight decay)는 0.001이고 모멘텀은 0.9입니다. 학습률은 0.1에서 시작하고, 검증 손실이 포화되면 10으로 나누는 것을 3회 반복합니다. ActivityNet 데이터셋에서의 예비 실험에서는 큰 학습률과 배치 크기가 좋은 인식 성능을 달성하는 데 중요했습니다.

**3.2.2 인식**

훈련된 모델을 사용하여 비디오에서 행동을 인식합니다. 슬라이딩 윈도우 방식을 채택하여 입력 클립을 생성합니다(즉, 각 비디오는 겹치지 않는 16 프레임 클립으로 나눕니다). 각 클립은 최대 스케일의 중심 위치를 기준으로 자릅니다. 훈련된 모델을 사용하여 각 클립의 클래스 확률을 추정하고, 비디오의 모든 클립에서 이를 평균내어 비디오의 행동을 인식합니다.

**4. 실험**

**4.1. 데이터셋**

실험에서는 ActivityNet (v1.3)과 Kinetics 데이터셋을 사용했습니다.

- **ActivityNet** 데이터셋은 200개의 인간 행동 클래스에서 샘플을 제공합니다. 각 클래스당 평균 137개의 잘리지 않은 비디오와 비디오당 1.41개의 활동 인스턴스가 포함되어 있습니다. 총 비디오 길이는 849시간이며, 총 활동 인스턴스 수는 28,108개입니다. 데이터셋은 무작위로 세 개의 하위 집합(훈련, 검증, 테스트)으로 나뉘며, 50%는 훈련에, 25%는 검증 및 테스트에 사용됩니다.
- **Kinetics** 데이터셋은 400개의 인간 행동 클래스를 가지고 있으며, 각 클래스당 400개 이상의 비디오로 구성됩니다. 비디오는 시간적으로 잘려 있어 비행동 프레임이 포함되지 않으며, 길이는 약 10초입니다. 총 비디오 수는 30만 개 이상입니다. 훈련, 검증, 테스트 세트의 수는 각각 약 24만 개, 2만 개, 4만 개입니다.

Kinetics의 활동 인스턴스 수는 ActivityNet의 10배 더 많지만, 두 데이터셋의 총 비디오 길이는 유사합니다. 두 데이터셋 모두 비디오의 높이를 360픽셀로 조정하되, 종횡비는 변경하지 않고 저장했습니다.

![](/assets/images/posts/256/img_3.png)

**그림 2: ActivityNet 데이터셋에서의 모델 훈련. ActivityNet의 크기는 Kinetics(30만 개 비디오)와 Sports-1M(100만 개 비디오)에 비해 상대적으로 작습니다(2만 개 비디오). 3D ResNet은 상대적으로 작은 데이터셋으로 인해 오버피팅되었으며, C3D는 오버피팅 없이 더 나은 정확도를 달성했습니다.**

![](/assets/images/posts/256/img_4.png)

**그림 3: Kinetics 데이터셋에서의 모델 훈련. 3D ResNet은 대규모 Kinetics 데이터셋을 사용하여 오버피팅 없이 좋은 성능을 달성했습니다.**

![](/assets/images/posts/256/img_5.png)

\*표 2: Kinetics 데이터셋에서의 정확도. 평균은 Top-1과 Top-5 정확도의 평균입니다. 는 방법이 Sports-1M 데이터셋에서 사전 훈련을 수행하는 것을 나타냅니다. 우리의 3D ResNet은 상대적으로 얕은 아키텍처를 가진 C3D보다 높은 정확도를 달성했습니다.

**4.2. 결과**

먼저 ActivityNet 데이터셋에 대한 예비 실험을 설명합니다. 이 실험의 목적은 상대적으로 작은 데이터셋에서 3D ResNets의 훈련을 탐색하는 것입니다. 이 실험에서는 표 1에 설명된 18층 3D ResNet과 Sports-1M에서 사전 훈련된 C3D를 훈련했습니다. 그림 2는 훈련에서의 훈련 및 검증 정확도를 보여줍니다. 정확도는 전체 비디오가 아닌 16 프레임 클립을 기준으로 계산되었습니다. 그림 2(a)에서 볼 수 있듯이, 3D ResNet-18은 오버피팅되어 검증 정확도가 훈련 정확도보다 현저히 낮았습니다. 이 결과는 ActivityNet 데이터셋이 3D ResNets를 처음부터 훈련하기에는 너무 작다는 것을 나타냅니다. 반면에, 그림 2(b)에서는 Sports-1M에서 사전 훈련된 C3D가 오버피팅되지 않았으며 더 나은 인식 정확도를 달성했습니다. C3D의 상대적으로 얕은 아키텍처와 Sports-1M 데이터셋에서의 사전 훈련이 C3D의 오버피팅을 방지합니다.

그 다음, Kinetics 데이터셋에 대한 실험을 보여줍니다. 여기에서는 ActivityNet보다 활동 인스턴스 수가 현저히 많은 Kinetics의 특성에 맞춰 18층 대신 34층 3D ResNet을 훈련했습니다. 그림 3은 훈련에서의 훈련 및 검증 정확도를 보여줍니다. 정확도는 그림 2와 유사하게 16 프레임 클립을 기준으로 계산되었습니다. 그림 3(a)에서 볼 수 있듯이, 3D ResNet-34는 오버피팅되지 않았으며 좋은 성능을 달성했습니다. Sports-1M에서 사전 훈련된 C3D도 좋은 검증 정확도를 달성했지만, 훈련 정확도는 검증 정확도보다 명확히 낮았으며(즉, C3D는 언더피팅되었습니다). 또한, 3D ResNet은 Sports-1M 데이터셋에서 사전 훈련 없이도 C3D와 경쟁력 있는 성능을 보였습니다. 이러한 결과는 C3D가 너무 얕고, Kinetics 데이터셋을 사용할 때 3D ResNets가 효과적임을 나타냅니다.

표 2는 우리의 3D ResNet-34와 최신 기술의 정확도를 보여줍니다. C3D w/ BN은 각 합성곱 및 완전 연결 층 뒤에 배치 정규화를 사용하는 C3D입니다. RGB-I3D w/o ImageNet은 ResNets 기반의 CNNs에서 3D 합성곱 커널을 사용하는 매우 깊은 네트워크(22층)와 유사한 inception입니다. 여기에서는 ImageNet에서 사전 훈련되지 않은 RGB-I3D의 결과를 보여줍니다. ResNet-34는 Sports-1M에서 사전 훈련된 C3D와 배치 정규화가 포함된 C3D를 처음부터 훈련한 것보다 높은 정확도를 달성했습니다. 이 결과는 3D ResNets의 효과를 지지합니다. 반면, RGB-I3D는 ResNet-34의 깊이보다 더 깊은 네트워크임에도 불구하고 최고의 성능을 달성했습니다. 이 결과의 이유는 사용된 GPU 수의 차이일 수 있습니다. 배치 정규화를 사용한 좋은 모델 훈련에는 큰 배치 크기가 중요합니다. Carreira et al.은 32개의 GPU를 사용하여 RGB-I3D를 훈련했으나, 우리는 4개의 GPU와 256 배치 크기로 훈련했습니다. 그들은 훈련에서 더 큰 배치 크기를 사용했을 가능성이 있으며, 이는 최고의 성능에 기여할 수 있습니다. 또 다른 이유는 입력 클립의 크기 차이일 수 있습니다. 3D ResNet의 크기는 GPU 메모리 제한으로 인해 3 × 16 × 112 × 112인 반면, RGB-I3D는 3 × 64 × 224 × 224입니다. 높은 공간 해상도와 긴 시간적 지속 기간은 인식 정확도를 개선합니다. 따라서 많은 GPU를 사용하고 배치 크기, 공간 해상도 및 시간적 지속 기간을 늘리면 3D ResNets의 추가 개선이 이루어질 수 있습니다.

그림 4는 3D ResNet-34의 분류 결과 예시를 보여줍니다.

![](/assets/images/posts/256/img_6.png)

**그림 4: Kinetics에서 3D ResNet-34의 인식 결과 예시. 각 행의 프레임은 중심 위치에서 잘라낸 것으로 원본 비디오의 일부를 보여줍니다. 상위 세 개의 행은 올바르게 인식된 결과입니다. 하단 행은 잘못 인식된 결과입니다.**

**5. 결론**

우리는 3D 합성곱 커널을 사용하는 ResNets의 효과를 탐구했습니다. 우리는 대규모 비디오 데이터셋인 Kinetics를 사용하여 3D ResNets를 훈련했습니다. Kinetics에서 훈련된 모델은 많은 파라미터에도 불구하고 오버피팅 없이 좋은 성능을 발휘했습니다. 우리의 코드와 사전 훈련된 모델은 <https://github.com/kenshohara/3D-ResNets>에서 공개됩니다.

3D ResNets의 훈련 시간(3주)이 매우 높기 때문에, 우리는 주로 ResNets-34에 집중했습니다. 향후 연구에서는 더 깊은 모델(ResNets-50, -101) 및 DenseNets-201 [8]과 같은 다른 깊은 아키텍처에 대한 추가 실험을 진행할 예정입니다.
