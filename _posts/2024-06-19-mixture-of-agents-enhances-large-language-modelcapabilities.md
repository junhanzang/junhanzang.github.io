---
title: "Mixture-of-Agents Enhances Large Language ModelCapabilities"
date: 2024-06-19 21:32:53
categories:
  - 인공지능
---

<https://github.com/togethercomputer/MoA?tab=readme-ov-file>

[GitHub - togethercomputer/MoA](https://github.com/togethercomputer/MoA?tab=readme-ov-file)

초록

최근 대형 언어 모델(LLM)의 발전은 자연어 이해 및 생성 작업에서 상당한 능력을 보여주고 있다. 증가하는 LLM의 수와 함께 여러 LLM의 집합적 전문 지식을 활용하는 방법은 흥미로운 개방적 방향이다. 이를 목표로 우리는 다중 LLM의 집합적 강점을 활용하는 Mixture-of-Agents (MoA) 방법론을 제안한다. 우리의 접근법에서 우리는 각 계층이 여러 LLM 에이전트로 구성된 계층적 MoA 아키텍처를 구축한다. 각 에이전트는 이전 계층의 에이전트들의 모든 출력을 보조 정보로 활용하여 응답을 생성한다. MoA 모델은 AlpacaEval 2.0, MT-Bench 및 FLASK에서 최첨단 성능을 달성하여 GPT-4 Omni를 능가한다. 예를 들어, 오픈 소스 LLM만을 사용하는 우리의 MoA는 AlpacaEval 2.0에서 65.1%의 점수를 달성하여 57.5%를 기록한 GPT-4 Omni를 상당한 격차로 앞서고 있다.

### 소개

대형 언어 모델(LLM)(Zhang et al., 2022a; Chowdhery et al., 2022; Touvron et al., 2023a; Team et al., 2023; Brown et al., 2020; OpenAI, 2023)은 최근 몇 년 동안 자연어 이해 및 생성 분야에서 상당한 발전을 이루었습니다. 이 모델들은 방대한 양의 데이터로 사전 훈련을 거치며, 이후 인간의 선호도에 맞게 조정되어 유용하고 일관된 출력을 생성합니다(Ouyang et al., 2022). 그러나 많은 LLM과 그들의 인상적인 성과에도 불구하고, 여전히 모델 크기와 훈련 데이터에 대한 본질적인 제약이 존재합니다. 이러한 모델을 더욱 확장하는 것은 매우 비용이 많이 들며, 종종 수조 개의 토큰을 재훈련해야 합니다.

동시에, 다양한 LLM은 고유한 강점을 가지고 있으며 다양한 작업 측면에서 특화되어 있습니다. 예를 들어, 일부 모델은 복잡한 지침을 따르는 데 뛰어나고(Xu et al., 2023a), 다른 모델은 코드 생성에 더 적합할 수 있습니다(Roziere et al., 2023; Guo et al., 2024). 이러한 다양한 LLM의 기술 세트는 다음과 같은 흥미로운 질문을 제기합니다: 여러 LLM의 집합적 전문 지식을 활용하여 더 유능하고 견고한 모델을 만들 수 있을까요?

![](/assets/images/posts/167/img.png)

그림 1: 다른 모델의 응답을 제공하면 AlpacaEval 2.0 LC의 승률이 향상됩니다.

우리의 대답은 '예'입니다. 우리는 'LLM의 협력성'이라는 내재된 현상을 확인했습니다. 이는 LLM이 다른 모델의 출력을 제공받을 때 더 나은 응답을 생성하는 경향이 있다는 것을 의미합니다. 비록 이러한 다른 모델들이 스스로는 덜 유능할지라도 말입니다. 그림 1은 AlpacaEval 2.0 벤치마크(Dubois et al., 2024)에서 6개의 인기 있는 LLM의 LC 승률을 보여줍니다. 이러한 모델들이 독립적으로 생성된 답변을 제공받았을 때, 그들의 LC 승률이 크게 향상되었습니다. 이는 협력성 현상이 LLM 사이에서 널리 퍼져 있음을 나타냅니다. 놀랍게도, 이러한 향상은 다른 모델들이 제공하는 보조 응답의 품질이 개별 LLM이 독립적으로 생성할 수 있는 것보다 낮은 경우에도 발생합니다.

![](/assets/images/posts/167/img_1.png)

그림 2: 에이전트 혼합 구조의 그림. 이 예에서는 각 레이어에 에이전트가 3개씩 있는 4개의 MoA 레이어를 보여줍니다. 여기서 에이전트는 동일한 모델을 공유할 수 있습니다.

이 발견을 바탕으로, 이 논문은 다중 LLM을 활용하여 반복적으로 생성 품질을 향상시키는 Mixture-of-Agents(MoA) 방법론을 소개합니다. MoA의 구조는 그림 2에 나와 있습니다. 처음에, 첫 번째 계층의 LLM(에이전트 A1,1, ...A1,n)은 주어진 프롬프트에 대해 독립적으로 응답을 생성합니다. 이러한 응답은 다음 계층의 에이전트 A2,1, ...A2,n(첫 번째 계층의 모델을 재사용할 수 있음)에게 제공되어 추가 정제를 진행합니다. 이러한 반복적인 정제 과정은 더 견고하고 포괄적인 응답을 얻을 때까지 여러 사이클 동안 계속됩니다.

모델 간의 효과적인 협력을 보장하고 전체 응답 품질을 향상시키기 위해 각 MoA 계층에 대한 LLM의 신중한 선택이 중요합니다. 이 선택 과정은 두 가지 주요 기준에 의해 안내됩니다: (a) 성능 지표: i 계층의 모델의 평균 승률은 i+1 계층에 포함될 모델의 적합성을 결정하는 데 중요한 역할을 합니다. 따라서 성능 지표에 따라 모델을 선택하면 더 높은 품질의 출력을 보장할 수 있습니다. (b) 다양성 고려: 모델 출력의 다양성도 중요합니다. 이질적인 모델이 생성한 응답은 동일한 모델이 생성한 응답보다 훨씬 더 많이 기여합니다. 나중에 3.3절에서 보여드리겠습니다. 이러한 기준(성능 및 다양성)을 활용하여 MoA는 개별 모델의 결점을 완화하고 협력적 합성을 통해 전체 응답 품질을 향상시키는 것을 목표로 합니다.

우리는 다양한 차원에서 응답 품질을 평가하기 위해 AlpacaEval 2.0, MT-Bench(Zheng et al., 2023), FLASK(Ye et al., 2023) 벤치마크를 사용하여 포괄적인 평가를 수행했습니다. 결과는 우리의 제안된 방법이 상당한 개선을 보여주었으며, AlpacaEval 2.0에서 65.8%의 새로운 SOTA 승률을 달성하여 이전에 GPT-4 Omni가 달성한 57.5%를 넘어섰습니다.

이 연구의 기여는 다음과 같이 요약할 수 있습니다: (1) 새로운 프레임워크: 우리는 여러 LLM의 강점을 활용하여 그들의 추론 및 언어 생성 능력을 향상시키는 Mixture-of-Agents 프레임워크를 제안합니다. (2) 언어 모델의 협력성 발견: 우리는 다른 모델의 출력에 접근할 때 모델들이 더 나은 품질의 응답을 생성하는 경향이 있는 LLM 간의 내재된 협력성을 강조합니다. (3) 최첨단 LLM 성능: 우리는 AlpacaEval 2.0, MT-Bench, FLASK와 같은 다수의 경쟁력 있는 벤치마크를 사용하여 광범위한 실험을 수행했으며, 우리의 MoA 프레임워크는 이러한 벤치마크에서 최첨단 성능을 달성했습니다.

### 2 Mixture-of-Agents 방법론

이 섹션에서는 여러 모델을 활용하여 성능을 향상시키기 위한 제안된 방법론을 소개합니다. 먼저 LLM들이 협력성을 가지고 있으며, 다른 모델의 출력을 기반으로 응답을 개선할 수 있음을 보여줍니다. 그 후 Mixture-of-Agents 방법론을 소개하고 그 설계 의미를 논의합니다.

#### 2.1 LLM의 협력성

먼저, LLM들이 다른 모델의 출력을 참고할 수 있을 때 더 높은 품질의 응답을 생성할 수 있는 협력성을 가지고 있음을 보여줍니다. 서론과 그림 1에서 보여주었듯이, 오늘날 사용 가능한 많은 LLM들이 이러한 협력 능력을 가지고 있습니다.

여러 LLM의 협력에서 최대의 이익을 얻기 위한 중요한 방법은 다양한 협력 측면에서 서로 다른 모델들이 어떤 점에서 뛰어난지를 특성화하는 것입니다. 협력 과정에서 우리는 LLM을 두 가지 역할로 구분할 수 있습니다:

- **제안자**는 다른 모델이 사용할 수 있는 유용한 참조 응답을 생성하는 데 뛰어납니다. 좋은 제안자는 반드시 높은 점수의 응답을 생성하지는 않지만, 더 많은 맥락과 다양한 관점을 제공하여 최종 응답이 더 나아지도록 기여해야 합니다.
- **집계자**는 다른 모델의 응답을 통합하여 단일의 고품질 출력을 생성하는 데 능숙합니다. 효과적인 집계자는 자신보다 낮은 품질의 입력을 통합할 때도 출력 품질을 유지하거나 향상시킬 수 있어야 합니다.

3.3절에서는 제안자와 집계자의 역할을 실험적으로 검증합니다. 특히, 많은 LLM들이 제안자와 집계자 모두로서의 능력을 가지고 있으며, 특정 모델들이 특정 역할에서 전문성을 발휘하는 것을 보여줍니다. GPT-4o, Qwen1.5, LLaMA-3는 보조 및 집계 작업 모두에서 효과적인 다재다능한 모델로 나타났습니다. 반면에 WizardLM은 제안자 모델로서 뛰어난 성능을 보였지만, 다른 모델의 응답을 집계하는 데는 어려움을 겪었습니다.

집계자가 다른 모델의 출력을 기반으로 더 높은 품질의 응답을 생성할 수 있음을 고려할 때, 추가적인 집계자를 도입하여 이러한 협력 잠재력을 더욱 향상시킬 것을 제안합니다. 한 가지 직관적인 아이디어는 여러 집계자를 사용하여 더 나은 응답을 처음 집계하고, 그런 다음 이러한 집계된 응답을 다시 집계하는 것입니다. 이 과정을 통해 더 많은 집계자를 도입하여 응답을 반복적으로 합성하고 정제함으로써 여러 모델의 강점을 활용하여 우수한 결과를 도출할 수 있습니다. 이것이 우리가 제안하는 Mixture-of-Agents의 설계로 이어집니다.

### Table 1: Aggregate-and-Synthesize Prompt to integrate responses from other models.

당신은 마지막 사용자 요청에 대해 여러 오픈 소스 모델의 다양한 응답 세트를 받았습니다. 당신의 작업은 이러한 응답을 높은 품질의 단일 응답으로 합성하는 것입니다. 이 과정에서 정보를 정확하게 활용하고, 각 응답의 강점을 합치는 것이 중요합니다. 단순히 주어진 응답을 결합하는 대신, 최종 응답이 명확하고 정확하며 포괄적이어야 합니다. 최종 응답이 잘 구조화되어 있고, 각 요소가 높은 정확성과 신뢰성을 가지고 있는지 확인해야 합니다.

![](/assets/images/posts/167/img_2.png)

2.2 Mixture-of-Agents

![](/assets/images/posts/167/img_3.png)

![](/assets/images/posts/167/img_4.png)

![](/assets/images/posts/167/img_5.png)

### 2.3 Mixture-of-Experts와의 유사성

Mixture-of-Experts (MoE) (Shazeer et al., 2017)은 여러 전문가 네트워크가 다른 기술 세트에 특화된 기계 학습에서 유명하고 잘 확립된 기술입니다. MoE 접근 방식은 문제 해결 작업에서 다양한 전문가 네트워크의 능력을 활용하는 능력 덕분에 다양한 응용 분야에서 상당한 성공을 거두었습니다. 우리의 MoA 방법은 이 방법론에서 영감을 받았습니다.

![](/assets/images/posts/167/img_6.png)

높은 수준에서, 우리의 제안된 MoA 프레임워크는 MoE 접근 방식을 모델 출력을 결합하여 작업을 해결하는 방법으로 확장합니다. 특히, 우리의 MoA 접근 방식은 게이팅 네트워크와 전문가 네트워크의 역할을 LLM으로 대체하여 기존 모델의 성능을 활용합니다. 이는 외부의 새로운 전문가 네트워크를 훈련시키지 않고도 상용 LLM의 능력을 활용합니다.

또한, 우리의 접근 방식은 미세 조정을 필요로 하지 않는 상용 LLM의 오프 더 쉘프(on-the-shelf) 능력을 효율적으로 활용합니다. 이는 필요한 경우 전체 모델의 재훈련 없이 LLM의 협력성을 활용할 수 있습니다. 따라서, 우리의 MoA 접근 방식은 매우 유연하며 다양한 LLM을 통해 높은 성능과 확장성을 제공합니다.

### 3 평가

이 섹션에서는 제안된 Mixture-of-Agents (MoA) 방법론에 대한 종합적인 평가를 제공합니다. 우리의 연구 결과는 다음과 같습니다:

1. **주요 개선 성과**:
   - AlpacaEval 2.0, MT-Bench 및 FLASK 벤치마크에서 상당한 성능 향상을 달성했습니다. 특히 오픈 소스 모델만을 사용하여 우리의 접근 방식은 AlpacaEval 2.0과 FLASK에서 GPT-4o를 능가했습니다.
2. **내부 메커니즘 이해**:
   - MoA의 내부 메커니즘에 대한 더 나은 이해를 제공하기 위해 광범위한 실험을 수행했습니다.
3. **비용 분석**:
   - 자세한 예산 분석을 통해, 여러 MoA 구현이 GPT-4 Turbo와 비교할 만한 성능을 제공하면서도 비용 효율성이 두 배 더 높다는 것을 보여주었습니다.

### 표 2: AlpacaEval 2.0 및 MT-Bench 결과

AlpacaEval 2.0의 경우, MoA와 MoA-Lite는 각각 3계층과 2계층의 6개 제안자를 나타냅니다. MoA w/ GPT-4o는 최종 집계자로 GPT-4o를 사용하는 MoA를 의미합니다. 우리는 실험을 세 번 수행하였고, 평균 점수와 표준 편차를 함께 보고했습니다. †는 AlpacaEval 결과의 우리의 복제를 나타냅니다. 우리는 모든 MT-Bench 점수를 직접 계산하여 턴 기반 점수를 얻었습니다.

![](/assets/images/posts/167/img_7.png)

### 3.1 설정

#### 벤치마크

우리는 주로 AlpacaEval 2.0 (Dubois et al., 2024)에서 모델을 평가합니다. 이 벤치마크는 LLM이 인간의 선호도에 얼마나 잘 맞추어져 있는지를 평가하는 주요 벤치마크입니다. 805개의 실제 사용 사례를 대표하는 지침이 포함되어 있습니다. 각 모델의 응답은 GPT-4 (gpt-4-1106-preview)의 응답과 직접 비교되며, GPT-4 기반 평가자가 평가된 모델의 응답을 선호할 가능성을 판단합니다. 공정성을 보장하기 위해, 평가에서는 길이 편향을 효과적으로 중립화하는 길이 제어(Len-Controlled, LC) 승률을 사용합니다.

추가로, 우리는 MT-Bench (Zheng et al., 2023)와 FLASK (Ye et al., 2023)에서도 평가를 수행합니다. MT-Bench는 GPT-4를 사용하여 모델의 답변에 점수를 매기고, FLASK는 12개의 특정 기술 점수로 더 세분화된 평가를 제공합니다.

#### 모델

우리의 연구에서는 오픈 소스 모델만을 사용하여 기본 MoA를 구축하고 경쟁력 있는 성능을 달성했습니다. 포함된 모델은 다음과 같습니다: Qwen1.5-110B-Chat (Bai et al., 2023), Qwen1.5-72B-Chat, WizardLM-8x22B (Xu et al., 2023a), LLaMA-3-70B-Instruct (Touvron et al., 2023b), Mixtral-8x22B-v0.1 (Jiang et al., 2024), dbrx-instruct (The Mosaic Research Team, 2024). 우리는 3개의 MoA 계층을 구성하고 각 MoA 계층에서 동일한 모델 세트를 사용합니다. 마지막 계층에서 Qwen1.5-110B-Chat을 집계자로 사용합니다.

또한, 최종 MoA 계층에서 GPT-4o를 집계자로 사용하여 높은 품질의 출력을 우선시하는 변형 버전인 MoA w/ GPT-4o도 개발했습니다. 또 다른 변형 버전인 MoA-Lite는 비용 효율성을 강조합니다. 이 버전은 제안자로 동일한 모델 세트를 사용하지만, 2개의 MoA 계층만 포함하고 Qwen1.5-72B-Chat을 집계자로 사용합니다. 이는 AlpacaEval 2.0에서 1.8% 품질 향상을 달성하면서도 GPT-4o보다 더 비용 효율적입니다. 우리는 이 연구에서 사용된 모든 모델의 라이선스 조건을 엄격히 준수합니다. 오픈 소스 모델의 경우 모든 추론은 Together Inference Endpoint를 통해 수행되었습니다.

### 3.2 벤치마크 결과

이 섹션에서는 AlpacaEval 2.0, MT-Bench, FLASK의 세 가지 표준 벤치마크에서 우리의 평가 결과를 제시합니다. 이 벤치마크들은 우리의 접근 방식의 성능을 종합적으로 평가하고 최첨단 LLM들과 비교하기 위해 선택되었습니다.

#### AlpacaEval 2.0

우리는 GPT-4 및 다른 최첨단 오픈 소스 모델과 비교를 수행했습니다. 상세한 결과는 표 2a에 제시되어 있으며, 우리 MoA 방법론이 AlpacaEval 2.0 리더보드에서 상위 위치를 차지하여 이전 최고 모델인 GPT-4o에 비해 8.2%의 절대적 향상을 달성했습니다. 더욱 주목할 만한 것은, 우리의 모델이 오직 오픈 소스 모델만을 사용하여 GPT-4o를 능가했다는 점으로, 57.5% (GPT-4o)에서 65.1% (MoA)로 7.6%의 절대적 향상을 이루었습니다. MoA-Lite 설정은 더 적은 계층을 사용하여 비용 효율성을 높였습니다. 이 가벼운 접근 방식으로도 여전히 최고의 모델을 1.8% 능가하여 57.5% (GPT-4o)에서 59.3% (MoA-Lite)로 향상되었습니다. 이는 다양한 계산 예산으로 오픈 소스 모델의 능력을 최대한 활용하는 우리의 방법의 효과를 더욱 강조합니다.

#### MT-Bench

개별 모델에 대한 개선 사항은 비교적 미미하지만, 이는 현재 모델들이 이미 이 벤치마크에서 매우 뛰어난 성능을 발휘하고 있기 때문입니다. 단일 모델만으로도 10점 만점에 9점 이상의 점수를 달성할 수 있습니다. 미미한 향상에도 불구하고, 우리의 접근 방식은 여전히 리더보드에서 최고 위치를 확보했습니다. 이는 이미 최적화된 벤치마크에서도 우리의 방법이 경계를 확장하고 리더십을 유지할 수 있음을 보여줍니다.

#### FLASK

FLASK는 모델을 세밀하게 평가합니다. 이러한 메트릭 중에서 MoA는 여러 주요 측면에서 뛰어난 성과를 보였습니다. 특히, 우리의 방법론은 견고성, 정확성, 효율성, 사실성, 상식성, 통찰력, 완전성에서 상당한 개선을 보여줍니다. 이는 집계자인 Qwen-110B-Chat의 단일 모델 점수와 비교하여 두드러집니다. 또한, MoA는 정확성, 사실성, 통찰력, 완전성, 메타인지 측면에서 GPT-4 Omni를 능가합니다. 한 가지 덜 우수한 메트릭은 간결성으로, 모델이 다소 더 장황한 출력을 생성했습니다.

![](/assets/images/posts/167/img_8.png)

그림 3: 6개의 제안자 MoA 설정을 사용하고 Qwen1.5-110B-Chat이 애그리게이터인 FLASK의 결과.

### 3.3 Mixture-of-Agents가 효과적인 이유는 무엇일까요?

이 섹션에서는 Mixture-of-Agents (MoA)의 내부 메커니즘을 더 잘 이해하기 위해 실험을 수행합니다. 주요 통찰을 아래에 요약합니다.

![](/assets/images/posts/167/img_9.png)

#### Figure 4

**(a) AlpacaEval 2.0에서 다른 집계자를 사용하는 6-모델 Mixture-of-Agents 설정의 LC 승률**:

- 모든 곡선은 동일한 6개의 제안자 에이전트를 사용하며, 최종 집계자의 선택만 다릅니다.
- LLM 랭커는 부록 표 5의 프롬프트 형식을 사용하여 Qwen1.5-110B-Chat 모델을 사용합니다.
- GPT-4o 모델은 평가 목적으로만 출력을 집계하며, 다음 계층의 제안자로는 참여하지 않습니다.

**(b) 제안된 출력의 BLEU 점수(3-그램, 4-그램, 5-그램 메트릭을 사용하여 계산)와 승률 사이의 스피어만 상관관계**:

- 제안된 출력의 승률과 BLEU 점수 사이의 긍정적인 상관관계를 보여줍니다.

#### Mixture-of-Agents가 LLM 랭커보다 뛰어난 성능을 발휘합니다.

먼저, 제안자들이 생성한 답변 중 하나를 선택하는 집계자 모델을 사용하는 LLM 기반 랭커와 MoA를 비교합니다. 결과는 그림 4에 나와 있으며, MoA 접근 방식이 LLM 랭커 기반을 크게 능가하는 것을 볼 수 있습니다. MoA가 랭킹 접근 방식을 능가한다는 사실은 집계자가 제안자들이 생성한 답변 중 하나를 단순히 선택하는 것이 아니라, 모든 제안된 생성물을 정교하게 집계할 수 있음을 시사합니다.

#### MoA는 최상의 제안 답변을 통합하는 경향이 있습니다.

우리는 또한 BLEU (Papineni et al., 2002)와 같은 유사성 점수를 통해 집계자의 응답과 제안자의 응답을 비교합니다. 각 샘플 내에서 제안자들이 제안한 n개의 답변을 고려하여, GPT-4 기반 평가자가 결정한 n개의 선호 점수와 n개의 유사 점수 간의 스피어만 순위 상관 계수를 계산합니다. 그림 4의 결과는 승률과 BLEU 점수 사이에 긍정적인 상관관계가 있음을 확인합니다. 부록 A에서는 텍스트 유사성에 대한 대체 접근 방식으로 Levenshtein 유사성(RapidFuzz, 2023) 또는 TF-IDF를 사용한 결과도 제시합니다. 두 대체 접근 방식 모두 선호 점수와 긍정적인 상관관계를 나타냈습니다.

#### Table 3: 제안자 모델 수의 효과

**AlpacaEval 2.0에서 제안자 모델 수의 효과**:

- nnn을 MoA 계층의 에이전트 수 또는 단일 제안자 설정에서 제안된 출력 수로 나타냅니다.
- Qwen1.5-110B-Chat을 집계자로 사용하고, 이 표의 모든 설정에서 2개의 MoA 계층을 사용합니다.

![](/assets/images/posts/167/img_10.png)

#### 모델 다양성과 제안자 수의 효과

우리는 각 계층에서 제안자의 수를 변화시켜 제안자의 수가 최종 출력 품질에 미치는 영향을 분석합니다. 표 3에 나타난 결과는 n이 증가할수록 점수가 단조롭게 증가하여 더 많은 보조 정보의 이점을 반영함을 보여줍니다. 또한, 다양한 LLM 세트를 제안자로 사용하는 영향도 정량화합니다. 각 n에 대해, 동일한 LLM이 온도 0.7로 n개의 응답을 생성하는 "단일 제안자" 설정과, 각각 다른 LLM이 응답을 생성하는 "다중 제안자" 설정을 비교합니다. 전반적으로, 다양한 LLM을 사용하는 것이 일관되게 더 나은 결과를 냈습니다. 두 결과 모두 각 MoA 계층에 더 많은 수의 다양한 LLM 에이전트를 보유하는 것이 성능을 향상시킬 수 있음을 시사합니다. MoA의 폭을 확장하는 것은 미래의 유망한 연구 방향입니다.

#### Table 4: 제안자 대 집계자로서의 다른 모델의 영향

**다른 모델이 제안자 대 집계자로서의 영향**:

- 다양한 집계자를 평가할 때, 모든 6개의 모델이 제안자로 작동합니다.
- 제안자를 평가할 때, Qwen1.5-110B-Chat이 집계자로 작동합니다.
- 이 표에서는 2개의 MoA 계층을 사용합니다.

![](/assets/images/posts/167/img_11.png)

#### Mixture-of-Agents 생태계 내 모델의 전문화

우리는 또한 특정 역할에서 뛰어난 모델을 결정하기 위한 실험을 수행했습니다. 표 4는 GPT-4o, Qwen, LLaMA-3가 보조 및 집계 작업 모두에서 효과적인 다재다능한 모델로 나타났음을 보여줍니다. 반면, WizardLM은 제안자 모델로서 뛰어난 성능을 보였지만, 다른 모델의 응답을 집계하는 데에는 효과를 유지하는 데 어려움을 겪었습니다.

![](/assets/images/posts/167/img_12.png)

### 그림 5

- **(a) 성능 대비 비용 트레이드오프**:
  - 특정 성능 수준에서 점진적으로 높은 점수를 제공하는 모델을 선택할 수 있는 파레토 프론트를 보여줍니다. Mixture-of-Agents 접근 방식은 이 파레토 프론트에 위치하며, 같은 LC 승률에서 GPT-4 Turbo와 GPT-4o보다 비용 효율성이 뛰어납니다.
  - 단일 제안자: 각 MoA 계층에서 동일한 모델을 사용하여 여러 응답을 생성.
  - 다중 제안자: 각 MoA 계층에서 다른 모델을 사용하여 응답을 생성.
- **(b) 성능 대비 Tflops 수 트레이드오프**:
  - 성능과 Tflops 수 간의 트레이드오프를 보여줍니다. 우리는 각 MoA 계층에서 제안자들 중 최대 Tflops 수의 합계를 계산하여 플롯을 작성했습니다. 이는 다중 제안자들이 병렬로 실행될 수 있기 때문입니다.
  - GPT-4의 실제 Tflops는 알려지지 않았으므로, 8x220B 아키텍처라는 소문을 기반으로 계산했습니다.

### 3.4 예산 및 토큰 분석

예산, 토큰 사용량, 그리고 LC 승률 간의 관계를 이해하기 위해 예산 및 토큰 분석을 수행했습니다. 그림 5a와 그림 5b는 이러한 관계를 보여줍니다.

#### 비용 효율성

그림 5a에서는 AlpacaEval 2.0 벤치마크의 각 인스턴스에 대한 평균 추론 비용과 LC 승률을 비교하여 플롯을 작성했습니다. 비용은 API 제공업체 웹사이트에서 제공되는 가격 정보를 기반으로 계산되었습니다. 이 플롯은 비용 효율적인 모델을 식별하는 데 도움이 되며, 높은 성능을 과도한 비용 없이 달성할 수 있습니다. 차트는 특정 모델들이 비용과 성능 간의 최적의 균형을 이루는 파레토 프론트(Pareto front)를 보여줍니다. 이 파레토 프론트에 가까운 모델들은 낮은 비용으로 높은 LC 승률을 제공함으로써 더 나은 경제적 가치를 제공합니다. 특히, 품질을 우선시한다면 MoA가 최고의 구성입니다. 하지만, 품질과 비용 간의 좋은 균형을 원한다면 MoA-Lite가 GPT-4o의 비용과 일치하면서 더 높은 수준의 품질을 달성할 수 있습니다. 주목할 만한 점은 GPT-4 Turbo보다 약 4% 더 우수하면서도 두 배 이상의 비용 효율성을 자랑한다는 것입니다.

#### Tflops 소비

그림 5b는 LC 승률과 Tflops 수 간의 관계를 나타냅니다. 여기서 우리는 Tflops 수를 지연 시간(latency)의 대리(proxy)로 사용합니다. 이는 추론 시스템에 따라 지연 시간이 달라질 수 있기 때문입니다. 이 분석은 성능 수준을 유지하거나 개선하면서 다양한 모델이 예산을 어떻게 관리하는지 이해하는 데 중요합니다. 비용 효율성 분석과 유사하게, 여기서도 파레토 프론트를 관찰할 수 있습니다. 이 프론트에 있는 모델들은 자신들의 계산 자원을 효과적으로 활용하여 LC 승률을 극대화합니다.

### 4 관련 연구

#### 4.1 LLM 추론

LLM의 생성 품질을 향상시키기 위해, 최근 연구들은 프롬프트 엔지니어링을 통해 다양한 다운스트림 작업에 LLM을 최적화하는 데 큰 진전을 보였습니다. Chain of Thought (CoT) (Wei et al., 2022; Kojima et al., 2022) 프롬프트 기술은 각 단계가 이전 단계를 기반으로 하는 선형 문제 해결 접근 방식을 나타냅니다. Fu et al. (2022)은 다단계 추론 작업에 CoT를 적용했습니다. CoT 프롬프트를 자동화하기 위해, Auto-CoT (Zhang et al., 2022b)는 다양한 질문을 샘플링하고 추론 체인을 생성하여 데모를 구성합니다. Active-Prompt (Dia et al., 2023)는 작업별 주석을 위해 가장 불확실한 질문을 선택하는 데 중점을 둡니다. PS Prompt (Wang et al., 2023)는 작업을 하위 작업으로 분해합니다. Tree-of-Thought (ToT) (Yao et al., 2023a)는 여러 추론 경로와 자기 평가 선택을 고려하여 추론 과정을 확장합니다. Effective Graph-of-Thought (Yao et al., 2023b)는 생각을 그래프로 구성합니다. Natural Program 프롬프트 (Ling et al., 2023)는 연역적 추론 작업을 더 잘 해결하기 위해 제안되었습니다. Re-reading 프롬프트 (Xu et al., 2023b)는 입력 프롬프트 내에 포함된 질문 정보를 재검토합니다.

#### 4.2 모델 앙상블

여러 모델의 강점을 활용하기 위한 간단한 해결책은 다른 모델의 출력을 재랭킹하는 것입니다. 예를 들어, Jiang et al. (2023)은 후보 출력에 대해 쌍별 비교를 수행하여 최상의 출력을 선택하는 PAIRRANKER를 소개하여 자체 구축한 지침 데이터 세트에서 개선된 결과를 보였습니다. 다중 LLM 추론과 관련된 상당한 계산 비용을 해결하기 위해, 다른 연구들은 주어진 입력에 대해 고정된 LLM 세트 중 최상의 성능을 발휘하는 모델을 예측하는 라우터를 훈련하는 방안을 탐구했습니다 (Wang et al., 2024a; Shnitzer et al., 2024; Lu et al., 2023). 추가로, FrugalGPT (Chen et al., 2023b)는 다양한 모델을 계단식으로 사용하여 LLM 사용 비용을 줄이는 방법을 제안했습니다. 여러 모델의 응답을 더 잘 활용하기 위해, Jiang et al. (2023)은 여러 후보의 강점을 활용하여 개선된 응답을 생성하도록 훈련된 GENFUSER 모델을 훈련했습니다. Huang et al. (2024)은 다른 모델의 출력 확률 분포를 평균화하여 출력을 결합하는 방법을 제안했습니다.

또 다른 연구 방향은 다중 에이전트 협업입니다. 여러 대형 언어 모델을 에이전트로 사용하여 주어진 문제를 상호 작용하며 논의하고 추론하는 방식을 탐구한 연구가 있습니다. Du et al. (2023)은 에이전트 간 대칭적 논의를 위한 메커니즘을 확립했습니다. 비슷한 시기에, MAD (Liang et al., 2023)는 다른 역할, 즉 논쟁자와 심판을 가진 비대칭 메커니즘 설계를 도입했습니다. 이와 유사한 연구로는 (Chan et al., 2023) 등이 있습니다. 또한, ReConcile (Chen et al., 2023a)는 가중 투표를 포함하는 비대칭 논의를 예시합니다. 논의를 더 깊이 이해하기 위해, Zhang et al. (2023)은 사회 심리학적 관점에서 이러한 협력 메커니즘을 설명하는 것을 목표로 합니다. Wang et al. (2024b)은 다중 에이전트 접근 방식을 체계적으로 비교하였고, 자세한 데모를 포함한 강력한 프롬프트를 가진 단일 에이전트가 다중 에이전트 접근 방식에 필적하는 응답 품질을 달성할 수 있음을 발견했습니다.

### 5 결론

이 논문은 다중 LLM의 능력을 활용하여 반복적인 협력을 통해 성능을 향상시키는 Mixture-of-Agents 접근 방식을 소개합니다. 우리의 방법은 Mixture-of-Agents 계열의 에이전트의 집합적 강점을 활용하여 각 개별 모델의 출력 품질을 크게 개선할 수 있습니다. AlpacaEval 2.0, MT-Bench 및 FLASK에서 수행된 실증적 평가에서 응답 품질의 상당한 향상을 보여주었으며, 우리의 접근 방식은 최대 65%의 LC 승률을 달성했습니다. 이러한 결과는 다양한 모델의 다양한 관점을 통합하는 것이 단일 모델에 의존하는 것보다 우수한 성능을 이끌어낼 수 있다는 가설을 입증합니다. 또한, 우리는 MoA의 설계를 개선하는 통찰을 제공합니다. MoA 아키텍처의 체계적인 최적화는 미래 연구의 흥미로운 방향입니다.

#### 한계점

제안된 방법은 모델 응답의 반복적인 집계를 필요로 하므로, 마지막 MoA 계층에 도달할 때까지 첫 번째 토큰을 결정할 수 없습니다. 이는 잠재적으로 높은 '첫 번째 토큰까지의 시간(Time to First Token, TTFT)'을 초래하여 사용자 경험에 부정적인 영향을 미칠 수 있습니다. 이 문제를 완화하기 위해 MoA 계층의 수를 제한할 수 있으며, 첫 번째 응답 집계가 생성 품질에 가장 큰 영향을 미칩니다. 향후 연구에서는 전체 응답을 한 번에 집계하는 대신 청크별로 집계하는 방법을 탐구하여 TTFT를 줄이면서도 응답 품질을 유지할 수 있을 것입니다.

#### 더 넓은 영향

이 연구는 LLM 기반 채팅 어시스턴트의 효과를 향상시켜 AI 접근성을 높이는 잠재력을 가지고 있습니다. 또한, 자연어로 표현된 중간 출력을 통해 MoA는 모델의 해석 가능성을 개선합니다. 이 개선된 해석 가능성은 인간의 추론과 더 잘 맞추도록 도와줍니다.

[2406.04692v1.pdf

1.10MB](./file/2406.04692v1.pdf)
