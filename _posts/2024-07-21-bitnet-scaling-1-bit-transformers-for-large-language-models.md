---
title: "BitNet: Scaling 1-bit Transformers for Large Language Models"
date: 2024-07-21 16:00:16
categories:
  - 인공지능
---

<https://arxiv.org/abs/2310.11453>

[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

### 초록

대형 언어 모델의 크기가 커짐에 따라 배포에 어려움이 생기고 높은 에너지 소비로 인한 환경 영향에 대한 우려가 커지고 있습니다. 본 연구에서는 대형 언어 모델을 위한 확장 가능하고 안정적인 1비트 Transformer 아키텍처인 BitNet을 소개합니다. 구체적으로, 우리는 nn.Linear 레이어를 대체하기 위해 BitLinear를 도입하여 1비트 가중치를 처음부터 학습할 수 있도록 합니다. 언어 모델링 실험 결과, BitNet은 메모리 사용량과 에너지 소비를 크게 줄이면서도 최첨단 8비트 양자화 방법과 FP16 Transformer 기준과 비교하여 경쟁력 있는 성능을 보여줍니다. 또한, BitNet은 완전 정밀도의 Transformer와 유사한 확장 법칙을 보이며, 효율성과 성능 이점을 유지하면서도 더 큰 언어 모델로 효과적으로 확장할 수 있는 잠재력을 제시합니다.

![](/assets/images/posts/214/img.png)

### 그림 1

BitNet은 처음부터 1비트 Transformer를 학습하여 에너지 효율적인 방식으로 경쟁력 있는 결과를 얻습니다. BitNet은 최첨단 양자화 방법을 크게 능가합니다. 모델 크기가 커질수록 비용 절감 효과가 더 커지며 FP16으로 학습된 모델과 경쟁력 있는 성능을 달성합니다.

"인간 지능에 대해 독특한 점이 있다고 생각하지 않습니다. 지각과 감정을 구성하는 뇌의 모든 뉴런은 이진 방식으로 작동합니다."

- William Henry Gates

### 1. 서론

대형 언어 모델의 급속한 성장은 다양한 작업에서 큰 개선을 이루었습니다 [BMR+20, Ope23, CND+22, ADF+23, TLI+23, TMS+23]. 그러나 이러한 모델을 호스팅하는 것은 높은 추론 비용과 에너지 소비로 인해 비쌉니다. 모델의 크기가 커질수록 모델 매개변수를 액세스하고 처리하는 데 필요한 메모리 대역폭이 주요 병목 현상이 되어 전체 추론 성능을 제한합니다. 또한, 이러한 모델을 분산 시스템이나 다중 장치 플랫폼에 배포할 때, 장치 간 통신 오버헤드가 추론 지연 시간과 에너지 소비에 크게 영향을 미칠 수 있습니다. 모델 양자화 [FAHA23, CCKS23, XLS+23]는 메모리 사용량과 계산 비용을 크게 줄이면서도 경쟁력 있는 성능을 유지할 수 있는 유망한 솔루션으로 떠오르고 있습니다.

대부분의 기존 대형 언어 모델 양자화 접근법은 훈련 후 양자화입니다. 이는 훈련 파이프라인을 변경하거나 모델을 재훈련할 필요가 없기 때문에 간단하고 적용하기 쉽습니다. 그러나 이는 정밀도가 낮아질수록 더 큰 정확도 손실을 초래합니다. 모델이 훈련 중에 양자화된 표현에 최적화되지 않기 때문입니다.

깊은 신경망을 양자화하는 또 다른 접근법은 양자화 인식 훈련입니다. 훈련 후 양자화에 비해, 이는 일반적으로 더 나은 정확도를 제공합니다. 모델이 처음부터 감소된 정밀도를 고려하여 훈련되기 때문입니다. 또한, 이는 모델이 계속해서 훈련하거나 미세 조정을 수행할 수 있도록 하여, 대형 언어 모델에 필수적입니다. 양자화 인식 훈련의 주요 도전 과제는 최적화에 있으며, 정밀도가 낮아질수록 모델의 수렴이 더 어려워집니다. 또한, 양자화 인식 훈련이 신경 언어 모델의 확장 법칙을 따르는지 여부는 알려져 있지 않습니다.

본 연구에서는 대형 언어 모델에 적용된 양자화의 극단적인 경우인 1비트화에 중점을 둡니다. 이전의 이진화 신경망 연구 [RORF16, BT19]는 대부분 컨볼루션 신경망에 집중되어 있었습니다. 최근에는 이진화된 Transformer에 대한 연구가 일부 있었으나, 이러한 연구는 주로 기계 번역 또는 BERT 사전 훈련에 초점을 맞추었으며, 이는 대형 언어 모델과는 상당히 다릅니다. 예를 들어, 기계 번역은 인코더-디코더 아키텍처를 사용하고, BERT 사전 훈련은 양방향 인코더를 활용하며, 대형 언어 모델은 단방향 디코더를 사용합니다. 더욱이, 대형 언어 모델은 일반적으로 훨씬 더 큰 모델 크기로 확장되는 반면, BERT와 기계 번역 모델은 그렇게 광범위한 확장을 겪지 않습니다.

우리가 아는 한, 이 연구는 1비트 대형 언어 모델에 대한 양자화 인식 훈련을 조사한 최초의 연구입니다. 우리는 BitNet을 제안하는데, 이는 대형 언어 모델을 위한 1비트 Transformer 아키텍처로, 메모리와 계산 면에서 효율적으로 확장하는 것을 목표로 합니다. BitNet은 훈련 중에 최적화 상태와 그래디언트에 높은 정밀도를 유지하면서, 저정밀도의 이진 가중치와 양자화된 활성화를 사용합니다. 우리의 접근법은 확장 가능하고 안정적으로 설계되어 대형 언어 모델을 효율적으로 처리할 수 있습니다. BitNet 아키텍처의 구현은 매우 간단하며, Transformer의 선형 투영(nn.Linear in PyTorch)만 교체하면 됩니다. 또한, BitNet은 PagedAttention [KLZ+23], FlashAttention [DFE+22, Dao23], 및 추측 디코딩 [LKM23]과 같은 대형 언어 모델을 위한 다른 가속 방법과 상호 보완적입니다.

우리는 BitNet을 다양한 언어 모델링 벤치마크에서 평가하여 최첨단 양자화 방법 및 FP16 Transformer와 비교했습니다. 실험 결과, BitNet은 퍼플렉시티(perplexity)와 다운스트림 작업 정확도 측면에서 경쟁력 있는 성능을 달성했습니다. 더 중요한 것은, BitNet이 메모리 사용량과 에너지 소비를 기준 모델에 비해 크게 줄인다는 것입니다. 또한, BitNet이 완전 정밀도의 Transformer와 유사한 확장 법칙을 따르며, 성능과 효율성 면에서 잠재적인 이점을 가지고 더 큰 언어 모델로 효과적으로 확장할 수 있음을 보여줍니다.

![](/assets/images/posts/214/img_1.png)

### 그림 2

(a) BitLinear의 계산 흐름. (b) BitNet의 아키텍처로, BitLinear로 구현된 행렬 곱셈이 포함된 주의(attentions)와 FFNs의 스택으로 구성되어 있습니다.

### 2. BitNet

BitNet은 Figure 2에서 보여주는 것처럼, 자가 주의 및 피드포워드 네트워크 블록을 쌓는 방식으로 Transformer와 동일한 레이아웃을 사용합니다. 기존 Transformer와 비교하여, BitNet은 일반적인 행렬 곱셈 대신 BitLinear(Eq. 11)을 사용하며, 이는 이진화된(즉, 1비트) 모델 가중치를 채택합니다. 우리는 실험에서 다른 구성 요소를 높은 정밀도로 유지했습니다. 예를 들어, 8비트로 설정했습니다. 이를 요약하면 다음과 같습니다. 첫째, 잔차 연결과 레이어 정규화는 대형 언어 모델에 거의 계산 비용을 추가하지 않습니다. 둘째, QKV 변환의 계산 비용은 모델이 커질수록 매개변수 투영에 비해 훨씬 적습니다. 셋째, 입력/출력 임베딩의 정밀도를 유지해야 언어 모델이 샘플링을 수행할 때 높은 정밀도의 확률을 사용할 수 있습니다.

### 2.1 BitLinear

우리는 먼저 가중치를 signum 함수를 사용하여 +1 또는 -1로 이진화합니다. [LOP+22]를 따라, 이진화 전에 가중치를 중앙 집중화하여 제한된 수치 범위 내에서 용량을 늘립니다. 이진화 후 스케일링 계수 β를 사용하여 실수형 가중치와 이진화된 가중치 사이의 l2 오차를 줄입니다. 가중치 W ∈ R^(n×m)의 이진화는 다음과 같이 공식화할 수 있습니다:

![](/assets/images/posts/214/img_2.png)

우리는 또한 활성화를 b비트 정밀도로 양자화합니다. [DLBZ22]를 따라, absmax 양자화를 사용하여 활성화를 [−Q\_b, Q\_b] (Q\_b = 2^(b−1)) 범위로 스케일링하고 입력 행렬의 절대 최대값으로 나눕니다:

![](/assets/images/posts/214/img_3.png)

여기서 ϵ는 클리핑 수행 시 오버플로우를 방지하는 작은 실수입니다. 비선형 함수(예: ReLU) 이전의 활성화에 대해서는, 모든 값을 비음수로 만들기 위해 입력의 최소값을 빼서 [0, Qb] 범위로 스케일링합니다:

![](/assets/images/posts/214/img_4.png)

본 연구에서는 활성화를 8비트로 양자화하며, 낮은 정밀도는 향후 연구 과제로 남깁니다. 더욱이, 양자화는 훈련 중에는 텐서 단위로, 추론 중에는 토큰 단위로 수행되어 안정성과 효율성을 모두 확보합니다.

위 양자화 방정식을 사용하여 행렬 곱셈은 다음과 같이 작성할 수 있습니다:

![](/assets/images/posts/214/img_5.png)

우리는 W와 x의 요소가 상호 독립적이며 동일한 분포를 공유하고, W와 x가 서로 독립적이라고 가정합니다. 그러면 출력 y의 분산은 다음과 같이 추정됩니다:

![](/assets/images/posts/214/img_6.png)

정밀한 계산의 경우, 출력 Var(y)의 분산은 표준 초기화 방법(예: Kaiming 초기화 또는 Xavier 초기화)으로 1의 규모로 설정되며, 이는 훈련 안정성에 큰 이점을 제공합니다. 양자화 후 분산을 유지하기 위해, 활성화 양자화 전에 LayerNorm[BKH16] 함수를 도입합니다. 이 방식으로, 출력 y의 분산은 Var(y) ≈ E[LN(x)^2] = 1로 추정되며, 이는 정밀한 대응물 Var(y)와 동일한 규모를 가집니다. Transformer의 문맥에서, 이는 SubLN[WMH+22]과 동일한 구현을 가집니다. SubLN과 위의 양자화 방법을 사용하여, BitLinear는 다음과 같이 공식화됩니다:

![](/assets/images/posts/214/img_7.png)

Figure 2는 BitLinear의 계산 흐름을 보여줍니다. SubLN 연산 후, absmax 함수를 사용하여 활성화를 양자화합니다. 행렬 곱셈은 1비트 가중치와 양자화된 활성화 사이에서 수행됩니다. 출력 활성화는 {β, γ}로 재스케일되어 원래 정밀도로 디양자화됩니다.

### 모델 병렬 처리와 그룹 양자화 및 정규화

대형 언어 모델을 확장하는 데 필수적인 기술 중 하나는 모델 병렬 처리입니다 [SPP+19]. 이는 행렬 곱셈을 여러 장치에 분할하는 방식입니다. 기존 모델 병렬 처리 접근 방식의 전제 조건은 텐서들이 분할 차원에서 독립적이어야 한다는 것입니다. 그러나 α, β, γ, η 모든 매개변수는 전체 텐서에서 계산되므로 독립성 전제 조건을 깨뜨립니다. 이를 해결하는 한 가지 방법은 각 매개변수에 대해 전체-감소(all-reduce) 연산을 도입하는 것입니다. 하지만 각 매개변수에 대한 통신이 적더라도 모델이 깊어짐에 따라 동기화의 양이 증가하여 전방 패스를 크게 느리게 합니다. 이 문제는 평균과 분산이 분할 차원에서 추정되어야 하는 SubLN에서도 발생합니다.

이를 해결하기 위해, 우리는 모델 병렬 처리를 보다 효율적으로 만드는 간단한 접근 방식을 제안합니다. 가중치와 활성화를 그룹으로 나눈 후 각 그룹의 매개변수를 독립적으로 추정합니다. 이렇게 하면 매개변수를 추가 통신 없이 로컬에서 계산할 수 있습니다. 이 접근 방식은 그룹 양자화(Group Quantization)라고 하며, 다음과 같이 공식화됩니다:

![](/assets/images/posts/214/img_8.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/214/img_9.png)

![](/assets/images/posts/214/img_10.png)

![](/assets/images/posts/214/img_11.png)

![](/assets/images/posts/214/img_12.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

![](/assets/images/posts/214/img_13.png)

### 표 1: BitNet과 Transformer의 에너지 소비 (모델 크기별)

결과는 입력 길이를 512로 설정하여 보고되었습니다.

### 2.2 모델 훈련

**직접 통과 추정기(Straight-through Estimator, STE)**: 1비트 모델을 훈련하기 위해, 우리는 역전파 동안 기울기를 근사하기 위해 직접 통과 추정기(STE)[BLC13]를 사용합니다. 이 방법은 역방향 패스 동안 Sign(Eq. 2)과 Clip(Eq. 5) 함수와 같은 비분화 가능 함수들을 우회합니다. STE는 이러한 비분화 가능 함수들에 영향을 받지 않고 네트워크를 통해 기울기가 흐르도록 하여, 양자화된 모델을 훈련할 수 있게 합니다.

**혼합 정밀도 훈련**: 가중치와 활성화가 낮은 정밀도로 양자화되는 동안, 기울기와 옵티마이저 상태는 훈련의 안정성과 정확성을 보장하기 위해 높은 정밀도로 저장됩니다. 이전 연구[LSL+21]를 따라, 우리는 학습 가능한 매개변수의 업데이트를 축적하기 위해 높은 정밀도의 잠재 가중치를 유지합니다. 잠재 가중치는 전방 패스 동안 즉석에서 이진화되며, 추론 과정에서는 사용되지 않습니다.

**큰 학습률**: 최적화의 한 가지 문제는 잠재 가중치의 작은 업데이트가 1비트 가중치에 거의 변화를 주지 않는다는 것입니다. 이는 1비트 가중치에 기반하여 추정된 편향된 기울기와 업데이트를 초래합니다. 이 문제는 훈련 초기 단계에서 더욱 심각해지며, 이 단계에서는 모델이 가능한 한 빨리 수렴해야 합니다. 이 문제를 해결하기 위해 다양한 방법을 탐구한 결과, 학습률을 높이는 것이 최적화를 가속화하는 가장 간단하고 최선의 방법이라는 결론을 내렸습니다. 우리의 실험은 BitNet이 큰 학습률에서 수렴하는 반면, FP16 Transformer는 같은 학습률에서 훈련 초기 단계에 발산한다는 것을 보여줍니다. 자세한 내용은 3장에서 확인할 수 있습니다.

### 2.3 계산 효율성

우리는 산술 연산 에너지와 메모리 사용량 측면에서 BitNet의 계산 효율성을 추정합니다. 주로 대형 언어 모델의 비용에 가장 크게 기여하는 행렬 곱셈의 계산에 중점을 둡니다.

**산술 연산 에너지**: [Hor14, ZZL22]의 에너지 모델에 따르면, 다양한 산술 연산의 에너지 소비는 다음과 같이 추정할 수 있습니다:

![](/assets/images/posts/214/img_14.png)

표 2: 45nm 및 7nm 공정 노드에서 다양한 비트 표현에 대한 ADD 및 MUL 에너지 소비[Hor14, ZZL22].

기본 Transformer의 경우, 차원이 m × n 및 n × p인 행렬 곱셈의 에너지 소비는 다음과 같이 계산할 수 있습니다:

![](/assets/images/posts/214/img_15.png)

### 3. FP16 Transformer와의 비교

#### 3.1 설정

우리는 125M에서 30B까지 다양한 규모의 BitNet을 사용하여 일련의 자동 회귀 언어 모델을 훈련합니다. 모델은 Pile 데이터셋, Common Crawl 스냅샷, RealNews, CC-Stories 데이터셋으로 구성된 영어 코퍼스를 사용하여 훈련됩니다. 데이터 전처리를 위해 Sentencpiece 토크나이저를 사용하며, 어휘 크기는 16K입니다. 공정한 비교를 위해 동일한 데이터셋과 설정을 사용하여 Transformer 기준 모델도 훈련합니다. 자세한 내용은 부록에서 확인할 수 있습니다.

![](/assets/images/posts/214/img_16.png)

그림 3: BitNet과 FP16 Transformer의 스케일링 곡선.

#### 3.2 추론 최적화 스케일링 법칙

신경 언어 모델은 일반적인 Transformer 아키텍처로 예측 가능하게 확장된다는 것이 입증되었습니다[KMH+20]. 손실은 훈련에 사용된 계산량에 따라 멱법칙으로 확장됩니다. 이를 통해 계산 예산의 최적 할당을 결정하고, 더 작은 모델로부터 대형 언어 모델의 성능을 예측할 수 있습니다.

이진화된 Transformer의 스케일링 법칙을 연구하기 위해, 우리는 BitNet과 FP16 Transformer 기준 모델의 파라미터 수에 따른 스케일링 곡선을 그리기 시작합니다. 훈련 토큰 수를 고정하고 모델 크기를 다양하게 설정합니다. Figure 3은 BitNet의 손실 스케일링이 FP16 Transformer와 유사하며, 이는 멱법칙을 따름을 보여줍니다. 그런 다음 우리는 비축소 손실 항을 포함한 스케일링 법칙에 맞춥니다:

![](/assets/images/posts/214/img_17.png)

스케일링 법칙이 손실을 정확하게 예측할 수 있는지 평가하기 위해, 우리는 125M에서 6.7B 모델을 선택하여 멱법칙의 매개변수를 맞추고, 이 법칙을 사용하여 13B와 30B 모델의 손실을 예측합니다. 결과는 스케일링 법칙이 BitNet의 손실을 높은 정확도로 예측함을 보여줍니다. 또한 모델 크기가 커짐에 따라 BitNet과 FP16 Transformer 간의 격차가 줄어듭니다.

위의 멱법칙은 BitNet의 스케일링 추세를 측정하지만, 손실과 실제 계산량 간의 관계를 적절히 모델링하지는 않습니다. 이전 연구[KMH+20, HKK+20, HBM+22]는 FLOPs를 계산하여 계산량을 추정합니다. 그러나 이는 정수 계산이 주를 이루는 1비트 모델에는 적용되지 않습니다. 또한, 이는 주로 훈련 계산량을 측정하며 추론을 측정하지는 않습니다. 신경 언어 모델의 스케일링 효율성을 더 잘 이해하기 위해, 우리는 추론 최적화 스케일링 법칙을 도입합니다. 이는 에너지 소비에 따른 손실을 예측합니다. 모델 사용에 따라 스케일링되는 추론 에너지 비용에 중점을 둡니다. 에너지 소비는 2.3절에서 설명한 대로 추정합니다. Figure 3은 7nm 공정 노드에서 추론 에너지 비용에 따른 스케일링 곡선을 보여줍니다. 이는 BitNet이 훨씬 높은 스케일링 효율성을 가짐을 입증합니다. 고정된 계산 예산을 감안할 때, BitNet은 훨씬 더 나은 손실을 달성합니다. 동시에, 동일한 성능을 얻기 위해 FP16 모델보다 추론 비용이 훨씬 적습니다.

![](/assets/images/posts/214/img_18.png)

**Figure 4**: BitNet과 FP16 Transformer의 추론 비용 대비 0-shot(왼쪽) 및 few-shot(오른쪽) 성능.

![](/assets/images/posts/214/img_19.png)

**Figure 5**: 동일한 학습률에서 FP16 Transformer보다 더 안정적인 BitNet(왼쪽). 훈련 안정성 덕분에 BitNet은 더 큰 학습률을 가능하게 하여 더 나은 수렴을 이끌어냄(오른쪽).

### 3.3 다운스트림 작업 결과

손실뿐만 아니라 BitNet의 확장 가능성에도 관심이 있습니다. 손실과 비교하여, 신경 언어 모델의 출현적 특성 때문에 용량을 예측하는 것은 더 어렵습니다. 해석 가능한 메트릭으로 용량을 평가하기 위해, Hellaswag [ZHB+19], Winogrande [SBBC20], Winograd [LDM12], Storycloze [MCH+16]를 포함한 네 가지 다운스트림 작업에서 0-shot 및 4-shot 결과를 테스트합니다. Figure 4는 다양한 규모의 BitNet과 FP16 Transformer의 평균 결과를 보고합니다. 손실 스케일링 곡선과 유사하게, 다운스트림 작업의 성능도 계산 예산이 증가함에 따라 확장될 수 있습니다. 또한, 용량의 스케일링 효율성은 0-shot 및 few-shot 성능 모두에서 FP16 Transformer 기준 모델보다 훨씬 높습니다.

### 3.4 안정성 테스트

저비트 Transformer를 훈련할 때 주요 도전 과제는 최적화의 안정성입니다. 따라서 BitNet과 FP16 기준 모델 모두에 대해 피크 학습률을 다양하게 설정하여 일련의 모델을 훈련하는 안정성 테스트를 수행합니다. Figure 5a는 안정성 테스트 결과를 보여줍니다. 결과는 BitNet이 큰 학습률로 수렴할 수 있는 반면, FP16 Transformer는 그렇지 않음을 보여주며, BitNet의 더 나은 훈련 안정성을 입증합니다. 이 최적화의 장점은 더 큰 학습률로 훈련을 가능하게 합니다. Figure 5b는 BitNet이 학습률 증가로부터 이점을 얻어, PPL 측면에서 더 나은 수렴을 달성함을 보여줍니다.

### 4. 사후 훈련 양자화와의 비교

#### 4.1 설정

우리는 3.1절에서 설명한 동일한 설정으로 BitNet을 훈련합니다. BitNet을 최첨단 양자화 방법인 Absmax [DLBZ22], SmoothQuant [XLS+23], GPTQ [FAHA23], QuIP [CCKS23]와 비교합니다. 이러한 방법들은 FP16 Transformer 모델에 대한 사후 훈련 양자화 방법으로, BitNet과 동일한 훈련 설정과 데이터를 따릅니다. 이들 중 Absmax와 SmoothQuant는 가중치와 활성화 모두를 양자화하는 반면, GPTQ와 QuIP는 가중치의 정밀도만 감소시킵니다. 우리는 다양한 양자화 수준으로 이러한 방법들을 적용합니다. 가중치만 양자화하는 경우(GPTQ와 QuIP)에는 W4A16과 W2A16을 실험합니다. 가중치와 활성화를 양자화하는 경우(Absmax와 SmoothQuant)에는 FP16 Transformer를 W8A8, W4A4, W1A8로 양자화합니다. BitNet의 구현은 이진 가중치와 8비트 활성화(W1A8)로, 기준 모델보다 낮거나 동일한 비트를 사용합니다.

![](/assets/images/posts/214/img_20.png)

**Figure 6**: 다운스트림 작업에서 BitNet과 사후 훈련 양자화 기준 모델의 제로샷(왼쪽) 및 few-shot(오른쪽) 결과.

![](/assets/images/posts/214/img_21.png)

**표 3**: BitNet과 기준 모델의 제로샷 결과 (PTQ: 사후 훈련 양자화, WGe: Winogrande, WG: Winograd, SC: Storycloze, HS: Hellaswag 데이터셋).

#### 4.2 결과

표 3은 제안된 방법인 BitNet과 여러 기준 접근법을 네 가지 벤치마크 데이터셋(Winogrande, Winograd, Storycloze, Hellaswag)에서 비교한 상세한 분석을 제공합니다. 모든 모델의 크기는 공정한 비교를 위해 6.7B입니다. 이 방법들은 16비트에서 1비트까지 다양한 가중치 비트 수준에서 평가됩니다. 다운스트림 작업에서의 제로샷 정확도 외에도, 각 방법의 성능을 종합적으로 이해하기 위해 검증 세트에서의 언어 모델 perplexity도 평가 지표로 포함되었습니다.

결과는 특히 낮은 비트 수준에서 BitNet이 기준 접근법과 비교하여 경쟁력 있는 성능 수준을 달성함을 보여줍니다. BitNet의 제로샷 점수는 8비트 모델과 비교할 만하며, 추론 비용은 훨씬 낮습니다. 4비트 모델의 경우, 가중치만 양자화하는 방법이 가중치와 활성화 모두를 양자화하는 방법보다 우수합니다. 이는 활성화의 양자화가 더 어렵기 때문입니다. 1비트 모델인 BitNet은 가중치와 활성화 양자화 방법과 가중치만 양자화하는 방법보다 훨씬 더 나은 결과를 달성합니다. 낮은 비트 모델의 경우, BitNet은 모든 기준 모델보다 일관되게 우수한 점수를 기록합니다. 이는 사후 훈련 양자화 방법보다 양자화 인식 훈련 접근법의 장점을 입증합니다. Figure 6은 모델 크기를 1.3B에서 6.7B로 확장하면서 제로샷 정확도와 few-shot 정확도를 요약합니다. 이 결과는 다양한 규모에서 BitNet의 우위가 일관됨을 입증합니다.

![](/assets/images/posts/214/img_22.png)

**표 4**: BitNet의 분할 실험 결과 (WGe: Winogrande, WG: Winograd, SC: Storycloze, HS: Hellaswag 데이터셋). Elastic은 [LOP+22]에서 온 활성화 양자화 방법이며, BMT는 [ZGC+23]에서 저비트 모델의 훈련 안정성을 위해 사용된 아키텍처입니다.

### 5. 소거 연구

표 4에서는 여러 대체 접근법과 비교한 소거 연구(ablation study) 결과를 제시합니다. 우리는 활성화 양자화 접근법과 모델 훈련 안정화 기술 선택의 효과를 조사합니다. BitNet은 활성화를 양자화하기 위해 absmax를 사용하고, 훈련 안정성을 위해 SubLN을 사용합니다. 한 가지 양자화 대안은 학습 가능한 매개변수로 스케일을 동적으로 조정하는 탄력 함수(elastic function) [LOP+22]입니다. 실험 결과, absmax가 탄력 함수보다 성능이 더 좋다는 것을 발견했습니다. 또한, absmax 함수는 훈련을 더 안정적으로 만들어 BitNet에 더 큰 학습률을 가능하게 합니다. 우리는 또한 SubLN을 Pre-LN과 BMT 아키텍처 [ZGC+23]와 비교했습니다. Pre-LN은 GPT 사전 훈련의 기본 아키텍처이고, BMT는 이진화된 모델의 안정성을 향상시키는 것으로 입증되었습니다. 실험 결과, SubLN이 Pre-LN과 BMT 모두를 능가하는 것으로 나타났습니다. 따라서 우리는 BitNet의 구현에서 absmax와 SubLN을 선택합니다.

### 6. 결론 및 향후 과제

우리는 대형 언어 모델을 위한 새로운 1비트 Transformer 아키텍처인 BitNet을 소개합니다. 우리의 접근법은 확장 가능하고 안정적이며, 대형 언어 모델을 효율적으로 처리할 수 있도록 설계되었습니다. 실험 결과, BitNet은 perplexity와 다운스트림 작업 성능 모두에서 경쟁력 있는 성능을 달성하면서, 메모리 사용량과 에너지 소비를 기준 모델에 비해 크게 줄였습니다. 또한, BitNet은 완전 정밀도 Transformer와 유사한 스케일링 법칙을 따르므로, 성능과 효율성 측면에서 잠재적인 이점을 가지고 더 큰 언어 모델로 효과적으로 확장될 수 있음을 나타냅니다. 향후에는 모델 크기와 훈련 단계를 확장하여 BitNet을 더욱 발전시키고자 합니다. 또한, BitNet을 다른 아키텍처(예: RetNet [SDH+23])에 적용하여 대형 언어 모델을 훈련하는 데 관심이 있습니다.

[2310.11453v1.pdf

0.56MB](./file/2310.11453v1.pdf)
