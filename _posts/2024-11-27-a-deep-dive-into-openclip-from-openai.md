---
title: "A Deep Dive Into OpenCLIP from OpenAI"
date: 2024-11-27 21:27:13
categories:
  - 인공지능
---

<https://wandb.ai/johnowhitaker/openclip-benchmarking/reports/A-Deep-Dive-Into-OpenCLIP-from-OpenAI--VmlldzoyOTIzNzIz>

[A Deep Dive Into OpenCLIP from OpenAI](https://wandb.ai/johnowhitaker/openclip-benchmarking/reports/A-Deep-Dive-Into-OpenCLIP-from-OpenAI--VmlldzoyOTIzNzIz)

2021년 초, OpenAI는 자연어 감독을 통해 시각적 개념을 효율적으로 학습하는 모델인 CLIP을 발표했습니다. CLIP은 매우 강력하며, 데이터 필터링과 검색부터 AI 예술 창작까지 원래 창작자들이 예상했던 범위를 훨씬 넘어서는 다양한 용도로 사용되고 있습니다. 이 글에서는 CLIP이 어떻게 작동하는지와, 오픈 소스 구현을 만들고 훈련한 OpenCLIP 프로젝트에 대해 알아보겠습니다.

![](/assets/images/posts/347/img.png)

CLIP과 스테이블 디퓨전을 사용하여 "대조적 언어-이미지 사전 훈련을 CLIP으로 시각화한 디지털 아트"라는 프롬프트로 생성한 CLIP의 자화상.

CLIP 모델은 어떻게 작동할까?

CLIP(Contrastive Language-Image Pre-training)은 OpenAI에서 개발한 방법으로, 이미지와 텍스트 표현을 정렬할 수 있는 모델을 훈련하는 기술입니다. 이미지와 텍스트는 완전히 다른 양상(modality)을 가지고 있지만, CLIP은 이 둘을 공유된 공간으로 매핑하여 여러 흥미로운 활용을 가능하게 합니다.

![](/assets/images/posts/347/img_1.png)

대조적 사전 훈련 (CLIP 블로그 포스트에서 발췌)

훈련 과정에서 CLIP은 이미지-캡션 쌍을 입력으로 받습니다. 이미지는 이미지 인코더 모델(ResNet 또는 ViT 백본을 기반으로 함)을 통해 처리되어 임베딩 벡터로 변환됩니다(위 다이어그램에서 I로 표시). 두 번째 모델(일반적으로 Transformer 계열의 모델)은 텍스트를 받아 이를 임베딩 벡터(T로 표시)로 변환합니다. 중요한 점은 이 두 임베딩이 같은 형태를 가진다는 점으로, 이를 통해 직접적인 비교가 가능합니다. 이미지-캡션 쌍의 배치를 주어진 상황에서, CLIP은 서로 연결된 이미지와 텍스트 임베딩의 유사성(다이어그램의 파란색 대각선)을 최대화하려 하며, 관련이 없는 쌍 사이의 유사성은 최소화하려고 합니다. 그 결과로 이미지 인코더와 텍스트 인코더는 공유된 공간에서 임베딩을 생성하게 되며, 여기서 비슷한 개념(시각적이거나 텍스트적인)이 서로 가깝고, 관련 없는 개념은 멀어지게 됩니다. 이러한 임베딩은 이미지와 텍스트에서 풍부한 의미론적 정보를 캡처하며, 이는 다양한 다운스트림 작업, 예를 들어 제로샷 이미지 분류(아래에서 탐구할 예정)와 같은 작업에 매우 유용하게 사용됩니다.

OpenCLIP & LAION: 로망 보몽과의 인터뷰

OpenAI는 4억 개의 이미지-캡션 쌍으로 구성된 내부 데이터셋으로 여러 가지 모델 변형을 훈련했습니다. 그들은 모델의 가중치는 공유했지만 훈련에 사용된 데이터는 공개하지 않았습니다. 여기에서 등장하는 OpenCLIP은 오픈 소스 복제 시도로, 성공적으로 CLIP 구현체를 만들어 여러 훈련된 모델을 누구나 사용할 수 있도록 공개했습니다. 이 같은 프로젝트가 어떻게 시작되었는지 더 알아보기 위해, LAION을 설립하고 데이터셋 생성에서 모델 훈련에 이르기까지 OpenCLIP 프로세스의 모든 단계에 깊이 관여한 로망 보몽(Romain Beaumont)과 이야기를 나누었습니다.

<https://x.com/rom1504/status/1570498422515564544>

Q: OpenAI는 4억 개의 이미지-캡션 쌍을 이용해 훈련했기 때문에, 그들의 모델을 재창조하는 데 중요한 부분은 비슷한 데이터셋을 구성하는 것이었습니다. LAION과 그 과정이 어떻게 이루어졌는지 간단히 말씀해 주실 수 있나요? 이 글을 쓰고 있는 현재로부터 약 1.5년 전, OpenAI는 CLIP과 DALL-E에 대한 훌륭한 블로그 게시물과 관련 논문을 발표하며, 이러한 데이터를 바탕으로 제로샷 분류, 검색, 이미지 생성이 가능한 모델을 구축할 수 있음을 증명했습니다. EleutherAI 디스코드에서 이야기를 나누던 한 그룹은 이러한 대규모 데이터셋을 공개적으로 제공하는 것이 멋진 일일 것이라 생각했고, 그것을 목표로 시작했습니다. 결국 LAION 조직이 설립되었고, 4억 쌍의 데이터셋이 만들어졌으며, 그 후 50억 쌍의 더 큰 버전이 공개되었습니다. 이 데이터셋은 스테이블 디퓨전(Stable Diffusion), OpenCLIP, Imagen, Phenaki, Make-A-Video 등 많은 텍스트+이미지 모델에서 사용되었습니다.

<https://x.com/laion_ai/status/1509629327599706115>

Q: 당신과 로스 와이트먼(Ross Wightman)은 최근에 몇 가지 대규모 모델을 훈련했는데, 그 중 하나는 CLIP 모델을 이용한 제로샷 ImageNet 분류에서 새로운 최첨단 성능을 기록했다고 들었습니다. 큰 모델을 훈련하는 것은 단순히 train.py를 실행하고 결과를 기대하는 것보다 더 많은 것이 있을 것 같습니다! 제가 본 당신의 open-clip Weights & Biases 작업 공간에는 약 350번의 실행이 기록되어 있던데, 어떤 실험들을 시도하고 있는지 궁금합니다. 약 1년 동안 CLIP을 재현하기 위해 노력해 왔습니다. 처음에는 OpenCLIP 초기 논문에서 언급된 대로 약 1,000 GPU 시간을 사용해 약 1천만 샘플로 저규모로 훈련했습니다. 이렇게 초기 코드가 개발되었고, 이를 통해 해당 방향에서 뭔가 해볼 가치가 있다는 것을 입증했습니다. 우리는 JUWELS 슈퍼컴퓨터에서 그랜트를 받아 OpenAI의 CLIP 모델을 L/14까지 재현했으며, 이는 약 10만 GPU 시간을 사용한 가장 큰 실험이었습니다. 이후 Stability 클러스터에서 추가적인 컴퓨팅 그랜트를 받아 최첨단 H/14 모델을 훈련하는 데 20만 GPU 시간을 사용했습니다. 이 과정의 각 단계마다 우리는 올바른 하이퍼파라미터(학습률, 배치 크기, 손실 함수의 형태, 그라디언트 체크포인팅 여부, 데이터셋 리샘플링 여부 등)를 찾기 위해 많은 실험을 했습니다. H/14 모델의 최신 반복에서는 20만 GPU 시간 규모에서 안정적으로 작동하도록 하는 새로운 도전과제가 있었습니다. 이를 위해 불량 GPU를 찾기 위한 도구를 개발했습니다. 또한 큰 모델은 불안정하다는 큰 도전과제가 있었으며, 결국 부동소수점 16(float16)에서 bfloat16으로 정밀도를 변경하는 것이 해결책임을 발견하기까지 약 15가지 매개변수 변형을 시도해야 했습니다. Weights & Biases는 손실 곡선이 제대로 동작하지 않을 때 이를 빨리 파악하는 데 매우 유용했습니다.

Q: 전체 훈련이 시작되면, 그 과정에 얼마나 참여하시나요? 로그를 지속적으로 확인하고, 새로운 설정으로 중지하고 다시 시작하며, 메트릭을 확인해 성능이 향상되는지 체크하는 것 같은데, 훈련이 진행되는 동안 당신의 하루 일과는 어떻게 되나요? 작업 흐름은 보통 이렇게 진행됩니다:

- 실행 시작 후 텍스트 로그에서 충돌이 없는지 확인
- W&B에서 손실 곡선이 감소하는지, 큰 스파이크가 없는지 확인
- W&B에 보고하는 동반 평가 실행 시작, 메트릭이 실제로 개선되고 있는지 확인
- 작업이 충돌할 경우 자동으로 대부분 재개
- 문제가 있을 경우 완전히 중지하고 조사

H/14 실행 기록에 이에 대한 자세한 내용이 문서화되어 있습니다.

Q: 시간을 내 주셔서 감사합니다. 이런 대규모 프로젝트를 스스로 시작하려는 다른 사람들에게 마지막으로 드릴 말씀이나 팁이 있나요? LAION은 여전히 더 많은 데이터셋(오디오, 비디오, 텍스트 등)을 구축하고 있으며, 다중 언어 CLIP 모델과 같은 흥미로운 모델을 훈련하기 위해 다른 사람들과 협력하고 있습니다. LAION은 열린 조직과 디스코드 채널을 운영 중이니, 관심이 있다면 언제든 참여해 도움을 제안해 주세요. 우리는 사람들이 기여하는 데 가장 좋은 방법을 안내할 수 있습니다.

OpenCLIP을 사용한 제로샷 분류

기억하세요, 이 모델들은 이미지 캡션을 정렬하는 것만 훈련되었습니다. 그럼 어떻게 이를 분류에 사용할 수 있을까요? 훈련된 CLIP 모델은 이미지와 텍스트 설명 간의 '유사도 점수'를 제공합니다. 이를 분류에 사용하려면 먼저 클래스 레이블을 'a photo of a {label}'과 같은 프롬프트로 텍스트로 변환하고, 이 텍스트 설명들을 CLIP 텍스트 인코더로 임베딩합니다. 그 후 이미지를 임베딩하고, 코사인 유사도와 같은 측정 방법을 사용하여 어떤 텍스트 임베딩이 이미지 임베딩과 가장 잘 맞는지 확인합니다. 잘 된다면, 가장 잘 맞는 텍스트는 올바른 레이블을 포함한 캡션이 될 것입니다.

![](/assets/images/posts/347/img_2.png)

제로샷 예측을 위한 CLIP (CLIP 블로그 게시물에서 발췌)

이 접근 방식을 사용하면 몇 가지 이미지 분류 작업에서 다른 CLIP 모델들이 얼마나 잘 수행하는지 알 수 있습니다. OpenCLIP GitHub 리포지토리의 예제 코드를 참고하여, 한 번에 256개의 이미지를 로드하고 클래스 레이블을 기반으로 이를 분류하는 노트북을 설정했습니다. 우선 저는 잘 알려진 Imagenet 데이터셋의 10개의 쉽게 분류할 수 있는 클래스를 포함한 작은 부분집합인 Imagenette 데이터셋으로 시작했습니다. 1,200만 개의 캡션 이미지로 훈련된 Resnet50 모델(RN50 + cc12M), OpenAI의 내부 데이터셋(4억 개 이미지)에서 훈련된 'Base' 크기의 ViT 기반 모델(패치 크기 32), OpenCLIP이 LAION 4억 데이터셋에서 훈련한 동등한 모델, 그리고 20억 개 이미지에서 훈련된 더 큰('Huge' 크기, 패치 크기 14) ViT 모델(LAION2B)을 비교했습니다. 예상대로, 가장 큰 모델이 정확도에서 최고 자리를 차지했지만, OpenAI와 OpenCLIP의 더 작은 ViT 모델들도 크게 뒤처지지 않았으며 거의 10배 빠르고 다운로드 크기도 훨씬 작았습니다(600MB 대 거의 4GB).

조금 더 어려운 데이터셋인 Imagewoof로 이동해 봅시다. 이 데이터셋은 구별하기 어려운 10개의 개 품종으로 구성되어 있습니다. 여기서 상대적으로 작은 데이터셋으로 훈련된 ResNet 기반의 작은 모델은 다른 모델들에 비해 상당히 고전합니다.

놀랍게도, 가장 큰 모델은 약 94%의 확률로 이미지를 올바른 클래스와 매칭할 수 있었습니다. 개 품종 분류를 전혀 훈련하지 않은 모델 치고는 대단한 성과입니다! Imagewoof 데이터셋에서 처음부터 훈련된 최고의 모델도 약 90%의 정확도를 기록할 뿐입니다.

추가적인 사용 사례

CLIP이 출시되었을 때, 주로 이미지 분류의 사전 훈련 방법이나 제로샷 분류, 검색과 같은 작업에 사용될 것이라고 예상되었을 것입니다. 그러나 CLIP은 곧 열정적인 개발자들에 의해 수많은 다른 창의적인 용도로 활용되었으며, 그 중 많은 것이 이후 등장한 AI 예술 기법들에 포함됩니다. 한 모델이 이렇게 많은 창의적 폭발을 불러일으키는 경우는 매우 드뭅니다!

![](/assets/images/posts/347/img_3.png)

CLIP 프롬프트 '수중 잠수함의 수채화 그림'과 일치하도록 imstack의 파라미터를 최적화하여 생성한 이미지

추가적인 CLIP 사용 사례의 불완전한 목록:

1. **이미지 검색**: 이미지와 입력된 텍스트 쿼리를 비교하여 강력한 검색 기능을 제공. 좋은 예로는 LAION을 검색할 수 있게 해주는 Clip front가 있음.
2. **생성된 이미지 순위 매기기**: 일부 텍스트-이미지 모델(예: CRAIYON)은 여러 이미지를 생성하고, CLIP을 사용하여 텍스트 프롬프트와 가장 잘 맞는 결과를 선택함.
3. **텍스트-이미지 모델의 조건으로 사용되는 의미론적으로 풍부한 임베딩**: CLIP은 이미지와 텍스트의 풍부한 표현을 학습하며, 이는 종종 Stable Diffusion과 같은 텍스트-이미지 모델의 조건 정보로 사용됨.
4. **이미지 생성 가이드**: CLIP은 또한 직접적인 생성 과정의 가이드로 사용될 수 있으며, 예를 들어 디퓨전 과정이나 이미지 생성기(VQGAN)의 파라미터를 최적화하는 손실 함수로도 사용됨. 제 강의(“The Generative Landscape”)에서는 CLIP을 사용해 다양한 이미지 생성기를 이끌고, 무조건적 디퓨전 모델을 안내하며, Neural Cellular Automata의 파라미터를 조정하고 GAN의 잠재 공간을 탐색합니다.

잠재적인 문제점

인터넷에서 가져온 원시 데이터로 훈련된 모델들처럼, CLIP도 결함이 없는 것은 아닙니다. 표현은 이미지 생성과 업로드의 주체, 언어 사용, 캡션 트렌드와 관련된 역사적 요소에 의해 왜곡됩니다. 따라서 많은 모델 변형들이 특정 인구 집단에 부정적인 영향을 미치고 여러 방식으로 해를 끼칠 수 있는 편향을 포함하고 있습니다. 예를 들어, 성인 콘텐츠 사이트의 캡션에 포함된 SEO 스팸으로 인해 일부 CLIP 모델 변형은 겉보기에 무해한 단어를 NSFW 콘텐츠와 연관짓기도 합니다. 만약 이 모델들을 실제로 사용하려고 한다면, 충분히 테스트하고 최대한 책임감 있게 사용하는 것이 중요합니다.

마지막 생각

CLIP은 굉장히 강력하고 다재다능한 도구로 입증되었으며, OpenCLIP 프로젝트의 노력 덕분에 이제는 누구나 접근할 수 있는 도구가 되었습니다. 이 깊이 있는 탐구가 여러분에게 CLIP을 도구함에 추가하도록 영감을 주었기를 바랍니다. 앞으로 CLIP을 이용한 다른 사용 사례들이 어떤 것일지 정말 기대됩니다!
