---
title: "DINOv3"
date: 2025-08-19 16:39:35
categories:
  - 인공지능
tags:
  - DINOv3
---

<https://arxiv.org/abs/2508.10104>

[DINOv3](https://arxiv.org/abs/2508.10104)

초록  
자기지도학습(self-supervised learning)은 수작업 데이터 라벨링의 필요성을 제거하고, 모델이 거대한 데이터셋과 대규모 아키텍처로 손쉽게 확장할 수 있도록 하는 잠재력을 지닌다. 특정 작업이나 도메인에 맞추어지지 않은 이 학습 패러다임은 단일 알고리즘만으로 자연 이미지부터 항공 이미지에 이르기까지 다양한 소스에서 시각적 표현을 학습할 수 있는 가능성을 제공한다. 본 기술 보고서에서는 이러한 비전을 실현하기 위한 중요한 이정표로 **DINOv3**를 소개하며, 단순하지만 효과적인 전략을 활용한다.

첫째, 데이터 준비, 설계, 최적화를 신중하게 수행함으로써 데이터셋과 모델 규모를 동시에 확장하는 이점을 활용한다.  
둘째, **Gram anchoring**이라 불리는 새로운 방법을 도입하여, 긴 학습 과정에서 발생하는 밀집(dense) 특성 맵의 품질 저하라는 기존의 알려진 문제를 효과적으로 해결한다.  
셋째, 해상도, 모델 크기, 텍스트 정렬(text alignment)에 대한 유연성을 추가로 높이는 사후(post-hoc) 전략을 적용한다.

그 결과, 본 연구는 파인튜닝 없이도 다양한 환경에서 특화된 최신(state-of-the-art) 기법들을 능가하는 다재다능한 비전 기초 모델(vision foundation model)을 제시한다. DINOv3는 다양한 비전 과제에서 탁월한 성능을 보이는 고품질 밀집 특성을 생성하며, 기존의 자기지도 및 약지도 기반 기초 모델들을 크게 능가한다. 또한, 다양한 자원 제약과 배포 시나리오에 맞추어 확장 가능한 솔루션을 제공함으로써, 폭넓은 작업과 데이터에서 최신 성능을 이끌어낼 수 있도록 설계된 **DINOv3 비전 모델 모음(suite)**을 함께 공개한다.

### 1. 서론

파운데이션 모델(Foundation models)은 현대 컴퓨터 비전에서 핵심적인 빌딩 블록이 되었으며, 단일 재사용 가능한 모델을 통해 다양한 작업과 도메인에 걸쳐 광범위한 일반화를 가능하게 한다. 자기지도학습(Self-Supervised Learning, SSL)은 이러한 모델을 학습시키는 강력한 접근법으로, 원시 픽셀 데이터(raw pixel data)로부터 직접 학습하고 이미지 내 패턴의 자연스러운 동시 발생(co-occurrence)을 활용한다.

약지도(weakly supervised) 및 완전지도(fully supervised) 사전학습(Radford et al., 2021; Dehghani et al., 2023; Bolya et al., 2025)이 고품질 메타데이터와 함께 제공되는 이미지 쌍을 요구하는 것과 달리, SSL은 방대한 원시 이미지 컬렉션을 활용한 학습을 가능하게 한다. 이는 사실상 무한에 가까운 학습 데이터 가용성 덕분에 대규모 시각 인코더(visual encoders)를 학습시키는 데 특히 효과적이다.

**DINOv2** (Oquab et al., 2024)는 이러한 강점을 잘 보여주며, 이미지 이해(image understanding) 과제에서 뛰어난 성과(Wang et al., 2025)를 거두었을 뿐 아니라 병리학(histopathology)과 같은 복잡한 도메인에서도 사전학습을 가능하게 했다(Chen et al., 2024).

SSL로 학습된 모델은 추가적으로 여러 유용한 특성을 지닌다.

- 입력 분포 변화(distribution shift)에 강인하다.
- 전역(global) 및 지역(local) 수준에서 강력한 특성을 제공한다.
- 물리적 장면 이해를 촉진하는 풍부한 임베딩(embeddings)을 생성한다.

SSL 모델은 특정 다운스트림 과제에 맞추어 학습된 것이 아니기 때문에, 다재다능하고 강인한 범용(generalist) 특성을 생성한다. 예를 들어, DINOv2 모델은 작업별 파인튜닝 없이도 다양한 과제와 도메인에서 뛰어난 성능을 보여주며, 단일 고정된 백본(frozen backbone)이 여러 목적에 활용될 수 있도록 한다.

특히 SSL은 병리학(Vorontsov et al., 2024), 생물학(Kim et al., 2025), 의료 영상(Pérez-García et al., 2025), 원격 탐사(remote sensing, Cong et al., 2022; Tolan et al., 2024), 천문학(Parker et al., 2024), 고에너지 입자 물리학(Dillon et al., 2022) 등 메타데이터가 부족한 도메인에서 활용하기 적합하다. 이러한 도메인들은 이미 DINOv2와 같은 파운데이션 모델로부터 이점을 얻을 수 있음이 입증되었다.

마지막으로, SSL은 인간 개입을 요구하지 않기 때문에, 웹 데이터가 폭발적으로 증가하는 상황 속에서 **지속적 학습(lifelong learning)** 에도 적합하다.

![](/assets/images/posts/592/img.png)

**그림 1 설명**

- (a) 수년간 ImageNet1k(IN1k)에서의 선형 프로빙(linear probing) 결과 진화. 완전지도(SL), 약지도(WSL), 자기지도(SSL) 방법 비교. SSL은 후발주자로 등장했음에도 빠르게 발전하여 최근 수년간의 ImageNet 정확도 수준에 도달했다.
- (b) 밀집 과제(dense tasks)에서 SSL이 가지는 고유한 잠재력을 보여주며, DINOv3는 최신 WSL 모델을 크게 능가한다.
- (c), (d) DINOv3를 자연 이미지(c)와 항공 이미지(d)에 대해 학습한 후 고해상도 이미지에서 얻은 특성을 PCA로 시각화한 결과.

### 실제 과제와 문제점

실제로 SSL(Self-Supervised Learning)의 핵심 약속, 즉 제약 없는 방대한 데이터를 활용하여 임의로 크고 강력한 모델을 만드는 것은 대규모 학습에서 여전히 도전적이다. Oquab et al. (2024)이 제안한 휴리스틱(heuristics)을 통해 모델 불안정성과 붕괴(collapse)는 완화되었지만, 더 큰 스케일로 확장할수록 새로운 문제가 발생한다.

1. 라벨이 없는 데이터 컬렉션에서 유용한 데이터를 어떻게 수집할 것인지 불명확하다.
2. 일반적인 학습 방식에서 사용하는 코사인 스케줄(cosine schedule)은 최적화 구간(optimization horizon)을 사전에 알아야 하는데, 대규모 이미지 코퍼스 학습에서는 이를 파악하기 어렵다.
3. 초기 학습 이후 시간이 지남에 따라 특성(feature) 성능이 점차 저하되며, 이는 패치 유사도 맵(patch similarity map) 시각화를 통해 확인된다. 이러한 현상은 ViT-Large(약 3억 매개변수) 이상의 대형 모델에서 긴 학습을 진행할 때 나타나며, DINOv2의 확장성을 저해한다.

### DINOv3의 제안

이러한 문제들을 해결하기 위해 본 연구에서는 **DINOv3**를 제안하며, 이는 대규모 SSL 학습을 한 단계 발전시킨다. 우리는 단일 고정된(frozen) SSL 백본이 범용 비주얼 인코더로 활용될 수 있음을 보여주며, 이는 까다로운 다운스트림 과제에서 최신(state-of-the-art) 성능을 달성하고, 지도학습 및 메타데이터 의존형 사전학습 전략을 능가한다.

우리 연구는 다음 세 가지 목표를 바탕으로 진행된다.

1. 다양한 작업과 도메인에 걸쳐 범용적으로 사용할 수 있는 파운데이션 모델 학습
2. 기존 SSL 모델의 **밀집 특성(dense features)** 한계를 개선
3. 바로 활용 가능한(off-the-shelf) 모델군을 배포

이 세 가지 목표를 중심으로 이후 논의를 전개한다.

![](/assets/images/posts/592/img_1.png)

**그림 2 설명**

- DINOv3 모델군을 자기지도 및 약지도 기반 모델군과 다양한 벤치마크에서 비교한 성능.
- 특히 밀집 벤치마크(dense benchmarks)에서 큰 격차로 다른 모델들을 능가하며, AM-RADIO(Heinrich et al., 2025)처럼 마스크 주석 priors를 활용하는 모델보다도 뛰어난 성능을 보인다.

### 강력하고 다재다능한 파운데이션 모델

DINOv3는 **모델 크기와 학습 데이터 확장**을 통해 두 축에서 높은 수준의 범용성을 제공한다.

- **첫째**, SSL 모델의 핵심 요구 조건은 모델을 고정(frozen) 상태로 유지하면서도 최첨단 성능을 내는 것이다. 이렇게 되면 단일 forward pass로 여러 작업에서 최신 결과를 낼 수 있어, 상당한 연산 자원을 절약할 수 있다. 이는 특히 엣지 디바이스 환경에서 매우 중요한 장점이다. (Sec. 6에서 DINOv3가 성공적으로 적용된 다양한 과제를 제시함.)
- **둘째**, 메타데이터에 의존하지 않는 확장 가능한 SSL 학습 파이프라인은 수많은 과학적 응용을 가능하게 한다. 웹 이미지나 관찰 데이터 등 다양한 이미지로 사전학습을 수행하면, SSL 모델은 광범위한 도메인과 과제에 걸쳐 일반화할 수 있다. Fig. 1(d)에 나타난 것처럼, DINOv3가 고해상도 항공 이미지에서 추출한 특성의 PCA 결과는 도로, 건물, 녹지를 명확히 구분하며 모델의 특성 품질을 잘 보여준다.

### Gram Anchoring을 통한 우수한 특성 맵

DINOv3의 또 다른 핵심은 **밀집 특성 맵(dense feature maps)의 큰 개선**이다. DINOv3 SSL 학습 전략은 고수준 의미적 과제(semantic tasks)에서 뛰어난 성능을 내는 동시에, 깊이 추정(depth estimation)이나 3D 매칭과 같은 기하학적 과제(geometric tasks)에 적합한 고품질 특성 맵을 생성하는 것을 목표로 한다.

이러한 모델은 후처리(post-processing) 최소화 혹은 전혀 없이도 곧바로 사용할 수 있는 밀집 특성을 제공해야 한다. 하지만 방대한 이미지로 학습할 때, 고수준 의미 이해와 밀집 특성 맵 품질 간에는 본질적인 충돌이 존재한다. 이 상충되는 목적은 대형 모델과 긴 학습 일정에서 밀집 특성 붕괴(dense feature collapse)로 이어진다.

이를 해결하기 위해, 우리는 새로운 **Gram Anchoring** 전략을 도입했고(자세한 내용은 Sec. 4), 이는 이러한 붕괴를 효과적으로 완화한다. 결과적으로 DINOv3는 DINOv2보다 훨씬 나은 밀집 특성 맵을 생성하며, 고해상도 상황에서도 깔끔한 품질을 유지한다(Fig. 3 참고).

### DINOv3 모델 패밀리

Gram Anchoring을 통해 밀집 특성 맵(dense feature map)의 붕괴 문제를 해결하면서, SSL 기반 대규모 학습의 잠재력이 본격적으로 열리게 되었다. 그 결과, 훨씬 더 큰 모델을 SSL로 학습할 수 있고, 이는 성능의 큰 도약으로 이어진다. 본 연구에서는 무려 **70억(7B) 파라미터** 규모의 DINO 모델 학습에 성공하였다.

그러나 이렇게 거대한 모델은 실행에 막대한 자원이 필요하기 때문에, 우리는 지식을 더 작은 모델로 압축하는 **지식 증류(distillation)** 기법을 적용하였다. 그 결과, 다양한 컴퓨터 비전 과제를 다룰 수 있도록 설계된 **DINOv3 모델 패밀리**를 제시한다. 이 모델군은 다양한 자원 제약 및 배포 환경에 적응 가능한 확장형 솔루션을 제공함으로써, 비전 분야의 최신 성능을 한 단계 끌어올리는 것을 목표로 한다.

증류 과정에서는 여러 규모의 모델 변형을 생성했으며, 여기에는 **Vision Transformer (ViT) Small, Base, Large** 및 **ConvNeXt 기반 아키텍처**가 포함된다. 특히 효율적이면서 널리 활용되는 **ViT-L** 모델은 다양한 과제에서 7B 규모의 교사 모델(teacher)에 근접한 성능을 달성하였다.

전반적으로 DINOv3 패밀리는 광범위한 벤치마크에서 강력한 성능을 보이며, 전역(global) 과제에서는 경쟁 모델들과 비슷하거나 더 나은 정확도를 달성하고, 밀집 예측 과제(dense prediction tasks)에서는 이들을 압도적으로 능가한다(Fig. 2 참조).

![](/assets/images/posts/592/img_2.png)

**그림 3 설명**  
4096×4096 고해상도 입력 이미지에서, 빨간 십자가로 표시된 패치와 모든 다른 패치 간 코사인 유사도 맵을 시각화한 결과. DINOv3 출력 특성으로 얻은 결과는 매우 정교한 공간적 구조를 보여준다.

### 기여 요약

본 연구에서는 대규모 SSL을 대형 모델(frontier model)로 확장하기 위해 여러 기여를 제안한다.

1. **데이터 스케일링** (Sec. 3.1)
   - 최근 자동 데이터 큐레이션(Vo et al., 2024)을 활용하여 방대한 “백그라운드” 학습 데이터셋을 수집하고, 여기에 소량의 특수 데이터(ImageNet-1k)를 혼합.
   - 이를 통해 제약 없는 대규모 데이터를 활용하면서 모델 성능을 향상시킴.
2. **모델 아키텍처 및 학습** (Sec. 3.2)
   - ViT 아키텍처의 변형을 정의하여 모델 크기를 70억 파라미터로 확장.
   - 현대적 위치 임베딩(axial RoPE)을 도입하고, 위치 아티팩트를 방지하기 위한 정규화 기법 개발.
   - DINOv2에서 사용된 다중 코사인 스케줄 대신, 100만(1M) iteration 동안 **상수 하이퍼파라미터 스케줄**을 적용 → 더 강력한 성능 달성.
3. **Gram Anchoring 기반 학습** (Sec. 4)
   - 대규모 학습에서 밀집 특성이 붕괴되는 문제를 해결하기 위해 Gram Anchoring 단계 추가.
   - 이 방법은 특성 맵 내의 노이즈를 제거하고, 훨씬 선명한 유사도 맵을 생성하며, 매개변수 기반/비매개변수 기반 밀집 과제 모두에서 성능을 극적으로 향상시킴.
4. **사후(post-hoc) 고해상도 학습 및 증류** (Sec. 5.2)
   - 마지막 파이프라인 단계에서 고해상도 학습을 수행한 뒤, **단일 교사–다중 학생(single-teacher multiple-students)** 구조의 효율적 증류 기법을 개발.
   - 이를 통해 7B 규모 모델의 성능을 다양한 크기의 실용적 모델군으로 이전.

### 성과

- 광범위한 벤치마크에서 측정된 결과(Sec. 6)는, 제안된 방법이 밀집 과제에서 새로운 표준을 정의하며, 전역 과제에서도 CLIP 파생 모델들과 유사한 성능을 달성함을 보여준다.
- 특히 **frozen vision backbone** 상태에서도, 객체 탐지(COCO detection, mAP 66.1)와 이미지 분할(ADE20k, mIoU 63.0) 같은 오랜 컴퓨터 비전 과제에서 최신 성능을 달성하며, 특화된 파인튜닝 파이프라인을 능가한다.
- 또한, 위성 이미지(Sec. 8)에 DINOv3 알고리즘을 적용한 결과, 모든 기존 접근 방식을 초월하며 접근의 범용성을 입증했다.

### 2. 관련 연구

#### 자기지도학습(Self-Supervised Learning, SSL)

라벨이 없는 상태에서 학습을 진행하려면, 학습을 위한 감독 신호를 대신 제공할 **인공 학습 과제(pre-text task)** 가 필요하다. SSL의 핵심은 이러한 프리텍스트 과제를 정교하게 설계하여, 다운스트림 과제에서 유용한 강력한 표현을 학습하는 데 있다.

언어 도메인은 이산적(discrete) 특성을 가지므로 이러한 과제를 비교적 단순하게 정의할 수 있었고, 이로 인해 텍스트 데이터를 대상으로 한 비지도 사전학습이 큰 성공을 거두었다. 대표적인 예로는 **단어 임베딩(word embeddings)** (Mikolov et al., 2013; Bojanowski et al., 2017), **문장 표현(sentence representations)** (Devlin et al., 2018; Liu et al., 2019), 그리고 **언어 모델(language models)** (Mikolov et al., 2010; Zaremba et al., 2014) 등이 있다.

반면, 컴퓨터 비전은 연속적인 신호를 다루기 때문에 더 큰 어려움이 따른다. 초기 시도들은 언어 도메인을 모방하여 이미지 일부로부터 다른 부분을 예측하는 방식으로 감독 신호를 추출하였다. 예를 들어:

- **패치 상대 위치 예측** (Doersch et al., 2015),
- **패치 순서 복원** (Noroozi and Favaro, 2016; Misra and Maaten, 2020),
- **인페인팅(inpainting)** (Pathak et al., 2016),
- **재색칠(re-colorization)** (Zhang et al., 2016),
- **이미지 변환 예측** (Gidaris et al., 2018).

특히 인페인팅 기반 접근은 패치 단위 ViT 아키텍처(He et al., 2021; Bao et al., 2021; El-Nouby et al., 2021)와 잘 맞아 큰 관심을 받았다. 이는 이미지의 손상된 영역을 복원하는 것으로, 잡음 제거(auto-encoding)와 유사하며 BERT의 마스크 토큰 예측(Devlin et al., 2018)과 개념적으로 연결된다. He et al. (2021)은 **MAE(Masked Auto-Encoder)** 가 다운스트림 과제 파인튜닝을 위한 강력한 초기화를 제공할 수 있음을 보였다. 이후 연구들(Baevski et al., 2022; Assran et al., 2023)은 **픽셀 공간 대신 학습된 잠재 공간(latent space)** 을 예측하는 것이 더 강력하고 고수준의 특성을 학습하게 한다는 점을 보였으며, 이는 **JEPA(Joint-Embedding Predictive Architecture)** (LeCun, 2022)로 불린다. 최근에는 이 패러다임이 비디오 학습(Bardes et al., 2024; Assran et al., 2025)으로 확장되었다.

두 번째 계열, 즉 본 연구와 더 가까운 접근은 이미지 간 **구별 신호(discriminative signals)** 를 활용하여 시각 표현을 학습한다. 이는 초기 딥러닝 연구(Hadsell et al., 2006)에서 시작되었으며, **인스턴스 분류(instance classification)** (Dosovitskiy et al., 2016; Wu et al., 2018)로 이어졌다. 이후 대조 학습(contrastive learning) 목적 함수와 정보 이론적 기준(Hénaff et al., 2019; He et al., 2020; Chen and He, 2020; Grill et al., 2020) 등이 도입되었고, **자기 클러스터링(self-clustering)** 기반 전략(Caron et al., 2018; Asano et al., 2020; Caron et al., 2020; 2021)도 제안되었다. 최근 방법인 **iBOT** (Zhou et al., 2021)은 이러한 구별 손실과 마스크 복원 목적을 결합하였다. 이 모든 방법은 강력한 특성을 학습할 수 있으며, ImageNet(Russakovsky et al., 2015) 같은 표준 벤치마크에서 높은 성능을 보여주었다. 그러나 대부분은 **대규모 모델로의 확장성**에 한계를 가진다 (Chen et al., 2021).

#### 비전 파운데이션 모델(Vision Foundation Models)

딥러닝 혁명은 **AlexNet** (Krizhevsky et al., 2012)으로 시작되었다. 이는 ImageNet 챌린지(Deng et al., 2009; Russakovsky et al., 2015)에서 이전 모든 방법을 능가한 심층 CNN 모델이었다. 초기부터 ImageNet과 같은 대규모 라벨링 데이터셋에서 엔드투엔드로 학습된 특성이 전이학습(transfer learning)에 매우 효과적임이 밝혀졌다(Oquab et al., 2014). 이후 VGG(Simonyan and Zisserman, 2015), GoogleNet(Szegedy et al., 2015), ResNet(He et al., 2016) 등 아키텍처 연구가 이어졌다.

스케일링의 효과가 입증되자, 더 큰 모델을 더 큰 데이터셋에서 학습하려는 시도가 이어졌다. Sun et al. (2017)은 **JFT 데이터셋**(3억 장 라벨링 이미지)을 활용해 지도학습을 확장하며 인상적인 성과를 보여주었고, 이는 이후 연구(Kolesnikov et al., 2020)에도 큰 영향을 주었다. 지도와 비지도를 혼합하는 접근도 시도되었는데, 예를 들어 ImageNet으로 학습한 모델이 비지도 데이터에 의사 라벨(pseudo-label)을 붙여 대규모 네트워크 학습에 활용되었다(Yalniz et al., 2019).

또한 대규모 지도 데이터셋(JFT)의 가용성은 트랜스포머 아키텍처를 비전에 적용(Dosovitskiy et al., 2020)하는 데 큰 역할을 했다. 하지만 JFT 없이 원래의 ViT와 비슷한 성능을 달성하기 위해서는 상당한 노력이 필요했다(Touvron et al., 2020; 2022). 이후 연구(Zhai et al., 2022a)는 ViT-22B(Dehghani et al., 2023)와 같은 초대형 인코더 학습까지 확장되었다.

라벨링 비용 문제를 피하기 위해 **약지도 학습(weakly-supervised learning)** 도 활발히 연구되었다. 초기에는 이미지 캡션의 모든 단어를 타깃으로 예측하게 하는 단순한 접근(Joulin et al., 2016)이 제안되었고, 이후 문장 구조(Li et al., 2017), 다양한 메타데이터(Mahajan et al., 2018), 대규모 스케일(Singh et al., 2022)로 확장되었다. 그러나 약지도 학습이 진정한 잠재력을 발휘한 것은 **대조 학습 + 캡션 표현 공동 학습**이 결합되면서였고, 이를 대표하는 것이 **Align**(Jia et al., 2021)과 **CLIP**(Radford et al., 2021)이다.

CLIP의 성공은 수많은 오픈소스 재현과 확장 연구로 이어졌다. **OpenCLIP**(Cherti et al., 2023)은 LAION 데이터셋(Schuhmann et al., 2021)으로 CLIP을 재현했고, 이후 연구들은 사전학습된 백본을 CLIP 방식으로 파인튜닝하는 전략(Sun et al., 2023; 2024)을 활용했다. **MetaCLIP**(Xu et al., 2024)은 원래 CLIP 절차를 그대로 따라 결과를 재현했고, Fang et al. (2024a)은 지도 데이터셋을 활용해 사전학습 데이터를 큐레이션했다. 다른 연구들은 손실 함수 개선(Zhai et al., 2023의 Sigmoid 기반 손실 → SigLIP), 사전학습된 인코더 활용(Zhai et al., 2022b) 등 다양한 방향으로 발전했다. 가장 중요한 교훈은 **고품질 대규모 데이터 + 막대한 연산 자원**이 최신 파운데이션 모델의 성능을 결정한다는 점이다. 실제로 **SigLIP 2**(Tschannen et al., 2025), **Perception Encoder (PE)** (Bolya et al., 2025)는 각각 400억~860억 쌍 이상의 이미지–텍스트 데이터를 활용해 최첨단 성능을 달성했다.

더 나아가, 본질적으로 멀티모달인 접근도 연구되고 있으며, 여기에는 **대조적 캡셔닝(contrastive captioning)** (Yu et al., 2022), **잠재 공간 마스크 모델링(masked modeling)** (Bao et al., 2021; Wang et al., 2022b; Fang et al., 2023), **오토리그레시브 학습(auto-regressive training)** (Fini et al., 2024) 등이 포함된다.

반대로, **비지도 이미지 사전학습의 스케일 확장**에 대한 연구는 상대적으로 적다. 초기에는 YFCC 데이터셋(Thomee et al., 2016)을 활용한 Caron et al. (2019), Goyal et al. (2019)이 있었으며, 이후 더 큰 데이터셋과 모델(Goyal et al., 2021; 2022a), SSL을 위한 데이터 큐레이션 초기 시도(Tian et al., 2021) 등이 있었다. 학습 알고리즘의 정교한 튜닝, 대규모 아키텍처, 방대한 학습 데이터의 결합은 **DINOv2**(Oquab et al., 2024)의 인상적인 성과로 이어졌으며, 이는 처음으로 SSL 모델이 다양한 과제에서 오픈소스 CLIP 변형들을 능가하거나 동등한 성능을 달성한 사례였다. 최근에는 Fan et al. (2025)이 데이터 큐레이션 없이 대규모 모델로 확장하거나, Venkataramanan et al. (2025)이 오픈 데이터셋과 개선된 학습 방법을 활용해 이 흐름을 이어가고 있다.

### 밀집( Dense ) Transformer 특성

![](/assets/images/posts/592/img_3.png)

**그림 4**: 초고해상도에서의 DINOv3. DINOv3의 밀집 특성을 시각화하기 위해, 특성 공간에서 PCA로 계산된 상위 세 개 성분을 RGB에 매핑하였다. PCA가 대상(subject)에 집중하도록, 배경 제거(background subtraction)로 특성 맵을 마스킹하였다. 해상도가 증가할수록 DINOv3는 선명하면서도 의미론적으로 일관된 특성을 생성한다. 추가 PCA 시각화는 Sec. 6.1.1에 제시된다.

현대의 다양한 비전 응용은 사전학습된 트랜스포머의 밀집 특성을 활용한다. 예를 들어:

- **멀티모달 모델** (Liu et al., 2023; Beyer et al., 2024),
- **생성 모델** (Yu et al., 2025; Yao et al., 2025),
- **3D 이해** (Wang et al., 2025),
- **비디오 이해** (Lin et al., 2023a; Wang et al., 2024b),
- **로보틱스** (Driess et al., 2023; Kim et al., 2024).

또한, 전통적인 컴퓨터 비전 과제(탐지, 분할, 깊이 추정) 역시 정확한 지역(local) 서술자를 필요로 한다. 이를 위해 많은 연구들이 **지역 특성 강화를 위한 SSL 손실(local SSL losses)** 개발에 집중했다.  
예:

- 비디오 내 시공간적 일관성을 활용 (Jabri et al., 2020),
- 같은 이미지의 서로 다른 크롭(crop) 간 공간 정렬(spatial alignment)을 활용 (Pinheiro et al., 2020; Bardes et al., 2022),
- 인접 패치 간 일관성(consistency) 강제 (Yun et al., 2022).

Darcet et al. (2025)은 **지역 패치 클러스터링 예측**이 밀집 표현을 향상시킨다고 보고했고, DetCon (Hénaff et al., 2021)과 ORL (Xie et al., 2021)은 영역 제안(region proposal) 단위로 대조 학습을 수행했지만, 이는 사전에 제안(proposal)이 존재한다는 가정을 필요로 한다. 이 가정은 ODIN (Hénaff et al., 2022), SlotCon (Wen et al., 2022) 같은 접근에서 완화되었다.

추가로, Darcet et al. (2024)은 **입력 시퀀스에 register token을 추가**하는 것만으로 밀집 특성이 크게 향상됨을 보였으며, 최근 연구(Jiang et al., 2025; Chen et al., 2025)에서는 학습 자체 없이도 이를 구현할 수 있음을 보였다.

#### 증류(distillation) 기반 접근

최근에는 **“집합적(agglomerative)” 증류 방법**이 등장했는데, 서로 다른 수준의 감독(supervision)으로 학습된 글로벌/로컬 특성을 가진 여러 이미지 인코더의 정보를 결합하는 방식이다(Ranzinger et al., 2024; Bolya et al., 2025).

- **AM-RADIO** (Ranzinger et al., 2024): 완전지도(SAM, Kirillov et al., 2023), 약지도(CLIP), 자기지도(DINOv2)의 강점을 통합한 백본 제시.
- **Perception Encoder (PE)** (Bolya et al., 2025): SAM(v2)을 증류해 밀집 특화 변형인 **PEspatial** 생성.
  - 여기서는 교사(teacher)의 마스크 주석 학습을 활용하여, 학생–교사 패치 간 코사인 유사도를 높이는 목표 함수를 사용.

이러한 손실은 스타일 전이(style transfer) 연구(Gatys et al., 2016; Johnson et al., 2016; Yoo et al., 2024)에서도 효과적임이 확인되었다. 본 연구 역시 **Gram 기반 목적(Gram objective)** 을 도입하여 학생–교사 패치 간 코사인 유사도를 정규화하고, 두 표현이 가깝게 유지되도록 유도한다. 다만 우리 접근에서는 **SSL 모델의 초기 단계 결과물을 교사로 활용**하여, 초기 모델이 후속 학습을 전역/밀집 과제 모두에서 효과적으로 안내한다는 점을 보여준다.

#### 사후(post-hoc) 지역 특성 개선

또 다른 연구들은 SSL 학습 후 지역 특성을 개선하는 방향에 집중했다.

- Ziegler & Asano (2022): 사전학습 모델을 밀집 클러스터링 목표로 파인튜닝.
- Salehi et al. (2023): 패치 특성을 시간적으로 정렬하도록 파인튜닝.
- Pariza et al. (2025): 패치 정렬 기반 목표를 통해 학생–교사 간 인접 패치 순서를 일관되게 유지.
- **STEGO** (Hamilton et al., 2022): 파인튜닝 없이, 고정된 SSL 특성 위에 비선형 투영층을 학습시켜 클러스터를 압축하고 상관 구조를 강화.
- Simoncini et al. (2024): 서로 다른 SSL 손실에서 나온 그래디언트를 결합하여 고정된 SSL 특성에 보강.
- Wysoczańska et al. (2024): 노이즈가 심한 특성 맵을 패치 기반 가중 평균으로 개선.

#### 비SSL 기반 연구

SSL과 직접적으로 관련되진 않지만, 일부 연구(Fu et al., 2024)는 ViT의 패치화로 인해 낮은 해상도로 생성되는 특성 맵을 고해상도로 복원하려고 시도했다.

#### 차별점

이러한 다양한 시도와 달리, **우리 모델(DINOv3)은 본질적으로(natively) 고품질 밀집 특성 맵을 생성**하며, 해상도가 달라져도 안정적이고 일관성을 유지한다(Fig. 4 참고).

### 3. 감독 없는 대규모 학습

**DINOv3**는 자기지도학습(Self-Supervised Learning, SSL)의 한계를 넘어, 지금까지 가장 강력하고 유연한 시각 표현을 생성하도록 설계된 차세대 모델이다. 우리는 대규모 언어 모델(LLM)의 성공에서 영감을 받았다. LLM에서 모델 용량을 확장하면 놀라운 **emergent property(창발적 특성)** 가 나타나는 것처럼, DINOv3 역시 **모델 크기와 학습 데이터셋을 한 차원 더 크게 확장**함으로써 SSL의 잠재력을 최대한 발휘하고, 전통적인 지도학습이나 태스크 특화 접근의 한계를 벗어난 **비전 패러다임 전환**을 목표로 한다.

특히, SSL은 특정 감독 신호나 태스크에 치우치지 않은 풍부하고 고품질의 시각 특성을 생성하기 때문에, 다양한 다운스트림 응용의 **범용 기반(foundation)** 으로 이상적이다. 지금까지 SSL 모델의 대규모 확장은 불안정성 문제로 인해 가로막혀 왔지만, 본 절에서는 **데이터 준비, 설계, 최적화 전략**을 신중히 활용하여 확장의 이점을 끌어내는 방법을 설명한다.

우리는 먼저 **데이터셋 생성 절차**(Sec. 3.1)를 소개한 뒤, **DINOv3의 1단계 SSL 학습 레시피**(Sec. 3.2)를 설명한다. 여기에는 아키텍처 선택, 손실 함수, 최적화 기법이 포함된다. 이후 밀집 특성(dense features)에 초점을 맞춘 2단계 학습은 Sec. 4에서 다룬다.

### 3.1 데이터 준비

데이터 스케일링은 대규모 파운데이션 모델의 성공을 이끄는 주요 요인 중 하나다 (Touvron et al., 2023; Radford et al., 2021; Xu et al., 2024; Oquab et al., 2024). 그러나 **단순히 데이터 크기만 늘린다고 해서 모델 품질이 개선되거나 다운스트림 벤치마크 성능이 향상되는 것은 아니다** (Goyal et al., 2021; Oquab et al., 2024; Vo et al., 2024). 성공적인 데이터 스케일링은 보통 정교한 **데이터 큐레이션 파이프라인**을 필요로 한다.

이러한 큐레이션 알고리즘의 목표는 크게 두 가지다:

1. **데이터 다양성과 균형성 개선**
2. **데이터 유용성 향상** – 실제 응용과의 관련성 확보

**DINOv3 개발 과정에서는 이 두 접근을 결합**하여, 모델의 일반화 성능과 실제 성능 모두를 향상시키고 두 목표 간 균형을 맞추었다.

**표 1**: 학습 데이터가 특성 품질에 미치는 영향 (다운스트림 태스크 성능으로 측정).

- 클러스터링 기반 큐레이션(Vo et al., 2024),
- 검색 기반 큐레이션(Oquab et al., 2024),
- 원시(raw) 데이터,
- 우리가 제안한 혼합 데이터셋

을 비교하였다. 이 소거 실험(ablation study)은 200k iteration의 단축된 학습 스케줄로 수행되었다.

![](/assets/images/posts/592/img_4.png)

### 데이터 수집 및 큐레이션

우리는 **Instagram 공개 게시물로부터 수집된 웹 이미지 대규모 풀(pool)** 을 활용하여 사전학습용 데이터셋을 구축하였다. 이 이미지들은 이미 플랫폼 차원에서의 콘텐츠 모더레이션을 거쳤기 때문에 유해한 콘텐츠가 일정 부분 제거된 상태였으며, 초기 데이터 풀의 규모는 약 **170억 장**에 달한다.

이 원시(raw) 데이터 풀을 기반으로 세 가지 부분으로 구성된 데이터셋을 만들었다.

1. **클러스터링 기반 큐레이션 (Vo et al., 2024)**
   - DINOv2를 임베딩으로 사용하고, 5단계 **계층적 k-means**를 적용.
   - 클러스터 수는 단계별로 **200M → 8M → 800k → 100k → 25k**.
   - 계층적 클러스터링 후, Vo et al. (2024)가 제안한 균형 샘플링(balanced sampling) 알고리즘을 적용.
   - 그 결과, 웹상에 존재하는 모든 시각적 개념을 균형 있게 포함하는 **16.89억(LVD-1689M) 이미지 서브셋**을 확보.
2. **검색(retrieval) 기반 큐레이션 (Oquab et al., 2024 유사 절차)**
   - 선택된 시드(seed) 데이터셋과 유사한 이미지를 원시 데이터 풀에서 검색.
   - 이를 통해 다운스트림 과제에 관련된 시각 개념들을 포괄하는 데이터셋을 생성.
3. **공개 비전 데이터셋 활용**
   - ImageNet1k (Deng et al., 2009),
   - ImageNet22k (Russakovsky et al., 2015),
   - Mapillary Street-level Sequences (Warburg et al., 2020) 등을 포함.
   - Oquab et al. (2024) 방식과 유사하게, 모델 성능 최적화를 위해 사용.

### 데이터 샘플링

사전학습 과정에서는 **샘플러(sampler)** 를 통해 위 세 가지 데이터 파트를 혼합하여 사용한다.

- 한 가지 방식은, 각 iteration마다 하나의 데이터 파트에서만 뽑아 **동질적 배치(homogeneous batch)** 로 학습하는 것.
- 또 다른 방식은, 모든 데이터 파트에서 일정 비율로 섞어 **이질적 배치(heterogeneous batch)** 를 구성하는 것.

우리는 **Charton & Kempe (2024)** 의 관찰(작은 고품질 데이터셋에서 동질적 배치로 학습하는 것이 유리하다)을 반영하여, 각 iteration에서 **두 가지 중 하나**를 무작위 선택한다:

1. **ImageNet1k 단일 동질 배치**
2. **나머지 데이터 파트를 섞은 이질 배치**

실제 학습에서는 ImageNet1k 동질 배치가 전체 학습의 약 **10%** 를 차지한다.

### 데이터 소거(ablation) 실험

데이터 큐레이션 기법의 효과를 평가하기 위해, 우리는

- **클러스터링 기반 데이터셋**,
- **검색 기반 데이터셋**,
- **원시 데이터 풀**,
- **우리의 혼합 데이터셋**

을 각각 사용하여 모델을 학습시키고, 표준 다운스트림 태스크에서 성능을 비교하였다.

효율성을 위해 전체 100만 iteration 대신 **200k iteration**의 단축 스케줄로 학습하였다. **표 1**의 결과에 따르면, 어느 단일 큐레이션 기법도 모든 벤치마크에서 가장 좋은 성능을 내지 못했으며, **우리의 전체 파이프라인(혼합 접근)** 이 두 방식의 장점을 모두 살려 최적의 결과를 달성했음을 확인할 수 있었다.

### 3.2 자기지도학습 기반 대규모 학습

SSL(Self-Supervised Learning)로 학습된 모델은 흥미로운 특성들을 보여주었지만 (Chen et al., 2020b; Caron et al., 2021), 대부분의 SSL 알고리즘은 **대규모 모델로의 확장**이 이루어지지 못했다. 그 이유는 크게 두 가지다.

1. **학습 안정성 문제** (Darcet et al., 2025)
2. 시각 세계의 복잡성을 충분히 포착하지 못하는 **과도하게 단순한 접근**

실제로 대규모 데이터와 모델로 학습(Goyal et al., 2022a)을 시도했을 때, SSL 모델이 반드시 인상적인 성능을 내는 것은 아니었다.

예외적으로, **DINOv2**는 약 **11억(1.1B) 파라미터** 규모의 모델을 정교하게 큐레이션된 데이터로 학습시켜, CLIP(Radford et al., 2021) 같은 약지도 모델과 성능 면에서 대등한 수준에 도달했다.

최근에는 **DINOv2를 70억(7B) 파라미터** 규모로 확장한 연구(Fan et al., 2025)도 있었는데, 전역(global) 과제에서는 고무적인 결과를 보였으나, **밀집 예측(dense prediction) 과제에서는 실망스러운 성능**을 보였다.

본 연구에서는 모델과 데이터를 동시에 확장하면서도, **전역 특성과 지역(dense) 특성 모두가 향상된 더 강력한 시각 표현**을 얻는 것을 목표로 한다.

**표 2**: DINOv2와 DINOv3에서 사용된 교사(teacher) 아키텍처 비교.

- 모델 깊이는 동일하게 **40 블록**으로 유지.
- 임베딩 차원은 **4096**으로 증가.
- 중요한 차이는 **패치 크기를 16픽셀**로 설정하여, 주어진 해상도에서 **실질적 시퀀스 길이**가 달라지도록 한 점이다.

![](/assets/images/posts/592/img_5.png)

### 학습 목표 (Learning Objective)

우리는 **전역(global)과 지역(local) 손실 항을 결합한 자기지도(discriminative self-supervised) 전략**으로 모델을 학습한다.

- **이미지 단위 목표(ℒ\_DINO)** : DINOv2(Oquab et al., 2024)에서 사용된 이미지 레벨 목적(Caron et al., 2021)을 채택.
- **패치 단위 목표(ℒ\_iBOT)** : iBOT(Zhou et al., 2021)의 패치 수준 잠재 재구성(latent reconstruction) 목적을 함께 사용.
- **센터링 대체** : 두 목적 함수 모두에서 DINO의 centering 대신 SwAV(Caron et al., 2020)의 **Sinkhorn-Knopp**을 사용.

각 손실은 백본(backbone) 네트워크 위 전용 헤드(dedicated head)의 출력을 통해 계산되며, 이를 통해 손실 계산 전에 특성이 부분적으로 특화될 수 있다. 또한, 로컬/글로벌 크롭의 백본 출력에는 전용 **Layer Normalization**을 적용하였다.

실험적으로, 이 변경은

- ImageNet kNN 분류 후반부에서 학습을 안정화(+0.2 정확도)하고,
- 밀집 과제 성능을 향상(+1 mIoU on ADE20k segmentation, -0.02 RMSE on NYUv2 depth estimation)시켰다.

추가로, 배치 내 특성이 공간에서 균등하게 퍼지도록 유도하는 **Koleo 정규화(ℒ\_Koleo, Sablayrolles et al., 2018)** 를 사용하였다. 이는 GPU 간 분산(distributed) 환경에서 배치 크기 16 단위로 적용된다.

최종적으로, 초기 학습 단계에서의 목적 함수는 다음과 같다:

![](/assets/images/posts/592/img_6.png)

### 업데이트된 모델 아키텍처 (Updated Model Architecture)

- 모델 규모를 **7B 파라미터**로 확장 (DINOv2의 1.1B 대비).
- 깊이는 **40 블록**으로 동일하게 유지하되, 임베딩 차원을 **4096**으로 증가.
- 패치 크기를 **16 픽셀**로 설정 → 주어진 해상도에서 **시퀀스 길이 변화** 발생.

**RoPE 변형**:

- 각 패치에 좌표를 [−1,1] 박스 안에 할당한 뒤, 두 패치 간 상대 위치에 따라 Multi-Head Attention에 바이어스를 추가.
- 모델이 해상도, 스케일, 종횡비 변화에 더 강건해지도록 **RoPE-box jittering** 적용.
  - 좌표 박스를 [−s,s]로 무작위 스케일링, s∈[0.5,2].

이러한 변경은 모델이 더 세밀하고 강건한 시각적 특성을 학습하도록 하며, 성능과 확장성을 향상시킨다.

### 최적화 (Optimization)

대규모 모델을 초대형 데이터셋에서 학습하는 것은 복잡한 실험 워크플로우를 요구한다.

- 모델 용량과 데이터 복잡성 간의 상호작용을 사전에 평가하기 어려워, **최적의 학습 기간(optimization horizon)을 예측하기 불가능**.

따라서, 우리는 **모든 파라미터 스케줄링을 제거**하고 **상수(constant)** 로 설정하였다:

- 학습률(learning rate),
- weight decay,
- teacher EMA momentum.

이 방식의 장점:

1. 다운스트림 성능이 개선되는 한 계속 학습을 이어갈 수 있음.
2. 하이퍼파라미터 수가 줄어들어 선택이 용이해짐.

단, 학습 초기 단계에서는 **linear warmup**을 사용하여 학습률과 teacher temperature를 점진적으로 증가시킨다.

- 옵티마이저: **AdamW** (Loshchilov & Hutter, 2017).
- 총 배치 크기: **4096 이미지**, 256개 GPU에 분산.
- 학습 전략: **multi-crop** (Caron et al., 2020)
  - 이미지당 2개의 글로벌 크롭, 8개의 로컬 크롭.
- 크롭 해상도: 글로벌(256px), 로컬(112px).
- 패치 크기 변경과 함께, 이미지당 **실질적 시퀀스 길이**는 DINOv2와 동일하게 유지.
- 배치당 총 토큰 수: **370만(3.7M)**.

추가 하이퍼파라미터는 App. C 및 코드 릴리즈 참고.

![](/assets/images/posts/592/img_7.png)

**그림 5 설명**

- (a) 코사인 유사도의 변화,
- (b) ViT-g, (c) ViT-7B 모델의 ImageNet1k 선형 분류(IN1k linear) 정확도와 VOC segmentation 성능 변화.
- 관찰 결과: **패치 토큰과 클래스 토큰 간의 코사인 유사도가 낮을 때, segmentation 성능이 최고조**.
- 학습이 진행되면서 이 유사도가 증가하면, **밀집 과제 성능은 점차 저하**됨.

### 4. Gram Anchoring: 밀집 특성을 위한 정규화

대규모 학습의 이점을 최대한 활용하기 위해, 우리는 **7B 모델을 장시간 학습**하는 전략을 취한다. 이론적으로는 무한정 학습할 수도 있다고 가정한다. 예상대로, 장시간 학습은 **전역(global) 벤치마크** 성능을 지속적으로 향상시킨다. 그러나 시간이 지남에 따라 **밀집(dense) 과제 성능은 오히려 저하**되는 현상이 나타난다(Fig. 5 참고).

이는 **패치 수준 표현(patch-level representation)** 의 불일치가 점점 두드러지게 나타나는 현상 때문이며, 장시간 학습의 장점을 크게 훼손한다. (추가적으로, 학습이 계속되면 다른 유형의 이상치(outlier)도 발생함을 관찰했으며, 이에 대한 논의는 부록 A에서 다룬다.)

본 절에서는 먼저 **패치 수준 일관성(consistency)의 붕괴 현상**을 분석한 뒤, 이를 완화하기 위한 새로운 목적 함수인 **Gram Anchoring**을 제안한다. 마지막으로, 제안 기법이 학습 안정성과 모델 성능에 미치는 영향을 논의한다.

### 4.1 학습 과정에서의 패치 수준 일관성 손실

![](/assets/images/posts/592/img_8.png)

**그림 6**: 빨간색으로 표시된 기준 패치(reference patch)와 모든 다른 패치 간 코사인 유사도의 변화. 학습이 진행될수록, 특성 맵은 덜 국소화되고 유사도 맵은 점점 더 노이즈화된다.

장시간 학습을 진행하면 전역 메트릭(Global metrics)은 꾸준히 향상되지만, 밀집 예측(dense prediction) 과제에서는 뚜렷한 성능 저하가 발생한다. 이는 DINOv2 학습에서도 일부 관찰되었으며, Fan et al. (2025)의 스케일링 실험에서도 언급된 바 있으나, 아직 해결되지 못한 문제였다.

우리는 Fig. 5를 통해, 모델 학습이 진행됨에 따라 분류(Classification)와 분할(Segmentation) 성능이 어떻게 변화하는지 비교하였다.

- **분류(Classification)**: CLS 토큰을 사용해 ImageNet-1k 선형 분류기를 학습, Top-1 정확도 측정.
- **분할(Segmentation)**: Pascal VOC 데이터셋에서 추출한 패치 특성을 기반으로 선형 레이어를 학습, mIoU 측정.

그 결과, **ViT-g와 ViT-7B 모두 분류 정확도는 학습 내내 단조롭게 상승**했으나, **세그멘테이션 성능은 약 200k iteration 이후 감소**하기 시작했으며, ViT-7B의 경우 초반 성능보다도 낮아졌다.

이를 더 잘 이해하기 위해 패치 특성의 품질을 **코사인 유사도 맵**으로 분석했다.

- **200k iteration 시점**: 유사도 맵은 매끄럽고 국소화(localized)되어 있어 일관된 패치 수준 표현을 보여줌.
- **600k iteration 이후**: 점점 더 많은 무관한 패치들이 기준 패치와 높은 유사도를 보이며, 맵 품질이 크게 저하됨.

→ 이러한 패치 수준 일관성의 상실은 곧 **밀집 과제 성능 저하와 강하게 연관**됨.

이 현상은 Darcet et al. (2024)에서 지적된 **high-norm patch outlier**와는 다르다.

- Register token을 도입한 상태에서는 패치 norm 자체는 학습 내내 안정적으로 유지됨.
- 그러나, CLS 토큰과 패치 출력 간 코사인 유사도가 점차 증가 → 이는 패치 특성의 국소성이 점점 사라지고 있음을 의미.

이를 Fig. 5에 시각화했으며, 200k iteration과 1M iteration에서의 코사인 유사도 맵을 비교하면 이 현상이 분명하게 드러난다.

따라서, 우리는 **밀집 과제 성능 저하를 완화**하기 위해, 패치 특성을 정규화하고 일관성을 보장하는 새로운 목적 함수 **Gram Anchoring**을 제안한다. 이는 전역 성능을 유지하면서도 패치 수준의 품질 저하를 막는 것을 목표로 한다.

### 4.2 Gram Anchoring 목적 함수

![](/assets/images/posts/592/img_9.png)

**그림 7**: 학습 반복(iteration)에 따른 손실 변화. (a) 패치 수준 iBOT 손실, (b) 글로벌 DINO 손실, (c) 새롭게 도입한 Gram 손실. 그림에서는 Gram 목적을 사용하는 정제 단계(ℒ\_Ref)를 강조하였다.

우리의 실험 전반에서, **강력한 판별적 특성(discriminative feature)을 학습하는 것**과 **지역적 일관성(local consistency)을 유지하는 것** 사이에는 상대적 독립성이 있음을 확인하였다. 즉, 글로벌 성능과 밀집 성능 사이에 뚜렷한 상관관계가 없었다.

DINO의 글로벌 손실(ℒ\_DINO)과 iBOT의 로컬 손실(ℒ\_iBOT)을 결합하는 것으로 어느 정도 이 문제를 다룰 수 있었지만, 학습이 진행될수록 균형이 깨지고 글로벌 표현이 우세해졌다. 이에 착안하여, 우리는 이 **독립성을 명시적으로 활용**하는 새로운 해법을 제안한다.

### Gram Anchoring의 핵심 아이디어

- 목표: **패치 수준 일관성 품질**을 유지하여 붕괴를 완화하되, **특성 자체는 구속하지 않음**.
- 방법: 개별 특성(feature) 대신, **패치 특성 간의 상관 구조(Gram Matrix)** 에 규제를 가함.

![](/assets/images/posts/592/img_10.png)

### 학습 절차

- Gram 손실은 효율성을 위해 학습 **100만(1M) iteration 이후**에만 적용.
- 흥미롭게도, 뒤늦게 적용해도 이미 많이 붕괴된 로컬 특성을 “복구(repair)”할 수 있었음.
- Gram Teacher는 **매 10k iteration마다 갱신** → EMA teacher와 동일하게 업데이트.
- 이를 우리는 **정제 단계(refinement step)** 라고 부르며, 목적 함수는 다음과 같다:

![](/assets/images/posts/592/img_11.png)

### 관찰 결과

- Gram 목적을 적용하면 iBOT 손실이 빠르게 감소 → **안정적인 Gram Teacher가 iBOT에 긍정적 영향**을 미친다는 의미.
- 반대로, DINO 손실에는 큰 영향을 주지 않음 → Gram과 iBOT은 유사한 방식으로 특성에 작용하지만, DINO는 다른 차원을 다룬다는 것을 시사.

### 성능 영향

- **즉각적 효과**: Gram Anchoring을 적용한 직후, **밀집 과제 성능이 첫 10k iteration에서 크게 향상**.
- **ADE20k 세그멘테이션**: Gram Teacher 갱신 후 뚜렷한 성능 개선.
- **ObjectNet 및 기타 글로벌 벤치마크**: 장기 학습에서 추가적인 성능 향상.
- 글로벌 과제에서는 영향이 상대적으로 제한적이지만, 밀집 과제에서는 개선 효과가 매우 큼.

![](/assets/images/posts/592/img_12.png)

**그림 8**: Gram Anchoring 적용 이후의 벤치마크 성능 변화.

- (a) VOC, (b) ADE20k, (c) ObjectNet.
- 원래 학습을 이어가면서 Gram 목적을 적용한 정제 단계(ℒ\_Ref)를 시각화.
- 이후 Sec. 4.3에서 소개하는 고해상도 Gram 목적(ℒ\_HRef) 적용 결과도 함께 표시.
- Gram 목적이 사용된 iteration을 강조.

? 핵심 요약:

- **Gram Anchoring은 패치 특성 간 상관 구조를 유지하도록 정규화하는 손실**.
- 밀집 성능 붕괴를 막고, 심지어 뒤늦게 적용해도 degraded feature를 복구할 수 있음.
- 글로벌 성능에는 거의 영향이 없으면서 dense task 성능을 크게 끌어올림.

### 4.3 고해상도 특성(High-Resolution Features) 활용

최근 연구(Wysoczańska et al., 2024)에 따르면, **패치 특성의 가중 평균(weighted average)** 은 이상치 패치를 완화하고 패치 수준 일관성을 강화하여 더 나은 지역 표현(local representation)을 만들어낼 수 있다. 반면, 백본에 **고해상도 이미지를 직접 입력**하면 더욱 정교하고 세밀한 특성 맵을 얻을 수 있다.

우리는 이 두 가지 장점을 결합하여 **Gram Teacher**를 위한 고품질 특성을 계산한다. 구체적으로,

1. 입력 이미지를 일반 해상도의 2배 크기로 Gram Teacher에 입력,
2. 출력된 특성 맵을 **2× bicubic interpolation 다운샘플링**하여 학생(Student) 출력 크기와 맞춤.

이렇게 하면, 고해상도 특성에서 얻은 우수한 패치 수준 일관성이 다운샘플링 이후에도 유지되며, 결과적으로 **더 매끄럽고 일관된 패치 표현**을 확보할 수 있다. (Fig. 9(a))

추가적으로, 우리 모델은 Su et al. (2024)이 도입한 **RoPE(Rotary Positional Embedding)** 덕분에 별도의 적응 과정 없이 다양한 해상도의 이미지를 처리할 수 있다.

### 고해상도 Gram Anchoring 목적

다운샘플된 특성으로 Gram 행렬을 계산하여 기존 Gram Teacher의 X\_G​를 대체한다. 이렇게 정의된 새로운 정제 목적(refinement objective)을 **ℒ\_HRef**라 한다.

- 이 접근은 고해상도 특성의 **향상된 패치 일관성**을 학생 모델로 효과적으로 증류(distill)하는 효과를 낸다.
- 결과적으로, 밀집 과제(dense tasks)에서 성능이 추가적으로 개선된다.
- 예: Fig. 8 및 Fig. 9(b)에서 보이듯, **ℒ\_Ref 대비 ADE20k에서 +2 mIoU 개선**.

![](/assets/images/posts/592/img_13.png)

또한 Fig. 9(b)에서는 Gram Teacher 선택에 대한 ablation을 수행하였다.

- **100k 또는 200k iteration 시점의 Gram Teacher** → 성능에 큰 차이 없음.
- 그러나 **1M iteration 시점의 Gram Teacher**를 사용할 경우, 이미 패치 일관성이 저하된 상태라 성능이 떨어짐.

### 정성적 분석 (Qualitative Results)

![](/assets/images/posts/592/img_14.png)

Fig. 10은 초기 학습과 고해상도 Gram Anchoring 정제를 적용한 후의 패치 수준 Gram 행렬을 비교한다.

- 결과: 고해상도 정제 과정은 **특성 간 상관관계(feature correlation)** 를 크게 개선시킴.
- 입력 이미지는 **1024×1024** 해상도.

? 요약:

- **아이디어**: Gram Teacher를 고해상도 입력으로 학습 → 다운샘플링 → Gram Anchoring 적용.
- **효과**: 밀집 과제 성능 추가 향상 (특히 segmentation, +2 mIoU on ADE20k).
- **교훈**: 초기 단계 Gram Teacher가 더 유용, 후기(1M) Teacher는 패치 일관성 붕괴로 역효과.

### 5 사후 학습(Post-Training)

이 절에서는 **사후 학습 단계(post-training stages)** 를 다룬다.

- 다양한 입력 해상도에서 효과적인 추론을 가능하게 하는 **고해상도 적응 단계** (Sec. 5.1),
- 고품질이면서 효율적인 소형 모델을 생성하는 **모델 증류 단계** (Sec. 5.2),
- DINOv3에 제로샷 능력을 부여하는 **텍스트 정렬 단계** (Sec. 5.3) 가 포함된다.

#### 5.1 해상도 스케일링 (Resolution Scaling)

우리는 기본적으로 **해상도 256**에서 모델을 학습한다. 이는 **속도와 성능 간의 균형**을 제공한다.

- 패치 크기가 16일 때, 이는 해상도 224 & 패치 크기 14로 학습된 **DINOv2와 동일한 입력 시퀀스 길이**를 제공한다.
- 하지만 실제 컴퓨터 비전 응용에서는 **512×512 이상 고해상도 이미지**가 요구되는 경우가 많다. (더 정교한 공간 정보를 포착하기 위함)
- 또 추론 시 입력 해상도는 고정되지 않고 사용 사례에 따라 달라진다.

이를 해결하기 위해, 우리는 학습 과정에 **고해상도 적응 단계(high-resolution adaptation step)** 를 추가하였다 (Touvron et al., 2019).

? 고해상도 적응 학습 방법

- **혼합 해상도(mixed resolutions)** 전략을 사용 → 미니배치마다 다른 크기의 글로벌/로컬 crop을 샘플링.
- **글로벌 crop 크기**: {512, 768}
- **로컬 crop 크기**: {112, 168, 224, 336}
- **추가 학습 횟수**: 10k iterations

? Gram Anchoring의 핵심 역할

- 이 단계에서도 **Gram Anchoring**을 추가, 7B Teacher를 Gram Teacher로 활용.
- Gram Anchoring이 없을 경우, **밀집 예측(dense prediction) 성능이 크게 저하**됨.
- 이유: 고해상도 입력은 공간적 복잡성이 커지므로, 위치 간 일관되고 강인한 특성 상관관계(feature correlation)를 유지하는 것이 필수적임.

? 실험적 관찰 (Empirical Results)

- 비교적 짧지만 **타겟팅된 고해상도 적응 단계**만으로도 모델의 전반적인 품질이 크게 향상됨.
- 다양한 입력 크기에 대해 일반화 성능이 향상됨 (Fig. 4, Fig. 11).

![](/assets/images/posts/592/img_15.png)

**Fig. 11 결과 요약**

- (a) **ImageNet 분류**: 소폭 성능 향상, 해상도 변화에 대해 안정적.
- (b) **ObjectNet OOD 전이**: 저해상도에서는 성능이 다소 감소, 고해상도에서는 개선.
- (c) **ADE20k 의미 분할(segmentation)**: 고해상도에서 성능 증가 → 로컬 특성 품질 개선.
- (d) **DAVIS tracking**: 고해상도에서 성능 증가.

특히, 학습 시 최대 해상도(768)를 넘는 **4k 이상 초고해상도 입력**에서도 안정적인 특성 맵이 생성됨을 관찰 (Fig. 4).

? 정리:

- **고해상도 적응 단계**는 10k 반복만으로도 모델의 로컬 특성을 강화하여 **밀집 과제 성능을 개선**.
- Gram Anchoring이 핵심적 안정화 역할.
- 결과적으로, DINOv3는 **768 이상의 초고해상도 입력까지 일반화 가능**.

### 5.2 모델 증류 (Model Distillation)

#### ? 다양한 활용 사례를 위한 모델 패밀리

우리는 **ViT-7B 모델을 더 작은 Vision Transformer(ViT) 변형 모델들**(ViT-S, ViT-B, ViT-L)로 지식 증류(knowledge distillation)를 수행하였다.  
이 작은 모델들은 **관리 용이성과 효율성** 덕분에 커뮤니티에서 높은 가치를 가진다.

- 증류 시, 첫 학습 단계와 동일한 목적 함수(objective)를 사용하여 **일관된 학습 신호**를 유지한다.
- 하지만, 모델 가중치의 EMA(Exponential Moving Average)를 쓰는 대신, **7B 모델 자체를 고정된 teacher로 사용**하여 student 모델을 지도한다.
- 이 경우 patch-level consistency 문제는 관찰되지 않았으므로 **Gram Anchoring은 적용하지 않는다**.
- 이 방식은 큰 teacher의 **풍부한 표현력을 계승하면서도** 작은 모델을 **배포와 실험에 더 실용적**이게 만든다.

#### ? 모델 크기 구성

우리는 ViT-7B를 다양한 계산 예산(compute budget)을 아우르는 ViT 모델 계열로 증류하였다.

- 표준 모델: **ViT-S (21M 파라미터), ViT-B (86M), ViT-L (0.3B)**
- 맞춤형 모델: **ViT-S+ (29M), ViT-H+ (0.8B)**  
  → self-distilled 7B teacher와의 성능 격차를 줄이기 위해 설계

DINOv2에서도 관찰된 바와 같이, 작은 student 모델들이 증류를 통해 teacher와 유사한 성능에 도달할 수 있다.  
그 결과, 증류된 모델들은 **훨씬 적은 추론 비용으로 최첨단 수준의 성능**을 낼 수 있었다 (Tab. 14 참조).

- 학습: **100만 iterations**
- 추가 학습: **cosine 스케줄 기반 25만 iterations의 learning-rate cooldown**
- 이후 Sec. 5.1에서 소개한 **고해상도 학습 단계 수행 (Gram Anchoring 없이)**

#### ? 효율적인 다중 학생 증류 (Efficient Multi-Student Distillation)

큰 teacher 모델의 추론 비용은 작은 student 모델보다 **수십~수백 배 더 크다** (Fig. 16(a)).  
이를 해결하기 위해, 우리는 **병렬 증류 파이프라인(parallel distillation pipeline)**을 설계하였다.

- 이 방식은 **여러 학생(student) 모델을 동시에 훈련**하면서,
- **teacher 추론을 모든 노드에서 공유**하여 비용을 절감한다 (Fig. 12).

![](/assets/images/posts/592/img_16.png)

장점

1. 학생 모델을 추가할수록, **GPU당 iteration당 연산 비용 감소** → 증류 속도 향상
2. 전체 teacher 추론 비용은 고정 → 추가 비용은 오직 새로운 student의 학습 비용뿐
3. 구현은 GPU 프로세스 그룹 관리, 데이터로더 및 teacher inference를 **NCCL collective 연산**으로 동기화하는 정도만 필요
4. 모든 student 그룹이 iteration마다 동기화되므로, **GPU 할당을 학생별로 조정해 iteration 시간이 비슷하도록 최적화**  
   → 동기화 시 **대기 시간을 최소화**

? 정리하면, DINOv3는 **7B 모델을 teacher로 하여 소형 모델들을 효율적으로 증류**했고,  
이를 통해 **다양한 크기·효율의 모델 패밀리**를 구축했다.  
또한 **병렬 다중 학생 증류** 기법으로 compute 자원을 효율적으로 활용했다.

### 5.2 모델 증류 (Model Distillation)

#### 모델 패밀리: 다양한 활용 사례를 위한 설계

우리는 **ViT-7B 모델을 더 작은 Vision Transformer(ViT) 변형 모델들**(ViT-S, ViT-B, ViT-L)로 지식 증류(knowledge distillation)하였다.  
이러한 소형 모델들은 **관리 용이성과 효율성** 덕분에 커뮤니티에서 가치 있게 활용된다.

- 증류 방식은 **초기 학습 단계와 동일한 목적 함수(objective)**를 사용해 **학습 신호의 일관성**을 유지한다.
- 하지만, **모델 가중치의 EMA(Exponential Moving Average)**를 teacher로 사용하지 않고, **7B 모델 자체를 고정된 teacher**로 두어 작은 student 모델들을 지도한다.
- 이 경우 **patch-level consistency 문제는 발생하지 않으므로 Gram anchoring은 적용하지 않는다**.
- 결과적으로, student 모델은 대규모 teacher의 **풍부한 표현력(representational power)**을 계승하면서도, **배포 및 실험 측면에서 더 실용적**이 된다.

#### 모델 크기 스펙트럼

ViT-7B는 다양한 **연산 자원(compute budget)**을 고려해 여러 크기의 ViT 모델로 증류되었다.

- 표준 모델: **ViT-S (21M 파라미터), ViT-B (86M), ViT-L (0.3B)**
- 커스텀 모델: **ViT-S+ (29M), ViT-H+ (0.8B)**  
  → self-distilled 7B teacher와의 성능 격차를 줄이기 위해 설계

DINOv2에서와 마찬가지로, 작은 student 모델들도 증류를 통해 **teacher 수준의 성능**에 도달할 수 있었다.  
따라서 증류된 모델들은 **추론 비용의 일부만으로도 최첨단 성능**을 달성했다 (Tab. 14 참조).

- 학습: **100만 iterations**
- 추가 학습: **25만 iterations 동안 cosine 스케줄 기반 learning-rate 감소**
- 이후 Sec. 5.1에서 설명한 **고해상도 학습 단계 수행 (Gram anchoring 없음)**

#### 효율적인 다중 학생 증류 (Efficient Multi-Student Distillation)

![](/assets/images/posts/592/img_17.png)

큰 teacher 모델의 추론 비용은 student 모델보다 **수십 배에서 수백 배 더 크다** (Fig. 16(a)).  
이를 해결하기 위해, 우리는 **병렬 증류 파이프라인(parallel distillation pipeline)**을 설계했다.

- 이 방법은 **여러 학생(student) 모델을 동시에 학습**시키면서,
- **teacher 추론을 모든 노드에서 공유**해 연산 비용을 절감한다 (Fig. 12).

![](/assets/images/posts/592/img_18.png)

장점

1. **추가 student를 넣으면 GPU당 iteration 연산량이 감소** → 전체 증류 속도 향상
2. teacher 추론 비용은 고정 → **추가 비용은 새로운 student 학습 비용뿐**
3. 구현은 비교적 단순: GPU 프로세스 그룹 관리, 데이터로더 및 teacher inference를 **NCCL collective 연산**으로 동기화
4. 모든 student 그룹이 매 iteration 동기화되므로, **student별 GPU 수를 조정해 iteration 시간을 유사하게 맞춤** → 동기화 대기 시간 최소화

? 이 과정을 통해, 우리는 **다중 학생 증류를 무리 없이 수행**할 수 있었고,  
**7B 모델로부터 다양한 크기·효율의 모델 패밀리**를 완성했다.

### 5.3 DINOv3와 텍스트 정렬 (Aligning DINOv3 with Text)

**오픈 보캐블러리(Open-vocabulary) 이미지–텍스트 정렬**은 유연하고 확장 가능한 멀티모달 이해를 가능하게 할 잠재력 덕분에 연구 커뮤니티의 큰 관심을 받고 있다.  
많은 연구들은 원래 이미지와 텍스트 표현 간 **글로벌 정렬(global alignment)**만 학습했던 CLIP(Radford et al., 2021)의 품질 향상에 집중해왔다.

- CLIP은 인상적인 **제로샷(zero-shot) 능력**을 보여주었지만, **글로벌 특징에만 집중**했기 때문에 **세밀하고 지역적인 대응(fine-grained, localized correspondences)**을 포착하는 데는 한계가 있었다.
- 최근 연구(Zhai et al., 2022b)는 **사전 학습된 자기지도(Self-Supervised, SSL) 비주얼 백본**을 활용하면 효과적인 이미지–텍스트 정렬이 가능함을 보여주었다.  
  → 이 접근은 이미 학습된 시각 인코딩을 활용하므로 계산 비용을 줄이면서도, 글로벌 의미를 넘어서는 **더 풍부하고 정밀한 텍스트–이미지 연관**을 학습할 수 있게 한다.

#### DINOv3 정렬 전략

우리는 **Jose et al. (2025)**에서 제안한 학습 전략을 채택해, **텍스트 인코더를 DINOv3와 정렬**시킨다.

- 이 방식은 **LiT(Locked-image Text Tuning)** 학습 패러다임(Zhai et al., 2022b)에 기반한다.  
  → 텍스트 표현을 처음부터 학습하여 이미지와 캡션을 **대조적 목적 함수(contrastive objective)**로 정렬한다.  
  → 이때 **비전 인코더는 고정(frozen)**된 상태로 둔다.
- 다만, 비전 측에서 일정 수준의 유연성을 허용하기 위해,  
  **고정된 비주얼 백본 위에 트랜스포머 레이어 두 개**를 추가한다.
- 핵심 개선점:  
  **평균 풀링된 패치 임베딩(mean-pooled patch embeddings)**을 CLS 토큰 출력과 **연결(concatenation)**하여 텍스트 임베딩과 매칭한다.  
  → 이를 통해 **글로벌 특징 + 로컬 특징**을 동시에 텍스트와 정렬할 수 있으며,  
  → 별도의 휴리스틱이나 트릭 없이도 **세밀한 예측(dense prediction)** 과제에서 성능이 개선된다.
- 마지막으로, 데이터 준비는 **Jose et al. (2025)**에서 확립된 **데이터 큐레이션 프로토콜**을 그대로 따라 **일관성과 비교 가능성**을 보장한다.

? 정리하자면, DINOv3는 LiT 스타일의 학습을 통해 텍스트 정렬을 수행하며, 단순 글로벌 매칭을 넘어 **글로벌+로컬 정렬**을 달성해 밀집 예측(dense prediction) 성능까지 향상시켰다는 게 핵심입니다.

### 6 결과 (Results)

이 섹션에서는 **DINOv3 7B 플래그십 모델**을 다양한 컴퓨터 비전 과제에서 평가한다.  
실험 전반에서, 특별히 언급하지 않는 한 **DINOv3는 고정(frozen) 상태**로 두고 **그 표현(representations)만 활용**한다.

? 핵심 메시지: **DINOv3는 파인튜닝 없이도 강력한 성능을 낼 수 있다.**

섹션 구성은 다음과 같다:

1. **밀집 표현(dense representations) 평가 (Sec. 6.1)**
   - 가벼운 평가 프로토콜(lightweight evaluation)을 사용하여 **DINOv3의 dense 이미지 표현 품질**을 검증한다.
   - 현존하는 가장 강력한 비전 인코더들과 비교한다.
   - 결과: **DINOv3는 뛰어난 dense feature 학습 능력**을 보여준다.
2. **글로벌 표현(global representations) 평가 (Sec. 6.2)**
   - 동일한 방식으로 **DINOv3의 글로벌 이미지 표현 품질**을 검증한다.
   - 결과: **강건하고 다재다능한 글로벌 이미지 표현**을 제공한다.
3. **DINOv3 기반 복합 비전 시스템 확장 (Sec. 6.3)**
   - 단순히 DINOv3 위에 가벼운 추가만 해도,
     - 객체 탐지(object detection)
     - 의미론적 분할(semantic segmentation)
     - 3D 뷰 추정(view estimation)
     - 상대적 단안 깊이 추정(relative monocular depth estimation)  
       등 다양한 과제에서 **최첨단(SOTA)에 근접하거나 능가하는 성과**를 낼 수 있음을 보인다.

### 6.1 DINOv3는 뛰어난 Dense Feature를 제공한다

우리는 먼저 다양한 경량 평가(lightweight evaluation)를 통해 **DINOv3의 밀집 표현(dense representations) 품질**을 검증한다.  
모든 경우에서, **마지막 레이어의 frozen patch feature**를 사용하며, 다음과 같은 방법으로 평가한다:

1. **정성적 시각화(qualitative visualizations)** (Sec. 6.1.1)
2. **Dense Linear Probing** (Sec. 6.1.2: 의미론적 분할, 깊이 추정)
3. **비매개변수적 방법(non-parametric approaches)**
   - 3D 대응 추정 (Sec. 6.1.3)
   - 객체 발견(object discovery) (Sec. 6.1.4)
   - 트래킹(tracking) (Sec. 6.1.5)
4. **경량 Attention 기반 Probing** (Sec. 6.1.6: 비디오 분류)

#### 기준선(Baselines)

우리는 DINOv3의 dense feature를 현존하는 가장 강력한 공개 이미지 인코더들과 비교한다.  
여기에는 **약지도(weakly-supervised)**와 **자기지도(self-supervised)** 방식 모두 포함된다.

- **약지도 인코더**
  - Perception Encoder (PE) Core (Bolya et al., 2025)
  - SigLIP 2 (Tschannen et al., 2025)  
    (둘 다 CLIP 스타일의 이미지-텍스트 대조 학습을 활용)
- **자기지도 방법**
  - DINOv3의 전작인 **DINOv2** (Oquab et al., 2024)
  - Register 확장 버전 (Darcet et al., 2024)
  - **Web-DINO** (Fan et al., 2025) – DINO의 최신 확장 버전
  - **Franca** (Venkataramanan et al., 2025) – 공개 데이터 기반 SSL 모델 중 최고
- **집합 기반 모델 (Agglomerative Models)**
  - **AM-RADIOv2.5** (Heinrich et al., 2025) – DINOv2에서 증류(distillation)
  - **CLIP** (Radford et al., 2021)
  - **DFN** (Fang et al., 2024a)
  - **SAM (Segment Anything)** (Kirillov et al., 2023)
  - **PEspatial** – SAM 2 (Ravi et al., 2025)를 PEcore에 증류한 모델

각 기준선 모델에 대해, **가장 강력한 성능을 가진 버전**을 선택하고, 표에는 해당 아키텍처를 명시한다.

#### 6.1.1 정성적 분석 (Qualitative Analysis)

우리는 먼저 **DINOv3의 dense feature map**을 **정성적으로 분석**한다.

- 방법:
  - Dense feature 공간을 **PCA (주성분 분석)**를 통해 3차원으로 축소
  - 이를 **RGB 색상 공간**에 매핑하여 시각화
  - PCA의 부호 모호성(8가지 변형)과 주성분-색상 간 임의 매핑(6가지 변형)이 존재하므로,  
    가능한 모든 조합을 탐색 후 가장 시각적으로 우수한 결과를 선택
- 결과: Fig. 13에서 확인 가능
  - 다른 비전 백본과 비교했을 때,
    - DINOv3의 feature는 **더 선명(sharper)**
    - **노이즈가 훨씬 적음**
    - **우월한 의미적 일관성(semantic coherence)**을 보여줌

![](/assets/images/posts/592/img_19.png)

? **Figure 13 설명**: Dense Feature 비교

- 방법: PCA를 통해 dense output을 RGB로 매핑
- 비교 대상 (왼쪽→오른쪽):
  - SigLIP 2 ViT-g/16
  - PEspatial ViT-G/14
  - DINOv2 ViT-g/14 (register 포함)
  - **DINOv3 ViT-7B/16**
- 입력 해상도:
  - patch 16 → 1280×960
  - patch 14 → 1120×840
- 모든 feature map 크기: **80×60**

? 정리하면, **DINOv3는 단순 시각화만으로도 feature의 선명함·잡음 억제·의미적 일관성 측면에서 다른 모델들을 압도**한다는 게 핵심입니다.

### 6.1.2 Dense Linear Probing

우리는 **DINOv3의 dense feature 위에 선형 probing(linear probing)**을 적용하여 두 가지 작업을 평가한다:

1. **의미론적 분할(Semantic Segmentation)**
2. **단안(depth) 추정(Monocular Depth Estimation)**

두 경우 모두, DINOv3의 **frozen patch 출력 위에 선형 변환기(linear transform)**를 학습시킨다.

- **Semantic Segmentation**:
  - 데이터셋: **ADE20k** (Zhou et al., 2017), **Cityscapes** (Cordts et al., 2016), **PASCAL VOC 2012** (Everingham et al., 2012)
  - 성능 지표: **mIoU (mean Intersection-over-Union)**
- **Depth Estimation**:
  - 데이터셋: **NYUv2** (Silberman et al., 2012), **KITTI** (Geiger et al., 2013)
  - 성능 지표: **RMSE (Root Mean Squared Error)**

? **표 3: Dense Linear Probing 결과**

- 실험: frozen backbone을 사용하여 의미론적 분할과 단안 깊이 추정을 수행
- 분할(SEG) 성능: **ADE20k, Cityscapes, VOC** → **mIoU** 보고
- 깊이(Depth) 성능: **NYUv2, KITTI** → **RMSE** 보고
- 입력 해상도: **1024 patch 토큰**으로 맞춤
  - Patch size 14 → 448×448
  - Patch size 16 → 512×512

![](/assets/images/posts/592/img_20.png)

#### 결과 (Tab. 3)

- **Semantic Segmentation**
  - ADE20k에서, DINOv3는 자기지도(Self-supervised) 기준선을 **6 mIoU 이상**, 약지도(Weakly-supervised) 기준선을 **13점 이상** 초과
  - PEspatial보다 **6점 이상**, AM-RADIOv2.5보다 **3점 가까이** 우수
  - 주목할 점: PEspatial과 AM-RADIOv2.5는 강력한 기준선임에도, SAM (Kirillov et al., 2023)이라는 강력한 지도 모델에서 distillation된 모델임
  - Cityscapes에서도 동일한 경향:
    - DINOv3 → **81.1 mIoU**
    - AM-RADIOv2.5보다 **2.5점**, 다른 백본보다 **최소 5.5점** 더 높음
- **Monocular Depth Estimation**
  - DINOv3는 모든 모델을 큰 폭으로 초과
  - 약지도 기반 모델(PEcore, SigLIP 2)은 여전히 성능 열세
  - 가장 근접한 경쟁자는 DINOv2와 SAM 계열 모델
  - 흥미롭게도, PEspatial과 AM-RADIO는 **NYU에서는 강하지만**, KITTI에서는 DINOv2보다 낮음
  - 그럼에도 불구하고, DINOv3는 **DINOv2보다 0.278 RMSE 개선**

#### 합 분석

- 두 작업 모두에서, **DINOv3의 dense feature 표현력**이 매우 뛰어남을 입증
- 단순히 **선형 분류기(linear predictor)**만 사용했음에도:
  - 객체 카테고리/마스크 예측 가능
  - 장면의 물리적 특성(예: 상대적 깊이)까지 안정적으로 예측 가능
- 이는 DINOv3 feature가:
  - **시각적으로 선명(sharp)**
  - **정확히 지역화(localized)**
  - **선형적으로 분리(linearly separable) 가능한 중요한 관찰 속성들을 잘 표현**한다는 의미
- 마지막으로, ADE20k에서 선형 분류기로 얻은 성능 **55.9 mIoU** 자체가 놀라움
  - 이는 절대적 SOTA(63.0 mIoU)와도 그리 멀지 않은 수준

? 요약: DINOv3는 **frozen feature만으로도 강력한 성능**을 보이며, 선형 probing만으로도 거의 SOTA에 근접하는 표현력을 보여준다.

### 6.1.3 3D 대응(3D Correspondence) 추정

3D 세계를 이해하는 것은 컴퓨터 비전의 오랜 목표였습니다. 최근 이미지 기반 기초 모델들은 **3D 정보를 내포한 특징(3D-aware features)**을 제공하며 3D 이해 연구를 촉진하고 있습니다.  
이 절에서는 DINOv3의 **멀티뷰 일관성(multi-view consistency)**을 평가합니다. 즉, 같은 물체의 서로 다른 뷰에서 동일한 키포인트의 패치 특징이 유사한지를 Probe3D(Banani et al., 2024)에서 정의한 프로토콜에 따라 검사합니다.

우리는 **기하학적 대응(geometric correspondence)**과 **의미적 대응(semantic correspondence)**을 구분합니다.

- 기하학적 대응: 같은 **객체 인스턴스**의 키포인트를 일치시키는 것
- 의미적 대응: 같은 **객체 클래스** 내에서 서로 다른 인스턴스의 키포인트를 일치시키는 것

실험은 **NAVI 데이터셋**(Jampani et al., 2023)에서 기하학적 대응을, **SPair 데이터셋**(Min et al., 2019)에서 의미적 대응을 평가했습니다. 성능 평가는 두 경우 모두 **correspondence recall**(특정 거리 이내에 들어오는 대응 비율)로 측정합니다. 자세한 실험 세부 내용은 Sec. D.3을 참조하세요.

#### 표 4: DINOv3의 3D 일관성 평가

우리는 Probe3D의 프로토콜에 따라 뷰 간 3D 키포인트 대응을 추정합니다. 성능 평가는 **correspondence recall**(지정된 거리 안에 포함된 대응의 비율)로 보고합니다.

![](/assets/images/posts/592/img_21.png)

### 결과 (표 4)

- **기하학적 대응**에서 DINOv3는 모든 다른 모델을 능가하며, 2위 모델인 DINOv2보다 **+4.3% recall**을 기록했습니다.
- 다른 SSL 스케일링 시도들(Franca, WebSSL)은 DINOv2보다 성능이 낮아, DINOv2가 여전히 강력한 기준선임을 보여줍니다.
- **약지도 모델(PEcore, SigLIP 2)**은 해당 과제에서 좋은 성능을 보이지 못했으며, 이는 3D 인식 부족을 시사합니다.
- SAM을 기반으로 한 증류 모델들 중 **AM-RADIO**는 DINOv2에 근접했지만, **PEspatial**은 크게 뒤쳐졌습니다 (−11.6% recall). 심지어 Franca보다도 낮은 성능 (−0.8% recall)을 보였습니다. 이는 강력한 성능을 위해서는 **자기지도 학습(SSL)**이 핵심 요소임을 보여줍니다.
- **의미적 대응**의 경우도 같은 결론을 얻었습니다.
  - DINOv3는 최고의 성능을 기록하며,
  - DINOv2(+2.6% recall), AM-RADIO(+1.9% recall)를 모두 능가했습니다.

✅ 종합적으로, 이러한 키포인트 매칭 성능은 DINOv3가 **3D 중심 응용 분야**에서도 강력한 활용 가능성을 보여주는 매우 고무적인 신호라고 할 수 있습니다.

? 정리하면, DINOv3는 **2D 기반 모델이지만 3D 대응 능력**이 다른 모델 대비 매우 뛰어나서, 추후 **3D 재구성, AR/VR, 로보틱스** 같은 분야에도 쉽게 확장될 수 있다는 의미예요.

### 6.1.4 비지도 객체 발견 (Unsupervised Object Discovery)

강력한 자기지도(Self-Supervised) 특징은 **주석 없이 이미지에서 객체 인스턴스를 발견**하는 데 유용합니다 (Vo et al., 2021; Siméoni et al., 2021; Seitzer et al., 2023; Wang et al., 2023c; Siméoni et al., 2025).  
우리는 서로 다른 비전 인코더에 대해 **비지도 객체 발견(unsupervised object discovery)** 과제를 통해 이 능력을 검증합니다. 이 과제는 이미지에서 **클래스에 구애받지 않는(class-agnostic) 객체 분할**을 요구합니다 (Russell et al., 2006; Tuytelaars et al., 2010; Cho et al., 2015; Vo et al., 2019).

특히, 우리는 다양한 백본에서 강력한 성능을 보여온 **비매개변수적(non-parametric) 그래프 기반 TokenCut 알고리즘**(Wang et al., 2023c)을 사용합니다. 이 방법을 세 가지 널리 쓰이는 데이터셋에서 실행합니다:

- VOC 2007,
- VOC 2012 (Everingham et al., 2015),
- COCO-20k (Lin et al., 2014; Vo et al., 2020).

평가는 Siméoni et al. (2021)에서 정의한 프로토콜을 따르며, **CorLoc metric**을 보고합니다.  
다양한 백본이 서로 다른 특징 분포를 가질 수 있으므로, 공정한 비교를 위해 TokenCut의 핵심 하이퍼파라미터(패치 그래프를 구성할 때 사용하는 코사인 유사도 임계값)를 탐색합니다.

초기에는, **DINO (Caron et al., 2021)** 가 마지막 어텐션 레이어의 key 벡터를 사용해 최고의 객체 발견 성능을 기록했습니다. 그러나 이러한 **수작업 기반 선택(hand-crafted choice)**은 다른 백본에는 일관되게 일반화되지 않습니다. 단순화를 위해, 우리는 모든 모델에서 **출력 특징(output features)**을 사용합니다.

#### 결과 (그림 14)

- 원래 DINO는 이 과제에서 매우 높은 기준을 세웠습니다.
- 흥미롭게도, **DINOv2**는 픽셀 단위의 밀집(dense) 과제에서는 매우 강력한 성능을 보였지만, 객체 발견에서는 실패했습니다. 이는 부분적으로 **밀집 특징의 잡음(artifact)** 때문이라고 볼 수 있습니다 (그림 13 참조).
- 반면, **DINOv3**는 잡음이 적고 정밀한 출력 특징 맵을 통해 이전 버전들을 능가했습니다.
  - VOC 2007에서 **+5.9 CorLoc** 향상을 달성했고,
  - 자기지도, 약지도, 집성(agglomerative) 모델을 포함한 모든 다른 백본을 능가했습니다.

이 평가 결과는 DINOv3의 밀집 특징이 **의미적으로 강력하면서도 정확히 지역화(localized)** 되어 있음을 확인시켜 줍니다.  
우리는 이것이 **클래스 비의존적 객체 검출(class-agnostic detection)** 접근 방식을 발전시키는 길을 열 것이라 믿습니다. 특히 주석이 비싸거나 부족한 상황, 그리고 관련 클래스 집합이 사전에 정의되지 않은 시나리오에서 그 가치가 클 것입니다.

![](/assets/images/posts/592/img_22.png)

**그림 14: 비지도 객체 발견**  
TokenCut(Wang et al., 2022c)을 다양한 백본의 출력 패치 특징에 적용하고 CorLoc 지표를 보고합니다.  
또한, DINOv3를 활용해 얻은 예측 마스크(red overlay, 입력 해상도 1024)를 시각화했으며, 이 결과는 **주석(annotation)과 후처리(post-processing) 없이** 얻어진 것입니다.

? 요약하면, **DINOv3는 주석 없이도 객체를 잘 찾아낼 수 있는 강력한 특징을 학습했다**는 게 핵심이네요.

### 6.1.5 비디오 분할 추적 (Video Segmentation Tracking)

정적 이미지(static images)를 넘어, **시각 표현(visual representations)의 중요한 속성**은 **시간적 일관성(temporal consistency)**입니다. 즉, 특징(feature)이 시간이 지남에 따라 안정적으로 진화하는가 하는 문제입니다. 이를 검증하기 위해, 우리는 DINOv3를 **비디오 분할 추적(video segmentation tracking)** 과제에서 평가합니다.

이 과제의 목표는 비디오의 첫 번째 프레임에서 주어진 **GT(ground-truth) 인스턴스 분할 마스크**를 이후 프레임으로 전파(propagate)하는 것입니다.

- 데이터셋: DAVIS 2017 (Pont-Tuset et al., 2017), YouTube-VOS (Xu et al., 2018), MOSE (Ding et al., 2023)
- 성능 평가 지표: **? & ℱ-mean metric**
  - 영역 유사도(?, region similarity)와
  - 윤곽 정확도(ℱ, contour accuracy)를 결합한 지표 (Perazzi et al., 2016).

Jabri et al. (2020)을 따라, 프레임 간 패치 특징 간의 유사성을 고려하는 **비매개변수적(non-parametric) 레이블 전파 알고리즘**을 사용합니다.

또한 입력 해상도에 따라 세 가지 설정을 평가합니다.

- **Small (S)**: 패치 크기 14/16일 때, 영상 짧은 변 길이 420/480 픽셀
- **Medium (M)**: 840/960 픽셀
- **Large (L)**: 1260/1440 픽셀

(즉, 패치 토큰 개수에 맞게 설정).  
? & ℱ 점수는 항상 비디오의 원본 해상도(native resolution)에서 계산합니다.  
세부 실험 설정은 부록 Sec. D.5를 참조하세요.

![](/assets/images/posts/592/img_23.png)

#### 결과 (표 5)

- 지금까지와 일관되게, 약지도(weakly-supervised) 백본들은 설득력 있는 성능을 보여주지 못했습니다.
- **PEspatial** (비디오 모델 SAMv2에서 distillation된 모델)은 작은 해상도에서 DINOv2를 능가하는 만족스러운 성능을 보였으나, 더 큰 해상도에서는 성능이 떨어졌습니다.
- 해상도 전반에 걸쳐, **DINOv3는 모든 경쟁 모델을 압도**했습니다.
  - DAVIS-L에서 **83.3 ? & ℱ**, DINOv2보다 **+6.7 점** 향상.

또한 해상도에 따른 성능 추세도 긍정적이었는데, 이는 DINOv3가 **더 많은 입력 픽셀을 활용해 정밀하고 고해상도의 특징 맵을 산출할 수 있음을 확인**시켜 줍니다 (그림 3, 4 참조).

반대로,

- **SigLIP 2와 PEcore**는 해상도가 올라가도 성능이 거의 개선되지 않았고,
- **PEspatial**은 오히려 성능이 하락했습니다.

흥미롭게도, **DINOv3는 이미지 모델임에도 불구하고 비디오에 대해 어떤 튜닝도 없이 시간에 따라 객체를 제대로 추적**할 수 있었습니다 (그림 15 참조).  
이는 DINOv3가 **비디오를 임베딩(embedding)하는 기반 모델로서 매우 유망**하며, 그 위에 강력한 비디오 모델을 구축할 수 있음을 보여줍니다.

![](/assets/images/posts/592/img_24.png)

**그림 15: 분할 추적 예시**  
첫 프레임의 GT 인스턴스 분할 마스크가 주어졌을 때, 우리는 **DINOv3의 특징 공간에서의 패치 유사성**을 기반으로 인스턴스 라벨을 이후 프레임으로 전파합니다.  
입력 해상도는 **2048 × 1536 픽셀**, 결과적으로 **128 × 96 패치**를 생성합니다.

? 요약: **DINOv3는 이미지 전용으로 학습되었음에도 불구하고, 비디오 추적에서 최상위 성능을 달성하며 해상도 증가에도 강한 적응력을 보였다**는 점이 핵심입니다.

### 6.1.6 비디오 분류 (Video Classification)

앞선 결과들은 DINOv3의 표현이 **저수준 시간적 일관성(low-level temporal consistency)**을 가지고 있어, 시간에 따라 객체를 정확히 추적할 수 있음을 보여주었습니다. 이번 절에서는 한 단계 더 나아가, DINOv3의 **dense feature가 고수준 비디오 분류(high-level video classification)에 적합한지**를 평가합니다.

V-JEPA 2 (Assran et al., 2025)의 설정과 유사하게, 우리는 각 프레임에서 추출한 패치 특징 위에 **얕은 4층 트랜스포머 기반 분류기(attentive probe)**를 학습시킵니다. 이 방식은 프레임별로 독립적으로 추출된 특징을 이용하면서, 시공간적 차원 모두에서 추론할 수 있게 합니다.

평가 시에는

- 비디오당 하나의 클립만 사용하거나,
- **테스트 타임 증강(TTA)**을 적용하여, 비디오당 3개의 공간 crop과 2개의 시간 crop에서 얻은 예측값을 평균합니다.

세부 실험 설정은 부록 Sec. D.6을 참조하십시오.

데이터셋은 세 가지로 실험했습니다:

- **UCF101** (Soomro et al., 2012)
- **Something-Something V2 (SSv2)** (Goyal et al., 2017)
- **Kinetics-400 (K400)** (Kay et al., 2017)

성과 지표는 **Top-1 Accuracy**입니다.  
또한, 비교를 위해 비디오 이해용 최첨단 SSL 모델인 **V-JEPA v2**의 성능도 함께 보고합니다.

#### 결과 (표 6)

앞선 실험의 결론과 일치하게, **DINOv3는 강력한 비디오 특징 추출기로 성공적으로 활용될 수 있음**을 확인했습니다.

이 실험은 여러 층의 self-attention을 학습하는 것을 포함하기 때문에, 모델 간 차이가 다소 줄어드는 경향이 있습니다. 그러나,

- DINOv3는 **PEcore 및 SigLIP 2와 같은 성능 범위에 위치**하며,
- **DINOv2나 AM-RADIO보다 모든 데이터셋에서 명확하게 우수**했습니다.
- **UCF101과 K400**은 주로 **appearance-focused** 데이터셋으로, 객체의 범주 수준(category-level) 이해가 성능을 결정하는 주요 요인인데, DINOv3는 이 부분에서 강력한 성능을 발휘했습니다.
- 반면, **SSv2**는 **동작 이해(motion understanding)**가 더 중요한 데이터셋으로, 이 경우에는 전용 비디오 모델인 **V-JEPA v2**가 두드러진 성능을 보였습니다.

흥미롭게도, **DINOv3와 약지도(weakly-supervised) 모델들 간의 격차는 SSv2에서 더 크게 나타났습니다.**  
이는 다시 한번, **DINOv3가 비디오 관련 과제에도 적합한 모델임을 확인**시켜 줍니다.

![](/assets/images/posts/592/img_25.png)

? 요약: **DINOv3는 이미지 기반 모델임에도 불구하고, 비디오에서도 강력한 특징을 뽑아내며, 전용 비디오 모델이 아닌데도 충분히 경쟁력 있는 성능을 발휘한다**는 점이 핵심입니다.

### 6.2 DINOv3는 견고하고 다재다능한 전역 이미지 디스크립터를 가진다

이번 절에서는 **DINOv3가 전역(global) 이미지 통계를 포착하는 능력**을 평가한다. 이를 위해 우리는 **고전적인 분류 벤치마크(선형 프로브, Sec. 6.2.1)**와 **인스턴스 검색 벤치마크(Sec. 6.2.2)**를 고려한다. 비교 대상으로는 공개된 가장 강력한 이미지 인코더들을 포함했다. 앞 절의 모델들에 더해, **약지도(weakly supervised) 모델** 두 가지도 평가했다:

- **AIMv2** (Fini et al., 2024): 자동회귀 기반 픽셀 및 텍스트 예측을 결합하여 학습된 모델
- **EVA-CLIP-18B** (Sun et al., 2024): 초대규모 모델

#### 6.2.1 선형 프로빙을 통한 이미지 분류

DINOv3의 출력 **CLS 토큰** 위에 선형 분류기를 학습하여, 분류 벤치마크에서 성능을 평가한다.

- **ImageNet1k** (Deng et al., 2009) 데이터셋 및 변형들을 사용하여 분포 밖(out-of-distribution, OOD) 견고성을 평가하고,
- 다양한 도메인의 데이터셋으로 DINOv3의 세밀한 분류 능력을 검증한다.

평가 설정에 대한 세부 내용은 Sec. D.7을 참조하라.

![](/assets/images/posts/592/img_26.png)

ImageNet 기반 도메인 일반화 (표 7)

실험 과정:

- ImageNet-train으로 학습,
- ImageNet-val을 검증 세트로 사용해 하이퍼파라미터 선택,
- 선택된 분류기를 다양한 테스트 세트에 전이:
  - **ImageNet-V2** (Recht et al., 2019), **ReaL** (Beyer et al., 2020): ImageNet의 다른 버전 (기존 검증 세트에 대한 과적합 여부 확인)
  - **Rendition** (Hendrycks et al., 2021a), **Sketch** (Wang et al., 2019): 스타일/인공적 변형 버전
  - **Adversarial** (Hendrycks et al., 2021b), **ObjectNet** (Barbu et al., 2019): 의도적으로 어렵게 만든 예시
  - **Corruptions** (Hendrycks & Dietterich, 2019): 일반적인 이미지 손상 상황에 대한 견고성

참고로, **Dehghani et al. (2023)**에서 JFT(3B–4B 이미지)로 지도학습된 ViT에 대해 보고된 결과도 함께 표기했으나, **평가 프로토콜이 달라 직접 비교는 불가능**하다.

![](/assets/images/posts/592/img_27.png)

**결과:**

- DINOv3는 모든 이전 자가 지도 학습 백본을 크게 능가:
  - ImageNet-R에서 +10%,
  - Sketch에서 +6%,
  - ObjectNet에서 +13% (DINOv2 대비).
- 강력한 약지도 모델(SigLIP 2, PE)은 이제 대규모 지도 모델(ViT-22B)보다 어려운 OOD 작업(ImageNet-A, ObjectNet)에서 더 우수하다.
- DINOv3는 ImageNet-R과 Sketch에서 이들과 비슷한 성능을 기록, 어려운 OOD 작업에서는 PE 바로 뒤에 위치하면서도 SigLIPv2는 초과했다.
- ImageNet에서는 검증 점수가 SigLIPv2, PE 대비 0.7–0.9 낮았지만, “더 깨끗한” 테스트 세트(V2, ReaL)에서는 사실상 동일했다.
- 특히, DINOv3는 **ImageNet-C(손상 견고성)에서 최고 성능**을 달성했다.

➡️ 종합적으로, 이는 **SSL 모델이 처음으로 약지도·지도 학습 모델에 버금가는 성능을 분류 과제에서 달성한 사례**다.  
이는 ViT-22B, SigLIP 2, PE 같은 모델들이 방대한 **사람이 주석한 데이터셋**으로 학습된 것과 달리, DINOv3는 **이미지에서만 학습**했다는 점에서 더욱 인상적이다. 이 접근법은 향후 더 큰 확장 가능성을 시사한다.

![](/assets/images/posts/592/img_28.png)

세밀 분류 (표 9)

우리는 여러 데이터셋에서 선형 프로브를 학습하여, **세밀(fine-grained) 분류 성능**을 측정했다.

- **대규모 데이터셋 3개:**
  - **Places205** (Zhou et al., 2014): 장면 인식
  - **iNaturalist 2018** (Van Horn et al., 2018), **iNaturalist 2021** (Van Horn et al., 2021): 세밀한 식물/동물 종 분류
- **소규모 데이터셋 12개 평균 (Fine-S)**: 장면, 사물, 텍스처 포함 (Oquab et al., 2024 참조, 표 22에 개별 결과 포함).

**결과:**

- DINOv3는 다시 한번 모든 이전 SSL 방법을 능가했다.
- 약지도 방법과 비교해도 경쟁력 있는 성능을 보였으며, 이는 다양한 세밀 분류 작업에서의 견고성과 일반화 능력을 보여준다.
- 특히, 어려운 **iNaturalist21** 데이터셋에서 **89.8% 정확도**를 기록하며, 최고의 약지도 모델인 PEcore(87.0%)조차 초월했다.

? 요약:

- **ImageNet 분류**: 처음으로 SSL이 약지도·지도 모델에 견줄 성능 달성
- **세밀 분류**: 어려운 iNaturalist21에서 약지도 모델을 초월
- **의미**: DINOv3는 전역 이미지 디스크립터로서 견고하고, 범용적인 강점을 입증

### 6.2.2 인스턴스 인식 (Instance Recognition)

모델의 인스턴스 수준 인식 능력을 평가하기 위해 **비모수(non-parametric) 검색 접근법**을 사용했다.

- 데이터베이스 이미지는 주어진 쿼리 이미지와의 **코사인 유사도**를 기준으로 순위가 매겨지며, 이때 출력 **CLS 토큰**을 사용한다.
- 평가 데이터셋:
  - **Oxford, Paris**: 랜드마크 인식 (Radenović et al., 2018)
  - **Met**: 메트로폴리탄 박물관의 예술 작품 (Ypsilantis et al., 2021)
  - **AmsterTime**: 현대의 거리 뷰 이미지와 암스테르담의 역사적 아카이브 이미지를 매칭 (Yildiz et al., 2022)

검색 성능 측정:

- Oxford, Paris, AmsterTime → **mean average precision (mAP)**
- Met → **global average precision (gAP)**  
  평가 세부 내용은 Sec. D.8 참조.

#### 결과 (표 9, 23)

- 모든 벤치마크에서 **DINOv3는 압도적으로 가장 높은 성능**을 기록.
  - Met: 2위 모델(DINOv2) 대비 **+10.8포인트** 향상
  - AmsterTime: DINOv2 대비 **+7.6포인트** 향상
- 약지도 모델들은 대부분 DINOv3보다 크게 뒤처졌으며, 예외적으로 **AM-RADIO**는 DINOv2 피처에서 증류(distillation)된 덕분에 상대적으로 근접한 성능을 보였다.

➡️ 이러한 결과는 **DINOv3가 인스턴스 수준 검색(instance-level retrieval)에서 매우 견고하고 다재다능함**을 보여주며, 이는 전통적인 랜드마크 데이터셋뿐만 아니라 **예술 작품, 역사적 이미지 검색 같은 까다로운 도메인**에서도 일관되게 성능을 발휘함을 의미한다.

? 정리하면, 이 부분은 DINOv3가 **단순 landmarks → 예술·역사 이미지 retrieval까지 전 영역에서 통하는 전역 표현(global descriptor)**을 가진다는 점을 강조하는군요.

### 6.3 DINOv3는 복잡한 컴퓨터 비전 시스템을 위한 기반이 된다

앞선 두 섹션에서는 **DINOv3가 dense 및 global 작업 모두에서 뛰어난 품질**을 보인다는 강력한 신호를 확인했다. 그러나 이 결과들은 대부분 “모델 프로빙(model probing)” 방식의 실험 프로토콜 하에서 얻어진 것으로, 가벼운 선형 어댑터나 비모수 알고리즘을 사용해 특징 품질만을 평가한 것이다. 이러한 단순한 평가 방식은 혼란 요인을 제거하는 데는 유용하지만, **DINOv3가 대규모 컴퓨터 비전 시스템의 근간으로서 발휘할 수 있는 잠재력**을 온전히 보여주지는 못한다.

따라서 본 섹션에서는 단순 프로빙을 넘어, **더 복잡한 다운스트림 디코더를 학습**하고, **강력한 태스크 특화 베이스라인**들과 비교한다. 구체적으로, 우리는 DINOv3를 기반으로 다음 네 가지 작업을 다룬다:

1. 객체 탐지(Object Detection) — Plain-DETR (Sec. 6.3.1)
2. 의미론적 분할(Semantic Segmentation) — Mask2Former (Sec. 6.3.2)
3. 단일 뷰 깊이 추정(Monocular Depth Estimation) — Depth Anything (Sec. 6.3.3)
4. 3D 이해(3D Understanding) — Visual Geometry Grounded Transformer (Sec. 6.3.4)

이 작업들은 어디까지나 DINOv3로 가능한 활용 예시를 탐색하기 위한 것이지만, **적은 노력만으로도 최첨단 성능에 근접하거나 능가하는 결과**를 보여준다.

### 6.3.1 객체 탐지 (Object Detection)

첫 번째로 다루는 태스크는 오래된 컴퓨터 비전 과제인 **객체 탐지**이다.

- 목표: 입력 이미지에서 **사전 정의된 카테고리의 객체 인스턴스를 바운딩 박스(bounding box)**로 찾아내는 것.
- 요구사항:
  - **정확한 위치 추정(localization)**
  - **정확한 인식(recognition)**
  - 즉, 박스가 객체 경계와 잘 맞아야 하며, 올바른 카테고리로 분류되어야 함.

현재 COCO (Lin et al., 2014) 같은 표준 벤치마크에서 성능은 사실상 포화 상태다. 따라서 우리는 **백본(backbone)을 고정(frozen)**한 상태에서, **상단에 소형 디코더만 학습**하는 방식을 제안한다.

#### 데이터셋과 메트릭

- 평가: **COCO-VAL2017** split (Lin et al., 2014)
- 추가: **COCO-O** (Mao et al., 2023) → 동일 클래스지만 6가지 분포 이동(distribution shift)이 적용된 이미지 포함
- 메트릭: **mAP (mean Average Precision)**, IoU 임계값 [0.5:0.05:0.95]
- COCO-O에서는 추가로 **ER (Effective Robustness)** 보고
- 학습 데이터: COCO는 비교적 작은 데이터셋(118k 이미지)이므로, **Objects365 (Shao et al., 2019)**로 디코더 사전학습 → COCO로 파인튜닝 (일반적인 관행)

#### 구현

Plain-DETR (Lin et al., 2023b)를 기반으로 하되, **다음과 같은 수정**을 적용했다:

- Transformer 인코더를 백본에 융합하지 않고, 원래 DETR (Carion et al., 2020)처럼 **별도의 모듈로 유지**
- 따라서 학습 및 추론 중에도 **DINOv3 백본은 완전히 고정(frozen)** 상태 유지
- 이는 **고정된 백본을 사용하면서도 경쟁력 있는 탐지 모델을 구현한 최초 사례**로 보임

학습 프로세스:

- Objects365: 해상도 1536에서 22 epoch → 해상도 2048에서 1 epoch
- COCO: 해상도 2048에서 12 epoch
- 추론: 해상도 2048
- 추가적으로, **TTA(Test-Time Augmentation)** 적용 가능 (1536~2880 해상도의 멀티스케일 입력)
- 세부 실험 설정은 Sec. D.9 참조

#### 결과 (표 10)

- 비교 대상:
  - EVA-02 + Cascade Detector (Fang et al., 2024b)
  - EVA-02 + Co-DETR (Zong et al., 2023)
  - InternImage-G + DINO (Wang et al., 2023b)
  - PEspatial + DETA (Bolya et al., 2025)
- 결과 요약:
  - **백본을 동결한 상태**에서도, 우리 경량 탐지기(100M 파라미터)는 **최첨단 성능 달성**
  - 특히 **COCO-O에서의 성능 차이가 두드러지며**, 이는 DINOv3의 **견고한 표현력**을 탐지 모델이 잘 활용하고 있음을 의미
  - 우리의 모델은 **훈련된 파라미터 수가 훨씬 적음에도** 기존 모든 모델을 능가 (비교 모델 최소 300M 파라미터 이상)

➡️ 이는 **백본을 특화시킬 필요 없이 강력한 성능**을 달성할 수 있음을 보여주며,  
**단일 백본 forward만으로도 다수의 태스크를 지원**할 수 있음을 시사한다. 따라서 **계산 자원 절감과 실용적 응용 가능성**을 크게 높인다.

![](/assets/images/posts/592/img_29.png)

? 요약하면, 이 부분은 DINOv3가 **고정 백본(frozen backbone) + 소형 디코더만으로도 객체 탐지에서 SOTA급 성능**을 달성한다는 점을 강조하는군요.

### 6.3.2 의미론적 분할 (Semantic Segmentation)

앞선 실험에 이어, 이번에는 **의미론적 분할(semantic segmentation)**을 평가한다.  
이 작업 역시 **정확하고 잘 위치된 표현**을 필요로 하며, **픽셀 단위의 조밀한 예측(dense per-pixel prediction)**을 요구한다. 하지만 객체 탐지(object detection)와 달리, 같은 객체의 개별 인스턴스를 구분할 필요는 없다.  
탐지와 마찬가지로, 우리는 **동결된(frozen) DINOv3 모델 위에 디코더를 학습**한다.

#### 데이터셋 및 메트릭

- 주요 데이터셋: **ADE20k (Zhou et al., 2017)**
  - 150개의 의미론적 카테고리
  - 20k 학습 이미지, 2k 검증 이미지
- 평가 지표: **mIoU (mean Intersection over Union)**
- 추가 학습 데이터셋:
  - **COCO-Stuff (Caesar et al., 2018)** → 164k 이미지, 171 카테고리
  - **Hypersim (Roberts et al., 2021)** → 77k 이미지, 40 카테고리

#### 구현

DINOv3 특징을 의미론적 카테고리로 매핑하기 위해,

- **ViT-Adapter (Chen et al., 2022)** + **Mask2Former (Cheng et al., 2022)** 조합을 사용 (선행연구 Wang et al., 2022b; 2023b; 2023a와 유사)
- 그러나 우리 경우 **DINOv3 백본은 학습 동안 동결(frozen)**
- 백본의 특징을 변경하지 않기 위해, **원래 ViT-Adapter의 injector 컴포넌트를 제거**
- DINOv3의 **4096차원 출력**을 처리하기 위해, 임베딩 차원을 **1024 → 2048**로 확장

학습 절차:

1. COCO-Stuff에서 **80k iteration 사전학습**
2. Hypersim에서 **10k iteration 학습**
3. ADE20k 학습 split에서 **20k iteration 학습** 후, 검증 split에서 성능 보고

- 학습 해상도: 입력 크기 **896**
- 추론 시 두 가지 세팅:
  - **Single-scale** → 학습 해상도로 단일 전방향 수행
  - **Multi-scale** → 0.9배 ~ 1.1배 비율로 다양한 입력 크기를 적용 후 평균 (TTA)

#### 결과 (표 11)

- 비교 대상: **BEIT-3 (Wang et al., 2022b)**, **InternImage-H (Wang et al., 2023b)**, **ONE-PEACE (Wang et al., 2023a)**
- 추가 데이터셋 결과는 표 24 참고

**결과 요약:**

- 동결된 DINOv3 백본 위의 분할 모델은 **최첨단 성능을 달성**
- ADE20k에서 **ONE-PEACE와 동일한 63.0 mIoU**
- COCO-Stuff, VOC 2012에서도 기존 모든 모델보다 향상된 성능
- 의미론적 분할은 **픽셀 단위 정밀 예측**이 필요하기 때문에,
  - 비전 트랜스포머 백본의 **16픽셀 단위 패치 입력**은 본질적으로 예측의 세분성을 제한 → ViT-Adapter 같은 보완 필요
- 그러나 우리는 **4096 해상도까지 고품질 특징맵**을 얻을 수 있음을 보였음 (512 토큰 너비의 dense feature map)
- 따라서 향후 연구에서는 **무거운 디코더(예: ViT-Adapter + Mask2Former)에 의존하지 않고도**,  
  이러한 **고해상도 특징을 직접 활용해 SOTA 달성** 가능성을 기대

![](/assets/images/posts/592/img_30.png)

? 요약하면, 이 부분은 **“DINOv3를 고정한 상태에서도 ADE20k 의미론적 분할에서 SOTA급 성능을 달성했다”**는 메시지를 담고 있고, 특히 **더 고해상도 특징 활용 → 경량 디코더로도 가능할 것**이라는 비전을 제시하고 있네요.

### 6.3.3 단안 깊이 추정 (Monocular Depth Estimation)

이번에는 **단안(monocular) 깊이 추정 시스템**을 구축하는 것을 고려한다.  
이를 위해 최근 SOTA 기법인 **Depth Anything V2 (DAv2)** (Yang et al., 2024b)의 설정을 따른다.

DAv2의 핵심 혁신은 **대규모 합성 이미지 데이터셋**에 **정답 깊이 주석(ground truth depth annotation)**을 붙여 학습한다는 점이다. 중요한 점은, 여기서 **DINOv2를 특징 추출기(feature extractor)**로 사용하여 **시뮬레이션-현실(sim-to-real) 격차를 메우는 능력**을 확보한다는 것이다.  
다른 비전 백본(예: SAM (Kirillov et al., 2023))은 이러한 능력을 보이지 않는다 (Yang et al., 2024b).

따라서 우리는 **DAv2 파이프라인에서 DINOv2를 DINOv3로 교체**하여 유사한 결과를 달성할 수 있는지 확인한다.

#### 구현 (Implementation)

- DAv2와 마찬가지로, **Dense Prediction Transformer (DPT)** (Ranftl et al., 2021)을 사용하여 **픽셀 단위 깊이 맵**을 예측한다.
- 입력으로는 DINOv3의 **4개 층에서 동일 간격으로 추출한 특징**을 사용한다.
- 손실 함수는 DAv2에서 사용된 것을 그대로 적용하며, 합성 데이터셋으로 학습한다.
- 해상도는 **1024×768**로 높여, DINOv3의 **고해상도 특징 활용 능력**을 살린다.
- DAv2와 달리, **백본은 미세 조정(finetuning)하지 않고 동결(frozen)된 상태**로 두어 **DINOv3의 기본 능력(out-of-the-box capabilities)**을 평가한다.
- 또한, **DINOv3 7B의 대규모 특징을 충분히 활용**하기 위해 \*\*DPT 헤드를 확장(scale-up)\*\*하는 것이 효과적임을 발견했다.
- 세부 내용은 Sec. D.11 참고.

#### 데이터셋 및 평가 지표

모델을 **5개의 실제 데이터셋**에서 평가한다:

- **NYUv2** (Silberman et al., 2012)
- **KITTI** (Geiger et al., 2013)
- **ETH3D** (Schöps et al., 2017)
- **ScanNet** (Ke et al., 2025)
- **DIODE** (Vasiljevic et al., 2019)
- 평가 설정: **Zero-shot scale-invariant depth** (Ranftl et al., 2020; Ke et al., 2025; Yang et al., 2024b)
- 지표:
  - **절대 상대 오차 (ARel)** (낮을수록 좋음)
  - **δ1** (높을수록 좋음)
- 지표 설명은 Yang et al. (2024a) 참고.

#### 결과 (표 12)

- 비교 대상:
  - **MiDaS** (Ranftl et al., 2020)
  - **LeReS** (Yin et al., 2021)
  - **Omnidata** (Eftekhar et al., 2021)
  - **DPT** (Ranftl et al., 2021)
  - **Marigold (ensemble 버전)** (Ke et al., 2025)
  - **DAv2** (Yang et al., 2024b)
- **우리 모델 성능:**
  - **모든 데이터셋에서 새로운 SOTA 달성**
  - 단, **DIODE의 ARel 지표**에서는 DPT보다 약간 낮음
  - 주목할 점: **백본을 동결한 상태에서도 가능**  
    → 다른 모든 baseline은 깊이 추정을 위해 백본을 반드시 finetune함
  - 따라서 DINOv3는 **DINOv2가 보였던 sim-to-real 강력한 전이 능력**을 이어받았음을 검증함
  - 이는 **합성 데이터 기반 학습을 다양한 다운스트림 작업으로 확장**할 수 있는 가능성을 열어줌

![](/assets/images/posts/592/img_31.png)

? 요약하면:

- **DINOv3 + DAv2 조합 → 단안 깊이 추정에서 모든 벤치마크 SOTA 달성**
- **백본 동결 상태에서도 finetuned 모델을 능가**
- DINOv3의 **sim-to-real 일반화 능력**이 강력함을 보여줌

### 6.3.4 DINOv3 기반 Visual Geometry Grounded Transformer (VGGT)

마지막으로, 최근 제안된 **Visual Geometry Grounded Transformer (VGGT)** (Wang et al., 2025)을 사용한 **3D 이해(3D understanding)**를 다룬다.  
VGGT는 **대규모 3D 주석 데이터**로 학습되며, 단일 forward pass에서 장면의 주요 3D 속성(카메라 내·외부 파라미터, 포인트 맵, 깊이 맵 등)을 모두 추정할 수 있다.  
단순하고 통합된 파이프라인을 통해, VGGT는 다양한 3D 작업에서 SOTA 성능을 달성하며, 특화된 방법들보다 더 효율적이다. 이는 3D 이해에서 큰 진보를 의미한다.

#### 구현 (Implementation)

- VGGT는 **DINOv2로 사전학습된 백본**을 사용하여 장면의 다양한 뷰에서 표현을 추출하고, 이후 이를 Transformer로 융합한다.
- 본 연구에서는 단순히 **DINOv2 백본을 DINOv3로 교체**하였다.
  - DINOv2의 **ViT-L/14**와 대응하기 위해 **DINOv3 ViT-L 변형(variant)**을 사용 (Sec. 7 참고).
- 학습 파이프라인은 원래 VGGT와 동일하게 수행하며, **이미지 백본도 미세 조정(finetuning)** 한다.
- 이미지 해상도는 **518×518 → 592×592**로 변경하여 DINOv3의 **patch size 16**에 맞추었고, 결과가 VGGT와 비교 가능하도록 유지했다.
- 추가적으로 소수의 하이퍼파라미터 변경을 적용했으며, 세부 내용은 Sec. D.12에 기술되어 있다.

#### 데이터셋 및 평가 지표

Wang et al. (2025)을 따라 다음을 평가한다:

- **카메라 포즈 추정(Camera pose estimation)**
  - Re10K (Zhou et al., 2018)
  - CO3Dv2 (Reizenstein et al., 2021)
- **Dense multi-view 추정**
  - DTU (Jensen et al., 2014)
- **두-뷰 매칭(two-view matching)**
  - ScanNet-1500 (Dai et al., 2017)
- **평가지표:**
  - 카메라 포즈 추정 & 두-뷰 매칭 → **AUC (Area Under Curve)**
  - Multi-view 추정 →
    - 예측→정답의 최소 L2 거리 = **정확도(Accuracy)**
    - 정답→예측의 최소 L2 거리 = **완전성(Completeness)**
    - 두 지표 평균 = **Overall**
- 세부 평가 방법은 Wang et al. (2025) 참고.

#### 결과 (표 13)

- **DINOv3로 교체된 VGGT**는 모든 3D 작업에서 기존 VGGT가 세운 SOTA를 추가로 향상시킴.
- DINOv3 적용만으로 **명확하고 일관된 성능 개선**을 보였음.
- 이는 DINOv3에 대해 최소한의 튜닝만 적용했음을 고려할 때 특히 고무적임.
- 다룬 작업들은 **다양한 수준의 시각적 이해**를 포함한다:
  - **고수준 추상화**: 카메라 포즈 추정
  - **밀집 기하 예측**: 다중 뷰 깊이 추정
  - **정밀한 픽셀 단위 대응**: 뷰 매칭
- 앞서 살펴본 **대응 추정(Sec. 6.1.3)** 및 **깊이 추정(Sec. 6.3.3)** 결과와 함께, 이는 **DINOv3가 3D 작업의 기초(backbone)로 매우 적합하다는 경험적 증거**를 추가로 제공한다.
- 또한, **더 큰 모델인 DINOv3 7B**를 사용할 경우 성능이 더욱 향상될 것으로 기대된다.

![](/assets/images/posts/592/img_32.png)

? 요약:

- **DINOv2 → DINOv3 교체만으로 VGGT가 모든 3D 작업에서 SOTA 갱신**
- 카메라 포즈, 깊이 추정, 뷰 매칭까지 고르게 개선
- DINOv3는 3D 비전의 **범용 파운데이션 백본**으로 강력한 가능성을 입증

### 7 DINOv3 모델 전체 계열 평가

이 섹션에서는 **70억(7B) 파라미터 모델**(Sec. 5.2 참고)에서 증류(distillation)된 **DINOv3 모델 계열**에 대한 정량적 평가를 제공한다.  
이 계열은 **Vision Transformer (ViT)** 및 **ConvNeXt (CNX)** 아키텍처 기반 변형들을 포함한다.  
모든 모델의 세부 파라미터 수와 추론 시 FLOPs는 Fig. 16(a)에 제시하였다.  
이들 모델은 다양한 계산 예산을 커버하여 **폭넓은 사용자 및 배포 시나리오**를 지원한다.  
우리는 모든 ViT(Sec. 7.1) 및 ConvNeXt(Sec. 7.2) 변형 모델들을 평가하여, 다양한 작업에서의 성능을 철저히 분석한다.

Fig. 2는 **DINOv3 계열과 다른 모델 컬렉션을 비교한 개요**를 보여준다.  
DINOv3 계열은 **모든 dense prediction(밀집 예측) 작업에서 다른 모든 계열을 크게 능가**한다.  
여기에는 **AM-RADIO**, **PEspatial**처럼 **지도학습 기반 백본에서 증류된 특화 모델들**도 포함된다.  
동시에, DINOv3 모델은 **분류(classification) 작업에서도 유사한 결과**를 달성하여, **계산 자원 예산 전반에서 최적의 선택지**임을 보여준다.

- Sec. 7.1에서는 **ViT 모델의 세부사항**을 다루고, 다른 오픈소스 대안들과 비교한다.
- 이어서 Sec. 7.2에서는 **ConvNeXt 모델들**을 논의한다.
- 마지막으로 Sec. 5.3에 이어, 우리는 **ViT-L 모델 출력과 정렬된 텍스트 인코더**를 학습하였다.
- 이에 대한 **멀티모달 정렬 결과**를 Sec. 7.3에서 제시한다.

![](/assets/images/posts/592/img_33.png)

**Figure 16 설명**

- (a) 증류된 모델들의 특성:
  - CNX = ConvNeXt
  - 각 모델별 파라미터 수 및 GFLOPs를 입력 이미지 크기 **256×256**과 **512×512**에서 추정한 값과 함께 제시함.
- (b) DINOv3 **ViT-H+**와 **7B 규모의 teacher 모델** 비교:
  - **파라미터 수가 거의 10배 적음에도 불구하고**, ViT-H+는 DINOv3 7B에 근접한 성능을 보임.

? 요약하면, 이 섹션은 **DINOv3 계열 전체의 성능 스펙트럼을 공개적으로 제시하면서, dense prediction에서는 압도적 우위, classification에서는 동급 성능**을 입증했다는 내용이에요.

### 7.1 모든 활용 사례를 위한 비전 트랜스포머

우리의 ViT 계열은 **소형 ViT-S**부터 **8억 4천만 파라미터 규모의 ViT-H+**까지 다양한 아키텍처를 포함한다.

- **ViT-S**는 노트북과 같은 **자원 제약이 있는 기기**에서 효율적으로 실행되도록 설계되었으며,
- **ViT-H+**는 더 높은 요구가 필요한 응용을 위해 **최신(state-of-the-art) 성능**을 제공한다.

우리는 ViT 모델들을 유사한 규모의 **최고 수준 오픈소스 이미지 인코더**들과 비교한다.  
비교 대상은 **DINOv2 (Oquab et al., 2024)**, **SigLIP 2 (Tschannen et al., 2025)**, **Perception Encoder (Bolya et al., 2025)**이다.  
공정한 비교를 위해 입력 시퀀스 길이를 동일하게 맞추었다.  
구체적으로, **패치 크기 16 모델**은 입력 이미지 크기 **512×512**, **패치 크기 14 모델**은 **448×448** 이미지를 사용한다.

**표 14:**  
우리 ViT-{S, S+, B, L, H+} 모델을 다양한 글로벌 및 밀집 예측 벤치마크에서 비교한 결과.

- 분류: IN-ReAL, IN-R, ObjectNet
- 검색: Oxford-H
- 세그멘테이션: ADE20k
- 깊이 추정: NYU
- 추적: DAVIS (960px)
- 키포인트 매칭: NAVI, SPair

패치 크기가 다른 모델 간에도 **패치 토큰 수를 동일하게 맞춰** 비교하였다.

![](/assets/images/posts/592/img_34.png)

**실험 결과:**

- DINOv3 모델은 **모든 dense prediction 작업에서 일관적으로 경쟁 모델을 능가**한다.
- 특히, **ADE20k 벤치마크**에서 DINOv3 ViT-L은 **DINOv2 대비 6 mIoU 이상** 개선을 보였다.
- ViT-B도 차상위 경쟁자 대비 **약 3 mIoU** 향상을 달성했다.

이러한 큰 개선은 DINOv3의 **로컬 피처(local features)**가 세밀한 공간적 정보를 효과적으로 포착할 수 있음을 보여준다.  
또한, 깊이 추정 작업에서도 일관된 성능 향상이 나타나, DINOv3 계열이 다양한 밀집 비전 문제에서도 **범용성과 강력한 성능**을 입증하였다.

중요하게도, **ObjectNet 및 ImageNet-1k**와 같은 글로벌 인식 벤치마크에서도 경쟁력 있는 성능을 달성하였다.  
이는 **dense task 성능 향상이 global task 정확도를 희생하지 않았음**을 의미한다.  
따라서, DINOv3 모델은 **dense와 global 비전 과제 모두에서 뛰어난 균형 잡힌 해법**임을 확인할 수 있다.

**대규모 모델 증류 검증:**  
우리는 또한 가장 큰 증류 모델이 teacher의 정보를 충분히 포착했는지를 확인하고자 한다.  
이를 위해, **가장 큰 ViT-H+ 모델**을 **7B teacher 모델**과 비교했다.  
Fig. 16(b)에 나타난 바와 같이, **ViT-H+는 파라미터 수가 8배 적음에도 불구하고 7B 모델과 동등한 성능**을 보였다.

이 결과는

1. 우리의 증류 과정이 효과적임을 검증하며,
2. **고품질 teacher**가 존재할 경우 작은 모델도 비슷한 수준의 성능을 학습할 수 있음을 보여준다.

따라서, **초대규모 모델 학습은 더 넓은 커뮤니티에 이익**을 주며, 대규모 모델의 강점을 효율적이고 작은 모델로 증류하여 품질 손실을 거의 없이 전수할 수 있음을 시사한다.

![](/assets/images/posts/592/img_35.png)

**그림 17:**  
DINOv3 ViT 계열 모델의 **다중 해상도에서의 feature 안정성**.

- 위에서 아래로: ViT-S, S+, B, L, H+
- 입력 이미지를 여러 해상도로 추론한 뒤,  
  1792×1024 해상도(=112×64 토큰)에서 계산된 피처에 대해 **주성분 분석(PCA)**을 수행하고,  
  주성분 5–7을 RGB 공간에 투영하여 시각화.

관찰 결과:

- 모든 해상도에서 모델이 작동은 하지만, feature는 특정 해상도 구간에서 안정적으로 유지되다가 이후 점차 drift(변형)되기 시작한다.
- 예: **ViT-S+**는 입력이 896×512 ~ 3584×2048 구간에서 안정적,
- **ViT-L**은 최대 해상도인 7168×4096에서 약간 drift 시작.
- **ViT-H+**는 전체 실험 범위에서 **안정적으로 유지**됨.

? 정리하면, **DINOv3 ViT 계열은 dense vision task에서 압도적 우위, global task에서도 경쟁력 확보, 증류 효과 검증, 해상도 안정성까지 입증**한 결과라고 할 수 있습니다.

### 7.2 자원 제약 환경을 위한 효율적인 ConvNeXt

이 절에서는 **7B teacher 모델**에서 증류한 **ConvNeXt(CNX)** 모델들의 성능을 평가한다.  
ConvNeXt 모델은 **FLOPs 측면에서 매우 효율적**이며, 합성곱 연산에 최적화된 기기에서 배포하기 적합하다.  
또한, 트랜스포머 모델은 **양자화(quantization)에 적합하지 않은 경우가 많지만** (Bondarenko et al., 2021), 합성곱 신경망의 양자화는 이미 잘 연구된 주제이다.

우리는 **CNX-T, S, B, L** 크기의 아키텍처를 증류하였으며(Fig. 16(a) 참고), 이를 원본 **ConvNeXt 모델 (Liu et al., 2022)**과 비교하였다.  
이들 ConvNeXt 베이스라인은 **ImageNet-22k 레이블을 이용한 지도 학습**으로 훈련되었기 때문에, **ImageNet-1k에서 매우 높은 성능**을 내며 강력한 경쟁자가 된다.

실험 설정:

- **글로벌 태스크:** 입력 해상도 256, 512에서 평가
- **세그멘테이션(ADE20k):** 해상도 512
- **깊이 추정(NYU):** 해상도 640

**표 15:**  
증류된 DINOv3 ConvNeXt 모델의 평가.  
ImageNet-22k에서 지도학습된 ConvNeXt (Liu et al., 2022)와 비교.  
글로벌 태스크에서는 입력 해상도 256과 512의 결과를 제시.  
(지도학습 ConvNeXt는 해상도 512에서 성능이 크게 저하됨을 발견).

![](/assets/images/posts/592/img_36.png)

**결과 (Tab. 15):**

- **In-distribution 분류 (ImageNet):**
  - 해상도 256에서는 지도학습 모델이 소폭 더 우수 (예: CNX-T에서 IN-ReAL −0.7).
  - 그러나 해상도 512에서는 반대 현상이 나타남: 지도학습 ConvNeXt는 성능이 크게 저하되는 반면, **우리 모델은 해상도 증가에 따라 스케일링**함.
- **Out-of-distribution 분류 (IN-R, ObjectNet):**
  - 모든 크기에서 두 모델 계열 간 **큰 성능 차이**가 존재 → 이는 **DINOv3 CNX 모델의 강건성(robustness)**을 입증함.
- **Dense task (세그멘테이션 등):**
  - 매우 큰 개선이 나타남.
  - 예: CNX-T는 **+17.9 mIoU** 향상 (42.7 vs 24.8), CNX-L은 **+14.5 mIoU** 향상 (47.8 vs 33.3).

**요약:**  
높은 성능과 계산 효율성을 겸비한 증류 ConvNeXt 모델은 **자원 제약이 중요한 실제 응용 환경에서 특히 유망**하다.

또한, **ViT-7B 모델을 작은 ConvNeXt 모델로 증류한 점**도 매우 흥미롭다.

- ViT-7B는 **CLS 토큰을 사용하는 트랜스포머 블록** 기반,
- ConvNeXt는 **CLS 토큰이 없는 합성곱 연산** 기반.

즉, 두 아키텍처는 근본적으로 다르기 때문에 **지식 전이가 쉽지 않음**에도 불구하고 성공했다는 점은,  
우리 증류 과정의 **범용성과 효과성**을 잘 보여준다.

? 이걸 요약하면 **“DINOv3 CNX는 dense task에서 압도적 향상, 분류에서도 강건성 확보, ViT→ConvNeXt 증류까지 성공한 사례”** 정도로 정리할 수 있겠네요.

### 7.3 DINOv3 기반 **dino.txt**를 활용한 Zero-shot 추론

Sec. 5.3에서 설명한 바와 같이, 우리는 **증류된 DINOv3 ViT-L 모델의 CLS 토큰과 출력 패치들을 텍스트와 정렬**하기 위해 텍스트 인코더를 학습하였다. 이 과정은 Jose et al. (2025)의 **dino.txt** 레시피를 따른다.

우리는 표준 벤치마크에서 **글로벌 및 패치 수준**에서의 정렬 품질을 평가한다.

- **Zero-shot 분류:** CLIP 프로토콜(Radford et al., 2021)을 사용하여 **ImageNet-1k, ImageNet-Adversarial, ImageNet-Rendition, ObjectNet** 벤치마크에서 정확도를 측정.
- **이미지-텍스트 검색:** COCO2017 데이터셋(Tsung-Yi et al., 2017)에서 평가하며, **Recall@1**을 이미지→텍스트(I→T) 및 텍스트→이미지(T→I) 태스크 모두에서 보고.
- **패치 수준 정렬 품질:** open-vocabulary segmentation 태스크에서 **ADE20k, Cityscapes** 벤치마크를 사용, **mIoU**를 측정.

**결과 (표 16):**

- 우리 모델(DINOv3 ViT-L 텍스트 정렬)은 같은 크기 계열 모델들과 비교했을 때 모든 벤치마크에서 **유의미한 향상**을 보임.
- **Jose et al. (2025)**가 DINOv2를 텍스트에 정렬한 것과 비교하면, DINOv3는 **모든 벤치마크에서 크게 더 나은 성능**을 달성.
- **글로벌 정렬 태스크:** 원본 CLIP(Radford et al., 2021) 및 강력한 베이스라인(EVA-02-CLIP, Sun et al., 2023)보다 우수. 하지만 **SigLIP2 (Tschannen et al., 2025), Perception Encoder (Bolya et al., 2025)**에는 약간 뒤처짐.
- **Dense 정렬 태스크:** DINOv3의 **깔끔한 feature map** 덕분에, ADE20K와 Cityscapes 두 가지 어려운 벤치마크에서 매우 우수한 성능을 보임.

**표 16:**  
우리의 텍스트 정렬 DINOv3 ViT-L을 최신 기법과 비교.  
모델은 **Dense alignment 태스크에서 뛰어난 성능**을 달성하면서도, **Global alignment 태스크에서도 경쟁력 유지**.  
모든 비교 모델은 ViT-L 크기이며, 동일한 시퀀스 길이(576)에서 동작함.

![](/assets/images/posts/592/img_37.png)

? 요약하면, **“DINOv3 기반 dino.txt는 CLIP 계열을 능가하는 dense task 성능, global task에서도 준수한 경쟁력”**이라는 포인트네요.

## 8 DINOv3의 지리 공간(Geospatial) 데이터 적용

우리의 자기 지도 학습 레시피는 범용적이며, 어떤 이미지 도메인에도 적용될 수 있다. 이 섹션에서는 DINOv3가 처음 개발된 웹 이미지와는 **객체 질감, 센서 노이즈, 초점 뷰 등 특성이 매우 다른 위성 이미지**에 대해 **DINOv3 7B 모델**을 구축하여 이러한 보편성을 보여준다.

### 8.1 사전 학습 데이터 및 벤치마크

- 우리의 **위성 DINOv3 7B 모델**은 SAT-493M에서 사전 학습되었다.
  - SAT-493M은 **Maxar RGB 직교보정 영상(ortho-rectified imagery)**에서 무작위로 샘플링한 **512×512 크기의 이미지 4억 9천 3백만 장**으로 구성되며, 공간 해상도는 **0.6m**이다.
- 하이퍼파라미터는 웹 DINOv3 7B 모델과 동일하게 사용했으며, 예외적으로 **RGB 평균/표준편차 정규화(위성 특성 반영)**와 **훈련 길이**만 조정했다.
- 학습 파이프라인(웹 모델과 동일 구조):
  1. **100k iteration** 동안 전역 crop (256×256) 기반 사전 학습
  2. **10k iteration** 동안 Gram 정규화 적용
  3. **8k step** 동안 고해상도 파인튜닝(해상도 512)
  4. 학습된 7B 위성 모델은 **보다 사용하기 쉬운 ViT-Large 모델로 증류(distillation)**하여 저비용 환경에서도 활용할 수 있게 함.

### 평가

우리는 DINOv3 위성 모델과 웹 모델을 여러 **지구 관측(Earth Observation)** 태스크에서 평가했다.

#### (1) 글로벌 수관 높이 추정 (Canopy Height Mapping)

- **SatLidar 데이터셋** (Sec. D.13):
  - 100만 장의 512×512 이미지, LiDAR Ground Truth 포함
  - Train/Val/Test = 8:1:1 분할
  - Tolan et al. (2024)에서 사용된 **Neon, São Paulo 데이터셋** 포함
- **국가 규모 수관 높이 추정**: **Open-Canopy (Fogel et al., 2025)**
  - 프랑스 전역 **87,000 km²** 지역
  - SPOT 6-7 위성영상 + 항공 LiDAR 데이터
  - IR(적외선) 채널을 포함한 **4채널 영상** → patch embedding 모듈의 3채널 평균을 구해 4번째 채널 가중치로 추가
  - 입력 이미지를 **1667로 리사이즈 후 512 crop** → Maxar 샘플 해상도와 일치
  - **DPT decoder**를 사용하여 학습
- **표 17**:
  - 다양한 backbone에 대해 고해상도 수관 높이 예측 평가
  - 모든 모델은 DPT decoder로 학습
  - SatLidar (Val/Test, Neon, São Paulo)과 Open-Canopy 실험 결과 비교
  - 메트릭: **MAE(평균 절대 오차), 블록 R² (Tolan et al., 2024)**
  - 비교: Tolan et al. (2024)의 Neon 기반 원래 decoder(∗로 표시)

![](/assets/images/posts/592/img_38.png)

#### (2) 의미적 지리 공간 태스크 (Semantic Geospatial Tasks)

- **GEO-Bench (Lacoste et al., 2023)**
  - 6개 분류 + 6개 분할(segmentation) 태스크
  - 다양한 공간 해상도 및 광학 밴드 포함
  - 태스크 예시:
    - 지붕 태양광 시스템 탐지
    - 지역 기후 구역 분류
    - 산림 파괴 요인 측정
    - 수목 crown 탐지
- **고해상도 의미적 태스크**:
  - 토지 피복 분할 **LoveDA (Wang et al., 2022a)**
  - 객체 분할 **iSAID (Zamir et al., 2019)**
  - 수평 탐지 **DIOR (Li et al., 2020)**

? 요약하면, **DINOv3는 웹 이미지뿐만 아니라 위성/지리 공간 데이터에서도 강력한 성능을 발휘**하며, canopy height 추정과 같은 수치적 회귀 과제부터 다양한 분류·분할 태스크까지 잘 일반화된다는 점을 보여주고 있습니다.

## 8.2 수관 높이 추정 (Canopy Height Estimation)

위성 영상을 활용해 **수관 높이(canopy height)**를 추정하는 것은 매우 어려운 수치적 회귀 과제다. 이는 경사도, 관측 기하 구조, 태양 각도, 대기 산란, 양자화 아티팩트(quantization artifacts) 등 무작위적 요인에도 불구하고 **연속적인 공간 구조를 정확히 복원해야** 하기 때문이다.  
이 과제는 **전 지구 탄소 모니터링**, **산림 및 농업 관리**에 필수적이다 (Harris et al., 2021).

Tolan et al. (2024)은 위성 영상으로 학습한 SSL 백본을 처음으로 이 과제에 적용한 연구다. 이를 따라, 우리는 **SatLidar1M 훈련 세트**에서 **고정된 DINOv3 위에 DPT 헤드**를 학습시킨 후, i.i.d. 샘플로 구성된 **SatLidar1M 검증 세트**, 그리고 **OOD(Out-of-Distribution) 테스트 세트**(SatLidar1M test, Neon, São Paulo)에서 성능을 평가했다. 또한 **Open-Canopy 데이터셋**에서도 학습과 평가를 진행했다.

### 결과 (표 17 참조)

우리는 다양한 SSL 백본을 비교했으며, 여기서

- **“DINOv3 Sat”** = SAT-493M 데이터셋으로 학습된 모델
- **“DINOv3 Web”** = LVD-1689M 데이터셋으로 학습된 모델 (Sec. 3.1 참고)

로 표기한다.

**주요 성과:**

- **DINOv3 위성 모델**은 대부분의 벤치마크에서 **최첨단 성능(SOTA)**을 기록했다.
- 우리의 **7B 위성 모델**은 새로운 SOTA를 달성:
  - SatLidar1M val: MAE **2.4 → 2.2**
  - SatLidar1M test: MAE **3.4 → 3.2**
  - Open-Canopy: MAE **2.42 → 2.02**

이는 **DINOv3 학습 레시피가 범용적이며, 다른 도메인에도 효과적으로 적용될 수 있음**을 보여준다.

흥미롭게도, **증류된 ViT-L 위성 모델**은 7B 모델과 **거의 동등한 성능**을 보였으며, 특히 Neon 테스트 세트에서는 오히려 더 뛰어났다:

- Neon: MAE **2.4** (ViT-L) vs **2.6** (7B 모델) vs **2.9** (Tolan et al., 2024)

또한, 우리의 **DINOv3 7B 웹 모델**도 준수한 성능을 기록했으며,

- SatLidar1M val, Neon, Open-Canopy에서 Tolan et al. (2024)보다 우수했지만
- 위성 모델보다는 뒤처졌다.

? 이 결과는 **센서 특화 사전학습(domain-specific pretraining)**의 중요성을 잘 보여준다. 특히 **센서 특성에 따른 사전 지식(sensor priors)**과 **복사량(radiometric) 일관성**이 중요한 **물리 기반 태스크**(예: 수관 높이 추정)에서는, 웹 데이터가 아닌 도메인 맞춤형 위성 데이터로 학습된 모델이 더 강력함을 확인할 수 있다.

## 8.3 지구 관측(Earth Observation) SOTA와의 비교

**표 18**: Geo-Bench 과제에서 우리의 DINOv3 모델과 강력한 베이스라인인 **DOFA** (Xiong et al., 2024), **Prithvi-v2** (Szwarcman et al., 2024), **Tolan et al. (2024)**의 비교. Prithvi-v2와 DOFA는 모든 가용 광학 밴드를 활용하는 반면, **우리 모델은 RGB 입력만으로도 훨씬 더 우수한 성능**을 달성한다.

![](/assets/images/posts/592/img_39.png)

우리는 **표 18, 표 19**에서 지구 관측 과제에 대한 다양한 방법의 성능을 비교했다.

- **DINOv3 위성 및 웹 모델(백본 고정, frozen)**은 **15개 분류·분할·수평 객체 탐지 과제 중 12개에서 새로운 SOTA를 달성**했다.
- Geo-Bench 결과는 Prithvi-v2 (6개 이상의 밴드를 활용)와 DOFA(센티널-2, Landsat 과제에서 다중 밴드 활용 및 과제 특화 파인튜닝)보다 우수하다 (표 18 참조).
- RGB 입력만을 사용하는 **frozen DINOv3 위성 모델**은 3개의 분류 과제와 6개 분할 과제 중 5개에서 이전 방법을 능가했다.
- **DINOv3 7B 웹 모델** 또한 매우 경쟁력이 있으며, 다수의 Geo-Bench 과제뿐만 아니라 **대규모 고해상도 원격탐사 분할·탐지 벤치마크**에서도 동등하거나 더 강력한 성능을 보였다.
- 실제로 **표 18, 표 19**에 따르면, frozen DINOv3 웹 모델은 Geo-Bench, LoveDA, DIOR 데이터셋에서 새로운 최고 성능을 기록했다.

### 주요 시사점

이러한 발견은 **지리 공간(geospatial) 파운데이션 모델 설계**에 더 넓은 함의를 제공한다. 최근까지는

- 다중 시점 집계(multitemporal aggregation),
- 멀티센서 융합(multisensor fusion),
- 위성 전용 메타데이터 활용

과 같은 휴리스틱 기법이 강조되어 왔다 (Brown et al., 2025; Feng et al., 2025).

그러나 우리의 결과는 **범용 SSL이 객체 경계 정밀도가 중요한 과제(분할, 객체 탐지)에서도 위성 특화 접근법을 능가할 수 있음**을 보여준다. 이는 **도메인 비특화 사전학습(domain-agnostic pretraining)이 특화된 다운스트림 도메인에서도 강력한 일반화 성능을 제공한다**는 최근 연구 결과와도 일치한다 (Lahrichi et al., 2025).

### 결론

- **DINOv3 위성 모델**은 위성 특화 사전지식(priors)을 활용해 깊이 추정(depth estimation)과 같은 **수치적 회귀(metric) 과제**에서 우수하다.
- 반면, **DINOv3 웹 모델**은 다양하고 보편적인 표현을 통해 의미적 지리 공간 과제에서 새로운 SOTA를 기록한다.
- 두 모델의 상호보완적 강점은 **DINOv3 SSL 패러다임의 폭넓은 적용 가능성과 효과성**을 잘 보여준다.

![](/assets/images/posts/592/img_40.png)

**그림 18**: 단일 DINOv3 모델로 가능해진 원격탐사 응용 예시. DINOv3 특징 기반 PCA는 DINOv2보다 더 정밀한 세부 정보를 보여준다. 분할 맵은 GEO-Bench Chesapeake 라벨만으로 계산되었다. 수관 높이 모델 디코더는 Open-Canopy 데이터셋(4채널: RGB+IR)에서 학습했으나, 추론은 RGB 채널만으로 수행했다.

![](/assets/images/posts/592/img_41.png)

**표 19**: DINOv3를 Prithvi-v2 (Szwarcman et al., 2024), BillionFM (Cha et al., 2024), SkySense V2 (Zhang et al., 2025)와 비교.

- Segmentation (LoveDA, iSAID): mIoU 보고
- Detection (DIOR): mAP 보고

![](/assets/images/posts/592/img_42.png)

**그림 19**: Open Canopy 데이터셋에서 **DINOv3 7B 위성 모델**과 **Tolan et al. (2024)**의 정성적 비교. 두 모델 모두 448×448 입력 이미지로 디코더 학습. 결과적으로, DINOv3는 필드 내 나무의 정확한 높이와 같은 더 정밀한 지도를 생성함을 확인할 수 있다.

## 9 환경적 영향

**표 20**: 모델 학습의 탄소 발자국. 전체 모델 사전학습을 재현할 경우의 잠재적 탄소 배출량을 보고하며, 이는 **PUE 1.1**과 **탄소 집약도 계수 0.385 kg CO2eq/KWh**를 기준으로 계산하였다.

![](/assets/images/posts/592/img_43.png)

우리의 사전학습 과정에서의 탄소 배출량을 추정하기 위해, 자연어처리(Strubell et al., 2019; Touvron et al., 2023) 및 SSL(Oquab et al., 2024) 분야의 기존 연구에서 사용된 방법론을 따른다.

- **외생 변수**(전력 사용 효율 PUE, 전력망의 탄소 집약도)는 Touvron et al. (2023)에서 사용한 값과 동일하게 고정한다.
  - PUE: **1.1**
  - 미국 평균 탄소 집약도: **0.385 kg CO2eq/KWh**
- GPU 전력 소비량은 **열 설계 전력(TDP)**을 기준으로 한다.
  - A100 GPU: **400W**
  - H100 GPU: **700W**

우리의 **ViT-7B** 사전학습에 대한 계산 세부 내역은 **표 20**에 보고했다. 참고를 위해 **DINOv2**와 **MetaCLIP**의 유사 데이터도 함께 제공한다.

또한 비교 지점으로, **하나의 DINOv3 모델을 학습하는 데 필요한 에너지(47MWh)**는 **평균 전기차로 약 240,000km 주행하는 데 필요한 에너지**와 대략 동일하다.

### 프로젝트 전체의 탄소 발자국

프로젝트 전체의 탄소 발자국을 계산하기 위해, 총 **9M GPU 시간**을 사용했다고 가정한다.

- 위와 동일한 전력망 파라미터를 사용했을 때, 총 배출량은 약 **2600 tCO2eq**로 추정된다.
- 비교를 위해, 파리–뉴욕 간 보잉 777 왕복 비행의 탄소 배출량은 약 **560 tCO2eq**이다.
- 하루에 이 노선이 **12편 왕복**한다고 가정하면, 본 프로젝트의 환경적 영향은 **해당 노선 하루 운항량의 절반**에 해당한다.

이 추정치는 GPU 구동을 위한 전력만 고려한 것이며, 냉각, 제조, 폐기 과정에서 발생하는 다른 배출은 포함하지 않는다.

## 10 결론

**DINOv3**는 자기지도학습(Self-Supervised Learning) 분야에서 중요한 진전을 이루었으며, 다양한 도메인에서 시각적 표현 학습 방식을 혁신할 잠재력을 보여준다.

- 데이터셋과 모델 크기를 신중한 데이터 준비, 설계, 최적화를 통해 확장함으로써, DINOv3는 **수작업 레이블에 대한 의존성을 제거**할 수 있는 자기지도학습의 강점을 입증하였다.
- **Gram Anchoring 방법**을 도입하여, 장기 학습 시 조밀한 특징 맵(dense feature maps)이 열화되는 문제를 효과적으로 완화하고, 견고하고 신뢰할 수 있는 성능을 보장했다.
- 또한 고해상도 사후 학습(post-training) 및 지식 증류(distillation)와 같은 사후(polishing) 전략을 통해, 이미지 인코더의 추가 파인튜닝 없이도 광범위한 시각 과제에서 SOTA 성능을 달성하였다.

**DINOv3 비전 모델군**은 새로운 벤치마크를 세웠을 뿐 아니라, 다양한 **리소스 제약, 배포 시나리오, 응용 사례**에서 범용적인 솔루션을 제공한다.

DINOv3의 성과는 자기지도학습이 컴퓨터 비전 및 그 너머의 연구 분야에서 SOTA를 진전시키는 데 있어 얼마나 큰 가능성을 지니고 있는지를 잘 보여준다.
