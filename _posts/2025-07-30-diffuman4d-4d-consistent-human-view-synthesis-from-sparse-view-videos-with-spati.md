---
title: "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models"
date: 2025-07-30 17:36:37
categories:
  - 인공지능
tags:
  - Diffuman4D
---

<https://arxiv.org/abs/2507.13344>

[Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models](https://arxiv.org/abs/2507.13344)

**초록 (Abstract)**

본 논문은 **희소 시점(sparse-view) 비디오**를 입력으로 하는 인간 영상의 **고품질(view synthesis) 시점 합성** 문제를 다룬다. 기존 방법들은 관찰 정보가 부족한 문제를 해결하기 위해 **4D 확산 모델(4D diffusion model)**을 활용하여 새로운 시점에서의 영상을 생성해 왔다. 그러나 이러한 모델에서 생성된 비디오는 **시공간(spatio-temporal) 일관성**이 부족하여 시점 합성 품질이 저하되는 문제가 있다.

이를 해결하기 위해, 본 논문에서는 **새로운 슬라이딩 반복(Iterative Sliding) 기반의 디노이징(Denoising) 프로세스**를 제안하여 4D 확산 모델의 시공간 일관성을 강화한다. 구체적으로, 특정 시점과 타임스탬프(timestamp)에 해당하는 이미지, 카메라 포즈, 인체 포즈를 각각 잠재(latent)로 인코딩한 **잠재 그리드(latent grid)**를 정의한 후, 슬라이딩 윈도우(sliding window)를 사용하여 공간(spatial) 및 시간(temporal) 차원에서 교차적으로 반복 디노이징을 수행한다. 마지막으로, 디노이징된 잠재로부터 목표 시점의 영상을 디코딩하여 최종 비디오를 생성한다.

이러한 반복 슬라이딩 과정을 통해 잠재 그리드 내 정보가 충분히 흐르며, 확산 모델이 **넓은 수용 영역(receptive field)**을 확보할 수 있어 4D 일관성이 크게 향상된다. 동시에 GPU 메모리 소모를 실용적으로 유지할 수 있다. **DNA-Rendering** 및 **ActorsHQ** 데이터셋 실험 결과, 제안한 방법은 기존 접근법 대비 높은 품질과 일관성을 가진 새로운 시점의 비디오를 합성할 수 있음을 입증하였다.

자세한 인터랙티브 데모 및 비디오 결과는 프로젝트 페이지에서 확인할 수 있다: <https://diffuman4d.github.io/>

[Diffuman4D](https://diffuman4d.github.io/)

![](/assets/images/posts/588/img.png)

**Figure 1 설명:** Diffuman4D는 희소 시점 비디오로부터 사람의 동작을 고품질로 자유 시점 렌더링할 수 있다. 하단 행은 대표적인 결과를 보여준다.

**1. 서론 (Introduction)**

본 논문은 **희소 시점(sparse-view) 비디오**로부터 움직이는 인간의 **고품질 4D 시점 합성(view synthesis)** 문제를 다루며, 이는 증강현실, 영화 제작, 스포츠 중계 등 다양한 분야에서 응용될 수 있다. 기존의 전통적 **다중 시점 스테레오 기반 방법(multi-view stereo)** [53, 54, 17]과 최신 **신경 렌더링(neural rendering)** 기반 방법 [76, 67, 72]은 고품질 재구성을 위해 다수의 동기화된 카메라 배열을 필요로 하며, 이는 실제 환경에 적용하기 어렵다. 입력 시점이 희소해질수록 관찰 정보 부족으로 인해 재구성 문제가 **불량정의(ill-posed)** 되어 성능이 저하된다.

이 문제의 직관적 해결책은 **조건부 이미지/비디오 생성 모델(conditional generative models)**을 활용하여 입력 시점에 조건을 부여한 새로운 시점의 비디오를 생성하는 것이다 [68, 69, 65]. 주목(attention) 메커니즘을 활용해 이러한 방법들은 공간적 및 시간적 제어 신호를 비디오 생성 모델에 주입하여 목표 시점과 타임스탬프에서 인간 영상을 생성하고자 한다. 그러나 이러한 방법은 특히 인간의 **토폴로지(topology)** 및 **의복 변형(cloth deformation)**이 복잡할 때, 생성된 영상의 **시공간 일관성(spatio-temporal consistency)**을 유지하는 데 어려움을 겪는다. 핵심 원인은 GPU 메모리 제약으로 인해 목표 영상을 여러 패스로 나눠 생성해야 하고, 생성 모델의 확률적 특성으로 인해 출력 간 분산이 발생하기 때문이다.

본 논문에서는 **4D 시공간 일관성을 갖춘 멀티뷰 인간 비디오**를 생성하기 위한 새로운 **시공간 확산 모델(spatio-temporal diffusion model)**을 제안한다. 핵심 혁신점은 **슬라이딩 반복 디노이징(sliding iterative denoising)** 프로세스로, 모델 출력의 4D 일관성을 보장한다. 구체적으로, 희소 시점 비디오가 주어졌을 때 이미지 관측값과 카메라 파라미터를 조건부 잠재(latent)로 인코딩하고, 목표 시점과 타임스탬프에 해당하는 **노이즈 잠재(latent noise)**를 정의하여 **4D 잠재 그리드(latent grid)**를 형성한다. 디노이징 과정에서 우리는 **공간 및 시간 차원으로 슬라이딩하는 윈도우**를 전후로 교대로 이동시키며 반복적으로 디노이징한다. 이전 방법 [68]은 각 슬라이딩 단계에서 전체 디노이징 과정을 실행했지만, 우리 모델은 슬라이딩 중 일부 단계만 수행하여 효율성을 높였다. 이 반복 슬라이딩 전략은 잠재 그리드 전반에 걸쳐 정보가 충분히 전파되도록 하여, 모델이 주변 4D 신호를 활용해 각 목표 출력을 생성하도록 하고, 시공간 거리(spatio-temporal distance)에 따라 그 영향력을 동적으로 조정할 수 있게 한다.

모델의 일관성을 추가로 강화하기 위해, 우리는 **3D 인체 골격 시퀀스(3D human skeleton sequence)**를 구조적 사전 정보(structural prior)로 활용한다. 구체적으로, 주어진 희소 시점 비디오로부터 사전 학습된 3D 포즈 추정기를 통해 3D 골격 시퀀스를 추출한 뒤, 각 시점과 타임스탬프에서 이 골격을 이미지 공간으로 투영하여 **카메라 파라미터와 이미지 관측값과 함께 조건 신호(conditioning signal)**로 사용한다. 최종적으로, 디노이징된 잠재로부터 목표 시점의 비디오를 디코딩하고, 입력 시점 및 합성된 새로운 시점 비디오를 기반으로 **고품질 4D Gaussian Splatting(4DGS)** [73, 76, 67]을 재구성한다.

모델 학습을 위해, 우리는 **DNA-Rendering [10] 데이터셋**을 정교하게 전처리하여 카메라 파라미터 재보정, 이미지 색 보정 행렬(CCM) 최적화, 전경 마스크 추정, 인체 골격 추정을 수행하였다. DNA-Rendering 및 ActorsHQ 데이터셋에서 최첨단(state-of-the-art) 방법들과 비교 실험을 수행한 결과, 제안한 프레임워크가 희소 시점 입력에서도 정교한 인간의 동작과 외형을 포착하는 데 뛰어난 성능을 보임을 확인하였다.

### 본 논문의 주요 기여점은 다음과 같다:

- **Diffuman4D 제안**: 희소 시점 비디오 입력으로부터 시공간적으로 일관되고 고해상도(1024p)의 인간 비디오를 생성하는 새로운 확산 모델 제안.
- **슬라이딩 반복 디노이징 메커니즘**: 장기 비디오 생성 시 공간적, 시간적 일관성을 동시에 강화하며 효율적인 추론 가능.
- **인체 포즈 기반 조건부 설계**: 외형 품질 및 동작 정확도를 향상시키기 위한 인체 포즈 기반 조건화 설계.
- **DNA-Rendering 데이터셋 가공본 공개 계획**: 향후 연구에 기여할 수 있도록 가공된 DNA-Rendering 데이터셋 공개 예정.

### **2. 관련 연구 (Related Work)**

### 4D 재구성: 밀집 시점(dense views) 기반

비디오로부터 동적 3D 인간 퍼포먼스를 재구성하고 이를 기반으로 새로운 시점 합성을 통해 몰입형 재생(immersive playback)을 구현하는 것은 컴퓨터 비전과 그래픽스 분야에서 오랜 연구 주제였다. 전통적 방법은 **밀집 카메라 배열(dense camera arrays)** [60, 11, 25, 19, 59]이나 **깊이 센서(depth sensors)** [2, 56, 61, 5, 43]와 같은 복잡한 하드웨어를 활용해 고품질 인간 퍼포먼스를 재구성했다.

최근에는 **신경 장면 표현(neural scene representation)**, 특히 **NeRF (Neural Radiance Field)** [42]와 **3D Gaussian Splatting (3DGS)** [26]이 정적 3D 장면 재구성에서 뛰어난 성과를 보였다. [32, 63, 16, 7, 55, 76, 14, 73, 72]는 이러한 3D 표현(NeRF 또는 3DGS)에 **시간 차원(temporal dimension)**을 추가하여 4D로 확장, 동적 장면의 시간 변화를 모델링하는 방법을 제안했다. 그러나 이들 방법은 여전히 **촘촘하고 잘 동기화된 다중 시점 비디오 입력**에 크게 의존하며, 시점이 희소해질 경우 심각한 **오버피팅(overfitting)** 문제가 발생해 활용성이 크게 제한된다.

### 4D 재구성: 희소 시점(sparse views) 기반

밀집 다중 시점 입력 요구를 완화하기 위해, 일부 연구 [48, 66]는 **SMPL [40]과 같은 인간 사전 지식(human priors)**을 활용하여 재구성 과정을 보조한다. [47, 22, 71]은 **정적 3D 캐노니컬 공간(canonical space)**을 신경 필드로 구성한 뒤, SMPL 기반의 **변형 필드(deformation field)** [50, 15, 45, 46, 33]를 학습하여 동적 요소를 캐노니컬 공간으로 매핑하는 방법을 제안했다. 그러나 이러한 접근법은 복잡한 의상이나 빠른 움직임에서 **형태 변형(shape deformation)**을 정확히 추정하기 어렵다는 한계를 가진다.

또한, [34, 9, 8, 38, 85]와 같은 방법은 **스테레오(stereo)** [36] 또는 **다중 시점(multi-view) 깊이 추정(depth estimation)** [77, 78]과 같은 **깊이 사전 지식(depth priors)**을 활용해 일반화 가능한 장면 재구성과 새로운 시점 합성을 시도한다. 하지만 이러한 방법은 깊이 추정의 정확도에 크게 의존하므로 **가림(occlusion)**, **무질감(textureless) 영역**, **극도로 희소한 시점** 상황에서 성능이 저하된다.

### 4D 생성(4D generation)

최근의 **3D 콘텐츠 생성(3D content generation)** [49, 57, 64, 20, 37, 39, 18] 및 **비디오 확산(video diffusion) 기술** [6, 21, 75, 29]의 발전은 위에서 언급한 어려운 시나리오를 해결하기 위해 **생성적 데이터 사전(generative data priors)**을 재구성 파이프라인에 도입하는 새로운 방향을 제시한다.

[58, 3, 4, 51, 79, 83, 86, 81]은 **Score Distillation Sampling (SDS) [49]**을 활용해 이미지나 비디오 확산 모델로부터 4D 표현을 추출한다. 그러나 SDS는 **높은 연산 비용**으로 인해 대규모 4D 재구성 작업에 확장성이 떨어지고, **과도하게 부드러운 기하(geometry)와 비현실적 텍스처**를 생성하는 문제가 있다.

이러한 한계를 극복하기 위해 [31, 44, 74, 80, 69]는 확산 모델에 **시공간 일관성(spatio-temporal consistency)**을 갖춘 멀티뷰 비디오 생성을 조건화하여 4D 재구성에 활용하는 방법을 제안했다. 그러나 이들 접근법은 **객체 수준(object-level)** 생성에 국한되었다.

최근의 **CAT4D [68]**는 **시간 임베딩(time embedding)**과 **Plücker 임베딩 [24]**을 활용해 시공간 일관성을 확보한 **멀티뷰 일관성 비디오 확산 모델**을 제안하며, 일반 장면(general scenes)으로까지 확장했다. 하지만 CAT4D 역시 **머리카락, 옷 등 연질 구조의 움직임으로 인한 형태 왜곡(shape distortion)과 자체 가림(self-occlusion)** 문제로 인해 복잡한 인간 생성을 다루기 어렵고, 일반 조건만으로는 이러한 **불량정의(ill-posed)** 문제를 해결하기 힘들다.

이에 본 논문에서는 이러한 문제를 극복하기 위해 **인간 특화(human-specific) 사전 지식(priors)**을 추가 도입하는 새로운 접근을 제안한다.

## **3. 방법 (Method)**

본 연구에서는 **희소 시점 비디오(sparse-view videos)**로부터 인간 퍼포먼스를 재구성하는 과정을 두 단계로 수행한다.

1. **시공간 확산 모델(spatio-temporal diffusion model)**을 사용하여 입력된 희소 시점 비디오를 **조밀한 다중 시점 비디오(dense multi-view videos)**로 변환한다.
2. 이렇게 생성된 다중 시점 비디오를 활용해 **4D Gaussian Splatting (4DGS)** 최적화를 통해 인간 퍼포먼스를 재구성한다.

우선 3.1절에서 **시공간 확산 모델**의 구조를 설명하고, 3.2절에서는 **시공간 일관성이 유지된 다중 시점 비디오 생성**을 위한 디노이징 메커니즘을 다룬다. 이어서 3.3절에서는 **골격 조건화(skeleton conditioning) 방식**을, 3.4절에서는 이를 기반으로 인간 퍼포먼스를 재구성하는 방법을 설명한다.

### **3.1 시공간 확산 모델 (Spatio-Temporal Diffusion Model)**

### 파이프라인 (Pipeline)

그림 2에서 보이듯이, 제안하는 **시공간 확산 모델(spatio-temporal diffusion model)**은 **M개의 입력 시점 비디오**를 받아 **N개의 목표 시점 비디오**를 생성하며, 모든 비디오는 **T 프레임**으로 구성된다.

먼저, 입력 비디오는 **사전 학습된 VAE(Variational Autoencoder)**를 사용해 **잠재 공간(latent space)**으로 인코딩되며, 목표 시점 비디오는 **노이즈 잠재(noise latent)**로 초기화된다. 이 잠재들은 **(N+M) × T** 크기의 **샘플 그리드(sample grid)**로 구성되며, 그리드의 두 축은 각각 **공간적(다중 시점)** 차원과 **시간적(비디오)** 차원을 나타낸다.

그리드의 각 샘플은 입력 이미지 잠재(혹은 목표 노이즈 잠재)와 해당 샘플의 **조건부 임베딩(conditioning embeddings)**으로 구성된다. 이 임베딩은 **골격 잠재(skeleton latent)**와 **Plücker 좌표**(카메라 파라미터 표현)로 이루어진다(자세한 내용은 3.3절 참고). 이후 모델은 **슬라이딩 반복 디노이징(sliding iterative denoising)** 접근법을 통해 샘플 그리드를 점진적으로 디노이징한다(3.2절 참고).

디노이징이 완료되면 목표 이미지 잠재는 디코딩되어 최종 목표 시점 비디오로 변환된다. 마지막으로, 입력 시점 비디오와 생성된 목표 시점 비디오를 함께 활용하여 **고품질 4D Gaussian Splatting (4DGS)** 표현을 재구성하고, 이를 통해 **실시간 시점 렌더링**이 가능해진다.

### 아키텍처 (Architecture)

우리 모델은 **다중 시점 잠재 확산 모델(multi-view latent diffusion models)** [18, 57]의 아키텍처를 따른다. 구체적으로, 모델은 **3D self-attention 레이어**를 사용하여 시점 간 정보 교환을 수행한다. 입력 이미지는 사전 학습된 VAE를 통해 잠재 표현으로 인코딩되어 모델이 잠재 공간에서 **공동 분포(joint distribution)**를 학습할 수 있도록 한다. 텍스트 조건부 입력은 비활성화하며, 입력 프롬프트는 빈 문자열로 설정한다.

![](/assets/images/posts/588/img_1.png)

**그림 2 설명:**  
Diffuman4D의 개요. 모델은 M개의 입력 시점 비디오를 받아 N개의 목표 시점 비디오를 생성하고, 입력 및 생성된 비디오를 활용해 인간 퍼포먼스의 4DGS를 재구성한다. 구체적으로, 입력 시점 비디오는 사전 학습된 VAE를 통해 잠재 공간으로 인코딩된다. 3D 인간 골격 시퀀스는 각 시점에 투영되어 RGB 맵으로 렌더링되며, 동일한 잠재 공간으로 인코딩된다. 추가로, 카메라 파라미터는 **Plücker 좌표 [82]**로 인코딩된다. 이렇게 얻은 골격 잠재와 Plücker 좌표는 입력 시점의 이미지 잠재 또는 목표 시점의 노이즈 잠재와 **병합(concatenate)**되어 각각 입력 샘플과 목표 샘플을 형성한다. 모든 시점과 시간축에서의 샘플들은 하나의 **샘플 그리드**를 이루며, 이는 **시공간 확산 모델**에 입력되어 **슬라이딩 반복 디노이징 메커니즘**(3.2절 참고)을 통해 디노이징된다. 마지막으로, 목표 시점에 대한 디노이징된 이미지 잠재는 **사전 학습된 VAE 디코더**로 복원되어 목표 시점 비디오로 변환되며, 오프더셸프(Off-the-shelf) 방식 [73]을 통해 4DGS를 재구성하여 실시간 새로운 시점 렌더링이 가능해진다.

### **3.2 슬라이딩 반복 디노이징 (Sliding Iterative Denoising)**

### 문제 배경

고품질 4D 인간 재구성을 달성하기 위해서는 입력 데이터가 **공간적(다중 시점)** 및 **시간적(비디오)** 차원에서 충분히 조밀해야 한다. 예를 들어, 10초 분량의 4D 인간 퍼포먼스를 재구성하려면 **수만 장의 입력 이미지**가 필요하다. 그러나 **GPU 메모리 제약**으로 인해 기존 비디오 확산 모델은 한 번의 추론에서 소수의 이미지 그룹만 디노이징할 수 있으며, 전체 시퀀스를 **수백 개의 그룹**으로 나눠 처리해야 한다. 이 과정에서 디노이징 반복이 **서로 독립적으로 수행**되므로, 확산 모델의 확률적 특성 때문에 **출력 이미지 간 불일치(inconsistency)**가 발생한다.

최근 연구 [68]는 이를 완화하기 위해 **슬라이딩 윈도우(sliding window)** 전략을 도입했다. 각 슬라이딩 윈도우 내에서 **완전한 디노이징**을 수행한 후, 겹치는 영역에 대해 **중간값 필터링(median filtering)**을 적용하여 변동성을 줄이는 방식이다. 하지만 이 방법은 긴 시퀀스를 생성할 때 여전히 일관성 문제가 남고, **여러 번의 디노이징 반복**으로 인해 **추론 시간이 크게 증가**하는 한계를 가진다(Fig. 5 참조).

### 슬라이딩 반복 디노이징 (Sliding Iterative Denoising)

위 문제를 해결하기 위해, 본 연구에서는 디노이징 과정에서 풍부한 **문맥 정보(context information)**를 활용해 긴 시퀀스의 일관성을 강화하는 **슬라이딩 반복 디노이징 메커니즘**을 제안한다.

구체적으로, 목표 샘플 시퀀스가 주어졌을 때 길이 **W**의 컨텍스트 윈도우를 정의하고, 이를 **고정된 스트라이드(stride) S**로 시퀀스를 따라 슬라이딩시킨다. 각 반복에서, **목표 샘플과 입력 샘플을 결합(concatenate)**하여 확산 모델에 입력하고 **P 단계**의 디노이징을 수행한다.

Fig. 3(a)와 같이, 인간 중심 시퀀스(예: 카메라 시점이 원형으로 배열된 경우)에 대해 먼저 **반시계 방향(counter-clockwise)**으로 컨텍스트 윈도우를 슬라이딩하여 정보를 전파하고, 이후 **시계 방향(clockwise)**으로 방향을 반전시켜 양방향 문맥을 통합한다. 이 과정을 통해 각 샘플은 총 **D = 2 × P × W / S 단계**의 디노이징을 거치며, 이를 전체 확산 추론 단계로 설정하여 최종 생성을 완료한다.

동일한 연산을 **시간 축 시퀀스**에도 적용할 수 있어, 각 샘플이 과거와 미래 문맥을 모두 활용할 수 있다. 또한, 여러 시퀀스는 **멀티 GPU 환경에서 병렬 디노이징**이 가능하다. 이러한 메커니즘을 통해 모델은 공간적 및 시간적 차원에서 목표 샘플의 일관성을 조화롭게 유지하며, 고품질 4D 이미지 그리드를 생성할 수 있다.

### 교차 디노이징 (Alternating Denoising)

이전 연구 [68]를 따라, 우리는 공간적·시간적 일관성을 더욱 향상시키기 위해 **교차 디노이징(alternating denoising)** 전략을 도입했다(Fig. 3(b) 참조).

- **공간 디노이징 단계:**  
  M-뷰, T-프레임 비디오가 주어졌을 때, 먼저 대응 시간의 **M개의 입력 시점**을 조건으로 하여 목표 샘플을 **D/2 단계** 동안 공간 차원에서 디노이징한다.
- **시간 디노이징 단계:**  
  이후 가장 가까운 시점(view)에서 동일 시간 범위 내 **W 프레임**을 조건으로 하여 나머지 **D/2 단계** 동안 시간 차원에서 디노이징을 수행한다.

이 전략을 슬라이딩 반복 디노이징과 결합하면, 각 샘플은 인접 시점으로부터의 **공간 정보**와 인접 프레임으로부터의 **시간 정보**를 슬라이딩 윈도우를 통해 통합적으로 수용하게 된다. 또한, 각 목표 샘플은 **로컬 중심(local center)** 역할을 하며, 중심에 가까운 샘플일수록 더 많은 공동 디노이징 단계를 거치게 된다. 이는 **4D 데이터의 특성**—근접 샘플일수록 상관성이 높아 더 강한 일관성 제약이 필요하다는 점—에 부합한다.

![](/assets/images/posts/588/img_2.png)

**그림 3 설명:**  
(a) **슬라이딩 반복 디노이징 메커니즘:** 원형으로 배열된 시점 시퀀스에서 길이 W(예: W=3)의 컨텍스트 윈도우를 초기화한 후, 스트라이드 S(예: S=1)로 반시계 방향으로 슬라이딩하며 각 반복마다 P 단계 디노이징을 수행한다. 한 바퀴 완료 후 방향을 시계 방향으로 반전시켜 한 바퀴 더 슬라이딩하며 최종적으로 D = 2 × P × W / S 단계의 디노이징을 거친다.

![](/assets/images/posts/588/img_3.png)

(b) **시공간 디노이징 과정:** M-뷰, N-프레임 비디오(M=2, N=5)와 D 단계 추론이 주어졌을 때, 먼저 공간 차원에서 D/2 단계 디노이징을 수행해 공간 일관성을 확보하고, 이후 시간 차원에서 D/2 단계 디노이징을 수행해 최종적으로 시공간 일관성을 확보한다. 슬라이딩 반복 디노이징은 두 차원 모두에 적용되며, 각 행 또는 열은 병렬로 디노이징 가능하다.

### **3.3 골격 기반 확산(Skeleton-Conditioned Diffusion)**

### 기존 방법의 한계

이전의 4D 생성 기법들 [68, 65]은 **일반적인 장면(general scenes)**을 대상으로, **공간 신호(camera embeddings)**와 **시간 신호(timestamp embeddings)**를 직접 확산 모델에 주입하는 방식으로 설계되었다. 그러나 이러한 방법들은 **인간 이미지를 시공간적으로 일관되게 생성**하는 데 다음과 같은 문제를 겪는다.

1. **포즈 오차(pose error):**  
   Plücker 좌표는 픽셀 단위의 카메라 포즈 신호를 제공하지만, 생성된 이미지에는 입력 카메라 기준에서 눈에 띄는 포즈 오차가 발생하는 경우가 많다.
2. **형태 변형 및 자체 가림(self-occlusion):**  
   인간의 움직임은 종종 머리카락이나 헐렁한 옷과 같은 **크고 복잡한 변형(deformation)**을 동반하므로, 확산 모델은 안정적인 생성 과정을 유지하기 어렵다.

### 골격 기반 조건화(Skeleton Conditioning)

이러한 문제를 해결하기 위해, 우리는 **인간 특화(human-specific) 조건 신호**를 추가하여 모델의 생성 공간을 제약하고, 보다 정확하고 일관된 인간 이미지 합성을 달성한다.

인간 데이터는 **의상, 체형, 성별 등 매우 다양한 특성**을 가지므로, 모델에 정확한 조건 신호를 제공하기 위해 **중간 표현(intermediate representation)**이 필요하다. 이에 가장 적합한 표현은 **3D 인간 골격 시퀀스(3D human skeleton sequence)**이며, 이는 다음 장점을 가진다.

- **희소 시점 비디오(sparse-view videos)**에서도 손쉽게 추출 가능하다.
- 시간과 공간 전반에서 **일관된 4D 구조**를 제공한다.
- 각 시점과 타임스탬프에 투영하여 **조건 신호의 정밀도**를 높일 수 있다.

구체적으로, 우리는 **Sapiens [27]**을 사용해 **2D 인간 골격**을 추정한 뒤, 이를 **삼각측량(triangulation)**하여 **3D 골격 시퀀스**를 복원한다. 이후 이 골격을 각 시점(view)에 투영하고 **RGB 맵(RGB map)**으로 렌더링하며, **신체 부위별로 다른 색상**을 할당해 조건 정보를 풍부하게 한다. 이 RGB 맵은 **사전 학습된 VAE**를 통해 잠재 공간(latent space)으로 인코딩되며, **픽셀 정렬 특징(pixel-aligned feature)**로 작동하여 복잡한 인간 포즈에서도 생성 품질을 크게 향상시킨다.

### Skeleton-Plücker 혼합 조건화 (Mixed Conditioning)

그러나, 복잡한 의상을 착용한 사람의 경우 **골격 예측이 불완전**할 수 있어 포즈 제어 신호의 완전성이 저하될 수 있다. 또한, 골격 표현은 **명시적인 가림(occlusion) 정보**를 포함하지 않아 **앞뒤(front-back) 대칭성**으로 인한 모호성을 내포한다(Fig. 4 참고).

이 한계를 완화하기 위해, 우리는 여전히 **Plücker 좌표 조건화**를 유지하여 명시적인 **카메라 포즈 정보**를 제공함으로써 생성 과정의 **강건성(robustness)**을 높인다.

**3.4 4DGS 재구성 (4DGS Reconstruction)**

위에서 제안한 접근 방식을 통해, 우리 모델은 **시공간적으로 일관된 조밀 시점 비디오(dense-view videos)**를 생성할 수 있다. 이렇게 생성된 비디오는 기존의 **4D 재구성 파이프라인(4D reconstruction pipeline)**에 입력되어 **4D 인간 표현(4D human representation)**을 얻을 수 있다.

실험에서는 **LongVolcap [73, 70]**을 재구성 방법으로 사용하였다. LongVolcap은 **4D Gaussian Splatting (4DGS)**의 확장 버전으로, 시간적으로 계층화된 Gaussian 표현(temporally hierarchical Gaussian representation)을 통해 긴 볼류메트릭 비디오(long volumetric video)를 효과적으로 재구성할 수 있다.

## **4. 실험 (Experiments)**

### 4.1 구현 세부 사항 (Implementation Details)

우리 모델은 **공간 샘플 시퀀스(spatial sample sequence)** 또는 **시간 샘플 시퀀스(temporal sample sequence)** 중 하나를 대상으로 학습되며, 각 시퀀스의 총 길이는 **M + N = 16** 으로 설정된다.

- **공간 샘플 시퀀스 (Spatial Sequence):**  
  모든 샘플은 서로 다른 카메라에서 **동시에 촬영**되며, 모델은 **M = 4** 개의 조건 샘플을 기반으로 **N = 12** 개의 목표 샘플을 생성하도록 학습된다.
- **시간 샘플 시퀀스 (Temporal Sequence):**  
  목표 카메라 **C\_T**에서의 **N = 8** 연속 샘플을 생성하며, 같은 시간 범위에서 무작위로 선택된 참조 카메라 **C\_R**의 **M = 8** 샘플을 조건으로 사용한다.

학습 시, 위 두 시퀀스 각각을 **50% 확률**로 선택하여 균등하게 학습한다. 공간 및 시간 학습 시퀀스는 전체 공간(시간) 후보군에서 무작위로 샘플링된다.

또한, **classifier-free guidance**를 적용하기 위해 학습 중 **10% 확률**로 모든 조건(이미지 잠재, 골격 잠재, Plücker 좌표)을 무작위로 제거(drop)한다.

자세한 구현 세부 사항은 **부록(supplementary materials)**을 참고한다.

**표 1:** DNA-Rendering [10] 및 ActorsHQ [23] 데이터셋에서의 정량적 비교. Diffuman4D는 다양한 설정과 지표에서 기준 모델(baseline)들을 능가한다. 참고로, CAT4D†는 저자들이 재현한 버전이다.

![](/assets/images/posts/588/img_4.png)

**표 2:** 디노이징 전략 간의 정량적 비교.

![](/assets/images/posts/588/img_5.png)

**표 3:** 조건화 스킴(Conditioning Scheme) 간의 정량적 비교.

![](/assets/images/posts/588/img_6.png)

**4.2 데이터셋 및 베이스라인 (Datasets and Baselines)**

### 데이터셋 (Datasets)

우리 모델은 **DNA-Rendering [10]** 데이터셋에서 학습된다. 이 데이터셋은 다양한 의상과 동적인 동작을 포함한 인간 퍼포먼스의 **2,000개 이상의 시퀀스**로 구성되어 있다. 학습을 위해, 우리는 **복잡한 오브젝트와 상호작용하는 배우를 필터링**한 후 **1,000개 시퀀스**를 선택하며, 각 시퀀스는 **48 시점(view)**과 **시점당 225 프레임**으로 구성되어 총 **1천만 장의 이미지**를 포함한다.

정량적 비교를 위해 테스트 세트에서 다양한 의상 유형과 동작 카테고리를 포함한 **16개 시퀀스**를 사용한다. 추가적으로, **ActorsHQ [23]** 데이터셋의 **12개 인간 퍼포먼스 시퀀스**에서 모델을 평가하여 **제로샷 일반화(zero-shot generalization)** 성능을 측정한다.

### 베이스라인 (Baselines)

우리는 제안한 방법을 다음의 최첨단(state-of-the-art) 기법들과 비교한다.

- **최적화 기반(Optimization-based) 방법:** LongVolcap [73]
- **SMPL 기반 방법:** GauHuman [22]
- **Feed-forward 방법:** GPS-Gaussian [85]
- **생성 기반 방법:** CAT4D† [68]

(**†**: 저자들이 재현한 버전)

DNA-Rendering 및 ActorsHQ 데이터셋은 테스트 시퀀스에 대한 **SMPL 모델**을 제공하지 않으므로, 우리는 **EasyMocap [1]**을 사용해 SMPL 모델을 추출하고 이를 GauHuman [22]의 입력으로 사용한다.

GPS-Gaussian [85]의 경우, 원 논문에서 제시한 **뷰 선택 전략(view-selection strategy)**을 따른다. 즉, 목표 시점과 가장 가까운 두 입력 시점을 선택하여 각 시점을 생성하는 방식이다.

CAT4D† [68]은 우리와 동일한 학습 설정으로 전처리된 DNA-Rendering 데이터셋에서 학습되었으며, **희소 시점 비디오(sparse-view video)**에서 조건부 뷰를 선택해 각 행(row) 또는 열(column)을 디노이징한다. 이때 **3.2절에서 설명한 샘플링 시퀀스 및 조건부 뷰 선택 전략**을 동일하게 사용한다.

![](/assets/images/posts/588/img_7.png)

**그림 4 설명:** 조건화 방식에 따른 정성적 비교(qualitative comparison). **Skeleton-Plücker 혼합 조건화**는 확산 모델을 위한 강건한 인간 포즈 사전(prior)으로 작용한다.

## **4.3 베이스라인과의 비교 (Comparison to Baselines)**

### DNA-Rendering [10] 데이터셋 비교

우리는 **DNA-Rendering [10]** 데이터셋에서의 정량적(표 1) 및 정성적(그림 6) 비교를 제공한다. 시각화 결과와 평가 지표에서 확인할 수 있듯, 제안한 방법은 **시각적 품질(visual quality)**과 **시공간 일관성(spatio-temporal consistency)** 모두에서 베이스라인 대비 일관되게 우수한 성능을 보인다.

- **LongVolcap [73] (최적화 기반 방법):**  
  희소 시점 재구성 문제의 **불량정의(ill-posed)** 특성으로 인해 노이즈가 많은 렌더링 결과를 산출한다.
- **GauHuman [22] (SMPL 기반 방법):**  
  복잡한 의상을 착용하거나 동적인 움직임을 수행하는 퍼포머를 신뢰성 있게 재구성하지 못한다.
- **GPS-Gaussian [85] (Feed-forward 방법):**  
  희소 시점 설정에서 깊이 추정기(depth estimator)가 실패하여, 동적인 시퀀스에서 **단편화된 결과(fragmented results)**를 산출한다.

반면, 제안한 방법은 **확산 모델의 사전 지식(diffusion prior)**으로부터 합리적인 가이던스를 생성하여 희소 시점 환경의 어려움을 효과적으로 극복하며, 복잡한 인간의 동작 및 외형에도 **우수한 일반화 성능(generalization)**을 보인다.

특히, **입력 시점 4개만으로도** 제안한 방법은 LongVolcap [73]의 **48 시점 밀집 재구성(dense reconstruction)**과 유사한 수준의 시각적 품질을 달성한다.

![](/assets/images/posts/588/img_8.png)

**그림 5 설명:** 서로 다른 디노이징 전략 간의 정성적 비교. 제안한 **슬라이딩 반복 디노이징(sliding iterative denoising)** 방법은 긴 이미지 시퀀스 전반에서 **일관된 외형(consistent appearance)**을 유지한다.

**ActorsHQ [23] 데이터셋 비교**

표 1과 그림 6에서 보이듯이, 우리 모델은 **ActorsHQ 데이터셋의 보지 못한(unseen) 배우 외형과 동작**에도 잘 일반화된다. 반면, 베이스라인 방법들은 DNA-Rendering 데이터셋에서와 마찬가지로 **기하 구조와 외형의 일관성(coherent geometry and appearance)** 확보에 어려움을 겪는다.

우리 모델의 독창적인 설계 덕분에, ActorsHQ 데이터셋에서 관측 정보가 제한적인 상황에서도 Diffuman4D는 **더 선명하고(sharper) 시공간적으로 일관된(spatio-temporally consistent)** 인간 퍼포먼스 재구성 결과를 안정적으로 산출한다.

![](/assets/images/posts/588/img_9.png)

![](/assets/images/posts/588/img_10.png)

**그림 6 설명:**

(a) DNA-Rendering [10] 테스트 세트 결과  
(b) ActorsHQ [23]에서의 제로샷 일반화 결과

GPS-Gaussian은 **8개 입력 시점**을 사용하며, 나머지 방법들은 **4개 입력 시점**을 사용한다. CAT4D†는 저자들이 재현한 버전이다. Diffuman4D는 **시각적 품질**과 **시공간 일관성**에서 베이스라인 대비 일관되게 우수한 성능을 보인다.

**4.4 절제 실험 (Ablation Study)**

우리는 **DNA-Rendering [10] 데이터셋**을 사용해 두 가지 절제 실험을 수행하였다.

- **슬라이딩 반복 디노이징 메커니즘 평가:** 도전적인 동작 시퀀스 3개를 사용
- **Skeleton–Plücker 조건화 평가:** 복잡한 의상을 포함한 시퀀스 6개를 사용

### 확산 디노이징 전략 비교 (Diffusion Denoising Strategy)

우리는 세 가지 **확산 샘플링 전략(diffusion sampling strategies)**을 비교하였다.

1. **멀티 그룹 디노이징(multi-group denoising):**  
   데이터를 여러 그룹으로 나눠 각각 독립적으로 디노이징하는 방식.  
   → 그룹 간 시간적·공간적 상관 관계를 고려하지 않아, **구간(segment) 전환 시 불연속성(jumps)** 발생.
2. **중간값 필터링(median filtering):**  
   그룹 간 겹치는 이미지의 디노이징 결과를 중간값으로 결합해 멀티 그룹 방식의 불일치를 완화.  
   → 그러나 **계산 비용이 겹침 비율(overlap ratio)**에 반비례하며, 겹침이 충분하지 않으면 여전히 불일치 발생.
3. **제안한 슬라이딩 반복 디노이징(sliding iterative denoising):**  
   슬라이딩 윈도우와 디노이징 단계를 **결합(merge)**해, **계산 비용은 일정하게 유지**하면서도 디노이징 과정에서 **부드러움 유도 편향(smoothness-inductive bias)**을 추가.  
   → 결과적으로 **더 일관되고(global consistency) 정확한 결과**를 산출.

그림 5와 표 2에서 확인할 수 있듯, 제안한 방식은 두 기존 방법보다 우수한 시공간 일관성을 달성한다.

### 조건화 스킴 비교 (Conditioning Scheme)

그림 4와 표 3에서 보이듯, 우리는 세 가지 **조건화 스킴**을 비교했다.

- **Skeleton 미사용 (w/o skeleton):**  
  카메라 제어 신호가 제한적이어서, 생성된 콘텐츠가 **불량정의(ill-posed)** 문제로 인해 큰 불일치(misalignment)를 보임.
- **Plücker 미사용 (w/o Plücker):**  
  세밀한 인체 제어는 가능하지만, **앞/뒤(front-back)** 및 **좌/우(left-right)** 구분에 어려움이 있어 재구성 모듈에 **일관되지 않은 가이던스**를 제공.
- **Skeleton–Plücker 혼합 조건화 (우리 방법):**  
  Plücker 임베딩의 카메라 제어 신호와 Skeleton 임베딩의 포즈 제어 신호의 장점을 결합해, **일관적이고 제어 가능한 새로운 시점 결과(novel view results)**를 생성.

이를 통해 목표 인간 배우의 동작과 외형을 더 정확하게 재현할 수 있었다.

**5. 결론 (Conclusion)**

본 논문에서는 **Diffuman4D**를 소개하였다. 이는 **희소 시점 입력(sparse-view input)**으로부터 **고해상도(1024p)** 및 **4D 일관성(4D-consistency)**을 갖춘 인간 이미지를 생성할 수 있는 새로운 확산 모델이다. 우리는 **공간적 및 시간적 일관성(spatial and temporal consistency)**을 향상시키면서도 **높은 계산 효율성(computational efficiency)**을 유지하기 위해 새로운 **슬라이딩 반복 디노이징(sliding iterative denoising)** 전략을 제안했다. 또한, 인간 골격(human skeleton)을 활용한 **4D 포즈 조건화(4D pose conditioning)** 메커니즘을 도입하여 동작 정확도와 시각적 품질을 한층 개선하였다.

우리 방법은 기존 최첨단 접근법(state-of-the-art) 대비, 희소 입력에서도 세밀한 디테일과 복잡한 인간 동작을 더 정교하게 포착하는 우수한 성능을 보였다.

그러나 여전히 다음과 같은 한계가 존재한다.

1. **고해상도(4K) 비디오 미지원:**  
   기반 모델의 제약으로 인해 4K 영상 생성은 지원되지 않는다.
2. **복잡한 인간-객체 상호작용:**  
   인간과 객체가 복잡하게 상호작용하는 장면에서는 성능 저하가 발생할 수 있다.
3. **새로운 포즈(novel-pose) 렌더링 불가:**  
   입력 비디오를 통해 공간적 일관성을 유지해야 하므로, 현재 방법으로는 완전히 새로운 포즈 렌더링은 불가능하다.

이러한 한계들을 해결하는 것은 향후 연구에서 흥미로운 과제가 될 것이다.

**감사의 글 (Acknowledgments)**  
본 연구는 **중국 국가 중점 R&D 프로그램(National Key R&D Program of China, No.2024YFB2809105)**, **중국 국가자연과학재단(NSFC, No.U24B20154, No.62172364)**, **중국 저장성 자연과학재단(Zhejiang Provincial Natural Science Foundation of China, No.LR25F020003)**, **Ant Research**, 그리고 **저장대학교 CAD&CG 국가중점실험실 및 정보기술센터(State Key Lab of CAD&CG, Zhejiang University)**의 지원을 받아 수행되었다.
