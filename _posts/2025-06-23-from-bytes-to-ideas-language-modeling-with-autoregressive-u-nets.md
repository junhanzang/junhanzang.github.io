---
title: "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets"
date: 2025-06-23 15:46:08
categories:
  - 인공지능
---

<https://arxiv.org/abs/2506.14761>

[From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)

**초록 (Abstract)**  
토크나이제이션(tokenization)은 입력 텍스트에 고정된 세분화 수준을 강제로 부여하며, 이는 언어 모델이 데이터를 처리하는 방식과 미래를 예측하는 범위를 고정시킵니다. Byte Pair Encoding(BPE) 및 유사한 방식들은 텍스트를 한 번 나누고, 정적인 어휘집을 구축한 뒤, 모델을 그 선택에 묶어둡니다. 본 논문에서는 이러한 경직성을 완화하기 위해, 학습 과정에서 자체 토큰을 임베딩하는 방법을 학습하는 **오토리그레시브 U-Net(Autoregressive U-Net)**을 제안합니다.

이 네트워크는 원시 바이트(raw bytes)를 읽은 뒤, 이를 단어 단위로 풀링(pooling)하고, 그 다음에는 단어 쌍(pair of words), 더 나아가 최대 4단어까지 점진적으로 결합하여 시퀀스의 **다중 스케일 표현(multi-scale representation)**을 생성합니다. 모델이 더 깊은 단계로 들어갈수록, 더 멀리 있는 미래—즉, 다음 바이트가 아닌 **다음 여러 단어**를 예측해야 하므로, 깊은 단계에서는 **더 넓은 의미적 패턴(semantic pattern)**을 포착하고, 얕은 단계에서는 미세한 세부 사항(fine detail)을 처리하게 됩니다.

사전학습(pretraining) 연산량을 정밀하게 조정하고 제어하면, 얕은 계층 구조는 강력한 BPE 기준 모델들과 유사한 성능을 보이며, 더 깊은 계층 구조는 유망한 성능 향상 추세를 나타냅니다. 이제 토크나이제이션이 모델 내부에 포함되기 때문에, **동일한 시스템이 문자 수준(character-level)의 작업도 처리할 수 있고**, **저자원 언어 간에도 지식을 공유**할 수 있게 됩니다.

![](/assets/images/posts/574/img.png)

**그림 1: 세 단계로 구성된 오토리그레시브 U-Net (AU-Net)**  
모델은 왼쪽에서 오른쪽 방향으로 실행됩니다. 수축 경로(contracting path)는 시퀀스를 두 단계에 걸쳐 압축합니다:

- **1단계(Stage 1)**에서는 원시 바이트를 처리하고,
- **2단계(Stage 2)**에서는 각 단어 경계에서의 벡터만 유지하며,
- **3단계(Stage 3)**에서는 두 단어당 하나의 벡터만 유지합니다.

각 수축 및 확장 단계는 임의의 풀링(pooling) 및 업샘플링(upsampling) 패턴을 지원합니다. 가장 깊은 단계 이후에는, 확장 경로(expanding path)가 수축 경로를 역으로 따라가며 각 조밀한(coarse) 벡터를 복제하고, 위치별 선형 레이어를 적용합니다. 이들은 수축 경로에서의 skip connection과 결합되어 시퀀스 길이를 점진적으로 복원하고, 상위 수준의 정보를 혼합합니다. **깊은 단계는 더 먼 미래를 예측하고 넓은 의미론을 포착**하며, **얕은 단계는 지역적인 세부 사항을 정제**합니다.

### 1. 서론 (Introduction)

언어 모델의 본질은 시퀀스 내에서 패턴을 발견해 다음에 올 것을 예측하는 데 있다. 그러나 그 전에, 우리는 그 시퀀스를 구성하는 단위—즉 **토큰(token)**—이 무엇인지 먼저 정의해야 한다. 이 선택은 보통 학습이 시작되기 훨씬 전, **원시 텍스트(raw text)**를 개별 단위로 자르는 **토크나이저(tokenizer)**에 의해 고정된다.

예를 들어, 문장 “The quick brown fox.”를 생각해보자.

- **문자 수준(character-level)** 토크나이저는 {T, h, e, ␣, q, u} 같은 스트림을 모델에 제공하고, 다음 글자 i를 예측하도록 요구한다.
- 반면에 **단어 수준(word-level)** 토크나이저는 {The, quick}을 넘겨주고, 모델이 한 번에 brown을 예측하길 기대한다.

세분화가 정밀할수록 시퀀스는 길어지고 모델이 미리 볼 수 있는 창(window)은 짧아진다. 반대로, 세분화가 거칠수록 시퀀스는 짧아지지만, 각 토큰은 더 희귀해지고 서로 비교하거나 예측하기가 어려워진다. 세분화 수준과 무관하게, 어떤 형태든 **토크나이제이션은 필수**다—Transformer 모델이 실행되기 위해서는 반드시 시퀀스가 먼저 존재해야 하기 때문이다.

**Byte Pair Encoding(BPE)**은 단순한 임베딩 테이블과 함께 사용되며, 가장 널리 쓰이는 접근법이다. 이 방법은 학습 텍스트 내에서 가장 자주 등장하는 바이트 시퀀스를 반복적으로 병합하여, 설정된 어휘 크기까지 계속 확장해 나간다. 이 과정에서 연구자들은 직관적인 두 개의 조절 요소만 다룬다:

1. **학습 코퍼스(training corpus)**: 어떤 텍스트를 넣느냐—영어 문장, 소스코드, 다국어 혼합—에 따라 병합되는 패턴이 달라지고, 최종 토큰 모양도 결정된다.
2. **어휘 크기(vocabulary size)**: 이 값을 높이면 병합 과정을 더 많이 반복할 수 있어, 더 긴 토큰과 짧은 시퀀스를 얻을 수 있지만, 더 큰 임베딩 테이블과 출력 소프트맥스가 필요해진다.

토크나이제이션의 대부분 문제는 **텍스트 분할** 자체보다 **임베딩 과정**에서 비롯된다. 각 토큰은 보통 독립된 벡터에 매핑되며, 이로 인해 네트워크는 불투명한 식별자(예: "strawberry" vs. "strawberries")만 보고, 두 단어가 9개의 문자를 공유한다는 사실조차도 다시 학습해야 한다. 이러한 **고립된 임베딩** 의존성은 기호 수준의 추론을 방해하고, 방언이나 희귀 언어로의 전이를 어렵게 만든다. 게다가 이 분할은 대부분 **사전 처리(preprocessing)** 단계에서 수행되기 때문에, 이후 모든 모델 계층이 **하나의 고정된 세분화 수준**에 묶이게 된다 (2.2절 참고).

이러한 한계를 해결하기 위해, 우리는 **오토리그레시브 U-Net(Autoregressive U-Net, 2.1절)** 또는 **AU-Net(발음: 오우-넷 /óU nEt/)** 을 제안한다. 이 모델은 **원시 바이트(raw byte)**로부터 직접 정보를 임베딩하며, **여러 단계의 분할(multi-stage splitting)**을 허용한다.

임베딩의 목적은 토큰을 벡터로 매핑하는 것이다. 기존의 테이블 기반 룩업 대신, 우리는 **self-attention을 직접 사용해 토큰을 임베딩**한다. Self-attention은 어떤 위치의 벡터든 전체 앞선 컨텍스트를 요약할 수 있게 해준다. 이 특성을 활용해, 우리는 다음과 같은 **단순한 풀링 메커니즘**을 구성한다:

- 단어 경계에서 벡터를 선택(AU-Net-2)
- 단어 쌍에서 선택(AU-Net-3)
- 4단어 단위로 선택(AU-Net-4)

이렇게 해서 **다단계 임베딩 계층 구조(multi-stage embedding hierarchy)**를 만든다. 이 U-Net 구조는 시퀀스를 점차 압축하는 수축 경로(contracting path)를 따르고, skip connection으로 세부 정보를 유지한 뒤, 다시 확장 경로(expanding path)를 통해 시퀀스를 복원한다. 이 확장 과정에서는, 거친(coarse) 정보를 나타내는 벡터가 미세한(fine-grained) 표현으로 다시 주입된다.

깊은 단계는 압축된 표현에서 작동하기 때문에, 본질적으로 여러 단어를 미리 예측해야 하며, 이는 **보조 손실 없이 multi-token 예측**(Gloeckle et al., 2024)과 유사하다. 이 구조는 깊은 단계가 의미 수준에서 얕은 단계를 안내하게 하고, 얕은 단계는 철자 등 세부 사항을 담당하도록 한다.

### 기여 요약 (Section 3에서 정량적으로 평가됨):

- **C1. 적응형 다단계 계층 구조**: 고정된 풀링 또는 얕은 계층 구조에 의존했던 기존 연구와 달리, 임의의 사용자 정의 분할 함수와 함께 최대 4단계 임베딩을 종단(end-to-end) 학습한다.
- **C2. 무한한 어휘 크기**: 원시 바이트 단위로 작동하기 때문에, 사전 정의된 어휘나 메모리를 많이 사용하는 임베딩 테이블이 필요 없어지고, **무한한 수의 고유 토큰**을 다룰 수 있다.
- **C3. 강력한 성능 및 확장성**: 동일한 사전 학습 자원 내에서, 단일 계층 모델은 강력한 BPE 기준과 대등하며, 두세 단계 모델은 **유망한 확장 추세(scaling trend)**를 보인다. 결과 일부는 표 2에 제시된다.
- **C4. 실용적 효율성**: 이론적 계산량이 아니라, 실제 **GPU 처리량을 기준으로 벽시계 시간(wall-clock time)**에서 성능을 유지한다. 코드는 [Meta Lingua (Videau et al., 2024)](https://github.com/facebookresearch/lingua/tree/main/apps/aunet)에서 공개되어 있다.
- **C5. 안정적인 스케일링 법칙**: 토큰 수준에서 바이트 수준으로 넘어가면서 **새로운 배치 크기 및 학습률 공식**이 필요함을 보이고, 이를 통해 매끄러운 최적화가 가능해진다.

---

### ✏️ 요약하면

초록은 “기존 방식의 rigid함을 완화했다”는 말로 추상적으로 표현되어 있지만,  
**실제로는 토크나이저를 완전히 없애고, 바이트부터 4단어 단위까지 모델 내부에서 동적으로 처리한다는 점이 혁신적입니다.**

또한 self-attention 기반으로 토큰을 임베딩하고, multi-scale 계층 구조를 통해 **깊은 계층이 의미 수준을, 얕은 계층이 철자 수준을 처리**하는 방식은 Transformer 이후 나온 많은 구조 중에서도 **기존 임베딩 방식의 한계를 깨려는 시도**로 볼 수 있습니다.

---
