---
title: "YOLOE: Real-Time Seeing Anything"
date: 2025-08-07 14:36:29
categories:
  - 인공지능
tags:
  - yoloe
---

<https://arxiv.org/abs/2503.07465>

[YOLOE: Real-Time Seeing Anything](https://arxiv.org/abs/2503.07465)

초록  
객체 탐지(Object detection)와 분할(Segmentation)은 컴퓨터 비전 응용 분야에서 널리 사용되고 있다. 그러나 YOLO 시리즈와 같은 기존 모델은 효율적이고 정확하지만, 미리 정의된 카테고리에 의존하기 때문에 개방형(open) 환경에서의 적응성이 제한된다. 최근 개방형 방법들은 텍스트 프롬프트(text prompt), 비주얼 프롬프트(visual cue), 또는 프롬프트 없는(prompt-free) 패러다임을 활용해 이러한 한계를 극복하려 하지만, 높은 연산 요구나 복잡한 배포로 인해 성능과 효율성 사이에서 타협해야 하는 문제가 존재한다.

본 연구에서는 다양한 개방형 프롬프트 메커니즘을 단일의 고효율 모델 내에서 통합하여 탐지와 분할을 수행하며, 실시간으로 모든 객체를 인식할 수 있는 YOLOE를 제안한다. 텍스트 프롬프트의 경우, **재매개 가능한 영역-텍스트 정렬(RepRTA, Re-parameterizable Region-Text Alignment)** 전략을 제안한다. 이는 사전학습된 텍스트 임베딩을 재매개 가능한 경량 보조 네트워크를 통해 정제하고, 시각-텍스트 정렬을 강화하면서 추론 및 전이 과정에서의 추가 오버헤드를 제거한다.

비주얼 프롬프트의 경우, **의미 활성화 비주얼 프롬프트 인코더(SAVPE, Semantic-Activated Visual Prompt Encoder)**를 제안한다. 이는 의미 분기(semantic branch)와 활성화 분기(activation branch)를 분리하여 시각 임베딩을 개선하고 정확도를 향상시키면서도 복잡성을 최소화한다.

프롬프트 없는 시나리오에서는 **Lazy Region-Prompt Contrast (LRPC)** 전략을 도입한다. 이 방법은 내장된 대규모 어휘와 특화된 임베딩을 활용해 모든 객체를 식별하며, 언어 모델에 대한 비용 높은 의존성을 피한다.

광범위한 실험 결과, YOLOE는 뛰어난 제로샷(Zero-shot) 성능과 전이 가능성을 보이며, 높은 추론 효율성과 낮은 학습 비용을 동시에 달성함을 보여준다. 특히 LVIS 데이터셋에서 학습 비용을 3배 절감하고 추론 속도를 1.4배 향상시키면서도 YOLO-Worldv2-S 대비 3.5 AP 향상을 달성했다. COCO 전이 실험에서도 YOLOE-v8-L은 폐쇄형(closed-set) YOLOv8-L 대비 0.6 APb, 0.4 APm의 성능 향상과 함께 학습 시간을 약 4배 단축했다.

코드와 모델은 <https://github.com/THU-MIG/yoloe>에서 제공된다.

## 1 서론

객체 탐지(Object detection)와 분할(Segmentation)은 컴퓨터 비전의 기초 과제로서 [48, 15], 자율주행 [2], 의료 분석 [55], 로보틱스 [8] 등 다양한 분야에서 폭넓게 활용되고 있다. YOLO 시리즈 [47, 1, 21, 3]와 같은 전통적인 접근법은 합성곱 신경망(CNN)을 기반으로 실시간에서 뛰어난 성능을 보여왔으나, 사전에 정의된 객체 카테고리에 의존하기 때문에 실제 개방형(open) 환경에서의 유연성이 제한된다. 이러한 개방형 환경에서는 텍스트, 비주얼 큐(visual cue) 혹은 프롬프트 없는(prompt-free) 다양한 프롬프트 메커니즘을 통해 임의의 객체를 탐지하고 분할할 수 있는 모델이 점점 더 요구되고 있다.

![](/assets/images/posts/589/img.png)

그림 1은 YOLOE(본 논문)와 최신 YOLO-Worldv2 모델 간의 성능, 학습 비용, 추론 효율성을 개방형 텍스트 프롬프트 조건에서 비교한 것이다. LVIS AP는 minival 세트에서 평가되었으며, FPS는 TensorRT 환경에서 T4 GPU, CoreML 환경에서 iPhone 12에서 각각 측정되었다. 결과는 본 모델의 우수성을 보여준다.

이와 같은 배경에서 최근 연구들은 모델이 개방형 프롬프트에 대해 일반화할 수 있도록 하는 방향으로 전환되고 있다 [49, 20, 80, 5]. 이들 연구는 특정 프롬프트 유형만을 목표로 하거나(e.g., GLIP [32]), 여러 프롬프트 유형을 통합적으로 처리하는 방향으로(e.g., DINO-X [49]) 설계된다. 특히, 영역 단위의 비전-언어 사전학습(region-level vision-language pretraining) [32, 37, 65]을 통해 텍스트 프롬프트는 일반적으로 텍스트 인코더를 통해 처리되어 영역 특징(region feature)에 대한 대조적 학습 목표(contrastive objective)로 사용되며 [49, 20], 임의의 카테고리 인식을 달성한다 (예: YOLO-World [5]). 비주얼 프롬프트의 경우, 지정된 영역과 연계된 클래스 임베딩으로 인코딩되어 이미지 특징이나 언어 정렬된 시각 인코더와의 상호작용을 통해 유사 객체를 식별한다 [19, 30, 49, 5] (예: T-Rex2 [20]). 한편 프롬프트 없는 시나리오에서는, 기존 방법들은 언어 모델을 통합하여 모든 객체를 찾고 영역 특징에 기반해 순차적으로 카테고리 이름을 생성하는 방식을 채택한다 [49, 62] (예: GenerateU [33]).

하지만 여전히 다양한 개방형 프롬프트를 동시에 지원하면서 임의 객체를 높은 효율성과 정확도로 처리할 수 있는 단일 모델은 부족하다. 예를 들어, DINO-X [49]는 통합 아키텍처를 갖추었지만, 자원 집약적인 학습 및 추론 오버헤드가 발생한다. 또한, 서로 다른 프롬프트를 위해 별도로 설계된 기존 연구들은 성능과 효율성 간의 최적 균형을 이루지 못해 단일 모델로 직접 통합하기 어렵다. 예컨대, 텍스트 기반 접근법은 대규모 어휘를 포함할 때 교차 모달리티 융합(cross-modality fusion)의 복잡성으로 인해 상당한 계산 오버헤드를 유발한다 [5, 37, 32, 49]. 비주얼 기반 방법은 트랜스포머 중심 설계나 추가 시각 인코더 의존성 때문에 엣지 디바이스에서의 배포성이 저하된다 [20, 30, 67]. 프롬프트 없는 방법은 대형 언어 모델에 의존하여 상당한 메모리 및 지연 비용을 유발한다 [33, 49].

이러한 한계를 고려해 본 논문에서는 **YOLOE(ye)**를 제안한다. 이는 텍스트, 비주얼 입력, 프롬프트 없는 패러다임 등 다양한 프롬프트 메커니즘 하에서, 사람의 눈과 같이(open eye) 개방형 객체 탐지 및 분할을 수행할 수 있는 고효율 통합 모델이다. 본 모델은 검증된 YOLO 아키텍처를 기반으로 한다.

- **텍스트 프롬프트**:  
  **재매개 가능한 영역-텍스트 정렬(RepRTA, Re-parameterizable Region-Text Alignment)** 전략을 제안한다. 경량 보조 네트워크를 사용해 사전학습된 텍스트 임베딩을 개선하여 시각-의미 정렬을 향상시킨다. 학습 시 사전 캐시된 텍스트 임베딩은 보조 네트워크만 거치므로 폐쇄형(closed-set) 학습 대비 추가 비용이 적다. 추론 및 전이 시 보조 네트워크는 분류 헤드에 재매개화되어 YOLO와 동일한 아키텍처로 동작, 오버헤드가 없다.
- **비주얼 프롬프트**:  
  **의미 활성화 비주얼 프롬프트 인코더(SAVPE, Semantic-Activated Visual Prompt Encoder)**를 설계한다. 관심 영역을 마스크로 형식화하고, 이를 PAN의 다중 스케일 특징과 융합하여 저차원의 활성화 분기에서 프롬프트 인식 가중치를 생성하고, 의미 분기에서 프롬프트 비의존적 특징을 추출한다. 두 특징을 통합해 프롬프트 임베딩을 도출, 낮은 복잡도로 우수한 성능을 얻는다.
- **프롬프트 없는 시나리오**:  
  **Lazy Region-Prompt Contrast (LRPC)** 전략을 도입한다. 대형 언어 모델에 의존하지 않고, 특화된 프롬프트 임베딩을 활용해 모든 객체를 탐지하고 내장 대규모 어휘를 통해 카테고리를 검색한다. 식별된 객체의 앵커 포인트만 어휘와 매칭하여 성능을 유지하면서도 오버헤드를 낮춘다.

이러한 설계 덕분에 YOLOE는 다양한 개방형 프롬프트 메커니즘 하에서 탐지와 분할을 단일 모델로 수행하며, 높은 추론 효율성과 낮은 학습 비용을 달성한다. 특히 그림 1에서 보듯, 학습 비용을 3배 절감하면서 LVIS [14]에서 YOLO-Worldv2-S [5] 대비 3.5 AP 향상, T4 및 iPhone 12에서 각각 1.4배 및 1.3배의 추론 속도 향상을 달성했다. 또한 비주얼 프롬프트 및 프롬프트 없는 환경에서 YOLOE-v8-L은 T-Rex2 대비 3.3 APr, GenerateU 대비 0.4 AP 향상을 보였으며, 이는 2배 적은 학습 데이터와 6.3배 적은 파라미터로 달성된 결과이다. COCO [34] 전이 실험에서도 YOLOE-v8-M/L은 YOLOv8-M/L 대비 0.4/0.6 APb 및 0.4/0.4 APm 향상을 보이며, 학습 시간을 약 4배 단축했다.

우리는 YOLOE가 실시간 개방형 프롬프트 기반 비전 과제의 강력한 베이스라인으로 자리 잡고, 이후 연구 발전을 이끄는 역할을 하기를 기대한다.

## 2 관련 연구

### 전통적 탐지 및 분할 (Traditional detection and segmentation)

객체 탐지(Object detection)와 분할(Segmentation)의 전통적 접근법은 주로 폐쇄형(closed-set) 패러다임 하에서 동작한다. 초기의 2단계(two-stage) 프레임워크 [12, 48, 15, 4]는 **Faster R-CNN** [48]으로 대표되며, 영역 제안 네트워크(Region Proposal Network, RPN)를 통해 후보 영역을 생성한 후 관심 영역(ROI)에 대해 분류와 회귀를 수행한다. 한편, 단일 단계(single-stage) 탐지기 [38, 35, 56, 72, 10]는 단일 네트워크 내에서 그리드 기반 예측(grid-based prediction)을 통해 속도를 우선시한다.

**YOLO 시리즈** [47, 1, 27, 60, 21, 59]는 이러한 패러다임에서 중요한 역할을 수행하며 실제 응용에서 널리 사용되고 있다. 또한 **DETR** [28]와 그 변형 모델들 [77, 28, 69]은 트랜스포머 기반 아키텍처를 통해 휴리스틱 요소를 제거하면서 객체 탐지에서 큰 전환점을 이뤘다.

정밀한 결과를 달성하기 위해 기존의 인스턴스 분할(instance segmentation) 방법은 바운딩 박스 좌표가 아닌 **픽셀 단위 마스크**를 예측한다 [15]. 이를 위해 **YOLACT** [3]는 프로토타입 마스크와 마스크 계수를 통합해 실시간 인스턴스 분할을 지원한다. 또한 **MaskDINO** [29]는 DINO [69]를 기반으로 쿼리 임베딩(query embedding)과 고해상도 픽셀 임베딩 맵을 활용해 이진 마스크를 생성한다.

![](/assets/images/posts/589/img_1.png)

그림 2는 다양한 개방형 프롬프트 메커니즘에서 탐지 및 분할을 지원하는 YOLOE의 개요를 보여준다.

- 텍스트 프롬프트의 경우, 재매개 가능한 영역-텍스트 정렬 전략을 설계해 추론 및 전이 시 오버헤드 없이 성능을 향상시킨다.
- 비주얼 프롬프트의 경우, SAVPE를 사용해 시각적 단서를 인코딩하고, 최소 비용으로 향상된 프롬프트 임베딩을 생성한다.
- 프롬프트 없는 설정에서는 Lazy Region-Prompt Contrast 전략을 도입해 식별된 모든 객체에 대해 효율적으로 카테고리 이름을 검색 방식으로 제공한다.

### 텍스트 프롬프트 기반 탐지 및 분할

최근 개방 어휘 객체 탐지(open-vocabulary object detection) [68, 13, 76, 75, 25, 61, 74]의 발전은 시각적 특징과 텍스트 임베딩의 정렬을 통해 새로운 카테고리를 탐지하는 데 초점을 맞추고 있다. 예를 들어, **GLIP** [32]은 대규모 이미지-텍스트 쌍을 기반으로 한 사전학습(grounded pre-training)을 통해 객체 탐지와 구문 결합(phrase grounding)을 통합하여 강력한 제로샷 성능을 보여준다. **DetCLIP** [65]은 설명(descriptions)을 통해 개념을 풍부하게 하여 개방 어휘 학습을 지원한다. 또한 **Grounding DINO** [37]는 DINO에 교차 모달리티 융합(cross-modality fusion)을 통합해 텍스트 프롬프트와 시각 표현 간의 정렬을 향상시킨다.

**YOLO-World** [5]는 YOLO 아키텍처 기반으로 소형 탐지기를 개방형 인식 기능과 함께 사전학습(pretraining)하는 잠재력을 보여준다. **YOLO-UniOW** [36]는 YOLO-World를 확장하여 적응적 의사결정 학습 전략(adaptive decision-learning strategy)을 활용한다. 이와 유사하게 여러 개방 어휘 인스턴스 분할 모델들 [26, 11, 18, 63, 45]은 최신 파운데이션 모델을 통해 풍부한 시각-의미 지식을 학습하여 새로운 객체 카테고리에 대해 분할을 수행한다. 예를 들어, **X-Decoder** [79]와 **OpenSeeD** [71]는 개방 어휘 탐지와 분할 과제를 모두 탐구하고, **APE** [54]는 다양한 텍스트 프롬프트를 사용해 이미지 내 모든 객체를 정렬하고 프롬프트하는 범용 시각 인지 모델을 제안한다.

### 비주얼 프롬프트 기반 탐지 및 분할

텍스트 프롬프트는 일반적인 설명을 제공하지만, 특정 객체는 언어만으로 설명하기 어려운 경우가 있다. 특히 전문적인 도메인 지식이 필요한 객체들이 이에 해당하며, 이러한 상황에서는 비주얼 프롬프트가 텍스트 프롬프트를 보완하여 더 유연하고 구체적으로 탐지와 분할을 안내할 수 있다 [19, 20].

**OV-DETR** [67]와 **OWL-ViT** [41]는 CLIP 인코더를 활용해 텍스트와 이미지 프롬프트를 처리한다. **MQ-Det** [64]은 텍스트 쿼리에 쿼리 이미지로부터 클래스별 시각 정보를 추가한다. **DINOv** [30]는 시각 프롬프트를 범용 및 참조 비전 작업의 컨텍스트 예제로 탐구하며, **T-Rex2** [20]는 영역 단위 대조적 정렬(region-level contrastive alignment)을 통해 시각 및 텍스트 프롬프트를 통합한다.

분할 측면에서, **SAM** [23]은 대규모 데이터 기반으로 상호작용적이고 반복적으로 프롬프트 가능한 유연하고 강력한 모델을 제안한다. **SEEM** [80]은 더 다양한 프롬프트 유형으로 객체를 분할하는 것을 탐구하며, **Semantic-SAM** [31]은 의미 이해와 세분화 정밀도(granularity) 측면에서 뛰어나며, 범용적 분할(panoptic)과 부분 분할(part segmentation) 작업 모두를 처리할 수 있다.

### 프롬프트 없는 탐지 및 분할

기존 접근법은 개방형 탐지 및 분할에서 추론 시 여전히 명시적 프롬프트(explicit prompt)에 의존한다. 이러한 한계를 해결하기 위해 몇몇 연구 [62, 40, 66, 33, 49]는 생성형 언어 모델과의 통합을 탐구하여 탐지된 모든 객체의 설명을 생성하도록 한다. 예를 들어, **GRiT** [62]는 밀집 캡셔닝(dense captioning)과 객체 탐지 작업 모두를 위해 텍스트 디코더를 활용하며, **DetCLIPv3** [66]은 대규모 데이터에서 객체 캡셔너(object captioner)를 학습시켜 모델이 풍부한 라벨 정보를 생성할 수 있도록 한다. **GenerateU** [33]는 언어 모델을 활용해 자유 형식의 객체 이름을 생성한다.

### 마무리

우리의 지식에 따르면, **DINO-X** [49]를 제외하면 다양한 개방형 프롬프트 메커니즘에서 객체 탐지와 분할을 단일 아키텍처 내에서 달성한 연구는 거의 없다. 그러나 DINO-X는 방대한 학습 비용과 상당한 추론 오버헤드를 수반해 실제 엣지(edge) 배포에서의 실용성을 크게 제한한다. 반면, 본 논문의 **YOLOE**는 실시간 성능과 효율성을 갖춘 통합 모델로, 배포가 용이하면서도 효율적인 솔루션을 제공하는 것을 목표로 한다.

## 3 방법론

이 섹션에서는 YOLOE의 설계에 대해 상세히 설명한다. YOLOE는 YOLO 구조(Sec. 3.1)를 기반으로 하며, 텍스트 프롬프트는 **RepRTA**(Sec. 3.2), 비주얼 프롬프트는 **SAVPE**(Sec. 3.3), 프롬프트 없는 시나리오는 **LRPC**(Sec. 3.4)를 통해 지원한다.

### 3.1 모델 아키텍처

그림 2와 같이 YOLOE는 전형적인 YOLO 아키텍처 [47, 1, 21]를 채택하며, **백본(backbone)**, **PAN(Path Aggregation Network)**, **회귀 헤드(regression head)**, **분할 헤드(segmentation head)**, **객체 임베딩 헤드(object embedding head)**로 구성된다.

- **백본과 PAN**: 입력 이미지를 다중 스케일 특징으로 추출한다.
- **회귀 헤드**: 각 앵커 포인트(anchor point)에 대해 탐지를 위한 바운딩 박스를 예측한다.
- **분할 헤드**: 프로토타입(prototype)과 마스크 계수(mask coefficients)를 생성해 분할을 수행한다 [3].
- **객체 임베딩 헤드**: YOLO의 분류 헤드 구조를 따르지만, 폐쇄형(closed-set) 시나리오에서 클래스 개수였던 마지막 1×1 합성곱(convolution) 층의 출력 채널 수를 **임베딩 차원**으로 변경한 점이 다르다.

텍스트 및 비주얼 프롬프트가 주어지면, 각각 **RepRTA**와 **SAVPE**를 통해 이를 정규화된 프롬프트 임베딩 P으로 인코딩한다. 이 임베딩은 분류 가중치(classification weight)로 사용되며, 앵커 포인트의 객체 임베딩 O과 대조(contrast)되어 카테고리 라벨을 얻는다. 이 과정을 수식으로 나타내면 다음과 같다.

![](/assets/images/posts/589/img_2.png)

여기서 N은 앵커 포인트의 개수, C는 프롬프트의 개수, D는 임베딩의 특징 차원을 각각 의미한다.

### 3.2 재매개 가능한 영역-텍스트 정렬 (Re-parameterizable Region-Text Alignment)

개방형 시나리오에서 텍스트 임베딩과 객체 임베딩 간의 정렬(alignment)은 식별된 카테고리의 정확도를 결정짓는 핵심 요소이다. 기존 연구들은 주로 시각-텍스트 표현을 정렬하기 위해 복잡한 교차 모달리티 융합(cross-modality fusion) 기법을 도입해왔다 [5, 37]. 그러나 이러한 방식은 특히 텍스트의 수가 많을 때 상당한 계산 오버헤드를 야기한다. 이를 해결하기 위해 우리는 **재매개 가능한 영역-텍스트 정렬(RepRTA)** 전략을 제안한다. 이 전략은 재매개화(re-parameterization)가 가능한 경량 보조 네트워크를 활용해 학습 중 사전학습된 텍스트 임베딩을 개선하며, 추론 및 전이 시에는 추가 오버헤드 없이 시각-의미 정렬을 강화한다.

구체적으로, 길이가 CCC인 텍스트 프롬프트 TTT가 주어지면 CLIP 텍스트 인코더 [44, 57]를 사용해 사전학습된 텍스트 임베딩 P=TextEncoder(T)를 얻는다. 학습 전에 데이터셋 내 모든 텍스트 임베딩을 미리 캐시(cache)하고, 이후 학습 시 텍스트 인코더는 제거되어 추가 비용 없이 진행할 수 있다.

그림 3.(a)와 같이, 우리는 단일 피드포워드 블록(feed-forward block) [58, 53]으로 구성된 경량 보조 네트워크 f\_θ​를 도입한다. 여기서 θ는 학습 가능한 파라미터를 나타내며 폐쇄형 학습 대비 낮은 오버헤드를 가진다. 이 네트워크는 향상된 텍스트 임베딩

![](/assets/images/posts/589/img_3.png)

을 생성하여 학습 중 앵커 포인트의 객체 임베딩과 대조시킴으로써 시각-의미 정렬을 향상시킨다.

![](/assets/images/posts/589/img_4.png)

![](/assets/images/posts/589/img_5.png)

![](/assets/images/posts/589/img_6.png)

**그림 3 설명**

- (a) RepRTA의 경량 보조 네트워크 구조: 하나의 SwiGLU FFN 블록 [53]으로 구성된다.
- (b) SAVPE의 구조: 의미 브랜치(semantic branch)는 프롬프트와 무관한 의미 특징을 생성하고, 활성화 브랜치(activation branch)는 그룹화된 프롬프트 인식 가중치를 제공한다. 두 결과의 결합을 통해 비주얼 프롬프트 임베딩을 효율적으로 도출할 수 있다.

### 3.3 의미 활성화 비주얼 프롬프트 인코더 (Semantic-Activated Visual Prompt Encoder)

비주얼 프롬프트(Visual prompt)는 관심 있는 객체 카테고리를 시각적 단서(visual cue) — 예: 박스(box)나 마스크(mask) — 를 통해 나타내도록 설계된다. 기존 연구들은 비주얼 프롬프트 임베딩 생성을 위해 **트랜스포머 중심 설계** [20, 30] (예: deformable attention [78]) 또는 추가적인 **CLIP 비전 인코더** [44, 67]를 사용하는 경우가 많다. 그러나 이러한 방식은 복잡한 연산자와 높은 계산 요구로 인해 배포성과 효율성에 한계를 초래한다. 이를 해결하기 위해 우리는 **Semantic-Activated Visual Prompt Encoder (SAVPE)**를 제안하며, 이는 시각적 단서를 효율적으로 처리하도록 설계되었다. SAVPE는 두 개의 분리된 경량 브랜치(branch)로 구성된다.

1. **Semantic Branch**: 프롬프트와 무관한(agnostic) 의미적 특징을 DDD 채널에서 출력하며, 시각적 단서와의 융합 오버헤드가 없다.
2. **Activation Branch**: 시각적 단서와 이미지 특징의 상호작용을 통해 그룹화된 프롬프트 인식 가중치(prompt-aware weight)를 훨씬 적은 채널 수에서 저비용으로 생성한다.

이 두 브랜치의 결과를 집계(aggregation)하여 최소한의 복잡도로 풍부한 정보를 담은 프롬프트 임베딩을 생성한다.

![](/assets/images/posts/589/img_7.png)

![](/assets/images/posts/589/img_8.png)

### 3.4 Lazy Region-Prompt Contrast (LRPC)

프롬프트가 없는(prompt-free) 시나리오에서는 명시적인 가이드 없이 모델이 이미지 내의 모든 객체를 식별하고 이름을 부여해야 한다. 기존 연구들은 이러한 설정을 주로 **생성 문제(generative problem)**로 정식화하며, 언어 모델을 사용해 탐지된 밀집 객체(dense objects)의 카테고리를 생성하도록 했다 [33, 49, 62]. 그러나 이 방식은 언어 모델의 높은 파라미터 수로 인해 상당한 오버헤드를 유발한다. 예를 들어, GenerateU [33]의 FlanT5-base(250M 파라미터)나 DINO-X [49]의 OPT-125M 모델은 높은 효율성을 요구하는 실시간 환경에 적합하지 않다.

이 문제를 해결하기 위해, 우리는 이 시나리오를 **검색 문제(retrieval problem)**로 재정의하고 **Lazy Region-Prompt Contrast (LRPC)** 전략을 제안한다. LRPC는 내장된 대규모 어휘(built-in large vocabulary)로부터 객체가 포함된 앵커 포인트(anchor point)에 대해서만 카테고리 이름을 검색하여 효율적으로 처리한다. 이 방식은 언어 모델에 대한 의존성을 완전히 제거하면서도 높은 효율성과 우수한 성능을 제공한다.

구체적으로, 사전학습된 YOLOE 모델에서 **특화된 프롬프트 임베딩(specialized prompt embedding)**을 도입해 이를 전용으로 학습시켜 모든 객체를 탐지하도록 한다. 이때 객체들은 하나의 카테고리로 취급된다. 동시에 [16]을 따라 다양한 카테고리를 포함하는 대규모 어휘를 수집하고, 이를 검색용 내장 데이터 소스로 사용한다.

![](/assets/images/posts/589/img_9.png)

### 3.5 학습 목표 (Training Objective)

학습 시, [5]를 따라 **모자이크 샘플(mosaic sample)** 내 이미지에 포함된 텍스트를 양성 라벨(positive label)로 사용해 온라인 어휘(online vocabulary)를 구축한다. 또한 [21]을 참고하여 예측값과 정답(ground truth)을 매칭하기 위해 **task-aligned label assignment**를 활용한다.

- **분류(Classification)**: Binary Cross Entropy Loss 사용
- **회귀(Regression)**: IoU Loss와 Distributed Focal Loss 사용
- **분할(Segmentation)**: [3]을 따라 Binary Cross Entropy Loss로 마스크 최적화

이러한 다중 손실 함수의 조합을 통해 YOLOE는 개방형 프롬프트 시나리오에서도 효율적이고 정확한 객체 탐지 및 분할 학습을 수행한다.

## 4 실험

### 4.1 구현 세부 사항

#### 모델

[5]와의 공정한 비교를 위해 YOLOE는 동일한 **YOLOv8 아키텍처** [21]를 기반으로 구현된다. 또한 YOLOE의 **다른 YOLO 아키텍처에서의 일반화 능력**을 검증하기 위해 **YOLO11 아키텍처** [21]로도 실험을 수행했다. 두 아키텍처 모두 소형(S), 중형(M), 대형(L)의 세 가지 스케일을 제공해 다양한 응용 환경에 대응한다. 텍스트 프롬프트는 사전학습된 **MobileCLIP-B(LT)** [57] 텍스트 인코더로 인코딩한다. SAVPE의 경우, 실험적으로 A=16을 기본값으로 사용한다.

#### 데이터

[5]를 따라 **Objects365 (V1)** [52], **GoldG** [22] (GQA [17]와 Flickr30k [43] 포함)와 같은 탐지 및 그라운딩 데이터셋을 활용하며, COCO [34]의 이미지는 제외한다. 분할 데이터는 탐지 및 그라운딩 데이터셋의 **ground truth 바운딩 박스**를 기반으로 **SAM-2.1** [46] 모델을 사용해 **pseudo 인스턴스 마스크**를 생성하고, [9]의 방식을 통해 필터링 및 단순화를 거쳐 노이즈를 제거한다. 비주얼 프롬프트 데이터는 [20]을 따라 **ground truth 바운딩 박스**를 시각적 단서로 활용한다. 프롬프트 없는 작업에서는 동일한 데이터셋을 사용하지만, 모든 객체를 단일 카테고리로 라벨링해 특화된 프롬프트 임베딩을 학습한다.

#### 학습

연산 자원 제약으로 인해, YOLO-World가 100 에포크 학습한 것과 달리, YOLOE는 다음과 같은 절차로 학습한다.

1. **텍스트 프롬프트 학습**: 30 에포크 학습.
2. **비주얼 프롬프트 학습(SAVPE)**: 추가 비용을 최소화하기 위해 2 에포크만 학습하며, 나머지 파트는 고정(freeze).
3. **프롬프트 없는 시나리오 학습(LRPC)**: 특화 프롬프트 임베딩만 1 에포크 학습.

텍스트 프롬프트 학습 단계에서는 [5]와 동일한 설정을 사용한다. YOLOE-v8-S / M / L 모델은 **8×RTX4090 GPU**에서 각각 **12.0 / 17.0 / 22.5 시간** 만에 학습 가능하며, 이는 YOLO-World 대비 약 3배 낮은 비용이다. 비주얼 프롬프트 학습은 모든 다른 모듈을 고정한 상태에서 텍스트 프롬프트 학습과 동일한 설정으로 진행한다. 프롬프트 없는 기능을 위해 동일한 데이터를 사용해 특화된 임베딩을 추가로 학습한다.

또한 YOLOE의 **다운스트림 작업 전이 성능**을 검증하기 위해 COCO [34]에서 폐쇄형(closed-set) 탐지 및 분할에 대해 파인튜닝을 수행했다. 두 가지 실용적 파인튜닝 전략을 실험했다.

1. **Linear probing**: 분류 헤드만 학습 가능 상태로 두고 10 에포크 학습.
2. **Full tuning**: 모든 파라미터를 학습 가능 상태로 두며, 소형 모델(YOLOE-v8-S / 11-S)은 160 에포크, 중형 및 대형 모델(YOLOE-v8-M / L, YOLOE-11-M / L)은 80 에포크 학습.

#### 평가 지표

- **텍스트 프롬프트 평가**: 벤치마크의 모든 카테고리 이름을 입력으로 사용하여 open-vocabulary 객체 탐지 표준 프로토콜에 따라 평가.
- **비주얼 프롬프트 평가**: [20]을 따라 각 카테고리에서 N=16N = 16N=16개의 학습 이미지를 무작위 샘플링하여 ground truth 바운딩 박스로 시각 임베딩을 추출한 뒤 평균 프롬프트 임베딩을 계산.
- **프롬프트 없는 평가**: [33]과 동일한 프로토콜을 사용하며, 사전학습된 텍스트 인코더 [57]로 개방형 예측을 벤치마크 카테고리 이름과 의미적으로 매핑. 다만 [33]과 달리, 상위-k 선택이나 빔 서치를 생략하고 가장 확신(confidence)이 높은 예측만을 사용해 매핑을 간소화.

**내장 어휘(built-in vocabulary)**는 [16]의 태그 리스트를 사용하며 총 4,585개 카테고리를 포함한다. LRPC의 임계값 δ=0.001을 기본값으로 사용한다. 세 가지 프롬프트 유형 모두 [5, 20, 33]을 따라 **LVIS [14] 데이터셋의 1,203개 카테고리에 대해 제로샷 평가**를 수행하며, 기본적으로 **LVIS minival subset의 Fixed AP [7]**를 보고한다. COCO 전이 실험에서는 [1, 21]을 따라 표준 AP를 사용한다. 또한 TensorRT 환경의 Nvidia T4 GPU와 CoreML 환경의 iPhone 12에서 FPS를 측정해 모델의 효율성을 평가한다.

![](/assets/images/posts/589/img_10.png)

**표 1**: LVIS에서의 제로샷 탐지 평가 결과. 공정 비교를 위해 LVIS minival 세트에서 Fixed AP를 제로샷으로 보고한다. 학습 시간은 텍스트 프롬프트 기준이며, [32, 65]는 8×V100 GPU, YOLO-World와 YOLOE는 8×RTX4090 GPU 기준이다. FPS는 Nvidia T4 GPU(TensorRT)와 iPhone 12(CoreML)에서 각각 측정된다. 결과는 텍스트 프롬프트(T) 및 비주얼 프롬프트(V) 유형으로 제공된다. 학습 데이터 약어: OI(OpenImages [24]), HT(HierText [39]), CH(CrowdHuman [51]), OG(Objects365 [52]와 GoldG [22]), G-20M(Grounding-20M [50]).

### 4.2 텍스트 및 비주얼 프롬프트 평가

**표 1**에서 보듯, **LVIS에서의 객체 탐지 성능**은 YOLOE가 **다양한 모델 스케일**에 걸쳐 **효율성과 제로샷 성능 간의 균형**을 매우 우수하게 달성함을 보여준다. 특히 이러한 결과는 YOLO-Worldv2 대비 **약 3배 빠른 학습 시간** 내에 이루어진 것으로 주목할 만하다.

구체적으로,

- **YOLOE-v8-S / M / L**은 각각 **YOLO-Worldv2-S / M / L** 대비
  - **AP에서 3.5 / 0.2 / 0.4 포인트 향상**되었고,
  - 추론 속도는 T4 GPU 기준 **1.4× / 1.3× / 1.3×**,
  - iPhone 12 기준 **1.3× / 1.2× / 1.2×** 향상을 보였다.

또한, **희소 카테고리(rare category)**에 대해서는 YOLOE-v8-S와 YOLOE-v8-L이 각각 **5.2% 및 7.6% APr** 향상이라는 큰 개선을 보였다.

한편, YOLOE-v8-M / L은 **YOLO-Worldv2-M / L** 대비 **APf(자주 등장하는 category)**에서 소폭 낮은 성능을 보였는데, 이는 YOLOE가 탐지와 분할을 하나의 모델로 통합하는 **멀티태스크 학습 구조**로 인해 발생한 성능 트레이드오프 때문이며, 이는 **표 5**에서 자세히 분석된다.

또한 YOLO11 아키텍처 기반 YOLOE 역시 우수한 성능과 효율성을 보인다. 예를 들어, **YOLOE-11-L**은 YOLO-Worldv2-L과 유사한 AP를 달성하면서도, T4 및 iPhone 12에서 **1.6배 빠른 추론 속도**를 기록하며 YOLOE의 **우수한 일반화 성능**을 입증한다.

**비주얼 프롬프트**의 도입은 YOLOE의 범용성을 더욱 강화시킨다.

- 예를 들어, **YOLOE-v8-L**은 **T-Rex2** 대비
  - **3.3 APr**,
  - **0.9 APc** 향상을 보였으며,
  - 사용한 학습 데이터는 절반 수준 (3.1M → 1.4M),
  - 학습 리소스는 훨씬 적음 (16×A100 → 8×RTX4090).

또한 SAVPE는 단 **2 에포크만 학습**되고 나머지 파트는 고정된 상태였음에도 불구하고, **여러 모델 스케일에서 텍스트 프롬프트와 유사한 APr 및 APc 성능**을 달성했다. 이는 [20]에서도 관찰된 바와 같이, **텍스트로 설명하기 어려운 희소 객체**에 대해 **비주얼 프롬프트가 매우 효과적**임을 뒷받침한다.

### 세분화(분할) 성능

**LVIS val 세트**에서의 분할 성능(APm)은 **표 2**에 요약되어 있다. YOLOE는 **텍스트 프롬프트와 비주얼 프롬프트를 모두 활용**해 강력한 분할 성능을 보여준다.

- **YOLOE-v8-M / L**은 제로샷 조건에서 각각
  - **20.8 / 23.5 APm**을 기록하며,
  - LVIS-Base 데이터로 분할 헤드를 파인튜닝한 **YOLO-Worldv2-M / L** 대비
  - 각각 **3.0 / 3.7 APm 향상**을 보여준다.

이는 **YOLOE의 분할 성능에서도 경쟁 모델 대비 뚜렷한 우수성**을 입증하는 결과이다.

![](/assets/images/posts/589/img_11.png)

**표 2 설명**

- LVIS val 세트에서 **표준 APm** 기준으로 모든 모델을 평가했다.
- YOLOE는 **텍스트(T)** 및 **비주얼(V)** 프롬프트 모두를 입력으로 지원한다.
- † 표시는 해당 모델이 **LVIS-Base 데이터로 분할 헤드를 파인튜닝**한 것을 의미한다.
- 반면 YOLOE는 **학습 시 LVIS 이미지를 전혀 사용하지 않은 제로샷 방식**으로 평가된다.

### 4.3 프롬프트 없는 시나리오 평가 (Prompt-free Evaluation)

**표 3**에서 보듯, 프롬프트가 없는 시나리오에서도 **YOLOE는 뛰어난 성능과 효율성**을 모두 보여준다.  
구체적으로, **YOLOE-v8-L**은

- **27.2 AP**,
- **23.5 APr**를 기록하며,
- **Swin-T 백본을 사용하는 GenerateU**보다 각각 **0.4 AP, 3.5 APr** 더 높은 성능을 달성했다.  
  또한,
- **파라미터 수는 6.3배 적고**,
- **추론 속도는 무려 53배 빠르다.**

이 결과는 YOLOE가 **프롬프트 없는(open-ended) 문제를 내장 어휘 기반의 검색(retrieval) 문제로 재정의함으로써** 매우 효과적인 접근 방식을 제공함을 보여준다.  
뿐만 아니라, YOLOE는 **명시적 프롬프트에 의존하지 않고도 다양한 객체 카테고리에 일반화할 수 있는 잠재력**을 입증하며,

> **현실 세계의 다양한 활용 시나리오에서의 실용성을 크게 향상**시킨다.

**표 3 설명**

- 프롬프트 없는 시나리오에서는 [33]의 프로토콜을 따라 **LVIS minival 세트에서 Fixed AP**를 기준으로 평가한다.
- FPS는 **Nvidia T4 GPU에서 PyTorch [42]로 측정**되었으며, 실시간 적용 가능성을 보여준다.

![](/assets/images/posts/589/img_12.png)

### 4.4 다운스트림 전이 평가 (Downstream Transferring)

**표 4**에서 보듯, **YOLOE를 COCO 데이터셋에 전이(fine-tuning)하여 폐쇄형(closed-set) 탐지 및 분할** 작업에 적용한 결과,  
제한된 학습 에포크에서도 **두 가지 파인튜닝 전략 모두에서 우수한 성능**을 나타낸다.

#### Linear Probing (분류 헤드만 학습 가능하도록 설정)

- 전체 학습 시간의 **2% 미만**으로 학습했음에도,
- **YOLOE-11-M / L**은 각각 **YOLO11-M / L의 성능의 80% 이상**을 달성했다.  
  → 이는 **YOLOE의 강력한 전이 능력(transferability)**을 뒷받침한다.

#### Full Tuning (모든 파라미터 학습 가능)

YOLOE는 학습 비용을 크게 줄이면서도 성능을 더욱 향상시킬 수 있다.

예를 들어:

- **YOLOE-v8-M / L**은 학습 에포크 수를 **4배나 줄였음에도**,
  - **YOLOv8-M / L** 대비
    - 분할 성능(APm)은 **0.4포인트**,
    - 탐지 성능(APb)은 **0.6포인트** 더 높았다.

또한,

- **YOLOE-v8-S**는 학습 시간을 **3배 절감**하면서도
  - 탐지 및 분할 성능 모두에서 **YOLOv8-S보다 더 나은 결과**를 기록했다.

이러한 결과는 YOLOE가 단순히 제로샷(open-set) 모델로서뿐만 아니라,

> **다운스트림 폐쇄형 작업 전이에 있어서도 매우 강력한 출발점(strong initialization)**이 될 수 있음을 잘 보여준다.

**표 4 설명**

- COCO 데이터셋에서 YOLOE를 파인튜닝한 후
  - 탐지(APb)와 분할(APm) 모두에 대해 **표준 AP**를 보고한다.
- 실험은 두 가지 실용적인 파인튜닝 전략으로 수행되었다:
  1. **Linear probing**: 분류 헤드만 학습
  2. **Full tuning**: 전체 파라미터 학습 가능

![](/assets/images/posts/589/img_13.png)

### 4.5 설계 구성 요소에 대한 분석 (Ablation Study)

YOLOE의 구성 요소들이 실제로 성능 향상에 어떻게 기여하는지를 분석하기 위해, **YOLOE-v8-L**을 기준으로 **LVIS minival 세트에서 제로샷 표준 AP**를 보고한다. 모든 실험은 동일한 조건에서 비교되며, **표 5**에 텍스트 프롬프트 기반 실험의 전체적인 발전 경로(roadmap)를 정리하였다.

#### **표 5 설명**

- LVIS minival 세트에서의 **제로샷 탐지 성능(AP)**
- 추론 속도(FPS)는 각각 **Nvidia T4 GPU (TensorRT)** 및 **iPhone 12 (CoreML)** 환경에서 측정
- 모델은 YOLOv8-Worldv2-L → YOLOE-v8-L으로 발전됨

![](/assets/images/posts/589/img_14.png)

![](/assets/images/posts/589/img_15.png)

### 그림 4 설명

- **(a)** LVIS에서 제로샷 추론 결과
- **(b)** 커스텀 텍스트 프롬프트 결과
  - 예: "white hat, red hat, white car, sunglasses, mustache, tie"
- **(c)** 비주얼 프롬프트 사용 (빨간 점선 박스가 프롬프트 역할)
- **(d)** 프롬프트 없이 수행된 추론 결과

※ 더 많은 예시는 부록(supplementary material) 참조

![](/assets/images/posts/589/img_16.png)

이러한 실험은 YOLOE의 설계가 **실제 성능과 실용성 측면에서 잘 작동함을 단계별로 입증**하며, 특히:

- **추론 오버헤드 없이 성능을 향상시키는 RepRTA**,
- **멀티태스크 확장성 확보**,
- **속도-성능 균형 유지** 측면에서 매우 효과적임을 보여준다.

![](/assets/images/posts/589/img_17.png)

표 6: SAVPE의 효과 (Effectiveness of SAVPE)

![](/assets/images/posts/589/img_18.png)

표 7: LRPC의 효과 (Effectiveness of LRPC)

### ✅ SAVPE의 효과 분석

**SAVPE(Semantic-Activated Visual Prompt Encoder)**의 성능을 검증하기 위해, **activation branch를 제거한 단순 버전(Mask pool)**과 비교 실험을 수행했다.이때는 시각 프롬프트 마스크를 통해 semantic feature를 단순 집계(mask pooling)하는 구조로 대체했다.

- **결과**:  
  **SAVPE는 Mask pool 방식보다 1.5 AP 더 우수한 성능**을 기록했다.  
  → 이는 **Mask pool이 시각 프롬프트 내의 위치별 의미 차이(semantic importance)를 반영하지 못한 반면**,  
  → **SAVPE의 activation branch는 이를 효과적으로 모델링하여**, 더 정확한 프롬프트 임베딩과 향상된 대조(contrast) 성능을 이끌어냈기 때문이다.

또한, **activation branch 내 그룹 수 A** 변화에 따른 성능도 분석되었다.

- **결과 요약**:
  - **A=1** (그룹이 1개뿐인 경우)에서도 성능 개선 가능
  - **A=16**일 때 **31.9 AP를 달성**,  
    → 성능과 효율 사이에서 가장 균형 잡힌 결과  
    → 그룹 수를 늘려도 성능 향상은 제한적 (marginal)

### ✅ LRPC의 효과 분석

**프롬프트 없는 시나리오(prompt-free)**에서의 성능을 검증하기 위해, **YOLOE가 내장된 대규모 어휘 전체를 직접 텍스트 프롬프트로 사용**하도록 한 **기준선(baseline)**과 비교했다.

- **결과** (**표 7**):  
  YOLOE-v8-S / L에서 동일한 성능(AP)을 유지하면서도,
  - **YOLOE-v8-S**는 **1.7배**,
  - **YOLOE-v8-L**는 **1.3배** 빠른 추론 속도 달성  
    → 이는 LRPC가 **객체가 있는 앵커 포인트에 대해서만 선택적으로 카테고리 검색**을 수행하고,  
    → **불필요한 비교를 모두 건너뛰는 방식**이 효율적으로 작동함을 보여준다.

또한, **LRPC의 임계값 δ** 조정에 따른 **성능-효율 트레이드오프**도 관찰되었다.

- 예시:
  - δ를 조정해 **YOLOE-v8-S에서 1.9배 속도 향상**,
  - **단 0.2 AP만 감소**  
    → 실용적 적용에서 매우 유용한 조절 지표임을 시사

### 요약

- **SAVPE**는 위치별 의미를 반영하는 **activation branch 덕분에** 더 정교한 프롬프트 임베딩을 만들고, 단순 마스킹 대비 **1.5 AP 향상**을 달성함.
- **LRPC**는 명시적 프롬프트 없이도 **낮은 연산 비용으로 유사한 성능**을 유지하며, 임계값 조절을 통해 **유연한 속도-정확도 균형 조절**이 가능함.

두 기법 모두 YOLOE의 효율성과 범용성을 실질적으로 확장하는 핵심 구성 요소로 작동함을 실험적으로 입증했다.

### 4.6 시각화 분석 (Visualization Analyses)

YOLOE의 성능을 시각적으로 검증하기 위해, 다음 **4가지 시나리오**에 대한 분석을 진행하였다:

1. **(a) LVIS에서 제로샷 추론**
   - 카테고리 이름 전체를 **텍스트 프롬프트**로 사용하여 탐지 수행
2. **(b) 임의의 텍스트 프롬프트 사용**
   - 예: "white hat", "red hat", "white car", "sunglasses", "mustache", "tie" 등
   - 사용자가 입력한 자유로운 텍스트로 객체 탐지 가능
3. **(c) 비주얼 프롬프트 사용**
   - 빨간 점선 박스로 **시각적 단서(visual cue)**를 제공
4. **(d) 프롬프트 없는 시나리오**
   - **명시적 입력 없이** 이미지 내 모든 객체를 자동 탐지

이러한 시각적 결과를 통해 확인할 수 있는 핵심 내용은 다음과 같다:

- YOLOE는 **다양한 프롬프트 조건에서도 정확하게 객체를 탐지하고 분할**할 수 있으며,
- 실제 응용에 적합한 **높은 실용성과 범용성**을 갖춘 모델임을 입증한다.

### 5 결론 (Conclusion)

본 논문에서는 **YOLOE**를 소개하였다.  
YOLOE는 **다양한 개방형 프롬프트 방식**(텍스트, 비주얼, 프롬프트 없음)에 대응할 수 있도록

> 객체 탐지와 분할을 통합한 고효율 단일 모델이다.

특히, 세 가지 핵심 구성 요소를 설계하였다:

- **RepRTA**: 텍스트 프롬프트 정렬을 위한 재매개 가능한 영역-텍스트 정렬 전략
- **SAVPE**: 시각적 단서를 효율적으로 처리하는 의미 활성화 인코더
- **LRPC**: 명시적 프롬프트 없이 객체를 인식할 수 있는 지연형 카테고리 검색 전략

이러한 요소 덕분에 YOLOE는 **낮은 비용과 높은 효율성으로 다양한 프롬프트 방식에 대응**할 수 있으며,

> **“무엇이든 실시간으로 인식(seeing anything in real-time)”**하는 능력을 제공한다.

YOLOE가 **차세대 개방형 비전 작업을 위한 강력한 기준점(baseline)**으로 자리매김하여, 후속 연구들을 이끄는 역할을 하기를 기대한다.
